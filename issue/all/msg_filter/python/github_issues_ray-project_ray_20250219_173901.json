[
  {
    "number": 37677,
    "title": "[<Ray component: Cluster>] KeyError: 'CPU' error in Linux",
    "created_at": "2023-07-22T10:49:28Z",
    "closed_at": "2023-07-24T21:18:06Z",
    "labels": [
      "question",
      "triage",
      "core"
    ],
    "url": "https://github.com/ray-project/ray/issues/37677",
    "body": "### What happened + What you expected to happen\r\n\r\n**What I will do:**\r\nI tried to get the total number of cpus provided by the cluster;\r\n\r\n**What I got wrong:**\r\nThe specific error information is as follows:\r\n{cluster_resources()['CPU']} CPU resources in total;\r\nKeyError: 'CPU'\r\n\r\n**Update:**\r\n_I seem to have found the reason, when there is no available cpu in the cluster, the 'CPU' key is no longer in the returned dict; This leads to errors;_\r\n\r\n### Versions / Dependencies\r\n\r\nray: 2.3.1\r\nos: debian 11\r\npython: 3.9.2\r\n\r\n### Reproduction script\r\n\r\nfrom ray import init, cluster_resources\r\ninit()\r\nprint(f\"{cluster_resources()['CPU']}\")\r\n\r\n### Issue Severity\r\n\r\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/37677/comments",
    "author": "stevenhubhub",
    "comments": [
      {
        "user": "jjyao",
        "created_at": "2023-07-24T21:18:06Z",
        "body": "Yea, try to do `cluster_resources().get(\"CPU\", 0)`"
      },
      {
        "user": "stevenhubhub",
        "created_at": "2023-07-27T08:45:26Z",
        "body": "> Yea, try to do `cluster_resources().get(\"CPU\", 0)`\r\n\r\nThanks!"
      },
      {
        "user": "davide-russo-tfs",
        "created_at": "2024-09-30T10:51:06Z",
        "body": "Good morning, I have the same issue while trying to use Ray on Databricks cluster (with autoscaling). The runtime used is 15.1ML.\r\nI imported the following libraries:\r\n```\r\nfrom ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\r\nfrom ray.util.multiprocessing import Pool\r\nimport ray\r\n```\r\nThis is how I set up the environment:\r\n```\r\nsetup_ray_cluster(\r\n        num_worker_nodes  = 4,\r\n        num_cpus_per_node = 4,\r\n        autoscale          = True\r\n    )\r\nray.init(ignore_reinit_error = True)\r\n```\r\nthen I decorated a function to be run in parallel by using `@ray.remote` and tried to create a pool of processes this way:\r\n```\r\nwith Pool(processes = 8) as pool:\r\n        pool.starmap(foo, inputs)\r\n```\r\n\r\nHow can I solve this problem? Thank you for your help."
      }
    ]
  },
  {
    "number": 31151,
    "title": "ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...>] ",
    "created_at": "2022-12-16T06:43:57Z",
    "closed_at": "2022-12-16T12:58:13Z",
    "labels": [
      "question",
      "docs"
    ],
    "url": "https://github.com/ray-project/ray/issues/31151",
    "body": "### Description\n\nHellow,i'm sorry to bother you.I want to use ray 2.0.0.dev0.But I don't know where I can find it.There is only version ray 3.0.0.dev0 in the documentation.Can you tell me where i can get it,thanks!\n\n### Link\n\n_No response_",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/31151/comments",
    "author": "aa-oo",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2022-12-16T12:56:58Z",
        "body": "Hey @aa-oo , thanks for filing this issue.\r\n* All versions that have the \"dev\" in them are referring to the master branch (at that time). The current master branch version is called \"3.0.0.dev0\".\r\n* To get a stable version of Ray, you can simply try `pip install ray==2.2` (brand new one) or some older versions like `pip install ray==2.1` or `pip install ray==2.0`."
      },
      {
        "user": "aa-oo",
        "created_at": "2022-12-16T13:52:05Z",
        "body": "Ok,thank you!\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: \"Sven ***@***.***&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2022\u5e7412\u670816\u65e5(\u661f\u671f\u4e94) \u665a\u4e0a8:57\r\n\u6536\u4ef6\u4eba: ***@***.***&gt;; \r\n\u6284\u9001: ***@***.***&gt;; ***@***.***&gt;; \r\n\u4e3b\u9898: Re: [ray-project/ray] ray 2.0.0.dev0[<Ray component: Core|RLlib|etc...&gt;]  (Issue #31151)\r\n\r\n\r\n\r\n\r\n\r\n \r\nHey @aa-oo , thanks for filing this issue.\r\n  \r\nAll versions that have the \"dev\" in them are referring to the master branch (at that time). The current master branch version is called \"3.0.0.dev0\".\r\n \r\nTo get a stable version of Ray, you can simply try pip install ray==2.2 (brand new one) or some older versions like pip install ray==2.1 or pip install ray==2.0.\r\n  \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;"
      }
    ]
  },
  {
    "number": 30662,
    "title": "[tune] How to use an imported parameter via argparse in trainable function",
    "created_at": "2022-11-25T17:32:05Z",
    "closed_at": "2022-11-29T20:04:15Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/30662",
    "body": "### What happened + What you expected to happen\n\nI have a tuning task using an imported parameter via argparse in trainable function. The task crashes complaining the argument is not provided. It works fine If I use it outside the trainable function. \n\n### Versions / Dependencies\n\nRay 2.1.0\n\n### Reproduction script\n\nThe script being imported called \u201cinput_param.py\u201d:\r\n\r\n    import sys, argparse\r\n\r\n    parser = argparse.ArgumentParser(description='')\r\n    parser.add_argument('--ttt', type=int, required=True, help='anything > 1')\r\n    args = parser.parse_args()\r\n\r\n    ttt = args.ttt\r\n\r\nThe tuning task code is named as \u2018example.py\u2019:\r\n\r\n    import os\r\n    from ray import tune, air\r\n    from hyperopt import hp\r\n    from ray.tune.search.hyperopt import HyperOptSearch\r\n    import input_param as input_param\r\n\r\n    def trainable(config):\r\n        #print('!! ttt = ', input_param.ttt)\r\n        score = config[\"a\"] ** 2 + config[\"b\"]\r\n        tune.report(SCORE=score)\r\n\r\n\r\n    search_space = {\r\n        \"a\": hp.uniform(\"a\", 0, 1),\r\n        \"b\": hp.uniform(\"b\", 0, 1)\r\n        }\r\n\r\n    raw_log_dir = \"./ray_log\"\r\n    raw_log_name = \"example\"\r\n\r\n    algorithm = HyperOptSearch(search_space, metric=\"SCORE\", mode=\"max\", n_initial_points=1)\r\n\r\n\r\n    tuner = tune.Tuner(trainable,\r\n            tune_config = tune.TuneConfig(\r\n                num_samples = 10,\r\n                search_alg=algorithm,\r\n                ),\r\n            param_space=search_space,\r\n            run_config = air.RunConfig(local_dir = raw_log_dir, name = raw_log_name) #\r\n            )\r\n\r\n    print('!! ttt = ', input_param.ttt)\r\n    results = tuner.fit()\r\n    print(results.get_best_result(metric=\"SCORE\", mode=\"max\").config)\r\n\r\nI run the task via the following command:\r\n\r\n    py example.py --ttt 99\r\n\r\nThe following is part of the error:\r\n\r\n    (pid=19560) default_worker.py: error: the following arguments are required: --ttt\r\n    (pid=19560) 2022-11-23 20:45:01,769     ERROR worker.py:763 -- Worker exits with an exit code 2.\r\n\r\n\n\n### Issue Severity\n\nHigh: It blocks me from completing my task.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/30662/comments",
    "author": "wxie2013",
    "comments": [
      {
        "user": "justinvyu",
        "created_at": "2022-11-28T17:56:06Z",
        "body": "Is it possible to work around this by passing the arguments into the config? Is there a specific reason why the arguments need to be stored and accessed in the trainable as a separate python module?\r\n\r\n```python\r\nsearch_space = {\r\n    # ...\r\n    \"ttt\": input_param.ttt,\r\n}\r\n```"
      },
      {
        "user": "wxie2013",
        "created_at": "2022-11-28T20:19:02Z",
        "body": "Thanks for the follow-up.  It is possible to implement a walkaround.  It would be nice to understand the reason why above example code doesn't work so that I won't stumble into similar problems in the future. "
      },
      {
        "user": "Yard1",
        "created_at": "2022-11-29T18:45:27Z",
        "body": "Hey @wxie2013, as I mentioned in the discuss thread, this is because the trainable function is ran in a separate process on each Tune worker in parallel. Therefore, argparse will expect arguments that are simply not provided when Ray spawns those processes."
      },
      {
        "user": "wxie2013",
        "created_at": "2022-11-29T20:04:15Z",
        "body": "Hi @Yard1, got it. Thanks for the help"
      }
    ]
  },
  {
    "number": 11971,
    "title": "[rllib] PPO ICM learning rate",
    "created_at": "2020-11-12T13:05:46Z",
    "closed_at": "2020-11-14T11:21:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/11971",
    "body": "Hello, I know the default ppo learning rate is 5e-5, default curiosity learning rate is 0.001. \r\nI just want to know whether the two learning rate are same?   \r\n\r\nIf I use curiosity in ppotrainer, how do I set it?\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/11971/comments",
    "author": "zzchuman",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-11-13T08:10:26Z",
        "body": "Hey @zzchuman , no they are not the same. The curiosity module has its own optimizer and lr.\r\nYou can set the curiosity lr inside the exploration_config key, the same way as it's done in rllib/utils/explorations/tests/test_curiosity.py:\r\n```\r\n            config[\"exploration_config\"] = {\r\n                \"type\": \"Curiosity\",\r\n                \"eta\": 0.2,\r\n                \"lr\": 0.001,  # <- HERE\r\n                \"feature_dim\": 128,\r\n                \"feature_net_config\": {\r\n                    \"fcnet_hiddens\": [],\r\n                    \"fcnet_activation\": \"relu\",\r\n                },\r\n                \"sub_exploration\": {\r\n                    \"type\": \"StochasticSampling\",\r\n                }\r\n            }\r\n```"
      },
      {
        "user": "zzchuman",
        "created_at": "2020-11-13T08:14:21Z",
        "body": "Thank you! got it! @sven1977 ,  I have a try! Thank you! "
      }
    ]
  },
  {
    "number": 11376,
    "title": "[rllib] Multi-agent with different hyperparameters",
    "created_at": "2020-10-13T21:13:28Z",
    "closed_at": "2020-11-02T09:34:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/11376",
    "body": "\r\nI've been trying to get a simple **hierarchical system** working on some very basic problems and I am assuming I can't get it to work because I cannot set different hyperparameters for the two agents.\r\n\r\nI would like to **train them in the same time**, this means I'll have to settle for the same set of hyperparameters for both agents in any one trial. Is that correct? If so, how would I go about implementing such a feature, where I can set different hyperparameters for agents? Having fixed hyperparams for one of the agents would also work for me I think, but this is not possible either, or is it?\r\n\r\nI'd appreciate any help.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/11376/comments",
    "author": "m3t4n01a",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-11-02T08:26:07Z",
        "body": "There is a hierarchical_training.py example in the examples folder, where this is done with a single trainer and two policies (\"multi-agent\" setup). You can then specify config-overrides within the \"policies\" config key:\r\n```\r\nconfig:\r\n    multiagent:\r\n        policies:\r\n            pol1: {None, [obsspace], [actionspace], {extra config overrides for pol1}}\r\n            pol2: {None, [obsspace], [actionspace], {extra config overrides for pol2}}\r\n```"
      },
      {
        "user": "m3t4n01a",
        "created_at": "2020-11-02T09:34:47Z",
        "body": "Thanks @sven1977 ! \r\nI completely missed the policy config overriding.\r\n"
      }
    ]
  },
  {
    "number": 10416,
    "title": "Always getting .nan reward while training with PPO or DQN",
    "created_at": "2020-08-29T04:38:03Z",
    "closed_at": "2020-08-30T02:54:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10416",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### Can anyone please give me hints why I am always getting the following while training with PPO or DQN?\r\nepisode_len_mean: .nan\r\nepisode_reward_max: .nan\r\nepisode_reward_mean: .nan\r\nepisode_reward_min: .nan\r\nepisodes_this_iter: 0\r\nepisodes_total: 0\r\n\r\nRay Version: 0.8.7\r\nOS: macOS\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10416/comments",
    "author": "ashutosh1906",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-08-29T07:15:57Z",
        "body": "`episodes_total: 0`. This is the reason. Until an episode has finished, we can't calculate any rewards. Does your env eventually return done=True at some point?"
      },
      {
        "user": "ashutosh1906",
        "created_at": "2020-08-30T02:54:07Z",
        "body": "Thank you. After putting \"done = True\" at some points, episodes_total becomes non-zero and does not return any .nan."
      }
    ]
  },
  {
    "number": 10312,
    "title": "[ray] How to startup workers more than number of cores",
    "created_at": "2020-08-25T14:55:28Z",
    "closed_at": "2020-08-26T05:16:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/10312",
    "body": "How to set ray startup arguments to let 150 workers running on a 96 cores machine? I notice ray will auto-scale on the local machine, but how to set while running a cluster?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/10312/comments",
    "author": "Seraphli",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-08-26T01:24:12Z",
        "body": "Just set --num-cpus=150! "
      },
      {
        "user": "Seraphli",
        "created_at": "2020-08-26T05:16:19Z",
        "body": "I tried this and it works. Thank you."
      }
    ]
  },
  {
    "number": 9863,
    "title": "How to init Ray with a specified GPU id to run all trials of Tune?",
    "created_at": "2020-08-02T12:46:26Z",
    "closed_at": "2020-08-03T02:01:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9863",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\nSay, I have 4 GPUs with ids=[0, 1, 2, 3] and I only want to run all trials for Tune on id=2 and id=3 only. That means I can only maximize the use of the third and fourth GPU without touching the first two GPUs. How can I achieve this? \r\n\r\n```ray.init(num_cpus=num_cpus, num_gpus=num_gpus, temp_dir=ray_log)```\r\n\r\nThe attribute ```num_gpus``` is the number of GPUs ray can use. When setting ```num_gpus=1```, all the trials run on the first device (GPU id=0).  When increasing ```num_gpus```, all the trials will ordinally use GPUs from id=0 to id=3... I want to know how to specify the exact GPU ids, e.g., all trials run on id=2 and id=3.\r\n\r\nI've tried specifying GPU id in the training functions, but raised ```RuntimeError: CUDA error: invalid device ordinal```. \r\n\r\nI'm still new to this great project. Appreciate your warm help!\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nOS: Linux\r\nPython: 3.7.4\r\nRay: 0.8.6",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9863/comments",
    "author": "guoxuxu",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-08-02T17:24:52Z",
        "body": "Try setting the CUDA_VISIBLE_DEVICES before running the ray script?"
      },
      {
        "user": "guoxuxu",
        "created_at": "2020-08-03T02:01:24Z",
        "body": "Soga. It works now. Thanks very much!"
      },
      {
        "user": "ndvbd",
        "created_at": "2023-05-16T18:22:39Z",
        "body": "But is there a smarter way, to automatically choose the free gpus from the cluster?"
      }
    ]
  },
  {
    "number": 9309,
    "title": "[rllib] Cannot detect pybullet environments.",
    "created_at": "2020-07-06T00:56:25Z",
    "closed_at": "2020-07-06T03:43:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9309",
    "body": "### Pybullet Environments Cannot Be Detected By Ray/rllib\r\n\r\nHi, I'm trying to use rllib to train pybullet games. My environment is ray 0.8.4, ubuntu 16.04, Pytorch  1.2.0. It seems that ray cannot detect these games and said the game was not registered. But I can make the gym environment outside ray within the same script. I attached a simple code to show what's wrong. Could someone help with this? Thanks!!\r\n\r\n```\r\nimport ray\r\nfrom ray.rllib.agents.ppo import PPOTrainer\r\nfrom ray.tune.registry import register_env\r\nimport gym\r\nimport pybullet_envs\r\n\r\nenv = gym.make('HumanoidBulletEnv-v0')\r\nprint(\"Made Successfully\")\r\n\r\nclass MyEnv(gym.Env):\r\n    def __init__(self, env_config):\r\n        self.env = gym.make('HumanoidBulletEnv-v0')\r\n        self.action_space = self.env.action_space\r\n        self.observation_space = self.env.observation_space\r\n\r\n    def reset(self):\r\n        obs = self.env.reset()\r\n        return obs\r\n\r\n    def step(self, action):\r\n        action = self.action_space.high * action\r\n        obs, reward, done, info = self.env.step(action)\r\n        return obs, reward, done, info\r\n\r\nregister_env(\"myenv\", lambda config: MyEnv(config))\r\n\r\n\r\ndef main():\r\n    ray.init()    \r\n    trainer = PPOTrainer(env=\"myenv\", config={\r\n        \"use_pytorch\": True,\r\n        })\r\n\r\n    for i in range(100):\r\n        trainer.train()\r\n\r\n    trainer.stop()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\nWhen I run the code, the environment outside ray could be made successfully and 'Made Successfully' was printed. But then I get the error that \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"pybullet_train.py\", line 44, in <module>\r\n    main()\r\n  File \"pybullet_train.py\", line 39, in main\r\n    trainer.train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 502, in train\r\n    raise e\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 491, in train\r\n    result = Trainable.train(self)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\r\n    result = self._train()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 150, in _train\r\n    fetches = self.optimizer.step()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/optimizers/sync_samples_optimizer.py\", line 59, in step\r\n    for e in self.workers.remote_workers()\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/utils/memory.py\", line 29, in ray_get_and_free\r\n    result = ray.get(object_ids)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1513, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(UnregisteredEnv): ray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 118, in spec\r\n    return self.env_specs[id]\r\nKeyError: 'HumanoidBulletEnv-v0'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray::RolloutWorker.__init__() (pid=8430, ip=192.168.1.8)\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 414, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 450, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\r\n  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 287, in __init__\r\n    self.env = _validate_env(env_creator(env_context))\r\n  File \"pybullet_train.py\", line 27, in <lambda>\r\n    register_env(\"myenv\", lambda config: MyEnv(config))\r\n  File \"pybullet_train.py\", line 14, in __init__\r\n    self.env = gym.make('HumanoidBulletEnv-v0')\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 142, in make\r\n    return registry.make(id, **kwargs)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 86, in make\r\n    spec = self.spec(path)\r\n  File \"/home/xxx/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\", line 128, in spec\r\n    raise error.UnregisteredEnv('No registered env with id: {}'.format(id))\r\ngym.error.UnregisteredEnv: No registered env with id: HumanoidBulletEnv-v0\r\n```\r\n\r\nI know 'import pybullet_envs' will register the environments in gym. It looked like rollout workers didn't detect these environments. Could someone tell me how to solve this? Thank you!",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9309/comments",
    "author": "KarlXing",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-07-06T02:59:30Z",
        "body": "Can you try moving the import into the constructor for your class? The problem is the import only applies locally and not on the Ray workers."
      },
      {
        "user": "KarlXing",
        "created_at": "2020-07-06T03:43:13Z",
        "body": "Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply."
      },
      {
        "user": "Glaucus-2G",
        "created_at": "2020-07-15T02:25:16Z",
        "body": "> Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply.\r\n\r\n\r\nCould you show me your codes? I am learning how to use it! Thank you!\n\n---\n\n> > Great! Importing pybullet_envs inside MyEnv class works. Thank you for the quick reply.\r\n\r\n> Could you show me your codes? I am learning how to use it! Thank you!\r\n\r\nI think I have got it.  Thank you!"
      }
    ]
  },
  {
    "number": 9288,
    "title": "[autoscaler]  how to set `ssh_user` if `worker_ips` usernames are all different each other?",
    "created_at": "2020-07-03T07:01:53Z",
    "closed_at": "2020-07-06T01:16:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/9288",
    "body": "In local configuration, It looks like `ssh_user`  in `configuration.yaml` should be set as the user of `head_node`, right?\r\n\r\nBut what if the `worker_node`'s username is different with `head_node`, say head_node's user name is `user1` and `worker_node`'s is `user2`? How does `head_node` find out worker's username when connecting via ssh?\r\n\r\nWhen the nodes are connected within the same networks(192.168.1.x) and I submitted some jobs to the head node, only head node worked hard and nothing happened in workers.\r\n\r\nIn this situation, should I run ray manually in each worker?  like `ray start --address=<address>` ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/9288/comments",
    "author": "rightx2",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-07-04T17:21:37Z",
        "body": "Yep! In this situation, starting ray manually is the way to go."
      },
      {
        "user": "rightx2",
        "created_at": "2020-07-06T01:16:12Z",
        "body": "@richardliaw  Thanks. I've solved the problem.\r\n"
      }
    ]
  },
  {
    "number": 8545,
    "title": "[ray] Is it bad practice to use sockets (pyzmq) to communicate between ray remote functions?",
    "created_at": "2020-05-22T06:17:38Z",
    "closed_at": "2020-05-27T15:03:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/8545",
    "body": "I have a `send()` function that generates random numpy arrays at every time step, and a `recv()` function that receives and prints those generated arrays. I am using `zmq` for sending/receiving the numpy arrays across the processes, and `pyarrow` to serialize and deserialize arrays. I wasn't able to find any examples using ray and zmq together, so I would like to know whether this is bad practice. If so, is there a recommended way to have the distributed-ly running processes communicate with each other using ray?\r\n\r\nThank you so much! \r\n\r\nPasted below is minimal working code (on Ubuntu 18.0.4, python=3.6.9, pyzmq=19.0.1, ray=0.8.5, pyarrow=0.17.1):\r\n\r\n```python\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport ray\r\nimport zmq\r\nray.init()\r\n\r\n\r\n@ray.remote\r\ndef send():\r\n    port = 5556\r\n    context = zmq.Context()\r\n    send_socket = context.socket(zmq.PUSH)\r\n    send_socket.bind(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        msg = np.random.rand(1, 3) # this could be larger, e.g. numpy-ed torch neural network weights\r\n        object_id = pa.serialize(msg).to_buffer()\r\n        send_socket.send(object_id)\r\n\r\n@ray.remote\r\ndef recv():        \r\n    port = 5556\r\n    context = zmq.Context()\r\n    recv_socket = context.socket(zmq.PULL)\r\n    recv_socket.connect(f\"tcp://127.0.0.1:{port}\")\r\n\r\n    while True:\r\n        object_id = recv_socket.recv()\r\n        msg = pa.deserialize(object_id)\r\n        print(msg)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    ray.wait([send.remote(), recv.remote()])\r\n```\r\n## Note:\r\nI had to use pyarrow for serialization since ray object id's (obtained via `ray.put()`) could not be passed through zmq sockets; doing so gives the error below:  \r\n```\r\nObjectID(45b95b1c8bd3a9c4ffffffff0100008801000000) does not provide a buffer interface.\r\n```",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8545/comments",
    "author": "cyoon1729",
    "comments": [
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T06:33:20Z",
        "body": "Ray already handles inter-process communication as well as serialization using apache arrow. You can just do.\r\n\r\n```python3\r\nimport ray\r\nray.init()\r\n\r\n@ray.remote\r\nclass ReceiveServer:\r\n    def recv(self, msg):\r\n        print(msg)\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\nrecv_server_handle = ReceiveServer.remote()\r\nray.wait(send.remote(recv_server_handle))\r\n```\r\nThis should do the same thing."
      },
      {
        "user": "cyoon1729",
        "created_at": "2020-05-22T07:35:22Z",
        "body": "@rkooo567 Thank you so much for your response and the example above. I would like to ask another question:\r\n \r\nSay, for instance, I have the `ReceiveServer` above to store the `msg` in an internal storage `self.storage (deque)` when `recv()` is called in `send()`, while continuously (as in a `while: True` loop) sampling data from `self.storage` and processing it in another member function `process()`.\r\n\r\nIf I were to run `process.remote()` asynchronously with respect to `send()`, would a mutual exclusion of `ReceiveSercer.storage` be enforced? Is this legal? \r\n\r\nThe code below implements what I tried to describe, but does not print anything:\r\n```python\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    def process(self):\r\n        while True:\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```\r\nIf it is indeed acceptable to use ray, pyarrow, and zmq together as in the first example, I would like to proceed with that. Are there any glaring issues with doing so? In particular, ray will be used purely as an alternative to python multiprocessing. \r\n\r\nThank you so much again for your time.\r\n"
      },
      {
        "user": "rkooo567",
        "created_at": "2020-05-22T21:17:35Z",
        "body": "It doesn't print anything because Actor (class with @ray.remote) is running in a single process, and `recv` will never run because `process` is occupying the process (because it is running a while loop). \r\n\r\nmutual exclusion of ReceiveSercer.storage be enforced? Is this legal?: Yes. Ray handles this issue and you never need to worry about locking. \r\n\r\nThere's nothing wrong with using zmq and pyarrow if you have the right reason. It is just not efficient because what you try to achieve using zmq and pyarrow is what Ray exists for. Ray is a distributed computing framework that abstracts inter-process communication problems (and many others).    \r\n\r\nYou can make this work in this way.  \r\n```python3\r\nimport ray\r\nimport numpy as np\r\nfrom collections import deque\r\nimport random\r\nimport asyncio\r\n\r\nray.init()\r\n\r\n@ray.remote\r\ndef send(receive_server_handle):\r\n    while True:\r\n        msg = np.random.rand(1, 3)\r\n        receive_server_handle.recv.remote(msg)\r\n\r\n\r\n@ray.remote \r\nclass ReceiveServer:\r\n    def __init__(self):\r\n        self.storage = deque(maxlen=2000)\r\n\r\n    async def recv(self, data):\r\n        self.storage.append(data)\r\n        \r\n    async def process(self):\r\n        while True:\r\n            await asyncio.sleep(0.0)\r\n            if len(self.storage) > 0:\r\n                data = random.sample(self.buffer, 1)\r\n                \r\n                # do something to data\r\n                # ...\r\n\r\n                print(data)  # does not print anything \r\n\r\nreceive_server = ReceiveServer.remote()\r\nray.wait([send.remote(receive_server), receive_server.process.remote()])\r\n```"
      },
      {
        "user": "cyoon1729",
        "created_at": "2020-05-27T15:03:37Z",
        "body": "Thanks @rkooo567! This was very helpful. "
      },
      {
        "user": "uchiiii",
        "created_at": "2023-08-07T15:14:51Z",
        "body": "I am very new to ray-project and have a question regarding this.\r\n\r\nRay supports inter-process communication as suggested above. What kind of protocol is used under the hood, `zmq` or anything else? Or it shares data using object storage like Plasma? \r\n\r\nThank you for you reply in advance! "
      }
    ]
  },
  {
    "number": 8413,
    "title": "[sgd] Can TorchTrainer print out something every one or several iterations?",
    "created_at": "2020-05-12T08:36:16Z",
    "closed_at": "2020-06-11T20:23:53Z",
    "labels": [
      "question",
      "sgd"
    ],
    "url": "https://github.com/ray-project/ray/issues/8413",
    "body": "Seems by default TorchTrainer only returns stats after train() finishes? During the training, is there a way I get some information (for example loss values, or just something to indicate the training is happening in the background?) for each iteration or every several iterations?\r\nOtherwise if one epoch training takes a lot of time, then I probably don't know what's going on. I may doubt whether the program crashes indeed or the training is just long.\r\n\r\n@richardliaw \r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/8413/comments",
    "author": "hkvision",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-05-14T08:36:26Z",
        "body": "You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use."
      },
      {
        "user": "hkvision",
        "created_at": "2020-06-09T12:35:33Z",
        "body": "> You can specify `num_steps`, which allows you to short-circuit the training. Also, `use_tqdm=True` is usually what I use.\r\n\r\nThank you so much! @richardliaw Sorry for the late reply. `use_tqdm` works great!\r\nIf I specify `num_steps`, then every call for `train` only trains several batches, and would it be the case that some data won't get trained?"
      },
      {
        "user": "richardliaw",
        "created_at": "2020-06-11T20:23:53Z",
        "body": "Ah yes; there's a workaround but we should push this. I'll make a new PR."
      }
    ]
  },
  {
    "number": 7912,
    "title": "Details about the hyperparameter in PPO Algorithm?",
    "created_at": "2020-04-06T14:27:08Z",
    "closed_at": "2020-04-06T14:37:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7912",
    "body": "Hi, so I want to tune my hyperparameter for the PPO Algorithm but I've found difficulties when reading the docs about the configs, so I guess I want to ask you guys in here about:\r\n1. What is the value of `lr_schedule` in the PPO Algorithm? Suppose that my starting learning_rate is `'lr': 1e-4` and I want to decay its value to 0 when I train.\r\n2. Is it possible to set the hidden layer size in the PPO algorithm? If yes, what is the corresponding config as I didn't find it in the documentation (I found this kind of config in the SAC algorithm documentation but not in PPO).\r\n\r\nThank you very much guys! I really appreciate your help \ud83d\ude04 ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7912/comments",
    "author": "Nicholaz99",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-06T14:36:29Z",
        "body": "Yeah, sorry, it's not clearly documented. Here are the answers. We'll add this to the docs.\r\n1) You are basically configuring a PiecewiseSchedule.\r\nSo lr_schedule: [[0, 0.01], [1000, 0.0005]] means that you decay from ts=0 (lr=0.01) linearly to ts=1000 (lr=0.0005). After 1000ts your learning rate will stay at 0.0005. The config key \"lr\" is ignored in this setting.\r\n2) You can do e.g. config[\"model\"][\"fcnet_hiddens\"] = [16, 32, 64]. Change the activation by using config[\"model\"][\"fcnet_activation\"] (\"tanh\", \"relu\", or \"linear\")."
      },
      {
        "user": "Nicholaz99",
        "created_at": "2020-04-06T14:47:52Z",
        "body": "Thank you so\r\n\r\n> Yeah, sorry, it's not clearly documented. Here are the answers. We'll add this to the docs.\r\n> \r\n> 1. You are basically configuring a PiecewiseSchedule.\r\n>    So lr_schedule: [[0, 0.01], [1000, 0.0005]] means that you decay from ts=0 (lr=0.01) linearly to ts=1000 (lr=0.0005). After 1000ts your learning rate will stay at 0.0005. The config key \"lr\" is ignored in this setting.\r\n> 2. You can do e.g. config[\"model\"][\"fcnet_hiddens\"] = [16, 32, 64]. Change the activation by using config[\"model\"][\"fcnet_activation\"] (\"tanh\", \"relu\", or \"linear\").\r\n\r\nThank you so much for your help!!! It helps me a lot for my project \ud83d\ude04 "
      }
    ]
  },
  {
    "number": 7849,
    "title": "[rllib] Unable to configure exploration parameters in PPO: Unknown config parameter `explore`",
    "created_at": "2020-04-01T09:25:26Z",
    "closed_at": "2020-05-01T08:08:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7849",
    "body": "Hello,\r\n\r\nI am unable to configure exploration parameters when trying to create a PPO trainer. Dictionary entries \"explore\" and \"exploration_config\" is said to be unknown. Below are the relevant trainer definition and the traceback.\r\n\r\n`trainer = PPOTrainer(\r\n                      env=env_title,\r\n                      config={\r\n                          \r\n                        \"explore\": True,\r\n                        \"exploration_config\": {\r\n                          \"type\": \"EpsilonGreedy\",\r\n                          # Parameters for the Exploration class' constructor:\r\n                          # \"initial_epsilon\"=1.0,  # default is 1.0\r\n                          # \"final_epsilon\"=0.05,  # default is 0.05\r\n                          \"epsilon_timesteps\": max_steps,  # Timesteps over which to anneal epsilon, defult is int(1e5).\r\n                        },\r\n\r\n\r\n                        \"num_workers\": 5,\r\n                        \"num_gpus\": 2,\r\n                        \"model\": nw_model,\r\n                        \"multiagent\": {\r\n                          \"policy_graphs\": policy_graphs,\r\n                          \"policy_mapping_fn\": policy_mapping_fn,\r\n                          \"policies_to_train\": [\"ppo_policy{}\".format(i) for i in range(n_agents)],\r\n                        },\r\n                        \"callbacks\": {\r\n                          \"on_episode_start\": tune.function(on_episode_start),\r\n                          \"on_episode_step\": tune.function(on_episode_step),\r\n                          \"on_episode_end\": tune.function(on_episode_end),\r\n                        },\r\n                        \"log_level\": \"ERROR\",\r\n                      })`\r\n\r\n\r\nFull traceback:\r\n\r\n`Exception                                 Traceback (most recent call last)\r\n<ipython-input-9-252111d46b85> in <module>()\r\n    121                           \"on_episode_end\": tune.function(on_episode_end),\r\n    122                         },\r\n--> 123                         \"log_level\": \"ERROR\",\r\n    124                       })\r\n    125 \r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py in __init__(self, config, env, logger_creator)\r\n     88 \r\n     89         def __init__(self, config=None, env=None, logger_creator=None):\r\n---> 90             Trainer.__init__(self, config, env, logger_creator)\r\n     91 \r\n     92         def _init(self, config, env_creator):\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in __init__(self, config, env, logger_creator)\r\n    370             logger_creator = default_logger_creator\r\n    371 \r\n--> 372         Trainable.__init__(self, config, logger_creator)\r\n    373 \r\n    374     @classmethod\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py in __init__(self, config, logger_creator)\r\n     94         self._restored = False\r\n     95         start_time = time.time()\r\n---> 96         self._setup(copy.deepcopy(self.config))\r\n     97         setup_time = time.time() - start_time\r\n     98         if setup_time > SETUP_TIME_THRESHOLD:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py in _setup(self, config)\r\n    476         merged_config = deep_update(merged_config, config,\r\n    477                                     self._allow_unknown_configs,\r\n--> 478                                     self._allow_unknown_subkeys)\r\n    479         self.raw_user_config = config\r\n    480         self.config = merged_config\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/util.py in deep_update(original, new_dict, new_keys_allowed, whitelist)\r\n    158         if k not in original:\r\n    159             if not new_keys_allowed:\r\n--> 160                 raise Exception(\"Unknown config parameter `{}` \".format(k))\r\n    161         if isinstance(original.get(k), dict):\r\n    162             if k in whitelist:\r\n\r\nException: Unknown config parameter `explore` `\r\n\r\n\r\n\r\nI am using Google Colab and Tensorflow 2.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7849/comments",
    "author": "ZekiDorukErden",
    "comments": [
      {
        "user": "sven1977",
        "created_at": "2020-04-01T09:39:56Z",
        "body": "Hi, you are probably on an older version of ray? What's your version number?\r\nFor now, try to remove these two keys (`exploration_config `and `explore`) altogether. You probably should not run PPO with EpsilonGreedy anyways."
      },
      {
        "user": "ZekiDorukErden",
        "created_at": "2020-04-01T09:52:13Z",
        "body": "Thanks for the reply! Apparently I am using version 0.8.0.dev5 (I copied the code block for dependencies in Ray with Google Colab tutorial without changing)."
      },
      {
        "user": "sven1977",
        "created_at": "2020-04-01T09:59:33Z",
        "body": "Ok, cool. So it's working now?"
      },
      {
        "user": "ZekiDorukErden",
        "created_at": "2020-04-01T10:03:00Z",
        "body": "Yes, when I run without the exploration settings, it worked. Thanks for the help!"
      }
    ]
  },
  {
    "number": 7737,
    "title": "Why actor methods cannot be called directly?",
    "created_at": "2020-03-25T03:57:27Z",
    "closed_at": "2020-03-31T15:54:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7737",
    "body": "When calling a actor method, that is, call the `__call__` method of an `ActorMethod` object. And this method is implemented as raise an `Exception` directly\r\n```\r\nException: Actor methods cannot be called directly. Instead of running 'object.get()', try 'object.get.remote()'\r\n```\r\n\r\nBut why is it necessary? Why it can't be\r\n\r\n```python\r\nclass ActorMethod:\r\n    ...\r\n    def __call__(self, *args, **kwargs):\r\n        return ray.get(self._remote(args, kwargs))\r\n    ...\r\n```\r\n\r\nThen in some case, If do the following:\r\n```python\r\nclass Foo(object):\r\n    def foo(self):\r\n        return \"foo\"\r\n\r\nclass Bar(object):\r\n    def bar(self, foo_obj):\r\n        return foo_obj.foo()\r\n \r\nRayFoo = ray.remote(Foo)\r\nRayBar = ray.remote(Bar)\r\n\r\nif __name__ == \"__main__\":\r\n    f = Foo()\r\n    b = Bar()\r\n    print(b.bar(f))\r\n\r\n    ray.init(log_to_driver=False)\r\n    rf = RayFoo.remote()\r\n    rb = RayBar.remote()\r\n    print(rb.bar(rf))\r\n```\r\nwith the original `__call__` implementation, this is not possible, but with the proposed one, this works perfectly.\r\n\r\nIs there any design consideration?\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7737/comments",
    "author": "cloudhan",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-25T05:33:16Z",
        "body": "This is a design decision we made a couple years ago. The reason is to remain consistent across the API - tasks, methods, and class invocations.\r\n\r\nThe high level goal is to safeguard against user errors. I should note that commonly, new users often complain about the verbosity of this decision :) "
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T06:03:29Z",
        "body": "Are there any design pattern to walkaround the issue I mentioned, that is, what if I want to support both local and Ray decorated types. How to avoid implementing those types twice?"
      },
      {
        "user": "ericl",
        "created_at": "2020-03-25T07:59:50Z",
        "body": "You can do that with a wrapper class that automatically invokes .remote() under the hood, e.g., `h = Wrapper(handle)`."
      },
      {
        "user": "cloudhan",
        "created_at": "2020-03-25T11:16:55Z",
        "body": "Tried to hack a new decorator that replace the object constructor with a wrapper and then which replace the actor_method_obj.__call__ method with a new wrapper that return ray.get(actor_method_obj.<method_name>.remote()), too convoluted, will use @ericl 's wrapper.\r\n\r\nBTW, it is viable to add an option to allow this type of behavior, e.g.\r\n```python\r\n@ray.remote(allow_non_remote_calls=True)\r\nclass Foo(object): ...\r\n``` "
      }
    ]
  },
  {
    "number": 7490,
    "title": "[rllib] why the RolloutWorker uses the default config  everytime",
    "created_at": "2020-03-06T17:57:35Z",
    "closed_at": "2020-03-07T03:58:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7490",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nRay: 0.8.2\r\nPython: 3.6\r\nTF: 2.0\r\nOS: macOS Catalina\r\n\r\nI  create some RolloutWorker instances in our customized training flow, you can run the code.\r\n\r\n```python\r\nimport argparse\r\n\r\nimport ray\r\nimport gym\r\nimport copy\r\nimport random\r\nimport numpy as np\r\n\r\nfrom ray import tune\r\nfrom ray.rllib.utils import try_import_tf\r\n\r\nfrom ray.rllib.models import ModelCatalog\r\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\r\n\r\nfrom ray.rllib.models.tf.tf_action_dist import Categorical\r\nfrom ray.rllib.agents.pg.pg import PGTFPolicy\r\n\r\nfrom ray.rllib.evaluation import RolloutWorker\r\nfrom ray.rllib.evaluation.metrics import collect_metrics\r\nfrom ray.rllib.policy.sample_batch import SampleBatch\r\nfrom ray.rllib.policy.tests.test_policy import TestPolicy\r\nfrom ray.rllib.policy.tf_policy import TFPolicy\r\n\r\nfrom ray.rllib.offline import NoopOutput, IOContext, OutputWriter, InputReader\r\n\r\nfrom ray.rllib.agents.trainer import with_common_config\r\n\r\nfrom ray.rllib.evaluation.postprocessing import Postprocessing, compute_advantages\r\nfrom ray.rllib.policy.tf_policy_template import build_tf_policy\r\n\r\n\r\nfrom ray.rllib.models.tf.misc import normc_initializer, get_activation_fn\r\n\r\n\r\ntf = try_import_tf()\r\n\r\n\r\nclass CustomCategorical(Categorical):\r\n    def __init__(self, inputs, model=None, temperature=1.0):\r\n        \"\"\" The inputs are action logits \"\"\"\r\n        super().__init__(inputs, model, temperature)\r\n        self.softmax = tf.nn.softmax(inputs)\r\n\r\n    def prob(self, actions):\r\n        _prob_given_action = tf.one_hot(actions, depth=self.softmax.get_shape().as_list()[-1]) * self.softmax\r\n        return tf.reduce_sum(_prob_given_action, axis=-1)\r\n\r\n\r\nclass DemoModel(TFModelV2):\r\n    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\r\n        super(DemoModel, self).__init__(obs_space, action_space, num_outputs, model_config, name)\r\n\r\n        self.loss_inputs = [\r\n            ('taken_actions', tf.placeholder(tf.int32, (None,))),\r\n            ('returns', tf.placeholder(tf.float32, (None,)))\r\n        ]\r\n        self.ph_obs_input = tf.placeholder(tf.float32, (None,)+obs_space.shape)\r\n\r\n        self.ph_all_inputs = {k: v for _, (k, v) in enumerate(self.loss_inputs + [(\"obs\", self.ph_obs_input)])}\r\n        self.ph_all_inputs['prev_actions'] = tf.placeholder(tf.int32, (None,))\r\n        self.ph_all_inputs['prev_rewards'] = tf.placeholder(tf.float32, (None,))\r\n\r\n        inputs = tf.keras.layers.Input(\r\n            shape=(np.product(obs_space.shape), ))\r\n\r\n        _layer_out = tf.keras.layers.Dense(\r\n            128,\r\n            activation=tf.nn.tanh,\r\n            kernel_initializer=normc_initializer(1.0))(inputs)\r\n\r\n        self._layer_out = tf.keras.layers.Dense(\r\n            num_outputs,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(1.0))(_layer_out)\r\n\r\n        self._value_out = tf.keras.layers.Dense(\r\n            1,\r\n            activation=None,\r\n            kernel_initializer=normc_initializer(0.01))(_layer_out)\r\n\r\n        self.action_dist = CustomCategorical(self._layer_out, None)\r\n        self.predicted_actions = self.action_dist.sample()\r\n\r\n        # self.model and self.base_model are different\r\n        self.model = tf.keras.Model(inputs, [self._layer_out, self._value_out])\r\n        self.register_variables(self.model.variables)\r\n\r\n    def forward(self, input_dict, state, seq_lens):\r\n        model_out, self._value_out = self.model(input_dict[\"obs_flat\"])\r\n        return model_out, state\r\n\r\n    def custom_loss(self, policy_loss, loss_inputs):\r\n        return policy_loss\r\n\r\n    def from_batch(self, train_batch, is_training=True):\r\n        \"\"\"Convenience function that calls this model with a tensor batch.\r\n\r\n        All this does is unpack the tensor batch to call this model with the\r\n        right input dict, state, and seq len arguments.\r\n        \"\"\"\r\n\r\n        input_dict = {\r\n            \"obs\": train_batch[SampleBatch.CUR_OBS],\r\n            \"is_training\": is_training,\r\n        }\r\n        if SampleBatch.PREV_ACTIONS in train_batch:\r\n            input_dict[\"prev_actions\"] = train_batch[SampleBatch.PREV_ACTIONS]\r\n        if SampleBatch.PREV_REWARDS in train_batch:\r\n            input_dict[\"prev_rewards\"] = train_batch[SampleBatch.PREV_REWARDS]\r\n        states = []\r\n        i = 0\r\n        while \"state_in_{}\".format(i) in train_batch:\r\n            states.append(train_batch[\"state_in_{}\".format(i)])\r\n            i += 1\r\n        return self.__call__(input_dict, states, train_batch.get(\"seq_lens\"))\r\n\r\n    def value_function(self):\r\n        return tf.reshape(self._value_out, [-1])\r\n\r\n    def metrics(self):\r\n        \"\"\"Override to return custom metrics from your model.\r\n\r\n        The stats will be reported as part of the learner stats, i.e.,\r\n            info:\r\n                learner:\r\n                    model:\r\n                        key1: metric1\r\n                        key2: metric2\r\n\r\n        Returns:\r\n            Dict of string keys to scalar tensors.\r\n        \"\"\"\r\n        return {}\r\n\r\n\r\ndef pg_tf_loss(policy, model, dist_class, train_batch):\r\n    \"\"\"The basic policy gradients loss.\"\"\"\r\n    logits, _ = model.from_batch(train_batch)\r\n    action_dist = dist_class(logits, model)\r\n    return -tf.reduce_mean(\r\n        action_dist.logp(train_batch[SampleBatch.ACTIONS]) * train_batch[SampleBatch.REWARDS])\r\n\r\n\r\n# def post_process_advantages(policy,\r\n#                             sample_batch,\r\n#                             other_agent_batches=None,\r\n#                             episode=None):\r\n#     \"\"\"This adds the \"advantages\" column to the sample train_batch.\"\"\"\r\n#     return compute_advantages(\r\n#         sample_batch,\r\n#         0.0,\r\n#         policy.config[\"gamma\"],\r\n#         use_gae=False,\r\n#         use_critic=False)\r\n\r\n\r\nDEFAULT_CONFIG = with_common_config({\r\n    # # No remote workers by default.\r\n    # \"num_workers\": 0,\r\n    # # Learning rate.\r\n    # \"lr\": 0.0004,\r\n})\r\n\r\n\r\nCustomPolicy = build_tf_policy(\r\n    name=\"CustomPolicy\",\r\n    get_default_config=lambda: DEFAULT_CONFIG,\r\n    # postprocess_fn=post_process_advantages,\r\n    loss_fn=pg_tf_loss)\r\n\r\n\r\ndef set_variables(policy: CustomPolicy):\r\n    policy._variables = ray.experimental.tf_utils.TensorFlowVariables(\r\n        [], policy._sess, policy.variables())\r\n\r\n\r\ndef training_workflow(config, reporter):\r\n    sess = tf.Session()\r\n    env = gym.make(\"CartPole-v0\")\r\n\r\n    # policy = CustomPolicy(observation_space=env.observation_space, action_space=env.action_space, config=config,\r\n    #                       sess=sess, model=model, loss_inputs=model.loss_inputs, loss='Not None',\r\n    #                       action_sampler=model.predicted_actions, obs_input=model.ph_obs_input)\r\n\r\n    conf = {'config': config, 'sess': sess, 'model': model, 'loss_inputs': model.loss_inputs, 'loss': 'Not None',\r\n            'action_sampler': model.predicted_actions, 'obs_input': model.ph_obs_input}\r\n\r\n    policy = CustomPolicy(env.observation_space, env.action_space,\r\n                          config=config)  # , existing_inputs=model.ph_all_inputs)#, existing_model=model)\r\n    set_variables(policy)\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n\r\n    for i in range(config[\"num_iters\"]):\r\n        # Broadcast weights to the policy evaluation workers\r\n        weights = ray.put({\"default_policy\": policy.get_weights()})\r\n        for w in workers:\r\n            w.set_weights.remote(weights)\r\n\r\n        # Gather a batch of samples\r\n        T1 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Update the remote policy replicas and gather another batch of samples\r\n        # new_value = policy.get_weights()\r\n        # for w in workers:\r\n        #     w.for_policy.remote(lambda p: p.update_some_value(new_value))\r\n\r\n        # Gather another batch of samples\r\n        T2 = SampleBatch.concat_samples(\r\n            ray.get([w.sample.remote() for w in workers]))\r\n\r\n        # Improve the policy using the T1 batch\r\n        policy.learn_on_batch(T1)\r\n\r\n        # Do some arbitrary updates based on the T2 batch\r\n        # print(f'iter: {i}, sum_rewards: {(sum(T2[\"rewards\"])):.2f}')\r\n\r\n        reporter(**collect_metrics(remote_workers=workers))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--gpu\", action=\"store_true\")\r\n    parser.add_argument(\"--num-iters\", type=int, default=3)\r\n    parser.add_argument(\"--num-workers\", type=int, default=1)\r\n    parser.add_argument(\"--num-cpus\", type=int, default=0)\r\n\r\n    args = parser.parse_args()\r\n    ray.init(num_cpus=args.num_cpus or None)\r\n    ModelCatalog.register_custom_model(\"demo_model\", DemoModel)\r\n\r\n    tune.run(\r\n        training_workflow,\r\n        # resources_per_trial={\r\n        #     \"gpu\": 1 if args.gpu else 0,\r\n        #     \"cpu\": 1,\r\n        #     \"extra_cpu\": args.num_workers,\r\n        # },\r\n        config={\r\n            \"num_workers\": args.num_workers,\r\n            \"num_iters\": args.num_iters,\r\n            \"lr\": 1e-3,\r\n            \"model\": {\r\n                \"custom_model\": \"demo_model\",\r\n                \"max_seq_len\": 20,\r\n                \"custom_options\": {\r\n                    \"activation\": tf.nn.tanh,\r\n                }\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nIn\r\n```\r\n    workers = [\r\n        RolloutWorker.as_remote().remote(env_creator=lambda c: gym.make(\"CartPole-v0\"),\r\n                                         policy=CustomPolicy\r\n                                         )\r\n        for _ in range(config[\"num_workers\"])\r\n    ]\r\n```\r\nThe RolloutWorker creates a instance by using the default config not the config passed into the workflow. \r\n\r\nAre there any methods to fix it?",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7490/comments",
    "author": "GoingMyWay",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-03-06T21:56:48Z",
        "body": "You need to explicitly pass it via RolloutWorker.remote(policy_config=config), or use `WorkerSet(trainer_config=config)` to create the rollout workers."
      },
      {
        "user": "GoingMyWay",
        "created_at": "2020-03-07T02:34:41Z",
        "body": "> You need to explicitly pass it via RolloutWorker.remote(policy_config=config), or use `WorkerSet(trainer_config=config)` to create the rollout workers.\r\n\r\nExactly. Thanks."
      }
    ]
  },
  {
    "number": 7467,
    "title": "[tune][rllib] _InactiveRpcError Deadline Exceeded",
    "created_at": "2020-03-05T16:32:58Z",
    "closed_at": "2020-04-22T17:22:40Z",
    "labels": [
      "question",
      "tune"
    ],
    "url": "https://github.com/ray-project/ray/issues/7467",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### What is your question?\r\nI have VirtualBox running on Centos 7 and I am having trouble initializing Ray. After I run ray.init(), I get an _InactiveRpcError due to a Deadline Exceeded exception. What info should I provide in order to troubleshoot this error?\r\n\r\n\r\n*Ray version and other system information (Python version, TensorFlow version, OS):*\r\nray 0.8.2\r\nredis 3.4.1\r\nPython 3.6\r\nCentos 7 on VirtualBox",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7467/comments",
    "author": "Leonolovich",
    "comments": [
      {
        "user": "Leonolovich",
        "created_at": "2020-03-05T20:46:30Z",
        "body": "Running ray.init(local_mode=True) allows me to continue without errors to my tune.run() step, but I havent been able to resolve the Inactive Rcp Error. This is not ideal as I can only get one worker to perform training when using local_mode=True."
      },
      {
        "user": "richardliaw",
        "created_at": "2020-04-22T02:36:49Z",
        "body": "Can you please try again on the latest Ray version?"
      },
      {
        "user": "Leonolovich",
        "created_at": "2020-04-22T17:22:39Z",
        "body": "That appears to have made the issue go away. For documentation, I was having the issue on version 0.8.2 or Ray and I no longer have the issue on version 0.8.4.\r\n\r\nThanks"
      }
    ]
  },
  {
    "number": 7424,
    "title": "Actor method arguments",
    "created_at": "2020-03-03T19:27:19Z",
    "closed_at": "2020-03-03T21:06:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7424",
    "body": "Why do actor methods do not support passing arguments? There is an assertion that fails if the actor method function arguments are larger than 0.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7424/comments",
    "author": "commanderka",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-03T19:31:13Z",
        "body": "Can you please provide more context? i.e., a script and stack trace for reproducing this issue?"
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T20:20:46Z",
        "body": "I can only provide a code snippet, the problem is that there are too many dependencies. But I think its a more conceptual thing anyway.\r\n\r\n```\r\n@ray.remote(num_gpus=1)\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = bla\r\n        self.landmarkDetector = bla\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Improved)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    @ray.method\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/25/image_0409.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(currentActor,imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\ngives me \r\n\r\n> File \"testActors.py\", line 10, in <module>\r\n>     class PreprocessorActor(object):\r\n>   File \"testActors.py\", line 18, in PreprocessorActor\r\n>     @ray.method\r\n>   File \"/usr/local/lib/python3.6/dist-packages/ray/actor.py\", line 40, in method\r\n>     assert len(args) == 0\r\n> AssertionError\r\n\r\nThe idea is to have remote workers that are constantly fed with images to preprocess and to collect the preprocessed images. The problem is that the initialization of the preprocessing takes time, so I used the concept of Actors. Perhaps I have some conceptually wrong understanding, I dont know.\r\nRay version is 0.8.2"
      },
      {
        "user": "simon-mo",
        "created_at": "2020-03-03T20:26:05Z",
        "body": "`@ray.method` decorator is only there if you want to pass special parameters for a remote method, for example, [specifying the number of return values](@ray.method(num_return_vals=2)). By default, all methods for a `@ray.remote` actor can be called. \r\n\r\nYou can just remote the `@ray.method` decorator. "
      },
      {
        "user": "commanderka",
        "created_at": "2020-03-03T21:06:27Z",
        "body": "Works like this now. I think the error was just misleading. I will close the issue. Nevertheless I would encourage to update the doku with some practical samples, perhaps also concerning the ActorPool class.\r\n\r\n```\r\n@ray.remote\r\nclass PreprocessorActor(object):\r\n    def __init__(self):\r\n        self.detector = something\r\n        self.landmarkDetector = something\r\n        self.transformer = AffineTransformer((112, 112), TransformType.Relative)\r\n        self.preprocessor = IPCustomPreprocessor.IPCustomPreprocessor(self.detector, self.landmarkDetector, self.transformer)\r\n    def preprocess(self,imagePath):\r\n        return self.preprocessor.preprocess_from_path(imagePath)\r\n        \r\n\r\n\r\nif __name__=='__main__':\r\n    ray.init(address=\"sss-digits-1:6379\")\r\n    #create several actors\r\n    actorList = []\r\n    for nActor in range(5):\r\n        actor = PreprocessorActor.remote()\r\n        actorList.append(actor)\r\n\r\n    \r\n    imagePathToPreprocess = \"/media/sss_data/FaceDatabases/CaltecFaces/20/image_0308.jpg\"\r\n    preprocIds = []\r\n    for currentActor in actorList:\r\n        preprocId = currentActor.preprocess.remote(imagePathToPreprocess)\r\n        preprocIds.append(preprocId)\r\n    results = ray.get(preprocIds)\r\n    for preprocResultIndex,preprocResult in enumerate(results):\r\n        if preprocResult is not None and preprocResult.error_message is None:\r\n            preprocessedImage = preprocResult.preprocessed_image\r\n            cv2.imwrite(\"/media/sss_data_3/preprocImages/{0}.jpg\".format(preprocResultIndex),preprocessedImage)\r\n```\r\n"
      }
    ]
  },
  {
    "number": 7394,
    "title": "Does DQN \"rollout.py\" have exploration turned off?",
    "created_at": "2020-03-02T03:57:53Z",
    "closed_at": "2020-03-02T10:34:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7394",
    "body": "When I call \"rollout.py\" I am not sure if exploration is turned off or not. I've looked over the file and can't seem to find `explore=False` anywhere.\r\n\r\nSo, when we evaluate trained policy (e.g. DQN) with rollout script - does it actually turn off random actions or not?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7394/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-03-02T04:08:14Z",
        "body": "I don't think it's actually turned off by default right now."
      },
      {
        "user": "sven1977",
        "created_at": "2020-03-02T10:34:15Z",
        "body": "The default config for DQN for evaluation is `exploration=False` (greedy action selection).\n\n---\n\nHowever, in rollout.py, we do not use the evaluation_config, which is something, we should probably change.\n\n---\n\nThen again, rollout.py picks up an already trained DQN, so its timesteps should already be past the epsilon exploration period, which then means it's (almost) not exploring anymore (if `final_epsilon` is 0.0, it won't explore at all). So for your specific DQN case, it should be fine (as in: not picking random actions anymore). What's your `exploration_config`?\n\n---\n\nThe above PR makes sure that rollout.py always uses the evaluation_config (which for DQN, is explore=False).\r\nIn the meantime, you can add `--config '{\"explore\": false}'` to your rollout.py command line to make sure, your algo picks only greedy acitons."
      },
      {
        "user": "drozzy",
        "created_at": "2020-03-02T13:07:44Z",
        "body": "Awesome."
      }
    ]
  },
  {
    "number": 7389,
    "title": "[rlib] Changing Results directory",
    "created_at": "2020-03-01T20:17:17Z",
    "closed_at": "2020-03-02T15:53:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7389",
    "body": "By default, when training an agent ( ray.rllib.agents) results are saved to   ~/ray_results/RUN,  where the format of RUN seems to depend on the agent used. Is there  way to change this to a custom directory and change the aforementioned format ?\r\n\r\nI skimmed  through the documentation and couldn't find any option for this. \r\n\r\nNote: I am not using tune.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7389/comments",
    "author": "Degiorgio",
    "comments": [
      {
        "user": "richardliaw",
        "created_at": "2020-03-01T22:27:15Z",
        "body": "You can pass in a custom logger_creator - see `trainer.py:Trainer.__init__`:\r\n\r\n```\r\n\r\n# Create a default logger creator if no logger_creator is specified\r\nif logger_creator is None:\r\n    timestr = datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\r\n    logdir_prefix = \"{}_{}_{}\".format(self._name, self._env_id,\r\n                                      timestr)\r\n\r\n    def default_logger_creator(config):\r\n        \"\"\"Creates a Unified logger with a default logdir prefix\r\n        containing the agent name and the env id\r\n        \"\"\"\r\n        if not os.path.exists(DEFAULT_RESULTS_DIR):\r\n            os.makedirs(DEFAULT_RESULTS_DIR)\r\n        logdir = tempfile.mkdtemp(\r\n            prefix=logdir_prefix, dir=DEFAULT_RESULTS_DIR)\r\n        return UnifiedLogger(config, logdir, loggers=None)\r\n\r\n    logger_creator = default_logger_creator\r\n```"
      },
      {
        "user": "Degiorgio",
        "created_at": "2020-03-02T15:53:51Z",
        "body": "@richardliaw  Thanks  this worked!"
      }
    ]
  },
  {
    "number": 7251,
    "title": "[rllib] In multiagent environment, is timesteps_total the total timesteps per agent or over all agents?",
    "created_at": "2020-02-20T21:16:05Z",
    "closed_at": "2020-02-20T22:05:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7251",
    "body": "### In multiagent environment, is timesteps_total the total timesteps per agent or across all agents?\r\n\r\nFor example, I have 4 policies in my multiagent policy configuration, and after the first training iteration the timesteps_total is 4000.\r\n\r\nIs that number per agent or overall? I.e.:\r\n\r\n1. Per agent - each agent has run 4000 timesteps, so the total number of timesteps is 16000\r\n2. Overall - each agent has run 1000 timesteps, so the total number of timesteps is 4000\r\n\r\nWhich one is it?\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7251/comments",
    "author": "coreylowman",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-20T22:01:51Z",
        "body": "It's the number of times step has been called on the env (so probably it means each agent has run 4000 timesteps, assuming each agent participates in every step)."
      },
      {
        "user": "coreylowman",
        "created_at": "2020-02-20T22:05:44Z",
        "body": "Thanks, makes sense!"
      },
      {
        "user": "drozzy",
        "created_at": "2020-02-23T06:15:10Z",
        "body": "> it means each agent has run 4000 timesteps\r\n\r\nWouldn't his `timesteps_total` be 16,000 then?"
      }
    ]
  },
  {
    "number": 7194,
    "title": "[rllib]PPO with action branching?",
    "created_at": "2020-02-17T13:42:11Z",
    "closed_at": "2020-02-18T05:29:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/7194",
    "body": "<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\n\r\n### PPO with action branching?\r\nRay version: 0.8.0\r\nTensorflow Version: 1.14.0\r\nOS: Ubuntu 18.04\r\n\r\nI'm currently working on training action branching agents with PPO. What else do I need to do besides set the action space to something like `gym.spaces.Tuple([gym.spaces.Discrete(3), gym.spaces.Discrete(5)])`, or I need to write a custom loss function? I was wondering if the gradients would be correct. ",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/7194/comments",
    "author": "jinbo-huang",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-17T18:57:55Z",
        "body": "Yeah that's all you need for PPO. The action will be automatically computed for the space."
      },
      {
        "user": "jinbo-huang",
        "created_at": "2020-02-18T05:29:02Z",
        "body": "Thank you for your answer. It helped a lot."
      }
    ]
  },
  {
    "number": 6986,
    "title": "[Question][rllib] Stochastic Game tensorboard separate rewards",
    "created_at": "2020-01-31T07:44:54Z",
    "closed_at": "2020-02-06T16:32:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/6986",
    "body": "### What is your question?\r\n\r\nI am designing a simple stochastic game wherein I have two agents. The first agent (the good guy) is rewarded according to some task. The second agent (the adversary) is rewarded negative proportional to the first. This is to encourage the adversary to screw up the good guy.\r\n\r\nAs a first pass, I just set the reward of the adversary equal to negative the reward of the good guy. This seems to cause some issue with tensorboard, however, because it looks like the rewards are summed together, which results is a reward of 0 for each iteration.\r\n\r\nIt would be nice to be able to visualize the rewards of each agent individually. I imagine that this would be very useful for other MARL scenarios, not just SG. Is this something that is possible?\r\n\r\nThank you!\r\n\r\npython3.7\r\ntensorflow2.1\r\nray0.8.1\r\nmac10.14\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/6986/comments",
    "author": "rusu24edward",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2020-02-01T01:43:53Z",
        "body": "Are you using separate policies for each agent? You can view the individual policy scores under the `policy_X_reward_mean` etc keys."
      },
      {
        "user": "rusu24edward",
        "created_at": "2020-02-03T21:58:22Z",
        "body": "I am using separate policies for each agent. I'm not sure what you mean by `policy_x_reward_mean` key. Is that something in the tensorboard interface?"
      },
      {
        "user": "ericl",
        "created_at": "2020-02-03T22:00:29Z",
        "body": "Yep, you should be able to find those in tensorboard, `result.json`, or printed to stdout if you use the `-v` flag."
      },
      {
        "user": "rusu24edward",
        "created_at": "2020-02-06T16:32:01Z",
        "body": "Nice! I found them. For me, they are a few pages in stored as `policy_reward_mean/<policy_name>`\r\n\r\nThank you!"
      }
    ]
  },
  {
    "number": 5017,
    "title": "Confusion about action embedding in parametric_action_cartpole.py",
    "created_at": "2019-06-22T03:52:31Z",
    "closed_at": "2019-06-26T08:53:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/5017",
    "body": "### Describe the problem\r\n<!-- Describe the problem clearly here. -->\r\n\r\nDear all,\r\n  I want to ask about hot to **make the action embedding trainable,** instead of making them initialized randomly. Since my valid action space is large, should I put them into the model to learn them,  instead of the env?  If they are put in the env, when I set **num_workers** > 0, there would be  multiple envs. Under this situation, are the **action embeddings are different across the different envs**? They should be the same, right?\r\n\r\nThanks in advance.",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/5017/comments",
    "author": "yangysc",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-06-26T03:47:47Z",
        "body": "I think one solution here is to make the embedding network part of the model itself. That way, synchronization is taken care of automatically.\r\n\r\nOne way this could work is if the action are represented in the environment as just a number. Then, in the model there can be an embedding layer that transforms this into the action embedding."
      },
      {
        "user": "yangysc",
        "created_at": "2019-06-26T08:53:01Z",
        "body": "Thanks for your help.  I think that is a nice solution."
      }
    ]
  },
  {
    "number": 3785,
    "title": "Tune doesn't work with multi agent env",
    "created_at": "2019-01-15T19:50:26Z",
    "closed_at": "2019-01-16T17:55:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3785",
    "body": "<!--\r\nGeneral questions should be asked on the mailing list ray-dev@googlegroups.com.\r\n\r\nBefore submitting an issue, please fill out the following form.\r\n-->\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.1\r\n- **Ray installed from (source or binary)**: Source\r\n- **Ray version**: 0.6.1\r\n- **Python version**: 3.6.7\r\n- **Exact command to reproduce**:\r\n\r\n\r\n<!--\r\nYou can obtain the Ray version with\r\n\r\npython -c \"import ray; print(ray.__version__)\"\r\n-->\r\n\r\n### Describe the problem\r\nI am trying to use Tune in combination with RLlib to train with a custom multi agent environment. It works, when I am just using RLlib. But when I try to train using Tune i get `RecursionError: maximum recursion depth exceeded`. Does Tune currently support multi agent environments? Please find my code and the full stack trace here:\r\n\r\n### Source code / logs\r\n```python3\r\nimport ray\r\nimport ray.rllib.agents.ppo as ppo\r\nimport ray.tune as tune\r\nimport ray.tune.schedulers\r\nfrom ray.tune.logger import pretty_print\r\nfrom ray.tune.registry import register_env\r\nimport beer_distribution_game\r\n\r\ndef env_creator(env_config):\r\n    import gym\r\n    import beer_distribution_game\r\n    return beer_distribution_game.BeerDistributionGameV0()\r\n\r\ndef policy_mapper(agent_id):\r\n    return agent_id\r\n\r\nray.init(redis_address='localhost:6379')\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nspace_env = beer_distribution_game.BeerDistributionGameV0()\r\nspaces = space_env.get_spaces()\r\nsingle_config = {\r\n            'model' : {\r\n                'conv_filters' : None,\r\n                'fcnet_activation' : 'relu',\r\n                'fcnet_hiddens': [50, 100, 100]\r\n            },\r\n    'gamma': 0.7\r\n}\r\n\r\nregister_env(\"SimpleBeerGame\", env_creator)\r\n\r\nconfig = {\r\n    'beer-game-tune': {\r\n        'run': 'PPO',\r\n        'env': 'SimpleBeerGame',\r\n        'stop': {'episode_reward_mean' : -2000},\r\n        'config': {\r\n            'multiagent': {\r\n                'policy_mapping_fn': policy_mapper,\r\n                'policy_graphs': {\r\n                    'manufactorer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['manufactorer']['observation_space'], spaces['manufactorer']['action_space'], single_config),\r\n                    'distributor':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['distributor']['observation_space'], spaces['distributor']['action_space'], single_config),\r\n                    'supplier':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['supplier']['observation_space'], spaces['supplier']['action_space'], single_config),\r\n                    'retailer':\r\n                        (ppo.ppo.PPOPolicyGraph, spaces['retailer']['observation_space'], spaces['retailer']['action_space'], single_config)\r\n                    },\r\n                'policies_to_train': [\r\n                    'manufactorer', 'distributor', 'supplier', 'retailer']\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nscheduler = ray.tune.schedulers.AsyncHyperBandScheduler(time_attr='training_iteration', reward_attr='episode_reward_mean', max_t=100)\r\n\r\ntrials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n```\r\n\r\n\r\nThis is the stack trace:\r\n```\r\n== Status ==\r\nUsing AsyncHyperBand: num_stopped=0\r\nBracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None\r\nBracket: Iter 90.000: None | Iter 30.000: None\r\nBracket: Iter 90.000: None\r\nResources requested: 0/16 CPUs, 0/0 GPUs\r\nMemory usage on this node: 1.7/16.3 GB\r\n\r\nDeprecation warning: Function values are ambiguous in Tune configuations. Either wrap the function with `tune.function(func)` to specify a function literal, or `tune.sample_from(func)` to tell Tune to sample values from the function during variant generation: <function policy_mapper at 0x7fa09014a8c8>\r\n---------------------------------------------------------------------------\r\nRecursionError                            Traceback (most recent call last)\r\n<ipython-input-6-6efdb03a13a1> in <module>\r\n----> 1 trials = tune.run_experiments(experiments=config,scheduler=scheduler)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run_experiments(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\r\n    106     last_debug = 0\r\n    107     while not runner.is_finished():\r\n--> 108         runner.step()\r\n    109         if time.time() - last_debug > DEBUG_PRINT_INTERVAL:\r\n    110             logger.info(runner.debug_string())\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in step(self)\r\n    112             raise TuneError(\"Called step when all trials finished?\")\r\n    113         self.trial_executor.on_step_begin()\r\n--> 114         next_trial = self._get_next_trial()\r\n    115         if next_trial is not None:\r\n    116             self.trial_executor.start_trial(next_trial)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _get_next_trial(self)\r\n    252         trials_done = all(trial.is_finished() for trial in self._trials)\r\n    253         wait_for_trial = trials_done and not self._search_alg.is_finished()\r\n--> 254         self._update_trial_queue(blocking=wait_for_trial)\r\n    255         trial = self._scheduler_alg.choose_trial_to_run(self)\r\n    256         return trial\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py in _update_trial_queue(self, blocking, timeout)\r\n    362             timeout (int): Seconds before blocking times out.\r\n    363         \"\"\"\r\n--> 364         trials = self._search_alg.next_trials()\r\n    365         if blocking and not trials:\r\n    366             start = time.time()\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in next_trials(self)\r\n     48             trials (list): Returns a list of trials.\r\n     49         \"\"\"\r\n---> 50         trials = list(self._trial_generator)\r\n     51         self._finished = True\r\n     52         return trials\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/basic_variant.py in _generate_trials(self, unresolved_spec, output_path)\r\n     67             raise TuneError(\"Must specify `run` in {}\".format(unresolved_spec))\r\n     68         for _ in range(unresolved_spec.get(\"num_samples\", 1)):\r\n---> 69             for resolved_vars, spec in generate_variants(unresolved_spec):\r\n     70                 experiment_tag = str(self._counter)\r\n     71                 if resolved_vars:\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in generate_variants(unresolved_spec)\r\n     40         \"cpu\": {\"eval\": \"spec.config.num_workers\"}\r\n     41     \"\"\"\r\n---> 42     for resolved_vars, spec in _generate_variants(unresolved_spec):\r\n     43         assert not _unresolved_values(spec)\r\n     44         yield format_vars(resolved_vars), spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    138     for resolved_spec in grid_search:\r\n    139         resolved_vars = _resolve_lambda_vars(resolved_spec, lambda_vars)\r\n--> 140         for resolved, spec in _generate_variants(resolved_spec):\r\n    141             for path, value in grid_vars:\r\n    142                 resolved_vars[path] = _get_value(spec, path)\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _generate_variants(spec)\r\n    121 def _generate_variants(spec):\r\n    122     spec = copy.deepcopy(spec)\r\n--> 123     unresolved = _unresolved_values(spec)\r\n    124     if not unresolved:\r\n    125         yield {}, spec\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\n... last 1 frames repeated, from the frame below ...\r\n\r\n/usr/local/lib/python3.6/dist-packages/ray/tune/suggest/variant_generator.py in _unresolved_values(spec)\r\n    258         elif isinstance(v, dict):\r\n    259             # Recurse into a dict\r\n--> 260             for (path, value) in _unresolved_values(v).items():\r\n    261                 found[(k, ) + path] = value\r\n    262         elif isinstance(v, list):\r\n\r\nRecursionError: maximum recursion depth exceeded\r\n```\r\n\r\nThis is the whole config:\r\n```python\r\n{'beer-game-tune': {'config': {'multiagent': {'policies_to_train': ['manufactorer',\r\n                                                                    'distributor',\r\n                                                                    'supplier',\r\n                                                                    'retailer'],\r\n                                              'policy_graphs': {'distributor': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                Box(4,),\r\n                                                                                Discrete(20),\r\n                                                                                {'gamma': 0.7,\r\n                                                                                 'model': {'conv_filters': None,\r\n                                                                                           'fcnet_activation': 'relu',\r\n                                                                                           'fcnet_hiddens': [50,\r\n                                                                                                             100,\r\n                                                                                                             100]}}),\r\n                                                                'manufactorer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                                 Box(4,),\r\n                                                                                 Discrete(20),\r\n                                                                                 {'gamma': 0.7,\r\n                                                                                  'model': {'conv_filters': None,\r\n                                                                                            'fcnet_activation': 'relu',\r\n                                                                                            'fcnet_hiddens': [50,\r\n                                                                                                              100,\r\n                                                                                                              100]}}),\r\n                                                                'retailer': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}}),\r\n                                                                'supplier': (<class 'ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph'>,\r\n                                                                             Box(4,),\r\n                                                                             Discrete(20),\r\n                                                                             {'gamma': 0.7,\r\n                                                                              'model': {'conv_filters': None,\r\n                                                                                        'fcnet_activation': 'relu',\r\n                                                                                        'fcnet_hiddens': [50,\r\n                                                                                                          100,\r\n                                                                                                          100]}})},\r\n                                              'policy_mapping_fn': <function policy_mapper at 0x7f804434a1e0>}},\r\n                    'env': 'SimpleBeerGame',\r\n                    'run': 'PPO',\r\n                    'stop': {'episode_reward_mean': -2000}}}\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3785/comments",
    "author": "MariusDanner",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2019-01-16T02:06:59Z",
        "body": "The issue is that tune is trying to expand lambda functions to generate trial variants. To fix that, you can 'escape' the policy mapper function with tune.function(func).\r\n\r\nThis is an unfortunate gotcha of the tune API, we should eventually raise an error on raw functions passed in the config."
      }
    ]
  },
  {
    "number": 3660,
    "title": "extract the feature of conv2 or fc1",
    "created_at": "2018-12-29T04:11:59Z",
    "closed_at": "2019-01-04T07:20:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3660",
    "body": "Hi, based on the visionnet model in rllib, the structure of DNN is: input->conv1->conv2->fc1->fc2.\r\n\r\nMy question is that how to modify the codes in Rllib to extract the features on conv2 layer or fc1 layer? I have tried several ways but cannot pass the compiler. Thanks.\r\n\r\nFor ES codes, only the feature on fc2 can be extracted by the following two parts:\r\n1) From rllib/models/visionnet.py\r\nclass VisionNetwork(Model):\r\n\r\n    def _build_layers_v2(self, input_dict, num_outputs, options):\r\n        inputs = input_dict[\"obs\"]\r\n        filters = options.get(\"conv_filters\")\r\n        if not filters:\r\n            filters = get_filter_config(options)\r\n\r\n        activation = get_activation_fn(options.get(\"conv_activation\", \"relu\"))\r\n\r\n        with tf.name_scope(\"vision_net\"):\r\n            for i, (out_size, kernel, stride) in enumerate(filters[:-1], 1):\r\n                inputs = slim.conv2d(\r\n                    inputs,\r\n                    out_size,\r\n                    kernel,\r\n                    stride,\r\n                    activation_fn=activation, \r\n                    scope=\"conv{}\".format(i))\r\n\r\n            out_size, kernel, stride = filters[-1]\r\n           #out_size = filters[-1,0]\r\n            inputs = slim.flatten (inputs, scope=\"embed\")\r\n            fc1 = slim.fully_connected(inputs, out_size, activation_fn=activation, scope=\"fc1\")\r\n            fc2 = slim.fully_connected(fc1, num_outputs, activation_fn=None, normalizer_fn=None,  scope=\"fc2\")\r\n\r\n            return fc2, fc1\r\n\r\n2) From rllib/agents/es/policies.py:\r\nclass GenericPolicy(object):\r\n    def __init__(self, sess, env,env2,action_space, obs_space, preprocessor,\r\n                 observation_filter, model_options, action_noise_std):\r\n        self.sess = sess\r\n        self.action_space = action_space\r\n        self.action_noise_std = action_noise_std\r\n        self.preprocessor = preprocessor\r\n        self.observation_filter = get_filter(observation_filter,\r\n                                             self.preprocessor.shape)\r\n        self.inputs = tf.placeholder(tf.float32,\r\n                                     [None] + list(self.preprocessor.shape))\r\n\r\n        # Policy network.\r\n        dist_class, dist_dim = ModelCatalog.get_action_dist(\r\n            self.action_space, model_options, dist_type=\"deterministic\")\r\n\r\n        self.model = ModelCatalog.get_model({\r\n            \"obs\": self.inputs}, obs_space, dist_dim, model_options)\r\n\r\n        self.dist = dist_class(self.model.outputs)\r\n        self.sampler = self.dist.sample()\r\n        self.entro=self.dist.entropy()\r\n        self.prob=self.dist.softmax()\r\n\r\n        self.variables = ray.experimental.TensorFlowVariables(\r\n            self.model.outputs, self.sess)\r\n\r\n        self.num_params = sum(\r\n            np.prod(variable.shape.as_list())\r\n            for _, variable in self.variables.variables.items())\r\n\r\nHow to modify part 1) or part 2) to extract the feature layer of conv2 or fc1 especially for ES?\r\n\r\nThanks a lot.\r\n",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3660/comments",
    "author": "stellaxu",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-12-29T05:10:54Z",
        "body": "If you use a custom model, then you can return whatever tensor you want as the feature layer and access it via model.last_layer (or define it as an extra attribute on the model)."
      }
    ]
  },
  {
    "number": 3075,
    "title": "Multi-period observation input ",
    "created_at": "2018-10-17T16:25:39Z",
    "closed_at": "2018-10-17T19:55:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ray-project/ray/issues/3075",
    "body": "Hi, \r\n\r\nI was wondering is there any implementation to pass last m observation as input into either of rlLib algorithms? I checked to see if I can write a custom preprocessor function, but in that case I need a custom memory to keep previous observation and handling it with replay memory in off-policy algorithms would be easy. Since, the original DQN paper used such trick, I though probably it is developed and I just need to ask how to use it. \r\nI appreciate your comments.\r\n\r\nThanks,",
    "comments_url": "https://api.github.com/repos/ray-project/ray/issues/3075/comments",
    "author": "oroojlooy",
    "comments": [
      {
        "user": "ericl",
        "created_at": "2018-10-17T16:40:43Z",
        "body": "Hey @oroojlooy, you can do this with a gym environment wrapper (basically a stateful preprocessor). We already do this for Atari envs, and you can find the code for this in atari_wrappers.py and use it to create a custom env of your own."
      },
      {
        "user": "oroojlooy",
        "created_at": "2018-10-17T19:55:31Z",
        "body": "Thanks @ericl, I see that `FrameStack(env,k)` manages this operation. "
      }
    ]
  }
]