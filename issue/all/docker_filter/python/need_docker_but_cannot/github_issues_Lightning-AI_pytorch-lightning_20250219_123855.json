[
  {
    "number": 18589,
    "title": "Can't seem to change distributed backend to gloo on Windows",
    "created_at": "2023-09-19T14:08:12Z",
    "closed_at": "2023-09-19T18:28:28Z",
    "labels": [
      "question",
      "strategy: ddp",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18589",
    "body": "### Bug description\r\n\r\nI am trying to run a training module with CUDA using PyTorch Lightning, but Lightning keeps trying to use NCCL. I have tried every solution I have found online, from specifying it in the code to prepending `PL_TORCH_DISTRIBUTED_BACKEND=gloo` to the laucnh command in the terminal, but Lightning still seems to try to use NCCL. I have verified that gloo is available for use in my system. Any help would be greatly appreciated.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nmaster\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nos.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\r\nmy_data = MyDataModule(args...)\r\nmy_model = MyModel(args...)\r\ntrainer = Trainer()\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\n# also pops up when running PL_TORCH_DISTRIBUTED_BACKEND=gloo python train.py\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\n```\r\n$ PL_TORCH_DISTRIBUTED_BACKEND=gloo python train.py\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\r\n  warnings.warn(\"No audio backend is available.\")\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarnin\r\ng: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the\r\nML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip\r\n install lightning[extra]` or one of them to enable TensorBoard support by default\r\n  warning_cache.warn(\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:72: PossibleUserWarning: `max_epochs` was not set.\r\nSetting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\r\n  rank_zero_warn(\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:69: UserWarning: You passed in a `v\r\nal_dataloader` but have no `validation_step`. Skipping val loop.\r\n  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\r\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\nC:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\env\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\r\n  warnings.warn(\"No audio backend is available.\")\r\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n[W C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\distributed\\c10d\\socket.cpp:601] [c10d] The client socket has failed to connect to [system.intranet.company.ne\r\nt]:52432 (system error: 10049 - The requested address is not valid in its context.).\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntrain.py 62 <module>\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\ntrainer.py 532 fit\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntrain.py 62 <module>\r\ntrainer.fit(my_model, my_data.train_dataloader, my_data.val_dataloader)\r\n\r\ntrainer.py 532 fit\r\ncall._call_and_handle_interrupt(\r\n\r\ncall.py 42 _call_and_handle_interrupt\r\ncall._call_and_handle_interrupt(\r\n\r\ncall.py 42 _call_and_handle_interrupt\r\nreturn trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n\r\nsubprocess_script.py 93 launch\r\nreturn trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\r\n\r\nreturn function(*args, **kwargs)\r\nsubprocess_script.py 93 launch\r\n\r\ntrainer.py 571 _fit_impl\r\nself._run(model, ckpt_path=ckpt_path)\r\n\r\ntrainer.py 938 _run\r\nself.strategy.setup_environment()\r\n\r\nddp.py 143 setup_environment\r\nreturn function(*args, **kwargs)\r\n\r\ntrainer.py 571 _fit_impl\r\nself._run(model, ckpt_path=ckpt_path)\r\n\r\ntrainer.py 938 _run\r\nself.strategy.setup_environment()\r\nself.setup_distributed()\r\n\r\n\r\nddp.py 143 setup_environment\r\nddp.py 191 setup_distributed\r\n_init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\r\n\r\ndistributed.py 258 _init_dist_connection\r\nself.setup_distributed()\r\n\r\ntorch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\nddp.py 191 setup_distributed\r\n\r\ndistributed_c10d.py 907 init_process_group\r\n_init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)\r\n\r\ndistributed.py 258 _init_dist_connection\r\ntorch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\r\n\r\ndistributed_c10d.py 907 init_process_group\r\ndefault_pg = _new_process_group_helper(\r\n\r\ndistributed_c10d.py 1013 _new_process_group_helper\r\nraise RuntimeError(\"Distributed package doesn't have NCCL \" \"built in\")\r\n\r\nRuntimeError:\r\nDistributed package doesn't have NCCL built in\r\ndefault_pg = _new_process_group_helper(\r\n\r\ndistributed_c10d.py 1013 _new_process_group_helper\r\nraise RuntimeError(\"Distributed package doesn't have NCCL \" \"built in\")\r\n\r\nRuntimeError:\r\nDistributed package doesn't have NCCL built in\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA TITAN X (Pascal)\r\n                - NVIDIA GeForce GTX 970\r\n        - available:         True\r\n        - version:           11.8\r\n* Lightning:\r\n        - lightning:         2.0.8\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - pytorch-lightning: 2.0.8\r\n        - pytorchvideo:      0.1.5\r\n        - torch:             2.0.1\r\n        - torchaudio:        2.0.2\r\n        - torchmetrics:      1.1.1\r\n        - torchvision:       0.15.2\r\n* Packages:\r\n        - aiofiles:          22.1.0\r\n        - aiohttp:           3.8.5\r\n        - aiosignal:         1.3.1\r\n        - aiosqlite:         0.18.0\r\n        - annotated-types:   0.5.0\r\n        - ansicon:           1.89.0\r\n        - anyio:             3.5.0\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arrow:             1.2.3\r\n        - asttokens:         2.0.5\r\n        - async-timeout:     4.0.3\r\n        - attrs:             22.1.0\r\n        - av:                10.0.0\r\n        - babel:             2.11.0\r\n        - backcall:          0.2.0\r\n        - backoff:           2.2.1\r\n        - beautifulsoup4:    4.12.2\r\n        - bleach:            4.1.0\r\n        - blessed:           1.20.0\r\n        - boto3:             1.28.42\r\n        - botocore:          1.31.42\r\n        - bottleneck:        1.3.5\r\n        - brotlipy:          0.7.0\r\n        - certifi:           2023.7.22\r\n        - cffi:              1.15.1\r\n        - charset-normalizer: 2.0.4\r\n        - click:             8.1.7\r\n        - colorama:          0.4.6\r\n        - comm:              0.1.2\r\n        - contourpy:         1.0.5\r\n        - croniter:          1.4.1\r\n        - cryptography:      41.0.2\r\n        - cycler:            0.11.0\r\n        - dateutils:         0.6.12\r\n        - debugpy:           1.6.7\r\n        - decorator:         5.1.1\r\n        - deepdiff:          6.4.1\r\n        - deeplake:          3.6.23\r\n        - defusedxml:        0.7.1\r\n        - dill:              0.3.7\r\n        - einops:            0.6.1\r\n        - entrypoints:       0.4\r\n        - executing:         0.8.3\r\n        - fastapi:           0.103.1\r\n        - fastjsonschema:    2.16.2\r\n        - filelock:          3.12.3\r\n        - fonttools:         4.25.0\r\n        - frozenlist:        1.4.0\r\n        - fsspec:            2023.9.0\r\n        - fvcore:            0.1.5.post20221221\r\n        - h11:               0.14.0\r\n        - huggingface-hub:   0.17.1\r\n        - humbug:            0.3.2\r\n        - idna:              3.4\r\n        - inquirer:          3.1.3\r\n        - iopath:            0.1.10\r\n        - ipykernel:         6.25.0\r\n        - ipython:           8.12.2\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        8.0.4\r\n        - itsdangerous:      2.1.2\r\n        - jedi:              0.18.1\r\n        - jinja2:            3.1.2\r\n        - jinxed:            1.2.0\r\n        - jmespath:          1.0.1\r\n        - joblib:            1.2.0\r\n        - json5:             0.9.6\r\n        - jsonschema:        4.17.3\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    7.4.9\r\n        - jupyter-console:   6.6.3\r\n        - jupyter-core:      5.3.0\r\n        - jupyter-events:    0.6.3\r\n        - jupyter-server:    1.23.4\r\n        - jupyter-server-fileid: 0.9.0\r\n        - jupyter-server-ydoc: 0.8.0\r\n        - jupyter-ydoc:      0.2.4\r\n        - jupyterlab:        3.6.3\r\n        - jupyterlab-pygments: 0.1.2\r\n        - jupyterlab-server: 2.22.0\r\n        - jupyterlab-widgets: 3.0.5\r\n        - kiwisolver:        1.4.4\r\n        - lightning:         2.0.8\r\n        - lightning-cloud:   0.5.37\r\n        - lightning-utilities: 0.9.0\r\n        - llvmlite:          0.40.0\r\n        - lxml:              4.9.2\r\n        - markdown-it-py:    3.0.0\r\n        - markupsafe:        2.1.1\r\n        - matplotlib:        3.7.2\r\n        - matplotlib-inline: 0.1.6\r\n        - mdurl:             0.1.2\r\n        - mistune:           0.8.4\r\n        - mkl-fft:           1.3.6\r\n        - mkl-random:        1.2.2\r\n        - mkl-service:       2.4.0\r\n        - mpmath:            1.3.0\r\n        - multidict:         6.0.4\r\n        - multiprocess:      0.70.15\r\n        - munkres:           1.1.4\r\n        - nbclassic:         0.5.5\r\n        - nbclient:          0.5.13\r\n        - nbconvert:         6.5.4\r\n        - nbformat:          5.7.0\r\n        - nest-asyncio:      1.5.6\r\n        - networkx:          3.1\r\n        - notebook:          6.5.4\r\n        - notebook-shim:     0.2.2\r\n        - numba:             0.57.0\r\n        - numcodecs:         0.11.0\r\n        - numexpr:           2.8.4\r\n        - numpy:             1.24.3\r\n        - ordered-set:       4.1.0\r\n        - packaging:         23.1\r\n        - pandas:            2.0.3\r\n        - pandocfilters:     1.5.0\r\n        - parameterized:     0.9.0\r\n        - parso:             0.8.3\r\n        - pathos:            0.3.1\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.4.0\r\n        - pip:               23.2.1\r\n        - platformdirs:      3.10.0\r\n        - ply:               3.11\r\n        - portalocker:       2.7.0\r\n        - pox:               0.3.3\r\n        - ppft:              1.7.6.7\r\n        - pretty-errors:     1.2.25\r\n        - prometheus-client: 0.14.1\r\n        - prompt-toolkit:    3.0.36\r\n        - psutil:            5.9.0\r\n        - pure-eval:         0.2.2\r\n        - pycparser:         2.21\r\n        - pydantic:          2.1.1\r\n        - pydantic-core:     2.4.0\r\n        - pygments:          2.15.1\r\n        - pyjwt:             2.8.0\r\n        - pyopenssl:         23.2.0\r\n        - pyparsing:         3.0.9\r\n        - pyqt5:             5.15.7\r\n        - pyqt5-sip:         12.11.0\r\n        - pyrsistent:        0.18.0\r\n        - pysocks:           1.7.1\r\n        - python-dateutil:   2.8.2\r\n        - python-editor:     1.0.4\r\n        - python-json-logger: 2.0.7\r\n        - python-multipart:  0.0.6\r\n        - pytorch-lightning: 2.0.8\r\n        - pytorchvideo:      0.1.5\r\n        - pytz:              2022.7\r\n        - pywin32:           305.1\r\n        - pywinpty:          2.0.10\r\n        - pyyaml:            6.0\r\n        - pyzmq:             23.2.0\r\n        - qtconsole:         5.4.2\r\n        - qtpy:              2.2.0\r\n        - readchar:          4.0.5\r\n        - regex:             2023.8.8\r\n        - requests:          2.31.0\r\n        - rfc3339-validator: 0.1.4\r\n        - rfc3986-validator: 0.1.1\r\n        - rich:              13.5.2\r\n        - s3transfer:        0.6.2\r\n        - safetensors:       0.3.3\r\n        - scikit-learn:      1.2.2\r\n        - scipy:             1.11.1\r\n        - send2trash:        1.8.0\r\n        - setuptools:        68.0.0\r\n        - sip:               6.6.2\r\n        - six:               1.16.0\r\n        - sniffio:           1.2.0\r\n        - soupsieve:         2.4\r\n        - stack-data:        0.2.0\r\n        - starlette:         0.27.0\r\n        - starsessions:      1.3.0\r\n        - sympy:             1.11.1\r\n        - tabulate:          0.9.0\r\n        - termcolor:         2.3.0\r\n        - terminado:         0.17.1\r\n        - threadpoolctl:     2.2.0\r\n        - tinycss2:          1.2.1\r\n        - tokenizers:        0.13.3\r\n        - toml:              0.10.2\r\n        - torch:             2.0.1\r\n        - torchaudio:        2.0.2\r\n        - torchmetrics:      1.1.1\r\n        - torchvision:       0.15.2\r\n        - tornado:           6.3.2\r\n        - tqdm:              4.65.0\r\n        - traitlets:         5.7.1\r\n        - transformers:      4.33.1\r\n        - typing-extensions: 4.7.1\r\n        - tzdata:            2023.3\r\n        - urllib3:           1.26.16\r\n        - uvicorn:           0.23.2\r\n        - wcwidth:           0.2.5\r\n        - webencodings:      0.5.1\r\n        - websocket-client:  0.58.0\r\n        - websockets:        11.0.3\r\n        - wheel:             0.38.4\r\n        - widgetsnbextension: 4.0.5\r\n        - win-inet-pton:     1.1.0\r\n        - y-py:              0.5.9\r\n        - yacs:              0.1.8\r\n        - yarl:              1.9.2\r\n        - ypy-websocket:     0.8.2\r\n* System:\r\n        - OS:                Windows\r\n        - architecture:\r\n                - 64bit\r\n                - WindowsPE\r\n        - processor:         Intel64 Family 6 Model 85 Stepping 4, GenuineIntel\r\n        - python:            3.11.4\r\n        - release:           10\r\n        - version:           10.0.19041\r\n\r\n</details>\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18589/comments",
    "author": "amansingh427",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-09-19T17:27:00Z",
        "body": "Hey @amansingh427 \r\nIn the latest Lightning versions, the backend can no longer be set through the environment variable `PL_TORCH_DISTRIBUTED_BACKEND`. You can set it like so:\r\n\r\n```py\r\nfrom lightning.pytorch.strategies import DDPStrategy\r\n\r\ntrainer = Trainer(strategy=DDPStrategy(process_group_backend=\"gloo\"), ...)\r\n```"
      },
      {
        "user": "amansingh427",
        "created_at": "2023-09-19T18:28:29Z",
        "body": "Fixed. Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of the correct method to configure distributed backend in modern PyTorch Lightning versions",
      "Demonstration of backend configuration through Strategy API rather than environment variables",
      "Clarification on Windows compatibility requirements for distributed training backends",
      "Verification that the backend configuration prevents NCCL fallback attempts"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-04 23:58:37"
    }
  },
  {
    "number": 5897,
    "title": "Auto_scale_batch_size fails for to bigger batch sizes, cuDNN failure",
    "created_at": "2021-02-10T08:51:55Z",
    "closed_at": "2021-02-15T15:44:41Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5897",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI'm using a pre-trained ResNet50 on 224x224 images with 16-bit precision, I wanted to test the auto_scale_batch_size functionality.\r\n\r\nThe output in the terminal is the following:\r\nGPU available: True, used: True\r\nTPU available: None, using: 0 TPU cores\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\nUsing native 16bit precision.\r\nBatch size 2 succeeded, trying batch size 4\r\nBatch size 4 succeeded, trying batch size 8\r\nBatch size 8 succeeded, trying batch size 16\r\nBatch size 16 succeeded, trying batch size 32\r\nBatch size 32 succeeded, trying batch size 64\r\nBatch size 64 succeeded, trying batch size 128\r\nBatch size 128 succeeded, trying batch size 256\r\n\r\nAll good till then. On batch size 256 the GPU's memory certainly is not sufficient and it fails on a 2d convolution.\r\nThe auto_scaling should be aborted at this point and the batch_size fixed to 128.\r\nInstead the script fails with the message \"RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\".\r\n\r\n### To Reproduce\r\nI'm not doing anything exceptional:\r\n1. Parsing arguments with \"--auto_scale_batch_size\", \"true\"\r\n2. Initiating model, datamodule and trainer using the parsed arguments\r\n3. trainer.tune(model, dm)\r\n\r\n### Question\r\nIs this a problem on the Pytorch Lightning side not capturing the exception or is this anyhow linked to Cuda and installing a different version could be enough?\r\n\r\n\r\n### Environment\r\n1x GeForce RTX 2080Ti\r\nUbuntu 20.10\r\nPython 3.8.5\r\nPytorch Lightning 1.1.8\r\nPytorch 1.7.1\r\nCuda 11.2\r\nEnv created with miniconda, packages installed with pip\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5897/comments",
    "author": "FlorianMF",
    "comments": [
      {
        "user": "justusschock",
        "created_at": "2021-02-10T08:56:45Z",
        "body": "Hi @FlorianMF,\r\n\r\nHave you tried to run that network with batchsize 256 without the auto_scaling? Because we don't do anything special on the convolutional side and this seems to be a bug in PyTorch.\r\n\r\nAlso could you try with `torch.backends.cudnn.benchmark = True` and/or `torch.backends.cudnn.enabled = False`? "
      },
      {
        "user": "FlorianMF",
        "created_at": "2021-02-10T09:22:45Z",
        "body": "Hi @justusschock, \r\nI get the same error without auto_scaling for a batch_size=256, as well when adding 'torch.backends.cudnn.benchmark = True' at the top of the script.\r\nWhen adding \"torch.backends.cudnn.enabled = False\" the error message \"RuntimeError: CUDA out of memory\" confirms my assumption.\r\n\r\nSo, am I 'forced' to use 'auto_scale_batch_size' once to get the maximum batch size and then rerun the script without the flag? I think your goal proposing this functionality is that this is not needed though.\r\n"
      },
      {
        "user": "justusschock",
        "created_at": "2021-02-10T10:03:05Z",
        "body": "No, usually you don't have to rerun it. But with torch.backends.cudnn.enabled = False, PyTorch will use some other algorithm for convolutions, which may be more memory demanding. So for some reason there seems to be a bug within PyTorch for the cudnn convolution. Our tuner only listens to the `Runtime: CUDA out of memory`-Error as everything else could also be a user-error.\r\n\r\nUnfortunately there is nothing else we can do on that"
      },
      {
        "user": "FlorianMF",
        "created_at": "2021-02-10T10:24:26Z",
        "body": "You're right. The error is captured when 'torch.backends.cudnn.enabled = False' is added at the top and the script continues."
      }
    ],
    "satisfaction_conditions": [
      "Handles non-CUDA-memory-related errors during batch size scaling",
      "Provides clear error differentiation between memory limits and framework-specific computation errors",
      "Ensures auto-scaling gracefully reverts to last valid batch size for any unrecoverable error",
      "Documents limitations of error detection in batch scaling mechanism"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 00:09:11"
    }
  },
  {
    "number": 3169,
    "title": "RuntimeError: Cannot replicate if number of devices (1) is different from 8 Exception in device=TPU:2: Cannot replicate if number of devices (1) is different from 8",
    "created_at": "2020-08-25T17:49:27Z",
    "closed_at": "2020-08-26T04:48:31Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3169",
    "body": "I am getting the above (title) error on Kaggle when I am doing \r\n\r\n`trainer.fit(model, dm)`\r\n\r\nwhere `trainer = Trainer(tpu_cores = 8, logger = wandblogger, max_epochs = 20)`\r\n\r\nFollowing is the trace:\r\n\r\n>   File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\r\n    _start_fn(index, pf_cfg, fn, args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\r\n    _setup_replication()\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\r\n    _start_fn(index, pf_cfg, fn, args)\r\nException in device=TPU:2: Cannot replicate if number of devices (1) is different from 8\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/core/xla_model.py\", line 287, in xla_replication_devices\r\n    format(len(local_devices), len(kind_devices)))\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\r\n    xm.set_replication(device, [device])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/core/xla_model.py\", line 315, in set_replication\r\n    replication_devices = xla_replication_devices(devices)\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\r\n    _setup_replication()\r\n\r\n\r\n\r\nI am using Kaggle Kernel. Here is my LightningDataModule.\r\n\r\n```python\r\nclass ProteinModule(pl.LightningDataModule):\r\n  def __init__(self, config, path, invalid_proteins):\r\n    super().__init__()\r\n\r\n    self.path = path\r\n    self.bs = config['bs']\r\n    self.state = config['seed']\r\n    self.invalid = invalid_proteins\r\n\r\n  def setup(self, stage=None):\r\n\r\n    all_protein_ids = os.listdir(self.path)\r\n    all_protein_ids = [protein for protein in all_protein_ids if protein not in self.invalid]\r\n    train_ids, val_ids = train_test_split(all_protein_ids, test_size = 0.2, random_state = self.state)\r\n\r\n    self.traindataset = ProteinDataset(path, train_ids)\r\n    self.valdataset = ProteinDataset(path, val_ids)\r\n\r\n\r\n  def train_dataloader(self):\r\n\r\n    return DataLoader(self.traindataset, batch_size = self.bs, shuffle = True)\r\n\r\n  def val_dataloader(self):\r\n\r\n    return DataLoader(self.valdataset, batch_size = self.bs, shuffle = False)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3169/comments",
    "author": "VirajBagal",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-08-25T18:24:46Z",
        "body": "Did you call `xm.xla_device()` somewhere? Or maybe try restarting the kernel and run again."
      },
      {
        "user": "VirajBagal",
        "created_at": "2020-08-26T04:47:39Z",
        "body": "Thanks, @rohitgr7, I was using `xm.xla_device()` in one place. This error is resolved but I have got another error now. I'll open another issue for it. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of TPU device initialization requirements in PyTorch Lightning",
      "Diagnosis of device count mismatch between configuration and available resources",
      "Guidance on proper TPU core configuration for distributed training"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 00:11:13"
    }
  },
  {
    "number": 1221,
    "title": "Colab weird behaviour and error when passing values from collate_fn to validation_step",
    "created_at": "2020-03-24T08:37:03Z",
    "closed_at": "2020-04-04T14:33:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1221",
    "body": "The same code that runs perfectly on my local machine (MacOS with Python3.7 and tested on an Ubuntu docker 18.04 with Python3.6.9) fails to run on colab.\r\n\r\nIt performs weirdly, calling the Dataset init() multiple times. However, it fails when the collate function that (correctly) returns (printed out the results) the following:\r\n```\r\nreturn x_tensor, x_lengths, y_tensor\r\n```\r\nget passed in the sanity check right at start in the ``validation_step``:\r\n```\r\ndef validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\r\n        x_tensor, x_lengths, y_tensor = batch <-- here it fails\r\n        .. forward(x_tensor,x _lengths) .. etc.\r\n```\r\nwith the error:\r\n```\r\nline 234, in validation_step\r\n    x_tensor, x_lengths, y_tensor = batch\r\nValueError: too many values to unpack (expected 3)\r\n```\r\nit actually just return the first value (x_tensor). I tried packing them in a dict from the collate function, but in the validation step ``batch`` comes to me just as the first key (as string!) from the dict created in collate_fn.\r\n\r\nThe environment is the same except the Python version (latest torch and lightning versions) across local runtime, docker ubuntu and colab. To ensure the same code is running on all machines, I'm doing just git clone, pip3 install -r requirements and python3 train.py. Am I missing something with Colab that's just not working? I can provide full code if needed.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1221/comments",
    "author": "dumitrescustefan",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-24T08:37:46Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T12:46:55Z",
        "body": "Try running master?"
      },
      {
        "user": "dumitrescustefan",
        "created_at": "2020-04-04T14:33:24Z",
        "body": "@williamFalcon \r\n\r\nConfirm that running on colab with version ``pytorch-lightning==0.7.2.dev0`` (from master directly) fixes the problem above. \r\n\r\nThanks a lot!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the root cause of version incompatibility between local environment and Colab",
      "Explains framework-specific handling of data flow between collate_fn and validation_step",
      "Addresses environment parity between local development and Colab runtime"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 00:13:54"
    }
  }
]