[
  {
    "number": 18170,
    "title": "Basic ProgressBar does not work",
    "created_at": "2023-07-26T21:40:27Z",
    "closed_at": "2023-07-27T11:20:36Z",
    "labels": [
      "question",
      "progress bar: tqdm",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18170",
    "body": "### Bug description\r\n\r\nVersion of `pytorch_lightning==2.0.6`, `tqdm==4.65.0`\r\nI want to display the training progress of my models and the basic ProgressBar from pytorch_lightning.callbacks does not work (nothing shows up).\r\nHowever, when I switched to RichProgressBar, the rich progress bar shows up.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\r\n### How to reproduce the bug\r\n\r\n`python\r\npytorch_lightning.callbacks import ProgressBar` does not show up anything.\r\n\r\n`from pytorch_lightning.callbacks import RichProgressBar` can show the training progress.\r\n\r\n\r\n### Error messages and logs\r\n\r\n\r\nNothing shows up for ```ProgressBar```.\r\n\r\n### Environment\r\n`pytorch_lightning==2.0.6`\r\n`tqdm==4.65.0`\r\n\r\n### More info\r\nRunning things in a Linux environment, with an A40 GPU.\n\ncc @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18170/comments",
    "author": "dnaihao",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-26T22:22:25Z",
        "body": "Hi @dnaihao \r\n\r\nYou don't need to import the progress bar if you want to use tqdm. It gets enabled by default if you just run the Trainer :)\r\nThe reason why you don't see anything is because you accidentally imported the base class, but you probably wanted to import `pytorch_lightning.callbacks import TQDMProgressBar`. (But again, it is the default, so it's not necessary technically). \r\n\r\nLet me know if that resolves your problem :)"
      },
      {
        "user": "dnaihao",
        "created_at": "2023-07-27T01:30:52Z",
        "body": "Thanks for the explanation. \r\nBut for the previous versions of Pytorch Lightening (probably around 1.6.X), I imported ProgressBar and it worked out perfectly fine.\r\nBut sure, I am using the TODMProgressBar now and it works fine."
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-27T11:20:36Z",
        "body": "Yes, you are right, in previous versions prior to 2.0, ProgressBar was the tqdm-version. Later, the name of the base class was changed from ProgressBarBase to just ProgressBar #17058. Apologies if this caused confusion!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of version-specific class name changes in PyTorch Lightning progress bars",
      "Identification of the correct default progress bar class in v2.0+",
      "Clarification that explicit progress bar imports are unnecessary for default behavior"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-04 23:58:52"
    }
  },
  {
    "number": 18135,
    "title": "_PrefetchDataFetcher ignores prefetch_factor",
    "created_at": "2023-07-21T21:34:48Z",
    "closed_at": "2023-07-24T15:11:19Z",
    "labels": [
      "question",
      "data handling",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18135",
    "body": "### Bug description\r\n\r\nNo matter what prefetch_factor is set for the `DataLoader` in a `LightningDataModule` wrapper, when the `_PrefetchDataFetcher` is initialized, the value is always reset to 1.\r\n\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\n\ncc @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18135/comments",
    "author": "botcs",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-07-23T18:56:39Z",
        "body": "@botcs The prefetching done in the trainer is independent of the prefetching in the DataLoader. The trainer prefetches 1 batch just to know in advance whether the data loader is exhausted or not, that's all. But of course, you can set any value for `DataLoader(prefetch_factor=N)` and this will be handled by PyTorch. Let me know if you have any questions."
      },
      {
        "user": "botcs",
        "created_at": "2023-07-24T15:11:19Z",
        "body": "A fair! thanks for the explanation :)"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-07-24T15:26:24Z",
        "body": "You are welcome of course, happy to help :)"
      }
    ],
    "satisfaction_conditions": [
      "Clarification of the relationship between trainer prefetching and DataLoader prefetching",
      "Confirmation that DataLoader prefetch_factor settings remain functional",
      "Explanation of different prefetching purposes in the system architecture"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-04 23:59:03"
    }
  },
  {
    "number": 16654,
    "title": "dependencies issue working with torch and pytorch_lightning. ",
    "created_at": "2023-02-06T17:16:02Z",
    "closed_at": "2023-02-06T21:09:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16654",
    "body": "### Bug description\n\nI have trouble getting all needed packages to be compatible to run ResNet transfer learning. I trying the import from one of the sample lightening notebooks anf got error. And then trying to install the recommended packages got following incompatibility issue.       \r\ncommand: \r\n`! pip install --quiet \"torchmetrics>=0.7, <0.12\" \"seaborn\" \"ipython[notebook]>=8.0.0, <8.9.0\" \"pytorch-lightning>=1.4, <1.9\" \"torchmetrics >=0.11.0\" \"setuptools==65.6.3\" \"pandas\" \"torchvision\" \"torch>=1.8.1, <1.14.0\"\r\n`\r\nIssue: \r\n```\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nthinc 8.0.1 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.10.4 which is incompatible.\r\nspacy 3.0.1 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.10.4 which is incompatible.\r\nlightning 1.9.0.dev0 requires lightning-utilities<1.0,>=0.4.2, but you have lightning-utilities 0.3.0 which is incompatible.\r\nconda-repo-cli 1.0.27 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\r\nconda-repo-cli 1.0.27 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\r\n```\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16654/comments",
    "author": "nkay28",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-02-06T20:40:16Z",
        "body": "@nkay28 You have torchmetrics specified twice, once with torchmetrics>=0.7, <0.12 and once with torchmetrics >=0.11.0. Also, I suggest to remove setuptools==65.6.3 which could limit the pip dep resolver. "
      },
      {
        "user": "nkay28",
        "created_at": "2023-02-06T21:09:20Z",
        "body": "Thank you very much. Resolved. "
      }
    ],
    "satisfaction_conditions": [
      "Resolve dependency conflicts between specified packages",
      "Ensure compatible version ranges for PyTorch Lightning ecosystem packages",
      "Eliminate duplicate/conflicting package specifications",
      "Avoid unnecessary version pinning that limits dependency resolution"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-04 23:59:30"
    }
  },
  {
    "number": 16102,
    "title": "Torch sees GPU but does not use it",
    "created_at": "2022-12-17T17:36:44Z",
    "closed_at": "2022-12-18T20:54:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16102",
    "body": "### Bug description\n\nWhen I use ``torch.cuda.device_count()``\r\nit returns 1, which is correct\r\nBut then when using Lightning, it shows this in terminal\r\n``LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n``\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 1.10):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16102/comments",
    "author": "ThatGuyCalledJesse",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-12-18T20:47:01Z",
        "body": "@ThatGuyCalledJesse That's correct. If you have one GPU, then Lightning can only use one and that's device with index 0. \r\nIf you had multiple GPUs, it would show:\r\n\r\n`LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0, 1]`\r\n\r\netc.\r\n"
      },
      {
        "user": "ThatGuyCalledJesse",
        "created_at": "2022-12-18T20:54:22Z",
        "body": "I am so sorry, it's a very beginner mistake"
      },
      {
        "user": "yotamcons",
        "created_at": "2023-11-20T11:09:29Z",
        "body": "I made the same mistake, thank you for asking.\r\nWhat bothered me was the \"local rank\" which i couldn't figure out. This calls for better logging, especially in the uncomfortable new filed of gpu computations."
      },
      {
        "user": "ejkitchen",
        "created_at": "2024-06-20T19:09:28Z",
        "body": "Please keep this thread up because I made the exact same assumption as the original poster. I know CUDA devices start at 0 but I was still thrown off by this message."
      }
    ],
    "satisfaction_conditions": [
      "Clarification that [0] in CUDA_VISIBLE_DEVICES indicates normal single-GPU usage",
      "Explanation of LOCAL_RANK and CUDA_VISIBLE_DEVICES semantics in distributed training contexts",
      "Reassurance about correct GPU detection and utilization",
      "Acknowledgement of common confusion points in GPU logging outputs"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-04 23:59:43"
    }
  },
  {
    "number": 7775,
    "title": "training_epoch_end called before all steps of epoch were completed. always at about 0.25 size of steps.",
    "created_at": "2021-05-31T07:10:17Z",
    "closed_at": "2021-06-01T09:40:40Z",
    "labels": [
      "help wanted",
      "question",
      "working as intended"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7775",
    "body": "## \ud83d\udc1b Bug\r\n\r\n```bash\r\nGPU available: False, used: False\r\nTPU available: None, using: 0 TPU cores\r\nValidation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\r\n  | Name                | Type                 | Params\r\n-------------------------------------------------------------\r\n\r\n-------------------------------------------------------------\r\n\r\nEpoch 0:   0%|          | 0/13 [00:00<?, ?it/s] \r\nEpoch 0:  23%|\u2588\u2588\u258e       | 3/13 [01:38<05:27, 32.75s/it, loss=4.73, v_num=7]\r\n// training_epoch_end:  outputs = [{'loss': tensor(6.4593)}, {'loss': tensor(5.7653)}, {'loss': tensor(1.9642)}]\r\n\r\nValidating: 0it [00:00, ?it/s]\r\nValidating:   0%|          | 0/10 [00:00<?, ?it/s]\r\nEpoch 0:  38%|\u2588\u2588\u2588\u258a      | 5/13 [01:48<02:54, 21.78s/it, loss=4.73, v_num=7]\r\nEpoch 0:  46%|\u2588\u2588\u2588\u2588\u258c     | 6/13 [01:59<02:19, 19.91s/it, loss=4.73, v_num=7]\r\nEpoch 0:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 7/13 [02:10<01:51, 18.58s/it, loss=4.73, v_num=7]\r\nEpoch 0:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 8/13 [02:20<01:27, 17.60s/it, loss=4.73, v_num=7]\r\nEpoch 0:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 9/13 [02:31<01:07, 16.83s/it, loss=4.73, v_num=7]\r\nEpoch 0:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 10/13 [02:42<00:48, 16.21s/it, loss=4.73, v_num=7]\r\nEpoch 0:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 11/13 [02:52<00:31, 15.71s/it, loss=4.73, v_num=7]\r\nEpoch 0:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 12/13 [03:04<00:15, 15.34s/it, loss=4.73, v_num=7]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:15<00:00, 15.00s/it, loss=4.73, v_num=7]\r\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:16<00:00, 15.15s/it, loss=4.73, v_num=7]\r\nEpoch 1:  23%|\u2588\u2588\u258e       | 3/13 [01:42<05:42, 34.24s/it, loss=3.39, v_num=7]\r\n// training_epoch_end:  outputs = [{'loss': tensor(2.6766)}, {'loss': tensor(2.3010)}, {'loss': tensor(1.1722)}]\r\nEpoch 1:  31%|\u2588\u2588\u2588       | 4/13 [01:48<04:04, 27.22s/it, loss=3.39, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 1:  38%|\u2588\u2588\u2588\u258a      | 5/13 [02:02<03:15, 24.42s/it, loss=3.39, v_num=7]\r\nCompleted 6.8 MiB/327.9 MiB (48.7 KiB/s) with 2 file(s) remaining\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nEpoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:48<00:00, 17.54s/it, loss=3.39, v_num=7]\r\nEpoch 2:  23%|\u2588\u2588\u258e       | 3/13 [01:44<05:47, 34.72s/it, loss=2.72, v_num=7]\r\nNUM EL TRAINING: 3   [{'loss': tensor(1.2504)}, {'loss': tensor(1.4905)}, {'loss': tensor(1.4158)}]\r\nEpoch 2:  31%|\u2588\u2588\u2588       | 4/13 [01:49<04:07, 27.48s/it, loss=2.72, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\nEpoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [03:50<00:00, 17.75s/it, loss=2.72, v_num=7]\r\nEpoch 3:  23%|\u2588\u2588\u258e       | 3/13 [01:43<05:46, 34.62s/it, loss=2.27, v_num=7]\r\n//training_epoch_end:   outputs = [{'loss': tensor(0.6632)}, {'loss': tensor(0.9215)}, {'loss': tensor(1.1396)}]\r\nEpoch 3:  31%|\u2588\u2588\u2588       | 4/13 [01:49<04:06, 27.41s/it, loss=2.27, v_num=7]\r\nValidating: 0it [00:00, ?it/s]\r\n```\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): mac Catalina (this happens on all environments , linux etc)\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration: happens also with 0 gpus.\r\n - Any other relevant information:\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7775/comments",
    "author": "ganitps",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T11:47:30Z",
        "body": "As of now, `training_epoch_end` runs before validation starts (validation runs outside the training epoch). The progress bar just shows the combined training + validation steps. So this is fine and you get all the steps for the training epoch.\r\n\r\nAfter #7357, `training_epoch_end` will run after the last validation loop each epoch. \r\nHope this clears it up :) "
      },
      {
        "user": "ganitps",
        "created_at": "2021-05-31T12:44:23Z",
        "body": "Thanks @awaelchli  for your reply\r\nThis is not what I understood form the documentation:\r\n\r\n// the pseudocode for these calls\r\n```\r\ntrain_outs = []\r\nfor train_batch in train_data:\r\n        out = training_step(train_batch)\r\n        train_outs.append(out)\r\ntraining_epoch_end(train_outs)\r\n\r\n```\r\n\r\nSo If I want to take actions when epoch ends I must do it in the last training step? (training_step_end())\r\nCan I take the latest version After #7357?"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T13:53:00Z",
        "body": "No this is still all good. The pseudo code is correct, all outputs from training_step get passed into training_epoch_end. You get as many outputs as training steps. In your case, there seems to be three training steps, so you get 3 outputs, correct? And that's when the training epoch ends and the validation starts. "
      },
      {
        "user": "ganitps",
        "created_at": "2021-05-31T14:29:49Z",
        "body": "No... I have 13 steps.\r\ntraining_epoch_end called after the third one... with 3 outputs..."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-31T14:38:36Z",
        "body": "> No... I have 13 steps.\r\n\r\nHow do you know?\r\nHave you printed `len(dataloader)`? Have you set `limit_train_batches=13`?\r\nFrom the output you shared it just looks like you have 3 training steps and 10 validation steps. \r\n\r\nMaybe I can help better if you try to explain what you want to achieve. "
      },
      {
        "user": "ganitps",
        "created_at": "2021-06-01T09:40:32Z",
        "body": "Dear @awaelchli  \r\nThank you so much for sticking with me, your last answer helped me understand that the epoch printouts doesn't reflect epoch steps but also validation. and every thing really works as designed :) "
      }
    ],
    "satisfaction_conditions": [
      "Clarify the execution order between training steps, training_epoch_end, and validation phases",
      "Explain how progress bar indicators relate to actual training step completion",
      "Confirm proper collection of all training step outputs for epoch-level processing",
      "Address version-specific behavior differences in epoch lifecycle management"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:01:13"
    }
  },
  {
    "number": 4646,
    "title": "Loading samples to RAM with DDP.",
    "created_at": "2020-11-12T21:06:02Z",
    "closed_at": "2020-11-13T10:41:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4646",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\n\r\nI'm facing an IO bottleneck that can be fixed with a custom `torch.utils.data.Dataset` that loads each sample to RAM. Then training goes fast as I don't need to read my samples (images) from disk (slow). Everything works well with when I'm using 1 GPU, but I'm a bit lost when I switch to multiple GPUs with DDP.\r\n\r\nDDP divides the samples to each GPU and I'm wondering when/where I should load my samples to RAM so that each process only loads its own partition of the samples?\r\n\r\n#### Code\r\n\r\n```\r\nclass RAMDataset(data.Dataset):\r\n    def __init__(self, paths,labels,transform):\r\n        \"\"\"Dataset that loads all samples to RAM.\"\"\"\r\n        self.paths = paths\r\n        self.labels = labels\r\n        self.transform = transform\r\n\r\n    def __len__(self):\r\n        return len(self.samples)\r\n\r\n    def load_to_RAM(self):\r\n        self.images = []\r\n        for path in self.paths:\r\n            with open(path, \"rb\") as f:\r\n                str_encode = f.read()\r\n                nparr = np.frombuffer(str_encode, np.uint8)\r\n                self.images.append(cv2.imdecode(nparr, cv2.IMREAD_COLOR))\r\n\r\n    def __getitem__(self, index):\r\n        # Run self.load_to_RAM() first!\r\n        image = self.transform(self.images[index])\r\n        label = self.labels[index]\r\n        return image, label\r\n```\r\n\r\n#### What have you tried?\r\n\r\nWith 1 GPU `self.load_to_RAM()` can be excecuted as soon as the Dataset has been created.\r\n\r\n```\r\ndataset = RAMDataset(paths,labels)\r\ndataset.load_to_RAM()\r\nloader = DataLoader(dataset,...)\r\ntrainer.fit(model,loader)\r\n```\r\n\r\nBut obviously this would load the samples `num_gpus` times to the RAM of the node.\r\n\r\nI quickly tried to call `self.train_dataloader.dataset.load_to_RAM()`on the hook `setup()` but got the following error...\r\n```\r\nAttributeError: '_PatchDataLoader' object has no attribute 'dataset'\r\n```\r\n..and I'm 99% this solution would also load all of the samples to RAM.\r\n\r\n#### Possible solution?\r\n\r\n1. Find out which process (which GPU and which node) is currently running.\r\n2. Get the allocated slice of the samples for this process.\r\n3. Load only that slice of the `self.paths` to RAM.\r\n\r\nTried to go through the source code but couldn't find out how I could implement this.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4646/comments",
    "author": "jopo666",
    "comments": [
      {
        "user": "s-rog",
        "created_at": "2020-11-13T00:27:32Z",
        "body": "The OS should automatically cache the dataset after the first epoch (assuming you have enough ram) so you shouldn't need to do this.\r\n\r\nI'm actually not sure if distributed sampler shards the same data/item on the same GPU through different epochs... but that's more of a pytorch question than a lightning one."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T07:55:29Z",
        "body": "I do have enough RAM if I manage to divide my data to multiple nodes. If the `BasicDataset()` below is cached after the first epoch wouldn't that mean only the labels and paths are cached? What I would want is that the loaded images are cached. I'll look into PyTorch and see if the same data/items are kept for each epoch.\r\n\r\n```\r\nclass BasicDataset(data.Dataset):\r\n    def __init__(self, paths,labels,transform):\r\n        self.paths = paths\r\n        self.labels = labels\r\n\r\n    def __getitem__(self, index):\r\n        image = load(self.paths[index]) # This is my bottleneck.\r\n        label = self.labels[index]\r\n        return image, label\r\n```"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-13T08:01:31Z",
        "body": "Ah you're doing cross node DDP, I was referring to single node above... I'm afraid I don't have much experience with multi node DDP.\r\n\r\nBut if each node gets the same subset of data every epoch, the OS should cache the files all the same (without needing `load_to_RAM`) after the first epoch since you have enough RAM."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T08:06:21Z",
        "body": "Would the actual loaded images be cached even though they are loaded inside `__getitem__()` and not just the whole `Dataset` object with only the `paths` to these files?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-11-13T08:29:08Z",
        "body": "Yes, this is not stored in your program/script memory but instead it's simply file caching at the system level.\r\n\r\nI use Ubuntu but this should be the same for other linux ditros unless some memory/caching settings have been changed/limited. You can look at the cache part of your ram usage in htop as your data is loaded into memory during the first epoch."
      },
      {
        "user": "jopo666",
        "created_at": "2020-11-13T10:41:38Z",
        "body": "Perfect! Kept running tests that ran only for one epoch so I never noticed the seed-up on the second epoch... \ud83d\ude04 Well you learn something new everyday."
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to avoid loading full dataset multiple times across nodes/GPUs",
      "Validation that solution works with PyTorch's distributed data parallelism",
      "Confirmation of proper data access patterns after loading"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:03:00"
    }
  },
  {
    "number": 557,
    "title": "The gan template does not seem to set properly the gradients of the discriminator to zero",
    "created_at": "2019-11-28T14:16:20Z",
    "closed_at": "2020-01-22T13:15:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/557",
    "body": "In each iteration at the beginning of the discriminator training, the gradient is not set to zero. To investigate, just print the gradients of the discriminator after the line `if optimizer_i == 1:`.\r\nThe `optimizer.zero_grad()` for discriminator is then only called once all the gradients are accumulated, including the ones coming from the update of the generator.\r\n\r\nBy the way, it may be helpful to indicate somewhere that the script is performing simultaneous gradient descent instead of alternating updates.\r\n\r\nI am wondering if someone succeeds in training a GAN with this script because this does not seem to be possible to me. Thanks in advance for the clarification.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/557/comments",
    "author": "cyber-meow",
    "comments": [
      {
        "user": "kwanUm",
        "created_at": "2020-01-02T12:13:29Z",
        "body": "@cyber-meow  bumped into this as well. Does the discussion at #591 solves your issue?"
      },
      {
        "user": "cyber-meow",
        "created_at": "2020-01-22T13:15:32Z",
        "body": "Effectively this issue was addressed in #603 and some recent attempts have also been carried out to improve the solution (#712). Thank you for mentioning.  My own solution is pretty much similar to the modification of #603.\r\nAlthough I feel it is worth mentioning the templates is for simultaneous update, maybe it is trivial for most of the people working on this."
      }
    ],
    "satisfaction_conditions": [
      "Clarification of gradient reset timing for discriminator updates",
      "Explanation of simultaneous vs alternating update strategy",
      "Validation of successful GAN training viability"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:06:54"
    }
  },
  {
    "number": 8678,
    "title": "multigpu ddp: Code after fit executed many times",
    "created_at": "2021-08-02T13:28:33Z",
    "closed_at": "2021-08-03T08:37:52Z",
    "labels": [
      "bug",
      "help wanted",
      "distributed",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8678",
    "body": "## \ud83d\udc1b Bug\r\n\r\nAfter training model with the Trainer.fit on 4-gpu machine with the accelerator=\"ddp\", my code which goes after that executed 3 (?) times. \r\nI receive 2 exceptions \"FileNotFoundError\" and then printing of successful weights saving.\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n```py\r\n....\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    precision=16 if train_opt.get(\"fp16\", False) else 32,\r\n    accelerator=\"ddp\",\r\n    accumulate_grad_batches=train_opt.get(\"grad_accum\", 1),\r\n    max_epochs=train_opt.get(\"epochs\", 20),\r\n    default_root_dir=train_opt.get(\"root_dir\", None),\r\n    callbacks=callbacks,\r\n    logger=logger,\r\n    log_every_n_steps=1,\r\n)\r\n....\r\ntrainer.fit(model, dataloaders[0], dataloaders[1])\r\nif trainer.state.status != TrainerStatus.FINISHED:\r\n    raise InterruptedError()\r\n\r\npath = checkpoint_callback.best_model_path\r\n\r\nos.makedirs(os.path.dirname(target_path), exist_ok=True)\r\nmodel.load_state_dict(torch.load(str(path))[\"state_dict\"])\r\ntorch.save(model.model.state_dict(), target_path)\r\n```\r\n\r\n### Expected behavior\r\n\r\nA single execution of the code after trainer.fit\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t\t- Tesla V100-SXM2-16GB\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0\r\n\t- pytorch-lightning: 1.4.0rc0\r\n\t- tqdm:              4.61.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.7\r\n\t- version:           #1 SMP Tue May 11 20:50:07 UTC 2021\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8678/comments",
    "author": "johngull",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-08-03T08:37:52Z",
        "body": "Dear @johngull,\r\n\r\nThis is an expected behaviour.\r\n\r\nUsing accelerator `ddp, this will create multiple independent processes and you script will be run `world_size` times.\r\n\r\n```py\r\n....\r\ntrainer = pl.Trainer(\r\n    gpus=-1,\r\n    precision=16 if train_opt.get(\"fp16\", False) else 32,\r\n    accelerator=\"ddp\",\r\n    accumulate_grad_batches=train_opt.get(\"grad_accum\", 1),\r\n    max_epochs=train_opt.get(\"epochs\", 20),\r\n    default_root_dir=train_opt.get(\"root_dir\", None),\r\n    callbacks=callbacks,\r\n    logger=logger,\r\n    log_every_n_steps=1,\r\n)\r\n....\r\ntrainer.fit(model, dataloaders[0], dataloaders[1])\r\n\r\n#\u00a0You should manipulate checkpoints only on rank 0 :)\r\nif trainer.is_global_zero:\r\n    path = checkpoint_callback.best_model_path\r\n    os.makedirs(os.path.dirname(target_path), exist_ok=True)\r\n    model.load_state_dict(torch.load(str(path))[\"state_dict\"])\r\n    torch.save(model.model.state_dict(), target_path)\r\n```\r\n\r\nBest,\r\nT.C\n\n---\n\nDear @johngull,\r\n\r\nI will be closing this issue. Feel free to re-open it if you still have questions.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "johngull",
        "created_at": "2021-08-03T08:45:12Z",
        "body": "Hello @tchaton,\r\n\r\nThank you a lot for the clarification and the tip on how to fix it.\r\nI have several questions here.\r\n\r\n- Shall I wrap everything else before trainer.fit also?\r\n- Is there another acceleration method that is faster than data-parallel but doesn't have such behavior?\r\n\r\nThanks.\r\n"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-03T13:57:50Z",
        "body": "Hey @tchaton,\r\n\r\nMy pleasure :)\r\n\r\n`Shall I wrap everything else before trainer.fit also?`\r\n\r\nThe processes are being created on `trainer.fit` call, therefore the trainer isn't aware of its rank before. Alternatively, you could use `ddp_spawn`.\r\n\r\nYes, `ddp_spawn`.\r\n\r\nBest,\r\nT.C"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why DDP accelerator causes multiple executions of post-training code",
      "Clear guidance on ensuring single execution of post-fit code in DDP environment",
      "Identification of process rank checking mechanism (like is_global_zero)",
      "Comparison of DDP alternatives that avoid multiple process execution",
      "Clarification of code scope affected by DDP process spawning"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:08:14"
    }
  },
  {
    "number": 7726,
    "title": "on_load_checkpoint never called",
    "created_at": "2021-05-26T14:09:10Z",
    "closed_at": "2021-05-26T14:55:00Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1",
      "priority: 2"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7726",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI am a new user of PL, so this may be an error of API comprehension on my side.\r\n\r\nI fail to get anything done on the loading of the checkpoint when I resume:\r\n\r\n## Please reproduce using the BoringModel\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint\r\nimport torch\r\n\r\n\r\nclass Solver(pl.LightningModule):\r\n    def __init__(self,):\r\n        super(Solver, self).__init__()\r\n        dx = 10\r\n        dy = 1\r\n        n = 100\r\n        self.model = torch.nn.Linear(dx, dy)\r\n        self.dataset = list(zip(torch.rand(n, dx), torch.rand(n, dy)))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def configure_optimizers(self,):\r\n        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\r\n\r\n    def _step(self, batch):\r\n        x, y = batch\r\n        y_hat = self.model(x)\r\n        return torch.nn.functional.mse_loss(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self._step(batch)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._step(batch)\r\n\r\n\r\nclass Checkpoint(ModelCheckpoint):\r\n    def on_load_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"loading...\")\r\n        import pdb # <----------------- Never called?\r\n\r\n        pdb.set_trace()\r\n        foo = checkpoint['bar']\r\n\r\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"saving...\")\r\n        checkpoint[\"foo\"] = \"bar\"\r\n\r\n\r\nsolver = Solver()\r\ncheckpoint = Checkpoint(dirpath=\"./\", save_last=True)\r\ntrainer = pl.Trainer(callbacks=[checkpoint], max_epochs=3)\r\ntrainer.fit(solver)\r\n\r\ntrainer = pl.Trainer(\r\n    callbacks=[checkpoint], resume_from_checkpoint=\"last.ckpt\", max_epochs=5\r\n)\r\ntrainer.fit(solver)\r\n\r\n```\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - Quadro GP100\r\n                - Quadro GP100\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.8.1+cu102\r\n        - pytorch-lightning: 1.3.2\r\n        - tqdm:              4.50.2\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #57-Ubuntu SMP Thu Oct 15 10:57:00 UTC 2020\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7726/comments",
    "author": "kingjr",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-05-26T14:51:37Z",
        "body": "Dear @kingjr,\r\n\r\nThis is working.\r\n\r\nExplanation: `on_load_checkpoint` is called only if `on_save_checkpoint` returned something which isn't None. \r\n\r\n```\r\nfrom typing import Callable\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning.callbacks import ModelCheckpoint, Callback\r\nimport torch\r\n\r\n\r\nclass Solver(pl.LightningModule):\r\n    def __init__(self,):\r\n        super(Solver, self).__init__()\r\n        dx = 10\r\n        dy = 1\r\n        n = 100\r\n        self.model = torch.nn.Linear(dx, dy)\r\n        self.dataset = list(zip(torch.rand(n, dx), torch.rand(n, dy)))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=10)\r\n\r\n    def configure_optimizers(self,):\r\n        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\r\n\r\n    def _step(self, batch):\r\n        x, y = batch\r\n        y_hat = self.model(x)\r\n        return torch.nn.functional.mse_loss(y_hat, y)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        return self._step(batch)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        self._step(batch)\r\n\r\n\r\nclass Checkpoint(ModelCheckpoint):\r\n    def on_load_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"loading...\")\r\n        import pdb; pdb.set_trace()\r\n        foo = checkpoint['bar']\r\n\r\n    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\r\n        print(\"saving...\")\r\n        checkpoint[\"foo\"] = \"bar\"\r\n        return checkpoint\r\n\r\n\r\nsolver = Solver()\r\ncheckpoint = Checkpoint(dirpath=\"./\", save_last=True)\r\ntrainer = pl.Trainer(callbacks=[checkpoint], max_epochs=3)\r\ntrainer.fit(solver)\r\n\r\ntrainer = pl.Trainer(\r\n    callbacks=[checkpoint], resume_from_checkpoint=\"last.ckpt\", max_epochs=5\r\n)\r\ntrainer.fit(solver)\r\n```\n\n---\n\nIf you have no further questions, I will close this issue."
      },
      {
        "user": "kingjr",
        "created_at": "2021-05-26T14:55:00Z",
        "body": "excellent, thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of the relationship between on_save_checkpoint return value and on_load_checkpoint invocation",
      "Clarification of callback checkpoint lifecycle requirements",
      "Guidance on proper checkpoint state persistence mechanics"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:08:40"
    }
  },
  {
    "number": 5642,
    "title": "Apex with multiple optimizers error \"element 0 of tensors does not require grad and does not have grad_fn\"",
    "created_at": "2021-01-24T22:05:23Z",
    "closed_at": "2021-01-27T02:24:27Z",
    "labels": [
      "bug",
      "help wanted",
      "3rd party"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5642",
    "body": "## \ud83d\udc1b Bug\r\n\r\n```bash\r\n File \"repro apex.py\", line 51, in <module>\r\n    trainer.fit(model)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 481, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/accelerators/gpu_accelerator.py\", line 67, in train\r\n    results = self.train_or_test()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/accelerators/accelerator.py\", line 68, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/trainer.py\", line 532, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 572, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 729, in run_training_batch\r\n    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 505, in optimizer_step\r\n    model_ref.optimizer_step(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/lightning.py\", line 1263, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/optimizer.py\", line 278, in step\r\n    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/optimizer.py\", line 133, in __optimizer_step\r\n    trainer.precision_connector.backend.optimizer_step(trainer, optimizer, closure)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/plugins/apex.py\", line 138, in optimizer_step\r\n    closure()\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 719, in train_step_and_backward_closure\r\n    result = self.training_step_and_backward(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 827, in training_step_and_backward\r\n    self.backward(result, optimizer, opt_idx)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/trainer/training_loop.py\", line 847, in backward\r\n    result.closure_loss = self.trainer.accelerator_backend.backward(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/accelerators/accelerator.py\", line 97, in backward\r\n    closure_loss = self.trainer.precision_connector.backend.backward(\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/plugins/apex.py\", line 53, in backward\r\n    model.backward(closure_loss, optimizer, opt_idx)\r\n  File \"/home/aw18f408/repositories/pytorch-lightning/pytorch_lightning/core/lightning.py\", line 1155, in backward\r\n    loss.backward(*args, **kwargs)\r\n  File \"/home/aw18f408/.conda/envs/lightning/lib/python3.8/site-packages/torch/tensor.py\", line 221, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/aw18f408/.conda/envs/lightning/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 130, in backward\r\n    Variable._execution_engine.run_backward(\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n\r\n```\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n```python\r\nimport torch\r\nfrom torch import optim\r\nfrom torch.utils.data import Dataset, DataLoader\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass AMPModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx, optimizer_idx):\r\n        output = self(batch)\r\n        loss = output.mean()\r\n        return {\"loss\": loss}\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def configure_optimizers(self):\r\n        optimizer1 = torch.optim.Adam(self.parameters(), lr=0.01)\r\n        optimizer2 = optim.SGD(self.parameters(), lr=0.01)\r\n        return [optimizer1, optimizer2]\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = AMPModel()\r\n    trainer = Trainer(\r\n        max_epochs=1,\r\n        precision=16,\r\n        amp_backend='apex',\r\n        gpus=1,\r\n    )\r\n    trainer.fit(model)\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo crash\r\n\r\n### Environment\r\n\r\n\r\n* CUDA:\r\n        - GPU:\r\n               - GeForce RTX 2080 Ti\r\n               - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           11.0\r\n* Packages:\r\n        - numpy:             1.19.5\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.2.0dev\r\n        - tqdm:              4.56.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n               - 64bit\r\n               - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.3\r\n        - version:           #1 SMP Thu Apr 9 13:49:54 UTC 2020\r\n\r\n### Additional context\r\n\r\ndiscovered in #5507, in the `test tests/models/test_amp::test_amp_with_apex`\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5642/comments",
    "author": "awaelchli",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-24T22:52:09Z",
        "body": "Hey @awaelchli ,\r\n\r\nA fix is already in review. Blocked by will \ud83d\ude01\r\n\r\nLightning is toggling on the current optimizer (setting requieres_grad=True) and untoggling on the second one (setting to False). \r\n\r\nAs self.parameters is used for both optimizers, your model doesn t have any trainable parameters and the loss doesn t have graph_fn.\r\n\r\nThe fix will restore the pre-toggle state.\r\n\r\nClosing duplicated issue.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-27T02:24:27Z",
        "body": "Thanks, I was searching but couldn't find a related issue. Would be great if you could link it :) \n\n---\n\nFixed by the linked PR. Thanks @tchaton "
      },
      {
        "user": "dave-epstein",
        "created_at": "2021-07-31T22:14:42Z",
        "body": "Hi, I'm still getting this issue training a model with multiple optimizers. I get the issue when using FSDP or DeepSpeed. I'm really stumped on the bug, and help would be appreciated. Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how parameter gradient requirements are managed when switching between multiple optimizers",
      "Clarification on framework compatibility with multiple optimizers in mixed precision contexts",
      "Guidance on maintaining valid computation graphs across optimizer transitions",
      "Documentation of parameter state preservation between optimizer steps"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:09:26"
    }
  },
  {
    "number": 5013,
    "title": "Accuracy metric for preds at half precision is zero with pl=1.0.8",
    "created_at": "2020-12-08T10:28:16Z",
    "closed_at": "2020-12-08T10:49:07Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5013",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nThe accuracy metric is wrong if `preds` are given with half precision. See example. \r\n\r\n### To Reproduce\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n```python\r\nimport torch\r\nfrom pytorch_lightning.metrics import Accuracy\r\n\r\nacc = Accuracy(threshold=0.5)\r\ntarget = torch.Tensor([1, 1, 0, 0])\r\npreds = torch.Tensor([0.7, 0.4, 0.8, 0.4])\r\n\r\nprint(acc(preds, target))  -> 0.5\r\nprint(acc(preds.half(), target))  -> 0.0\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nThe accuracy metric should not fail silently. Either an Error needs to be raised when preds are half precision or it should work correctly.\r\n\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.7.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.8\r\n - CUDA/cuDNN version: 10.2\r\n - GPU models and configuration: ...\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nThis might already be fixed in master. I filed the issue regardless because I don't have time to check.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5013/comments",
    "author": "luzuku",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-08T10:29:01Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "luzuku",
        "created_at": "2020-12-08T10:48:58Z",
        "body": "It is indeed fixed in `master` with the change from `preds.dtype == torch.float` to `preds.is_floating_point()`. It is also a duplicate of #4840. Sorry, my bad."
      },
      {
        "user": "tadejsv",
        "created_at": "2020-12-08T11:36:10Z",
        "body": "It is fixed in master but would be broken again in #4838, so thanks for catching this :)\n\n---\n\n@luzuku And one small note: if you create tensors with `torch.Tensor`, the created tensor will be a float tensor. Targets as floats will not be supported as inputs in classification functions anymore. If you want to preserve dtypes, create the tensor with `torch.tensor` (note that `tensor` is not capitalized)"
      }
    ],
    "satisfaction_conditions": [
      "Handles half-precision (float16) inputs correctly for accuracy calculation",
      "Either provides correct computation or explicit error for unsupported input types",
      "Maintains compatibility with floating point checks beyond simple dtype comparison",
      "Preserves functionality across different tensor precisions without regression"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:10:03"
    }
  },
  {
    "number": 3430,
    "title": "Issue with trainer.test in \"ddp\" distributed mode",
    "created_at": "2020-09-09T20:36:18Z",
    "closed_at": "2020-09-10T16:21:19Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3430",
    "body": "Hi -\r\n\r\nI have the following pseudo code workflow:\r\n\r\n> trainer = Trainer(distributed_backend='ddp', ...)\r\nmodel = new custom LightningModule\r\ntrainer.fit(model, ...)\r\nmodel.freeze()\r\ntrain.test(model, ...)\r\n\r\nThe error that I get is this:\r\n\r\n`AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.`\r\n\r\nWhat is the best way to address this?\r\n\r\nThanks very much,\r\nGriffin\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3430/comments",
    "author": "griff4692",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2020-09-09T20:51:45Z",
        "body": "That is a PyTorch `AssertionError`. Try it without calling `model.freeze()`?"
      },
      {
        "user": "griff4692",
        "created_at": "2020-09-16T18:01:22Z",
        "body": "Thanks! this worked"
      }
    ],
    "satisfaction_conditions": [
      "Resolve the conflict between model freezing and DistributedDataParallel (DDP) requirements",
      "Explain the relationship between gradient requirements and distributed training wrappers"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:10:56"
    }
  },
  {
    "number": 2254,
    "title": "Single node DDP: \"Default process group is not initialized\"",
    "created_at": "2020-06-19T02:37:22Z",
    "closed_at": "2020-07-10T01:20:18Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2254",
    "body": "## \ud83d\udc1b Bug\r\nUnable to start single node ddp training on 0.8.0\r\n\r\n### To Reproduce\r\n~~was going to run the gpu_template but... #2235~~\r\nboth methods of running the template result in the same error\r\n```\r\n$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp_spawn\r\n$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp\r\n```\r\n```\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 860, in fit\r\n    self.barrier('fit_prepare_data')\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1261, in barrier\r\n    torch_distrib.barrier()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 1484, in barrier\r\n    _check_default_pg()\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 187, in _check_default_pg\r\n    \"Default process group is not initialized\"\r\nAssertionError: Default process group is not initialized\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2254/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T02:47:50Z",
        "body": "can you post code to reproduce? just a minimal example that breaks\r\n\r\nBTW, the GPU template is fixed..."
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T02:50:00Z",
        "body": "done, let me post my env as well"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T02:50:36Z",
        "body": "ok wait... i think i see it. one sec"
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T04:50:07Z",
        "body": "I just tested the merged changes with both ddp and ddp_spawn again got this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 891, in fit\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    self.ddp_train(task, model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\r\n    exec(code, run_globals)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\r\n    main(hyperparams)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\r\n    trainer.fit(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 907, in fit\r\n    self.setup()\r\nTypeError: setup() missing 1 required positional argument: 'stage'\r\n    self.spawn_ddp_children(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 441, in spawn_ddp_children\r\n    self.ddp_train(local_rank, model, is_master=True)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\r\n    self.setup()\r\nTypeError: setup() missing 1 required positional argument: 'stage'\r\n```"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T05:14:30Z",
        "body": "try again. that was a typo"
      },
      {
        "user": "s-rog",
        "created_at": "2020-06-19T05:47:52Z",
        "body": "cheers, works now!"
      },
      {
        "user": "armancohan",
        "created_at": "2020-06-23T05:35:19Z",
        "body": "Still having the `Default process group is not initialized` issue when using trainer.test \r\n"
      },
      {
        "user": "wukailu",
        "created_at": "2020-06-23T06:30:56Z",
        "body": "> Still having the `Default process group is not initialized` issue when using trainer.test\r\n\r\nI still have this bug as well. One temporary solution is creating a new single GPU trainer to do the test.\r\n\r\nLike\r\n\r\n```\r\ntrainer = Trainer(gpus=1, deterministic=True, logger=logger)\r\ntrainer.model = model\r\ntrainer.test()\r\n```"
      },
      {
        "user": "armancohan",
        "created_at": "2020-06-23T19:57:28Z",
        "body": "Right, I know it works on single gpu. I have a large test set and ideally want faster inference using multiple gpus."
      },
      {
        "user": "zackcarson",
        "created_at": "2020-07-02T15:11:23Z",
        "body": "Can we re-open this issue? I am still having the `Default process group is not initialized` issue when I hit `trainer.test()` with ddp (with any number of gpus, even 1). I'm using the latest release from yesterday."
      },
      {
        "user": "armancohan",
        "created_at": "2020-07-02T15:33:13Z",
        "body": "+1, doesn't look like the issue is resolved yet."
      },
      {
        "user": "jxchen01",
        "created_at": "2020-07-04T05:32:04Z",
        "body": "having the same problem..... I also tried to downgrade pl to an older version, like 0.7.5, and try to using the older version to do the inference. But, the model trained and saved using the 0.8.x seems to not directly be compatible with older version.  "
      },
      {
        "user": "channingxiao",
        "created_at": "2020-07-09T12:11:00Z",
        "body": "version: 0.8.4  train with ddp,  Got \"Default process group is not initialized\" when run trainer.test()"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-07-09T12:18:32Z",
        "body": "could you try master? this is fixed there"
      },
      {
        "user": "zackcarson",
        "created_at": "2020-07-09T19:06:49Z",
        "body": "Just tried it, it works fine now! Thank you!\r\n"
      },
      {
        "user": "jxchen01",
        "created_at": "2020-08-17T19:13:27Z",
        "body": "@williamFalcon Trying 0.8.5\r\n\r\nTrained with ddp, and testing with ddp, but got the following error message:\r\n\r\n```\r\nAssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.\r\n```\r\n\r\nAny idea?\r\n\r\nThanks!"
      }
    ],
    "satisfaction_conditions": [
      "Resolve distributed process group initialization errors during both training and testing phases",
      "Support multi-GPU testing workflow with DDP backend",
      "Maintain compatibility between training and testing environments",
      "Handle parameter-less module scenarios in distributed mode"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:12:20"
    }
  },
  {
    "number": 2081,
    "title": "RuntimeError: Address already in use on 'ddp' mode pl 0.8.0",
    "created_at": "2020-06-05T10:58:42Z",
    "closed_at": "2020-06-05T14:33:40Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2081",
    "body": " Trainer configuration:\r\n```\r\n    trainer = pl.Trainer(\r\n        logger= CometLogger( api_key=\"ID\"),\r\n        auto_select_gpus=True,\r\n        gpus=3,\r\n        distributed_backend=\"ddp\",\r\n   )\r\n```\r\nThe error:\r\n```\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2]\r\nCometLogger will be initialized in online mode\r\nCometLogger will be initialized in online mode\r\ninitializing ddp: LOCAL_RANK: 0/2 WORLD_SIZE:3\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 156, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"train.py\", line 148, in main_train\r\n    trainer.fit(model)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 884, in fit\r\n    self.spawn_ddp_children(model)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 395, in spawn_ddp_children\r\n    self.ddp_train(local_rank, model, is_master=True)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 425, in ddp_train\r\n    model.init_ddp_connection(self.proc_rank, self.world_size, self.is_slurm_managing_tasks)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 962, in init_ddp_connection\r\n    torch_distrib.init_process_group(torch_backend, rank=proc_rank, world_size=world_size)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 393, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/user/anaconda3/envs/docBert/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 172, in _env_rendezvous_handler\r\n    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\r\nRuntimeError: Address already in use\r\n```\r\nEnv\r\n```\r\n* CUDA:\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.4\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.8.0-dev\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.46.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         x86_64\r\n        - python:            3.7.7\r\n        - version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2081/comments",
    "author": "dvirginz",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-05T11:09:11Z",
        "body": "check ps -elf | grep python. maybe a previous run is occupying that port.  "
      },
      {
        "user": "dvirginz",
        "created_at": "2020-06-05T14:33:36Z",
        "body": "I'm not sure, because `pkill -f train.py` fixed it, but it makes sense.\r\nI will update if it will happen again, closing.\r\n\r\nthanks for the framework, and the responsiveness!"
      },
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-10T17:11:04Z",
        "body": "Does this mean I can't launch multiple DDP jobs on the same node with many GPUs because the port will conflict? (e.g. 2 independent DDP jobs requiring 4 GPU each in an 8 GPU machine)\r\n\r\nEDIT: it seems that I can set `MASTER_PORT` env var to avoid this issue, correct? If so it'd be nice if lightning can detect this and use a new port automatically :)"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-06-10T19:36:16Z",
        "body": "you can... just the the MASTER_PORT env var.\r\n\r\nLightning does in fact pick a random port... but you set the seed, so it's always the same haha. We need to disable the seed for the port choosing or continue trying ports if the ports are taken.\r\n\r\nThis would be a great PR!"
      },
      {
        "user": "ZhaofengWu",
        "created_at": "2020-06-10T20:35:49Z",
        "body": "@williamFalcon PR submitted at #2140"
      },
      {
        "user": "ShanakaRG",
        "created_at": "2023-12-19T01:55:53Z",
        "body": "` kill -9 $(ps aux | grep main.py | grep -v grep | awk '{print $2}')` \r\n\r\nThis solved my problem. However, if I did not use `kill -9` it does not work for me."
      },
      {
        "user": "chenfengshijie",
        "created_at": "2024-06-24T09:35:21Z",
        "body": "Use torchrun instead of python solve this problem?Since torchrun can choose a free port to launch."
      }
    ],
    "satisfaction_conditions": [
      "Ensure port availability for distributed training processes",
      "Handle multiple concurrent DDP jobs on the same node",
      "Provide automatic port conflict resolution",
      "Clear process cleanup mechanism"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:12:39"
    }
  },
  {
    "number": 1322,
    "title": "Training loop temporarily hangs after every 4 steps",
    "created_at": "2020-03-31T17:35:57Z",
    "closed_at": "2020-04-05T15:07:16Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1322",
    "body": "I am porting some of my code to pytorch lightning, and everything seems to work fine. However, for some reason after every 4 training steps I see some temporary hanging (~1 second), which is severely slowing down my overall training time. Am I missing some obvious configuration?  This is my Trainer configuration:\r\n\r\n```\r\n    trainer = pl.Trainer(\r\n        gpus=8\r\n        num_nodes=1,\r\n        distributed_backend='ddp',\r\n        checkpoint_callback=False,\r\n        max_epochs=50,\r\n        max_steps=None,\r\n        progress_bar_refresh_rate=1,\r\n        check_val_every_n_epoch=1,\r\n        val_check_interval=1.0,\r\n        gradient_clip_val=0.0,\r\n        log_save_interval=0,\r\n        num_sanity_val_steps=0,\r\n        amp_level='O0',\r\n    )\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1322/comments",
    "author": "VitorGuizilini-TRI",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-31T17:36:45Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T12:34:00Z",
        "body": "@PyTorchLightning/core-contributors "
      },
      {
        "user": "ethanwharris",
        "created_at": "2020-04-04T12:39:03Z",
        "body": "Thanks for the issue! Would it be possible to post the code that reproduces this error? I've only seen this sort of behaviour before when the number of data loading workers is low - are you working with large data here (e.g. big images)?"
      },
      {
        "user": "VitorGuizilini",
        "created_at": "2020-04-04T16:24:16Z",
        "body": "I increased the number of workers and it works perfectly now, thank you very much! You can close this issue."
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-04T16:27:14Z",
        "body": "should we throw a warning when users use few workers?"
      },
      {
        "user": "VitorGuizilini",
        "created_at": "2020-04-04T16:34:04Z",
        "body": "If possible, sure! Seems like an obvious solution now, but it could save a couple of hours for other people. :)"
      }
    ],
    "satisfaction_conditions": [
      "Identify the cause of periodic training delays in distributed PyTorch Lightning setups",
      "Address data loading bottlenecks affecting training speed",
      "Explain configuration requirements for optimal distributed training performance",
      "Provide solutions to prevent I/O-related synchronization delays"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:13:40"
    }
  },
  {
    "number": 1155,
    "title": "No validation checks when overfit_pct is set",
    "created_at": "2020-03-15T13:43:17Z",
    "closed_at": "2020-05-03T23:15:57Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1155",
    "body": "## \ud83d\udc1b Bug\r\n\r\nWhen setting the `overfit_pct` to any value between 0 and 1 (exclusive) in trainer, the validation checks are disabled.\r\n\r\n### To Reproduce\r\n\r\nI have worked on a minimal example to reproduce the bug:\r\n\r\n```python3\r\nimport pytorch_lightning as pl\r\nimport torch\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Dataset, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n    def __getitem__(self, idx):\r\n        X = torch.rand(1, self.input_dim)\r\n        y = torch.randint(0, self.output_dim, (1,))\r\n        return X, y\r\n\r\n    def __len__(self):\r\n        return 1000\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Model, self).__init__()\r\n        self.layer = torch.nn.Linear(input_dim, output_dim)\r\n        self.dataset = Dataset(input_dim, output_dim)\r\n\r\n    def forward(self, x, y):\r\n        yhat = torch.softmax(self.layer(x), -1)\r\n        return F.nll_loss(logits, y)\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'loss': loss, 'log': {'loss': loss}}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Model(100, 10)\r\n    trainer = pl.Trainer(overfit_pct=.01)\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nValidation checks occur normally\r\n\r\n### Environment\r\n```bash\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Manjaro Linux\r\nGCC version: (GCC) 8.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-lightning==0.7.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] mkl                       2020.0                      166  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.7.1                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1155/comments",
    "author": "qmeeus",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-15T13:43:56Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "Borda",
        "created_at": "2020-03-18T21:49:24Z",
        "body": "@jeffling @hadim @awaelchli mind check?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-03-21T06:30:40Z",
        "body": "~~Yes there is a bug here~~, but I had to fix @qmeeus's code sample to make it visible. \r\nThe sanity validation checks run, but the validation at the end of the epoch doesn't.\r\nWhen setting `overfit_pct=1`, validation checks work as expected.\r\nHere is the fixed minimal code sample:\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Dataset, self).__init__()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n\r\n    def __getitem__(self, idx):\r\n        X = torch.rand(self.input_dim)\r\n        y = torch.randint(0, self.output_dim, (1,))\r\n        return X, y\r\n\r\n    def __len__(self):\r\n        return 1000\r\n\r\n\r\nclass Model(pl.LightningModule):\r\n\r\n    def __init__(self, input_dim, output_dim):\r\n        super(Model, self).__init__()\r\n        self.layer = torch.nn.Linear(input_dim, output_dim)\r\n        self.dataset = Dataset(input_dim, output_dim)\r\n\r\n    def forward(self, x, y):\r\n        logits = torch.softmax(self.layer(x), -1)\r\n        return F.nll_loss(logits, y.flatten(0))\r\n\r\n    def train_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def val_dataloader(self):\r\n        return torch.utils.data.DataLoader(self.dataset, batch_size=64)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        return {'loss': loss, 'log': {'loss': loss}}\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self.forward(*batch)\r\n        print('see that validation runs only in sanity check')\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n    def validation_end(self, outputs):\r\n        loss = torch.stack([output['val_loss'] for output in outputs]).mean()\r\n        return {'val_loss': loss, 'log': {'val_loss': loss}}\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = Model(100, 10)\r\n    trainer = pl.Trainer(overfit_pct=0.1, max_epochs=10)\r\n    trainer.fit(model)\r\n```\r\nFor the record, @qmeeus your code had these issues:\r\n- No val_dataloader defined\r\n- Wrong shapes returned in dataloader\r\n- Wrong shape for nll_loss labels\n\n---\n\nActually `overfit_pct `argument is not documented in the Trainer class. We should fix that and say that setting `overfit_pct `is the same as setting `train_percent_check`, `val_percent_check `and `test_percent_check`.\n\n---\n\n**False alarm!** Turns out it is simply because you chose a too small value for `overfit_pct`. \r\nYour dataset has size 1000, and dataloader has batch_size 64. \r\n1000 / 64 ~= 15 batches\r\nWhen you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch. \r\n\r\n@qmeeus Please let me know if it isn't clear. I think the behaviour of `overfit_pct `is correct.\n\n---\n\n@williamFalcon Should we make it so that `overfit_pct `does not round to 0 batches?\r\n"
      },
      {
        "user": "qmeeus",
        "created_at": "2020-03-21T10:42:11Z",
        "body": "> **False alarm!** Turns out it is simply because you chose a too small value for `overfit_pct`.\r\n> Your dataset has size 1000, and dataloader has batch_size 64.\r\n> 1000 / 64 ~= 15 batches\r\n> When you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch.\r\n> \r\n> @qmeeus Please let me know if it isn't clear. I think the behaviour of `overfit_pct `is correct.\r\n\r\nAwesome, thanks ! "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how overfit_pct parameter affects validation batch calculation",
      "Clarification of relationship between overfit_pct and validation checks",
      "Documentation of minimum batch threshold behavior",
      "Explanation of overfit_pct's interaction with other percentage check parameters"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:14:06"
    }
  },
  {
    "number": 754,
    "title": "Allow a flag into trainer to save checkpoints at partial epochs",
    "created_at": "2020-01-26T13:20:57Z",
    "closed_at": "2020-01-26T14:44:53Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/754",
    "body": "## \ud83d\ude80 Feature\r\nAllow a flag into trainer to save checkpoints at partial epochs\r\n\r\n### Motivation\r\n\r\nWhen you have a large dataset that takes tens of hours per epoch, it's important to have checkpoints along the way. Right now we only get a checkpoint on_epoch_end.\r\n\r\n### Workaround\r\nAlso interested to see if there is a good workaround. I guess I can set a reference to trainer inside my model and manually call on_epoch_end, but that feels like a hack and won't work without changing lightning code because of \r\n\r\nif self.epochs_since_last_check >= self.period:\r\n\r\ninside on_epoch_end.\r\n\r\n### Other ideas?\r\nAlso interested if there are better ways to solve this. Also thought about taking samples out of the dataset to make 'mini epochs,' but this breaks epochs naming convention. (eg an epoch implies all data has been run through once)\r\n\r\nThank you!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/754/comments",
    "author": "thisisjeffchen",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-01-26T14:44:53Z",
        "body": "@thisisjeffchen val_check_interval does this. \r\n\r\n```\r\nTrainer(val_check_interval=0.1)\r\n```\r\n\r\nThis will check every 10% of the epoch and save checkpoints\r\n"
      },
      {
        "user": "thisisjeffchen",
        "created_at": "2020-01-26T15:34:09Z",
        "body": "Ah, thank you. Didn't realize it saves over the last checkpoint, so only one file per epoch despite many saves :)"
      }
    ],
    "satisfaction_conditions": [
      "Supports checkpointing at intervals within a single epoch",
      "Does not require modifying internal framework code",
      "Maintains standard epoch semantics (full dataset iteration)",
      "Provides configurable checkpoint frequency",
      "Manages checkpoint storage efficiently during partial saves"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:14:14"
    }
  }
]