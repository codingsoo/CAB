[
  {
    "number": 20086,
    "title": "module statistics has no attribute mean",
    "created_at": "2024-07-15T07:18:01Z",
    "closed_at": "2024-07-15T11:51:42Z",
    "labels": [
      "question",
      "ver: 2.2.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20086",
    "body": "### Bug description\r\n\r\nWhen updating to the new PyTorch-lightning version 2.3.3 and using the MlFlowLogger as logger arg in the trainer I\u2018m getting the error trace (see error section)\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nmaster\r\n\r\n### How to reproduce the bug\r\n\r\n_No response_\r\n\r\n### Error messages and logs\r\n\r\n```\r\ntrain.py 9 <module>\r\nimport mlflow.pytorch\r\n \r\n__init__.py 1190 <module>\r\nfrom mlflow.pytorch._lightning_autolog import MlflowModelCheckpointCallback  # noqa: F401\r\n \r\n_lightning_autolog.py 24 <module>\r\nimport pytorch_lightning as pl\r\n \r\n__init__.py 27 <module>\r\nfrom pytorch_lightning.callbacks import Callback  # noqa: E402\r\n \r\n__init__.py 29 <module>\r\nfrom pytorch_lightning.callbacks.pruning import ModelPruning\r\n \r\npruning.py 32 <module>\r\nfrom pytorch_lightning.core.module import LightningModule\r\n \r\n__init__.py 16 <module>\r\nfrom pytorch_lightning.core.module import LightningModule\r\n \r\nmodule.py 62 <module>\r\nfrom pytorch_lightning.loggers import Logger\r\n \r\n__init__.py 14 <module>\r\nfrom pytorch_lightning.loggers.comet import CometLogger\r\n \r\ncomet.py 30 <module>\r\nfrom pytorch_lightning.loggers.logger import Logger, rank_zero_experiment\r\n \r\nlogger.py 103 <module>\r\ndefault_func: Callable[[Sequence[float]], float] = statistics.mean,\r\n \r\nAttributeError:\r\nmodule 'statistics' has no attribute 'mean'\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.3.3\r\n#- PyTorch Version (e.g., 2.0): 2.3.1\r\n#- Python version (e.g., 3.9): 3.11\r\n#- OS (e.g., Linux): MacOS Sonoma 14.5\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source): pip\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20086/comments",
    "author": "FabianKuon",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-07-15T10:28:22Z",
        "body": "Hey @FabianKuon \r\nThe statistics module is a standard module in Python and it definitely has a mean function. Could you please check that you don't have a different statistics module in the python path? For example, if you have a `statistics.py` module in your code, it would cause a collision with the standard library package. In this case, please rename or delete it. "
      },
      {
        "user": "FabianKuon",
        "created_at": "2024-07-15T11:51:42Z",
        "body": "@awaelchli thank you very much for your quick response. You\u2018re right I do have a statistics.py file in my repo. Renaming that solved the issue. \r\n\r\nOnce again thank you very much "
      },
      {
        "user": "awaelchli",
        "created_at": "2024-07-15T14:04:16Z",
        "body": "Perfect!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the root cause of the 'statistics' module attribute error",
      "Explains how Python's module resolution prioritizes local files over standard libraries",
      "Provides a method to detect module import conflicts",
      "Addresses resolution of naming collisions without prescribing specific filenames"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:57:37"
    }
  },
  {
    "number": 20020,
    "title": "Teardown trying to copy \"meta\" tensors",
    "created_at": "2024-06-27T08:19:33Z",
    "closed_at": "2024-06-27T21:48:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/20020",
    "body": "### Bug description\n\nI've got a model template that I'm using with torch.vmap, and to use it I need to store a meta model in my lightning module. However lightning keeps trying to copy the meta model to devices / during teardown etc... This results in an error for the meta tensor, since it does not have a copy method. Any good way to work around this?\r\n\r\nThis code generates my MLP's template, and during any lightning method that copies things, it dies when ```self.base_model.copy()``` or ```self.base_model.to(\"device\")``` is called.\r\n\r\n```python\r\nself.base_model = copy.deepcopy(self.mlp)\r\nself.base_model.to('meta')\r\n```\n\n### What version are you seeing the problem on?\n\nv2.2\n\n### How to reproduce the bug\n\n```python\nRun any lightning model with a meta model as a lightning property\r\n\r\n\r\nself.base_model = copy.deepcopy(self.mlp)\r\nself.base_model.to('meta')\n```\n\n\n### Error messages and logs\n\n```\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.2\r\n#- Lightning App Version (e.g., 0.5.2): \r\n#- PyTorch Version (e.g., 2.0): 2.2\r\n#- Python version (e.g., 3.9): 3.10\r\n#- OS (e.g., Linux): Windows\r\n#- CUDA/cuDNN version: 12.1\r\n#- GPU models and configuration: 3090\r\n#- How you installed Lightning(`conda`, `pip`, source): conda\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/20020/comments",
    "author": "kvndhrty",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-06-27T18:35:26Z",
        "body": "Hey @kvndhrty \r\nI think a pretty easy way to work around this is to not register your meta-template model as a submodule. You can easily do that by packing it into a list:\r\n```py\r\ndef __init__(self):\r\n    super().__init__()\r\n    with torch.device(\"meta\"):\r\n        self._template_model = [TemplateModel()]\r\n        \r\n    # then access it like so in your other code: \r\n    self._template_model[0]\r\n    \r\n    # ... or write a getter to return you the template model without indexing\r\n```\r\n\r\nI think that the assumption Lightning makes about your model not being on the meta device after training is a reasonable one. Even so before training, since eventually Lightning moves the model to GPU before training. I think it would become quite complex if we had to add logic to ignore such submodules on the meta-device. More so, it would be error-prone, because meta-device initialization is needed for large model training.\r\nSo I would like to suggest we don't treat this as a bug.\r\n\r\nOne other thing you could do is ask yourself whether it is even necessary to have your template model as an attribute at all. Since the creation on meta-device is basically free, you could also just do that on-the-fly whenever you need that. Get the properties you need and store them somewhere. Then you don't need to keep that template model around.\r\n    "
      },
      {
        "user": "kvndhrty",
        "created_at": "2024-06-27T21:48:58Z",
        "body": "@awaelchli I think that is entirely reasonable, I'll pack my module into a list for now. The small re-factor required to init the meta module each time isn't something I'll do this week, but maybe in the near future.\r\n\r\nThank you for the quick response! "
      }
    ],
    "satisfaction_conditions": [
      "Prevent Lightning from automatically copying/moving meta tensors during teardown or device transfers",
      "Maintain access to meta model for template purposes while avoiding framework interference",
      "Require minimal code changes to existing LightningModule structure",
      "Work within Lightning's assumptions about module attributes being movable to devices"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:57:45"
    }
  },
  {
    "number": 19252,
    "title": "lighting.pytorch IS NOT pytorch_lightning",
    "created_at": "2024-01-09T11:29:48Z",
    "closed_at": "2024-01-11T11:26:39Z",
    "labels": [
      "question",
      "ver: 2.0.x",
      "package"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19252",
    "body": "### Bug description\n\nDocumentation mentions that `pytorch_lightning` is now `lightning`, and that the old API can be found at `lightning_pytorch`.\r\n\r\nAnd yet, if you do:\r\n\r\n```python\r\nimport lightning.pytorch as pl\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass MyModel(LightningModule):\r\n    pass\r\n\r\nmodel = MyModel()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model)\r\n```\r\nthis still won't work, you will get:\r\n\r\n```\r\nTypeError: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `LightningModule`\r\n```\r\n\r\nbecause the `Trainer` was imported from `lightning.pytorch`, while the model is a `LightningModule` from `pytorch_lightning`. Are they the same or not?\r\n\r\nWhy can't we \"mix\" them?\r\n\r\nThis is sometimes necessary when working with other packages where some rely on `lightning` or `lightning.pytorch`, and other rely on `pytorch_lightning`.\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n```python\nimport lightning.pytorch as pl\r\nfrom pytorch_lightning import LightningModule\r\n\r\nclass MyModel(LightningModule):\r\n    pass\r\n\r\nmodel = MyModel()\r\ntrainer = pl.Trainer()\r\ntrainer.fit(model)\n```\n\n\n### Error messages and logs\n\n```\r\n# Error messages and logs here please\r\n```\r\n\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9):\r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19252/comments",
    "author": "svnv-svsv-jm",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2024-01-09T12:54:51Z",
        "body": "Hi @svnv-svsv-jm \r\n\r\n> because the Trainer was imported from lightning.pytorch, while the model is a LightningModule from pytorch_lightning. Are they the same or not?\r\n> \r\n> Why can't we \"mix\" them?\r\n\r\nThey are two different packages, with the same source code and the only difference is the imports. But Python doesn't know that, so you can't mix them together in the same source code. If you import LightningModule from one package, but the Trainer from the other one, it won't work. They are not meant to be interchangeable. \r\n\r\n> This is sometimes necessary when working with other packages where some rely on lightning or lightning.pytorch, and other rely on pytorch_lightning.\r\n\r\nIt's not possible to mix them. You will have to rewrite the code so that all of it is only from one package.\r\n\r\n\r\n\r\n"
      },
      {
        "user": "svnv-svsv-jm",
        "created_at": "2024-01-10T10:33:14Z",
        "body": "Thanks a lot for your reply!\r\n\r\nSo it is literally source code duplication?\r\n\r\nI thought that in the `lightning.pytorch` module, you would just import `pytorch_lightning` so that just allows the import of the same source code `pytorch_lightning` from two places, but it actually is duplicated code! Why maintain two exact replicas?\r\n\r\nJust curious at this point."
      },
      {
        "user": "awaelchli",
        "created_at": "2024-01-10T10:41:16Z",
        "body": "We don't maintain two copies of the code. We maintain lightning (see the content in GitHub here) and generate the pytorch_lightning package automatically. \r\n\r\nThat there are two packages is a consequence of the decision to rename the package from pytorch-lightning to lightning. "
      },
      {
        "user": "svnv-svsv-jm",
        "created_at": "2024-01-11T11:26:39Z",
        "body": "Ok, thanks lot!"
      },
      {
        "user": "awaelchli",
        "created_at": "2024-01-11T12:05:40Z",
        "body": "Thanks and you're welcome. Sorry for the inconvenience. Over time, more users will switch to the new package name and these questions and struggles will hopefully disappear in history. "
      }
    ],
    "satisfaction_conditions": [
      "Clarify the relationship between lightning.pytorch and pytorch_lightning packages",
      "Explain why mixing imports from different packages causes compatibility issues",
      "Address the maintenance rationale behind having two package names",
      "Provide guidance for resolving dependency conflicts with third-party packages",
      "Confirm whether components are interchangeable across packages"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:57:51"
    }
  },
  {
    "number": 19119,
    "title": "StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'",
    "created_at": "2023-12-06T10:50:14Z",
    "closed_at": "2023-12-10T00:26:53Z",
    "labels": [
      "question",
      "waiting on author",
      "callback: finetuning",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/19119",
    "body": "### Bug description\r\n\r\nFinetunning with a pytorch_lightning Trainer does not work now.\r\nThe call to `self.finetune_function()` should pass the `opt_idx` as the last parameter.\r\nLine 313 in `pytorch_lightning/callbacks/finetuning.py`.\r\nAfter that fix, finetuning works again.\r\n\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.1\r\n\r\n### How to reproduce the bug\r\n\r\n```python\r\nRunning a baseline training run.\r\nI am using a `StagedFinetuning` finetuning object, not sure if that is related.\r\nUsing training with a simple data loader: num_workers=1, batch_size=1\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nMain error message:\r\n```StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'```\r\n\r\nDetailed trace:\r\n```\r\n  File \"/home/rob/projects/model-training/model_training/training/train.py\", line 83, in main\r\n    trainer.fit(model, dl_train, val_dataloaders=[dl_val])\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 544, in fit\r\n    call._call_and_handle_interrupt(\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 989, in _run\r\n    results = self._run_stage()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1035, in _run_stage\r\n    self.fit_loop.run()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 201, in run\r\n    self.on_advance_start()\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 341, in on_advance_start\r\n    call._call_callback_hooks(trainer, \"on_train_epoch_start\")\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 208, in _call_callback_hooks\r\n    fn(trainer, trainer.lightning_module, *args, **kwargs)\r\n  File \"/home/rob/.cache/pypoetry/virtualenvs/model-training-799lC6_n-py3.10/lib/python3.10/site-packages/pytorch_lightning/callbacks/finetuning.py\", line 313, in on_train_epoch_start\r\n    self.finetune_function(pl_module, trainer.current_epoch, optimizer)\r\nTypeError: StagedFinetuning.finetune_function() missing 1 required positional argument: 'opt_idx'\r\n```\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\nOnly installed the \"pytorch-lightning\" pip package, not \"lightning\".\r\n\r\n```\r\n#- Lightning Component:  Trainer\r\n#- PyTorch Lightning Version (e.g., 1.5.0): 2.1.2\r\n#- Lightning App Version (e.g., 0.5.2): NA\r\n#- PyTorch Version (e.g., 2.0): 2.1.1\r\n#- Python version (e.g., 3.9): 3.10\r\n#- OS (e.g., Linux): Linux, Ubuntu 22.04, kernel 6.2.0-37-generic #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\r\n#- CUDA/cuDNN version: 12.2\r\n#- GPU models and configuration: Nvidia GTX 1050 Ti 4 GiB\r\n#- How you installed Lightning(`conda`, `pip`, source): poetry (pip install)\r\n#- Running environment of LightningApp (e.g. local, cloud): local\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/19119/comments",
    "author": "robwijnhoven",
    "comments": [
      {
        "user": "robwijnhoven",
        "created_at": "2023-12-06T11:02:57Z",
        "body": "Downgraded pytorch-lightning to 2.1.1 to be in sync with the pytorch version, but no effect there.\r\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-12-10T00:26:53Z",
        "body": "@robwijnhoven You can remove `opt_idx` from your `finetune_function` definition. If this doesn't work, please share your entire `StagedFinetuning` implementation. Thanks\n\n---\n\n@robwijnhoven Did it work?\n\n---\n\nI didn't get a reply, but I'm confident that my answer will solve the issue, so I'm closing the issue. "
      },
      {
        "user": "robwijnhoven",
        "created_at": "2023-12-11T12:33:24Z",
        "body": "Yes it works, thanks for the quick reply! My bad in the end :+1: \r\nSOLVED!"
      }
    ],
    "satisfaction_conditions": [
      "Resolve the missing 'opt_idx' argument error in StagedFinetuning.finetune_function()",
      "Ensure compatibility with the current PyTorch Lightning version's API expectations",
      "Maintain functionality of staged finetuning after the fix"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:57:59"
    }
  },
  {
    "number": 18975,
    "title": "Training a simple XOR network yields incorrect, undeterministic behaviour",
    "created_at": "2023-11-09T10:17:57Z",
    "closed_at": "2023-11-10T12:10:01Z",
    "labels": [
      "question",
      "ver: 2.1.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18975",
    "body": "### Bug description\n\nHi, I am trying to train a simple DNN to solve the XOR problem. This can be trivially solved with a pure torch implementation. I cannot replicate the same simple model in lightning. Instead the trained model oscillates between different states, never managing to correctly produce XOR.\n\n### What version are you seeing the problem on?\n\nv2.1\n\n### How to reproduce the bug\n\n```python\n# import libraries\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass XOR(nn.Module):\r\n    def __init__(self):\r\n        super(XOR, self).__init__()\r\n        self.linear_sigmoid_stack = nn.Sequential(\r\n            nn.Linear(2, 2),\r\n            nn.Sigmoid(),\r\n            nn.Linear(2, 1)\r\n        )\r\n\r\n    def forward(self, x):\r\n        return self.linear_sigmoid_stack(x)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # create data\r\n    Xs = torch.Tensor([[0., 0.],\r\n                       [0., 1.],\r\n                       [1., 0.],\r\n                       [1., 1.]])\r\n\r\n    y = torch.Tensor([0., 1., 1., 0.]).reshape(Xs.shape[0], 1)\r\n\r\n    xor_network = XOR()\r\n\r\n    epochs = 1000\r\n    mseloss = nn.MSELoss()\r\n    optimizer = torch.optim.Adam(xor_network.parameters(), lr=0.03)\r\n    all_losses = []\r\n    current_loss = 0\r\n    plot_every = 50\r\n\r\n    for epoch in range(epochs):\r\n\r\n        # input training example and return the prediction\r\n        yhat = xor_network.forward(Xs)\r\n\r\n        # calculate MSE loss\r\n        loss = mseloss(yhat, y)\r\n\r\n        # backpropogate through the loss gradiants\r\n        loss.backward()\r\n\r\n        # update model weights\r\n        optimizer.step()\r\n\r\n        # remove current gradients for next iteration\r\n        optimizer.zero_grad()\r\n\r\n        # append to loss\r\n        current_loss += loss\r\n        if epoch % plot_every == 0:\r\n            all_losses.append(current_loss / plot_every)\r\n            current_loss = 0\r\n\r\n        # print progress\r\n        if epoch % 500 == 0:\r\n            print(f'Epoch: {epoch} completed')\r\n```\r\n\r\nI tried to use Lightning to simplify away the boilerplate code like so:\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\nimport lightning as L\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\n\r\n\r\nclass XORNetwork(L.LightningModule):\r\n    def __init__(self):\r\n        super(XORNetwork, self).__init__()\r\n        self.linear_sigmoid_stack = nn.Sequential(\r\n            nn.Linear(2, 2),\r\n            nn.Sigmoid(),\r\n            nn.Linear(2, 1)\r\n        )\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop.\r\n        x, y = batch\r\n        yhat = self.forward(x)\r\n        loss = F.mse_loss(yhat, y)\r\n        return loss\r\n\r\n    def forward(self, x):\r\n        return self.linear_sigmoid_stack(x)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    X = torch.Tensor([[0., 0.], [0., 1.], [1., 0], [1., 1]])\r\n    labels = torch.Tensor([0., 1., 1., 0])\r\n    dataset = TensorDataset(X, labels)\r\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\r\n\r\n    xor_network = XORNetwork()\r\n\r\n    # train model\r\n    trainer = L.Trainer(max_epochs=500, accelerator=\"cpu\")\r\n    trainer.fit(model=xor_network, train_dataloaders=dataloader)\r\n\r\n    xor_network.eval()\r\n    with torch.no_grad():\r\n        test_output = xor_network(X)\r\n        print(test_output.round())\r\n```\n```\n\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n* CUDA:\r\n\t- GPU:               None\r\n\t- available:         False\r\n\t- version:           None\r\n* Lightning:\r\n\t- lightning:         2.1.1\r\n\t- lightning-utilities: 0.9.0\r\n\t- pytorch-lightning: 2.1.1\r\n\t- torch:             2.1.0\r\n\t- torchmetrics:      1.2.0\r\n* Packages:\r\n\t- aiohttp:           3.8.6\r\n\t- aiosignal:         1.3.1\r\n\t- async-timeout:     4.0.3\r\n\t- attrs:             23.1.0\r\n\t- certifi:           2023.7.22\r\n\t- charset-normalizer: 3.3.2\r\n\t- filelock:          3.13.1\r\n\t- frozenlist:        1.4.0\r\n\t- fsspec:            2023.10.0\r\n\t- idna:              3.4\r\n\t- jinja2:            3.1.2\r\n\t- lightning:         2.1.1\r\n\t- lightning-utilities: 0.9.0\r\n\t- markupsafe:        2.1.3\r\n\t- mpmath:            1.3.0\r\n\t- multidict:         6.0.4\r\n\t- networkx:          3.2.1\r\n\t- numpy:             1.26.1\r\n\t- packaging:         23.2\r\n\t- pip:               22.3.1\r\n\t- pytorch-lightning: 2.1.1\r\n\t- pyyaml:            6.0.1\r\n\t- requests:          2.31.0\r\n\t- setuptools:        65.5.1\r\n\t- sympy:             1.12\r\n\t- torch:             2.1.0\r\n\t- torchmetrics:      1.2.0\r\n\t- tqdm:              4.66.1\r\n\t- typing-extensions: 4.8.0\r\n\t- urllib3:           2.0.7\r\n\t- wheel:             0.38.4\r\n\t- yarl:              1.9.2\r\n* System:\r\n\t- OS:                Darwin\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         arm\r\n\t- python:            3.10.13\r\n\t- release:           23.0.0\r\n\t- version:           Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:34 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T8103\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18975/comments",
    "author": "Fohlen",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-11-09T13:59:33Z",
        "body": "@Fohlen In your Lightning code,\r\n\r\n1. You didn't choose the same learning rate. Make it 0.03 in both cases.\r\n2. You didn't run for the same number of epochs. Make it 1000 in both cases.\r\n\r\nAnd in the raw PyTorch code you are missing the test code:\r\n\r\n```py\r\n    xor_network.eval()\r\n    with torch.no_grad():\r\n        test_output = xor_network(Xs)\r\n        print(test_output.round())\r\n```\r\n\r\nTo make both of them the same, the hyperparameters need to be the same of course. Can you try again? I get the correct predictions (i.e. 0 1 1 0) after these fixes. \r\n\r\nIn addition, to make it fully deterministic you can set the seed \r\n\r\n```py\r\nL.seed_everything(0)\r\n```"
      },
      {
        "user": "Fohlen",
        "created_at": "2023-11-10T12:10:01Z",
        "body": "Hi @awaelchli, this indeed produces the correct result. I can get the code to converge correctly within 100 epochs or less with pure Torch, any idea why that wouldn't be the case with lightning?"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-11-10T12:32:37Z",
        "body": "> I can get the code to converge correctly within 100 epochs or less with pure Torch\r\n\r\nThe code that you posted can't actually converge in 100 epochs. Please share what you changed to make that possible. "
      },
      {
        "user": "Fohlen",
        "created_at": "2023-11-10T17:07:55Z",
        "body": "Sorry for the imprecise wording. After some experimentation with epochs I could produce the correct result at `epoch=250` (not convergence). However, this appears to be extremely sensitive to the seed one uses when training. I find this interesting. According to the Deep Learning book, the correct weights should be learned with a single pass of this network. However, this behaviour is not lightning-specific. Thanks for your help, I will keep on digging in torch to find out the reason for this behaviour \ud83d\udc4d "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why PyTorch Lightning implementation behaves differently than pure PyTorch despite identical network architecture",
      "Identification of critical hyperparameters affecting convergence in small networks",
      "Guidance on achieving deterministic training results in Lightning"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:58:11"
    }
  },
  {
    "number": 18378,
    "title": "Fabric cannot launch with specified gpu indices",
    "created_at": "2023-08-24T00:21:36Z",
    "closed_at": "2023-08-24T12:48:57Z",
    "labels": [
      "question",
      "3rd party",
      "fabric",
      "ver: 2.0.x"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18378",
    "body": "### Bug description\n\nHi,\r\n\r\nI launch fabric with specified gpu indices, and get RuntineError messges as follows.\r\nHow can I solve this issue? Thanks.\r\n\r\nSoftware version:\r\n```\r\ndeepspeed 0.10.0\r\nlightning 2.0.7\r\npytorch 2.0.1\r\npython 3.10.9\r\n```\r\n\r\nThe code looks like:\r\n```python\r\nimport lightning as L\r\nfabric = L.Fabric(accelerator=\"cuda\", devices=\"0,1,4,5\", strategy='deepspeed')\r\nfabric.launch()\r\n```\r\n\r\nThe error messges and logs is:\r\n```\r\n[2023-08-24 07:56:20,780] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/test_fab.py\", line 45, in <module>\r\n    fabric.launch()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 687, in launch\r\n    return self._strategy.launcher.launch(function, *args, **kwargs)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/launchers/subprocess_script.py\", line 92, in launch\r\n    return function(*args, **kwargs)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 776, in _run_with_setup\r\n    self._strategy.setup_environment()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/ddp.py\", line 113, in setup_environment\r\n    self._setup_distributed()\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/deepspeed.py\", line 579, in _setup_distributed\r\n    _validate_device_index_selection(self.parallel_devices)\r\n  File \"/home/xxx/miniconda310/envs/ml/lib/python3.10/site-packages/lightning/fabric/strategies/deepspeed.py\", line 821, in _validate_device_index_selection\r\n    raise RuntimeError(\r\nRuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n```\r\n\n\n### What version are you seeing the problem on?\n\nv2.0\n\n### How to reproduce the bug\n\n_No response_\n\n### Error messages and logs\n\n_No response_\n\n### Environment\n\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n#- Lightning Component (e.g. Trainer, LightningModule, LightningApp, LightningWork, LightningFlow):\r\n#- PyTorch Lightning Version (e.g., 1.5.0):\r\n#- Lightning App Version (e.g., 0.5.2):\r\n#- PyTorch Version (e.g., 2.0):\r\n#- Python version (e.g., 3.9): \r\n#- OS (e.g., Linux):\r\n#- CUDA/cuDNN version:\r\n#- GPU models and configuration:\r\n#- How you installed Lightning(`conda`, `pip`, source):\r\n#- Running environment of LightningApp (e.g. local, cloud):\r\n```\r\n\r\n</details>\r\n\n\n### More info\n\n_No response_\n\ncc @carmocca @justusschock @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18378/comments",
    "author": "seraphzl",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-08-24T12:48:58Z",
        "body": "@seraphzl This is expected, and is not supported by DeepSpeed. The error comes directly from Fabric and is informing you about the limitation. In addition, it suggests how you can select the devices via the environment variable. If you read the error message carefully:\r\n\r\n```\r\nRuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n```\r\n\r\nSo basically you remove the setting for `devices=[0, 1, 4, 5]` and launch with `CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py`. Please let me know if that works.\r\n\n\n---\n\n@seraphzl I'm closing the issue but if you have any further questions, don't hesitate to ask. "
      },
      {
        "user": "seraphzl",
        "created_at": "2023-08-24T13:55:21Z",
        "body": "> @seraphzl This is expected, and is not supported by DeepSpeed. The error comes directly from Fabric and is informing you about the limitation. In addition, it suggests how you can select the devices via the environment variable. If you read the error message carefully:\r\n> \r\n> ```\r\n> RuntimeError: The selected device indices [0, 1, 4, 5] don't match the local rank values of processes. If you need to select GPUs at a specific index, set the `CUDA_VISIBLE_DEVICES` environment variable instead. For example: `CUDA_VISIBLE_DEVICES=0,1,4,5`.\r\n> ```\r\n> \r\n> So basically you remove the setting for `devices=[0, 1, 4, 5]` and launch with `CUDA_VISIBLE_DEVICES=0,1,4,5 python train.py`. Please let me know if that works.\r\n\r\n@awaelchli Using the CUDA_VISIBLE_DEVICES environment setting to launch the training works. Thank you for reply."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why specifying GPU indices directly in Fabric's devices parameter conflicts with DeepSpeed's requirements",
      "Clear guidance on proper GPU selection method when using DeepSpeed strategy",
      "Identification of DeepSpeed's device index handling limitations",
      "Solution that maintains multi-GPU functionality without device index conflicts"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:58:45"
    }
  },
  {
    "number": 16970,
    "title": "Start training using CLI on Slurm cluster",
    "created_at": "2023-03-06T16:12:59Z",
    "closed_at": "2023-04-14T08:48:10Z",
    "labels": [
      "bug",
      "question",
      "won't fix",
      "waiting on author",
      "lightningcli"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/16970",
    "body": "### Bug description\r\n\r\nHi,\r\nIm trying to run a simple pytorch lightning model training on mnist data using the pytorch CLI (with yaml config) as a slurm job.\r\n\r\n### How to reproduce the bug\r\nIm starting the slurm job using: `sbatch train_submit.sh`\r\ntrain_submit.sh:\r\n``` bash\r\n#!/bin/bash -l\r\n\r\n# SLURM SUBMIT SCRIPT\r\n#SBATCH --nodes=1             # This needs to match Trainer(num_nodes=...)\r\n#SBATCH --ntasks-per-node=1   # This needs to match Trainer(devices=...)\r\n#SBATCH --cpus-per-task=5\r\n#SBATCH --mem-per-cpu=5240\r\n#SBATCH --gpus=1\r\n#SBATCH --time=01:00:00\r\n#SBATCH --mail-type=BEGIN,END\r\n\r\n# activate conda env\r\n# source activate $1\r\n\r\n# debugging flags (optional)\r\n# export NCCL_DEBUG=INFO\r\n# export PYTHONFAULTHANDLER=1\r\n\r\n# on your cluster you might need these:\r\n# set the network interface\r\n# export NCCL_SOCKET_IFNAME=^docker0,lo\r\n\r\n# might need the latest CUDA\r\n# module load NCCL/2.4.7-1-cuda.10.0\r\n\r\n# run script from above\r\nsrun python3 cli_test.py fit --config config.yaml\r\n```\r\nconfig.yaml file:\r\n```\r\nseed_everything_default: null\r\ntrainer:\r\n  accelerator: gpu\r\n  limit_train_batches: 100\r\n  max_epochs: 500\r\n  devices: 1\r\n  logger: true\r\n  callbacks:\r\n    - class_path: pytorch_lightning.callbacks.ModelCheckpoint\r\n      init_args:\r\n        save_top_k: 1\r\n        monitor: 'val_loss'\r\n        mode: min\r\n        filename: 'vit-best'\r\n    - class_path: pytorch_lightning.callbacks.ModelCheckpoint\r\n      init_args:\r\n        save_last: true\r\n        filename: 'vit-last'\r\nckpt_path: null\r\nlog_dir: /cluster/dir/to/log\r\n```\r\n\r\ncli_test.py:\r\n```\r\n# main.py\r\nfrom pytorch_lightning.cli import LightningCLI\r\n\r\nimport os\r\nfrom torch import optim, nn, utils, Tensor\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision.transforms import ToTensor\r\nimport pytorch_lightning as pl\r\n\r\n# define any number of nn.Modules (or use your current ones)\r\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\r\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\r\n\r\n\r\n# define the LightningModule\r\nclass LitAutoEncoder(pl.LightningModule):\r\n    def __init__(self, encoder, decoder):\r\n        super().__init__()\r\n        self.encoder = encoder\r\n        self.decoder = decoder\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        # training_step defines the train loop.\r\n        # it is independent of forward\r\n        x, y = batch\r\n        x = x.view(x.size(0), -1)\r\n        z = self.encoder(x)\r\n        x_hat = self.decoder(z)\r\n        loss = nn.functional.mse_loss(x_hat, x)\r\n        # Logging to TensorBoard (if installed) by default\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\r\n        return optimizer\r\n\r\n\r\nclass MNISTDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str = os.getcwd(), batch_size: int = 32):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: str):\r\n        self.mnist_test = MNIST(self.data_dir, train=False)\r\n        self.mnist_predict = MNIST(self.data_dir, train=False)\r\n        mnist_full = MNIST(self.data_dir, train=True)\r\n        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\r\n\r\n\r\ndef cli_main():\r\n    cli = LightningCLI(LitAutoEncoder, MNISTDataModule)\r\n    # note: don't call fit!!\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cli_main()\r\n```\r\n\r\n\r\n### Error messages and logs\r\n\r\nslurm-9842342.out (File where std:output is printed)\r\n```\r\n2023-03-06 17:02:07.694344: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nusage: cli_test.py [-h] [-c CONFIG] [--print_config ^H[=flags]]\r\n                   {fit,validate,test,predict,tune} ...\r\ncli_test.py: error: 'Configuration check failed :: No action for destination key \"seed_everything_default\" to check its value.'\r\nsrun: error: eu-g2-16: task 0: Exited with exit code 2\r\n```\r\n\r\n\r\n### Environment\r\n\r\n<details>\r\n  <summary>Current environment</summary>\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - NVIDIA GeForce GTX 1080 Ti\r\n        - available:         True\r\n        - version:           11.3\r\n* Lightning:\r\n        - lightning-utilities: 0.7.1\r\n        - pytorch-ignite:    0.4.10\r\n        - pytorch-lightning: 1.9.4\r\n        - pytorch3dunet:     1.3.3\r\n        - torch:             1.11.0+cu113\r\n        - torch-cluster:     1.6.0\r\n        - torch-fidelity:    0.3.0\r\n        - torch-geometric:   2.0.4\r\n        - torch-scatter:     2.0.9\r\n        - torch-sparse:      0.6.13\r\n        - torch-spline-conv: 1.2.1\r\n        - torchaudio:        0.11.0+cu113\r\n        - torchmetrics:      0.11.3\r\n        - torchvision:       0.12.0+cu113\r\n* Packages:\r\n        - absl-py:           1.0.0\r\n        - accesscontrol:     5.3.1\r\n        - acquisition:       4.10\r\n        - affine:            2.3.1\r\n        - aiohttp:           3.8.1\r\n        - aiohttp-cors:      0.7.0\r\n        - aioredis:          2.0.1\r\n        - aiosignal:         1.2.0\r\n        - alabaster:         0.7.12\r\n        - alembic:           1.8.1\r\n        - amply:             0.1.5\r\n        - aniso8601:         9.0.1\r\n        - anndata:           0.8.0\r\n        - antlr4-python3-runtime: 4.9.3\r\n        - anyio:             3.6.1\r\n        - app-model:         0.1.1\r\n        - appdirs:           1.4.4\r\n        - apptools:          5.1.0\r\n        - argcomplete:       2.0.0\r\n        - argh:              0.26.2\r\n        - argon2:            0.1.10\r\n        - argon2-cffi:       21.3.0\r\n        - argon2-cffi-bindings: 21.2.0\r\n        - arviz:             0.12.1\r\n        - ase:               3.22.1\r\n        - asn1crypto:        1.5.1\r\n        - astor:             0.8.1\r\n        - asttokens:         2.0.5\r\n        - astunparse:        1.6.3\r\n        - async-generator:   1.10\r\n        - async-timeout:     4.0.2\r\n        - atomicwrites:      1.4.0\r\n        - attrs:             21.4.0\r\n        - audioread:         2.1.9\r\n        - authencoding:      4.3\r\n        - autopage:          0.5.1\r\n        - autopep8:          1.6.0\r\n        - aws-requests-auth: 0.4.3\r\n        - babel:             2.10.1\r\n        - backcall:          0.2.0\r\n        - beautifulsoup4:    4.11.1\r\n        - bidict:            0.22.0\r\n        - bids-validator:    1.9.3\r\n        - biopython:         1.79\r\n        - bitstring:         3.1.9\r\n        - black:             22.3.0\r\n        - bleach:            5.0.0\r\n        - blessings:         1.7\r\n        - blurhash:          1.1.4\r\n        - bokeh:             2.4.3\r\n        - boost:             0.1\r\n        - boto3:             1.23.10\r\n        - botocore:          1.26.10\r\n        - bottleneck:        1.3.4\r\n        - btrees:            4.10.0\r\n        - build:             0.10.0\r\n        - cachetools:        5.2.0\r\n        - cachey:            0.2.1\r\n        - cellmodeller:      b-v4.3-42-g96ab099-\r\n        - certifi:           2022.5.18.1\r\n        - certipy:           0.1.3\r\n        - cffi:              1.15.0\r\n        - cftime:            1.6.0\r\n        - chainer:           7.8.1\r\n        - chameleon:         3.10.1\r\n        - chardet:           4.0.0\r\n        - charset-normalizer: 2.0.12\r\n        - chex:              0.1.3\r\n        - clang:             14.0\r\n        - click:             8.1.3\r\n        - click-plugins:     1.1.1\r\n        - cligj:             0.7.2\r\n        - clikit:            0.6.2\r\n        - cloudpickle:       2.1.0\r\n        - cmaes:             0.9.1\r\n        - cmake:             3.24.1.1\r\n        - cmd2:              2.4.1\r\n        - codecov:           2.1.12\r\n        - colorama:          0.4.4\r\n        - coloredlogs:       15.0.1\r\n        - colorful:          0.5.4\r\n        - colorlog:          6.6.0\r\n        - colorlover:        0.3.0\r\n        - colormath:         3.0.0\r\n        - commonmark:        0.9.1\r\n        - configargparse:    1.5.3\r\n        - configobj:         5.0.6\r\n        - configparser:      5.2.0\r\n        - connection-pool:   0.0.3\r\n        - contextlib2:       21.6.0\r\n        - coverage:          6.4\r\n        - crashtest:         0.3.1\r\n        - cryptography:      38.0.4\r\n        - cucim:             23.2.0\r\n        - cufflinks:         0.17.3\r\n        - cupy-cuda11x:      11.1.0\r\n        - cutadapt:          4.0\r\n        - cutensor:          1.6.0.3\r\n        - cvxopt:            1.3.0\r\n        - cvxpy:             1.2.1\r\n        - cycler:            0.11.0\r\n        - cython:            0.29.30\r\n        - dask:              2022.5.2\r\n        - databricks-cli:    0.17.4\r\n        - datasets:          2.5.1\r\n        - datetime:          4.4\r\n        - datrie:            0.8.2\r\n        - deap:              1.3.1\r\n        - debtcollector:     2.5.0\r\n        - debugpy:           1.6.0\r\n        - decorator:         5.1.1\r\n        - deepdiff:          5.8.1\r\n        - defusedxml:        0.7.1\r\n        - deprecated:        1.2.13\r\n        - deprecation:       2.1.0\r\n        - descartes:         1.1.0\r\n        - dill:              0.3.5.1\r\n        - distributed:       2022.5.2\r\n        - distro:            1.8.0\r\n        - dm-tree:           0.1.7\r\n        - dnaio:             0.9.0\r\n        - dnspython:         2.2.1\r\n        - docker:            6.0.1\r\n        - docker-pycreds:    0.4.0\r\n        - docopt:            0.6.2\r\n        - docstring-parser:  0.15\r\n        - documenttemplate:  4.0\r\n        - docutils:          0.17.1\r\n        - dpath:             2.0.6\r\n        - easydict:          1.9\r\n        - ecos:              2.0.10\r\n        - einops:            0.4.1\r\n        - entrypoints:       0.4\r\n        - envisage:          6.0.1\r\n        - ephem:             4.1.3\r\n        - esda:              2.4.1\r\n        - et-xmlfile:        1.1.0\r\n        - etils:             0.8.0\r\n        - eventlet:          0.33.1\r\n        - evo:               1.18.1\r\n        - executing:         0.8.3\r\n        - extensionclass:    4.6\r\n        - extras:            1.0.0\r\n        - fasteners:         0.17.3\r\n        - fastjsonschema:    2.15.3\r\n        - fastprogress:      1.0.2\r\n        - fastrlock:         0.8\r\n        - filelock:          3.7.0\r\n        - findlibs:          0.0.2\r\n        - fiona:             1.8.22\r\n        - fire:              0.5.0\r\n        - flask:             2.1.2\r\n        - flask-cors:        3.0.10\r\n        - flask-json:        0.3.4\r\n        - flask-restplus:    0.13.0\r\n        - flask-restx:       0.5.1\r\n        - flatbuffers:       1.12\r\n        - flit:              3.7.1\r\n        - flit-core:         3.7.1\r\n        - flowvision:        0.2.0\r\n        - follicle-tracker:  0.1.dev221+gc3cd246\r\n        - fonttools:         4.33.3\r\n        - freetype-py:       2.3.0\r\n        - frozenlist:        1.3.0\r\n        - fsspec:            2022.5.0\r\n        - funcsigs:          1.0.2\r\n        - future:            0.18.2\r\n        - futurist:          2.4.1\r\n        - gast:              0.4.0\r\n        - gdown:             4.4.0\r\n        - geopandas:         0.12.2\r\n        - gevent:            21.12.0\r\n        - giddy:             2.3.3\r\n        - gitdb:             4.0.9\r\n        - gitdb2:            4.0.2\r\n        - gitpython:         3.1.27\r\n        - gmpy2:             2.1.5\r\n        - google-api-core:   2.8.1\r\n        - google-auth:       2.6.6\r\n        - google-auth-oauthlib: 0.4.6\r\n        - google-pasta:      0.2.0\r\n        - googleapis-common-protos: 1.56.2\r\n        - googledrivedownloader: 0.4\r\n        - gpaw:              22.8.0\r\n        - gprmax:            3.1.4\r\n        - gpustat:           0.6.0\r\n        - grabbit:           0.2.6\r\n        - graphtools:        1.5.2\r\n        - greenlet:          1.1.2\r\n        - grpcio:            1.46.3\r\n        - gunicorn:          20.1.0\r\n        - h3:                3.7.4\r\n        - h5py:              3.7.0\r\n        - haversine:         2.5.1\r\n        - hdbscan:           0.8.29\r\n        - heapdict:          1.0.1\r\n        - hiredis:           2.0.0\r\n        - hsluv:             5.0.3\r\n        - html5lib:          1.1\r\n        - httpstan:          4.8.2\r\n        - huggingface-hub:   0.7.0\r\n        - humanfriendly:     10.0\r\n        - hydra-core:        1.2.0\r\n        - hyperopt:          0.2.7\r\n        - idna:              3.3\r\n        - ifcfg:             0.22\r\n        - imagecodecs:       2023.1.23\r\n        - imageio:           2.19.3\r\n        - imageio-ffmpeg:    0.4.7\r\n        - imagesize:         1.3.0\r\n        - importlib-metadata: 4.11.4\r\n        - importlib-resources: 5.7.1\r\n        - in-n-out:          0.1.7\r\n        - inequality:        1.0.0\r\n        - iniconfig:         1.1.1\r\n        - install:           1.3.5\r\n        - iopath:            0.1.6\r\n        - ipdb:              0.13.9\r\n        - ipykernel:         6.13.0\r\n        - ipython:           8.4.0\r\n        - ipython-genutils:  0.2.0\r\n        - ipywidgets:        7.7.0\r\n        - isal:              0.11.1\r\n        - iso3166:           2.0.2\r\n        - iso8601:           1.0.2\r\n        - isodate:           0.6.1\r\n        - iteration-utilities: 0.11.0\r\n        - itk:               5.3.0\r\n        - itk-core:          5.3.0\r\n        - itk-filtering:     5.3.0\r\n        - itk-io:            5.3.0\r\n        - itk-numerics:      5.3.0\r\n        - itk-registration:  5.3.0\r\n        - itk-segmentation:  5.3.0\r\n        - itsdangerous:      2.1.2\r\n        - jax:               0.3.23\r\n        - jaxlib:            0.3.22+cuda11.cudnn82\r\n        - jedi:              0.18.1\r\n        - jeepney:           0.8.0\r\n        - jieba:             0.42.1\r\n        - jinja2:            3.1.2\r\n        - jmespath:          1.0.0\r\n        - joblib:            1.1.0\r\n        - json-tricks:       3.16.1\r\n        - json5:             0.9.8\r\n        - jsonargparse:      4.20.0\r\n        - jsonlines:         1.2.0\r\n        - jsonpickle:        2.2.0\r\n        - jsonpointer:       2.3\r\n        - jsonschema:        4.5.1\r\n        - jupyter:           1.0.0\r\n        - jupyter-client:    7.3.1\r\n        - jupyter-console:   6.4.3\r\n        - jupyter-contrib-core: 0.3.3\r\n        - jupyter-core:      4.10.0\r\n        - jupyter-highlight-selected-word: 0.2.0\r\n        - jupyter-server:    1.17.0\r\n        - jupyter-telemetry: 0.1.0\r\n        - jupyterlab:        3.4.2\r\n        - jupyterlab-pygments: 0.2.2\r\n        - jupyterlab-server: 2.14.0\r\n        - jupyterlab-widgets: 1.1.0\r\n        - keras:             2.9.0\r\n        - keras-preprocessing: 1.1.2\r\n        - keyring:           23.5.1\r\n        - kiwisolver:        1.4.2\r\n        - lazy-object-proxy: 1.7.1\r\n        - libclang:          14.0.1\r\n        - libpysal:          4.6.2\r\n        - lightning-utilities: 0.7.1\r\n        - llvmlite:          0.38.1\r\n        - lmdb:              1.4.0\r\n        - locket:            1.0.0\r\n        - logutils:          0.3.5\r\n        - loompy:            3.0.7\r\n        - lxml:              4.8.0\r\n        - lz4:               4.0.1\r\n        - lzstring:          1.0.4\r\n        - mageck:            0.5.9.4\r\n        - magicgui:          0.7.0\r\n        - mako:              1.2.0\r\n        - mapclassify:       2.4.3\r\n        - markdown:          3.3.7\r\n        - markupsafe:        2.1.1\r\n        - marshmallow:       3.18.0\r\n        - mastodon.py:       1.8.0\r\n        - matplotlib:        3.5.2\r\n        - matplotlib-inline: 0.1.3\r\n        - mccabe:            0.7.0\r\n        - mercantile:        1.2.1\r\n        - mgwr:              2.1.2\r\n        - mistune:           0.8.4\r\n        - mlflow:            2.2.1\r\n        - mock:              4.0.3\r\n        - monai:             1.1.0\r\n        - more-itertools:    8.13.0\r\n        - mpi4py:            3.1.4\r\n        - mpmath:            1.2.1\r\n        - msgpack:           1.0.3\r\n        - multidict:         6.0.2\r\n        - multimapping:      4.1\r\n        - multipart:         0.2.4\r\n        - multiprocess:      0.70.13\r\n        - multiqc:           1.13\r\n        - munch:             2.5.0\r\n        - mypy-extensions:   0.4.3\r\n        - napari:            0.4.17\r\n        - napari-console:    0.0.7\r\n        - napari-plugin-engine: 0.2.0\r\n        - napari-svg:        0.1.6\r\n        - natsort:           8.1.0\r\n        - nbclassic:         0.3.7\r\n        - nbclient:          0.6.3\r\n        - nbconvert:         6.5.0\r\n        - nbformat:          5.4.0\r\n        - nbsphinx:          0.8.8\r\n        - nest-asyncio:      1.5.5\r\n        - netaddr:           0.8.0\r\n        - netcdf4:           1.5.8\r\n        - netifaces:         0.11.0\r\n        - networkx:          2.8.2\r\n        - nibabel:           3.2.2\r\n        - ninja:             1.11.1\r\n        - nipy:              0.5.0\r\n        - nltk:              3.7\r\n        - nni:               2.10\r\n        - nose:              1.3.7\r\n        - nose-timer:        1.0.1\r\n        - notebook:          6.4.11\r\n        - notebook-shim:     0.1.0\r\n        - npe2:              0.6.2\r\n        - nptyping:          2.5.0\r\n        - num2words:         0.5.10\r\n        - numba:             0.55.2\r\n        - numexpr:           2.8.1\r\n        - numpy:             1.22.4\r\n        - numpy-groupies:    0.9.16\r\n        - numpy-quaternion:  2022.4.2\r\n        - numpydoc:          1.5.0\r\n        - nvidia-ml-py3:     7.352.0\r\n        - oauthlib:          3.2.0\r\n        - omegaconf:         2.2.2\r\n        - opencensus:        0.9.0\r\n        - opencensus-context: 0.1.2\r\n        - opencv-contrib-python: 4.5.5.64\r\n        - opencv-python:     4.5.5.64\r\n        - openpyxl:          3.0.10\r\n        - openseespy:        3.3.0.1.1\r\n        - openseespylinux:   3.4.0.1\r\n        - openslide-python:  1.1.2\r\n        - opt-einsum:        3.3.0\r\n        - optax:             0.1.2\r\n        - optuna:            3.1.0\r\n        - ordered-set:       4.1.0\r\n        - os-service-types:  1.7.0\r\n        - oslo.i18n:         5.1.0\r\n        - osmnx:             1.2.2\r\n        - osqp:              0.6.2.post5\r\n        - ovary-analysis:    0.0.3\r\n        - overpy:            0.6\r\n        - packaging:         21.3\r\n        - pamela:            1.0.0\r\n        - pandas:            1.4.2\r\n        - pandas-datareader: 0.10.0\r\n        - pandoc:            2.2\r\n        - pandocfilters:     1.5.0\r\n        - parso:             0.8.3\r\n        - partd:             1.2.0\r\n        - paste:             3.5.0\r\n        - pastedeploy:       2.1.1\r\n        - pastel:            0.2.1\r\n        - pathos:            0.2.9\r\n        - pathspec:          0.9.0\r\n        - pathtools:         0.1.2\r\n        - patsy:             0.5.2\r\n        - pbr:               5.9.0\r\n        - persistence:       3.3\r\n        - persistent:        4.9.0\r\n        - pert:              2019.11\r\n        - pexpect:           4.8.0\r\n        - pickleshare:       0.7.5\r\n        - pillow:            9.1.1\r\n        - pint:              0.19.2\r\n        - pip:               22.2.2\r\n        - pkginfo:           1.8.2\r\n        - plac:              1.3.5\r\n        - platformdirs:      2.5.2\r\n        - plotly:            5.8.0\r\n        - pluggy:            1.0.0\r\n        - plumbum:           1.7.2\r\n        - ply:               3.11\r\n        - pointpats:         2.2.0\r\n        - pooch:             1.6.0\r\n        - portalocker:       2.4.0\r\n        - pox:               0.3.1\r\n        - ppft:              1.7.6.5\r\n        - prettytable:       3.3.0\r\n        - prometheus-client: 0.14.1\r\n        - promise:           2.3\r\n        - prompt-toolkit:    3.0.29\r\n        - protobuf:          3.19.4\r\n        - psutil:            5.9.1\r\n        - psygnal:           0.8.1\r\n        - ptyprocess:        0.7.0\r\n        - pulp:              2.6.0\r\n        - pure-eval:         0.2.2\r\n        - py:                1.11.0\r\n        - py-spy:            0.3.12\r\n        - py4design:         0.28\r\n        - py4j:              0.10.9.5\r\n        - pyarrow:           9.0.0\r\n        - pyasn1:            0.4.8\r\n        - pyasn1-modules:    0.2.8\r\n        - pybind11:          2.9.2\r\n        - pybis:             1.35.2\r\n        - pybufrkit:         0.2.19\r\n        - pycocotools:       2.0.4\r\n        - pycodestyle:       2.8.0\r\n        - pycollada:         0.7.2\r\n        - pycparser:         2.21\r\n        - pydantic:          1.10.5\r\n        - pydicom:           2.3.1\r\n        - pydot:             1.4.2\r\n        - pyepsg:            0.4.0\r\n        - pyface:            7.4.1\r\n        - pyfaidx:           0.6.4\r\n        - pyflakes:          2.5.0\r\n        - pyglet:            1.5.26\r\n        - pygments:          2.12.0\r\n        - pygsp:             0.5.1\r\n        - pygsti:            0.9.10.1\r\n        - pyinotify:         0.9.6\r\n        - pyjwt:             2.6.0\r\n        - pylev:             1.4.0\r\n        - pymeshfix:         0.16.2\r\n        - pymf:              0.1.9\r\n        - pymongo:           4.1.1\r\n        - pynrrd:            1.0.0\r\n        - pyomo:             6.4.1\r\n        - pyopencl:          2022.1.5\r\n        - pyopengl:          3.1.6\r\n        - pyopenssl:         22.1.0\r\n        - pyparsing:         3.0.9\r\n        - pyperclip:         1.8.2\r\n        - pyproj:            3.4.1\r\n        - pyproject-hooks:   1.0.0\r\n        - pypsa:             0.19.3\r\n        - pyqt5:             5.15.6\r\n        - pyqt5-qt5:         5.15.2\r\n        - pyqt5-sip:         12.10.1\r\n        - pyro4:             4.82\r\n        - pyrsistent:        0.18.1\r\n        - pysam:             0.19.1\r\n        - pyshp:             2.3.0\r\n        - pysimdjson:        3.2.0\r\n        - pysocks:           1.7.1\r\n        - pystan:            3.5.0\r\n        - pytest:            7.1.2\r\n        - python-dateutil:   2.8.2\r\n        - python-engineio:   4.3.2\r\n        - python-gettext:    4.0\r\n        - python-json-logger: 2.0.4\r\n        - python-louvain:    0.16\r\n        - python-magic:      0.4.27\r\n        - python-socketio:   5.6.0\r\n        - pythonwebhdfs:     0.2.3\r\n        - pytoml:            0.1.21\r\n        - pytomlpp:          1.0.11\r\n        - pytools:           2022.1.9\r\n        - pytorch-ignite:    0.4.10\r\n        - pytorch-lightning: 1.9.4\r\n        - pytorch3dunet:     1.3.3\r\n        - pytz:              2022.1\r\n        - pyutilib:          6.0.0\r\n        - pyutillib:         0.3.0\r\n        - pyvista:           0.38.3\r\n        - pywavelets:        1.3.0\r\n        - pyxlsb:            1.0.9\r\n        - pyyaml:            6.0\r\n        - pyzmq:             23.0.0\r\n        - qdldl:             0.1.5.post2\r\n        - qtconsole:         5.3.0\r\n        - qtpy:              2.1.0\r\n        - quantecon:         0.5.3\r\n        - querystring-parser: 1.2.4\r\n        - quilt3:            5.0.0\r\n        - rasterio:          1.3.6\r\n        - rasterstats:       0.18.0\r\n        - ratelimiter:       1.2.0.post0\r\n        - rdflib:            6.1.1\r\n        - readme-renderer:   35.0\r\n        - recommonmark:      0.7.1\r\n        - redis:             4.3.1\r\n        - rednose:           1.3.0\r\n        - regex:             2022.4.24\r\n        - reportlab:         3.6.9\r\n        - repoze.lru:        0.7\r\n        - requests:          2.28.2\r\n        - requests-futures:  1.0.0\r\n        - requests-oauthlib: 1.3.1\r\n        - requests-toolbelt: 0.9.1\r\n        - requests-unixsocket: 0.3.0\r\n        - requestsexceptions: 1.4.0\r\n        - resampy:           0.2.2\r\n        - responses:         0.18.0\r\n        - restrictedpython:  5.3a1.dev0\r\n        - retry:             0.9.2\r\n        - retrying:          1.3.3\r\n        - rfc3986:           2.0.0\r\n        - rich:              12.4.4\r\n        - rich-click:        1.5.2\r\n        - roman:             3.3\r\n        - rosbags:           0.9.11\r\n        - routes:            2.5.1\r\n        - rsa:               4.8\r\n        - rtree:             1.0.0\r\n        - ruamel.yaml:       0.17.21\r\n        - ruamel.yaml.clib:  0.2.6\r\n        - rvlib:             0.0.6\r\n        - s3transfer:        0.5.2\r\n        - salib:             1.4.5\r\n        - schema:            0.7.5\r\n        - scikit-build:      0.16.7\r\n        - scikit-fmm:        2022.3.26\r\n        - scikit-image:      0.19.2\r\n        - scikit-learn:      1.1.1\r\n        - scipy:             1.8.1\r\n        - scons:             4.4.0\r\n        - scooby:            0.7.1\r\n        - scs:               3.2.0\r\n        - seaborn:           0.11.2\r\n        - secretstorage:     3.3.2\r\n        - semver:            2.13.0\r\n        - send2trash:        1.8.0\r\n        - sentence-transformers: 2.2.0\r\n        - sentencepiece:     0.1.96\r\n        - sentry-sdk:        1.5.12\r\n        - serpent:           1.40\r\n        - setproctitle:      1.2.3\r\n        - setuptools:        58.1.0\r\n        - setuptools-scm:    6.4.2\r\n        - shap:              0.41.0\r\n        - shapely:           1.8.5.post1\r\n        - shortuuid:         1.0.9\r\n        - simplegeneric:     0.8.1\r\n        - simplejson:        3.17.6\r\n        - six:               1.16.0\r\n        - slicer:            0.0.7\r\n        - smart-open:        6.0.0\r\n        - smmap:             5.0.0\r\n        - smmap2:            3.0.1\r\n        - snakemake:         7.8.0\r\n        - sniffio:           1.2.0\r\n        - snowballstemmer:   2.2.0\r\n        - snuggs:            1.4.7\r\n        - sortedcontainers:  2.4.0\r\n        - soupsieve:         2.3.2.post1\r\n        - spaghetti:         1.6.5\r\n        - spectra:           0.0.11\r\n        - spglm:             1.0.8\r\n        - sphinx:            4.5.0\r\n        - sphinxcontrib-applehelp: 1.0.2\r\n        - sphinxcontrib-devhelp: 1.0.2\r\n        - sphinxcontrib-htmlhelp: 2.0.0\r\n        - sphinxcontrib-jsmath: 1.0.1\r\n        - sphinxcontrib-qthelp: 1.0.3\r\n        - sphinxcontrib-serializinghtml: 1.1.5\r\n        - sphinxcontrib-websupport: 1.2.4\r\n        - spint:             1.0.7\r\n        - spreg:             1.2.4\r\n        - spvcm:             0.3.0\r\n        - sqlalchemy:        1.4.37\r\n        - sqlparse:          0.4.2\r\n        - stack-data:        0.2.0\r\n        - staticmap:         0.5.5\r\n        - statsd:            3.3.0\r\n        - statsmodels:       0.13.2\r\n        - stevedore:         3.5.0\r\n        - stopit:            1.1.2\r\n        - subprocess32:      3.5.4\r\n        - superqt:           0.4.1\r\n        - svg.path:          6.0\r\n        - sympy:             1.10.1\r\n        - tables:            3.7.0\r\n        - tabulate:          0.8.9\r\n        - tasklogger:        1.1.2\r\n        - tblib:             1.7.0\r\n        - tempita:           0.5.2\r\n        - tenacity:          8.0.1\r\n        - tensorboard:       2.9.0\r\n        - tensorboard-data-server: 0.6.1\r\n        - tensorboard-plugin-wit: 1.8.1\r\n        - tensorboardx:      2.5\r\n        - tensorflow-estimator: 2.9.0\r\n        - tensorflow-gpu:    2.9.1\r\n        - tensorflow-io-gcs-filesystem: 0.26.0\r\n        - termcolor:         1.1.0\r\n        - terminado:         0.15.0\r\n        - terminaltables:    3.1.10\r\n        - termstyle:         0.1.11\r\n        - testpath:          0.6.0\r\n        - testresources:     2.0.1\r\n        - texttable:         1.6.4\r\n        - theano:            1.0.5\r\n        - theano-pymc:       1.1.2\r\n        - threadpoolctl:     3.1.0\r\n        - tifffile:          2022.5.4\r\n        - timezonefinder:    6.0.0\r\n        - tinycss2:          1.1.1\r\n        - tokenizers:        0.12.1\r\n        - toml:              0.10.2\r\n        - tomli:             2.0.1\r\n        - tomli-w:           1.0.0\r\n        - tomlkit:           0.11.0\r\n        - toolz:             0.11.2\r\n        - toposort:          1.7\r\n        - torch:             1.11.0+cu113\r\n        - torch-cluster:     1.6.0\r\n        - torch-fidelity:    0.3.0\r\n        - torch-geometric:   2.0.4\r\n        - torch-scatter:     2.0.9\r\n        - torch-sparse:      0.6.13\r\n        - torch-spline-conv: 1.2.1\r\n        - torchaudio:        0.11.0+cu113\r\n        - torchmetrics:      0.11.3\r\n        - torchvision:       0.12.0+cu113\r\n        - tornado:           6.1\r\n        - tqdm:              4.64.0\r\n        - traitlets:         5.2.1.post0\r\n        - traits:            6.3.2\r\n        - traitsui:          7.3.1\r\n        - transaction:       3.0.1\r\n        - transformers:      4.19.2\r\n        - trimesh:           3.12.5\r\n        - twine:             4.0.1\r\n        - typeguard:         2.13.3\r\n        - typer:             0.7.0\r\n        - typeshed-client:   2.2.0\r\n        - typing-extensions: 4.2.0\r\n        - urllib3:           1.26.9\r\n        - utm:               0.7.0\r\n        - velocyto:          0.17.17\r\n        - vine:              5.0.0\r\n        - vispy:             0.11.0\r\n        - vtk:               9.2.6\r\n        - waitress:          2.1.1\r\n        - wandb:             0.12.17\r\n        - wcwidth:           0.2.5\r\n        - webargs:           8.2.0\r\n        - webencodings:      0.5.1\r\n        - webob:             1.8.7\r\n        - websocket:         0.2.1\r\n        - websocket-client:  1.3.2\r\n        - websockets:        10.4\r\n        - webtest:           3.0.0\r\n        - werkzeug:          2.1.2\r\n        - wget:              3.2\r\n        - wheel:             0.37.1\r\n        - widgetsnbextension: 3.6.0\r\n        - wntr:              0.4.1\r\n        - wrapt:             1.14.1\r\n        - wsgiproxy2:        0.5.1\r\n        - wsme:              0.11.0\r\n        - xarray:            2022.3.0\r\n        - xarray-einstats:   0.2.2\r\n        - xlrd:              2.0.1\r\n        - xlsxwriter:        3.0.3\r\n        - xlwt:              1.3.0\r\n        - xmlrunner:         1.7.7\r\n        - xopen:             1.5.0\r\n        - xxhash:            3.0.0\r\n        - xyzservices:       2022.4.0\r\n        - yacs:              0.1.8\r\n        - yappi:             1.3.5\r\n        - yarl:              1.7.2\r\n        - yaspin:            2.1.0\r\n        - yte:               1.4.0\r\n        - z3c.pt:            3.3.1\r\n        - zc.lockfile:       2.0\r\n        - zconfig:           3.6.0\r\n        - zexceptions:       4.2\r\n        - zict:              2.2.0\r\n        - zipp:              3.8.0\r\n        - zodb:              5.7.0\r\n        - zodbpickle:        2.3\r\n        - zope:              5.5.1\r\n        - zope.annotation:   4.7.0\r\n        - zope.browser:      2.4\r\n        - zope.browsermenu:  4.4\r\n        - zope.browserpage:  4.4.0\r\n        - zope.browserresource: 4.4\r\n        - zope.cachedescriptors: 4.3.1\r\n        - zope.component:    5.0.1\r\n        - zope.configuration: 4.4.1\r\n        - zope.container:    4.5.0\r\n        - zope.contentprovider: 4.2.1\r\n        - zope.contenttype:  4.5.0\r\n        - zope.datetime:     4.3.0\r\n        - zope.deferredimport: 4.4\r\n        - zope.deprecation:  4.4.0\r\n        - zope.dottedname:   4.3\r\n        - zope.event:        4.5.0\r\n        - zope.exceptions:   4.5\r\n        - zope.filerepresentation: 5.0.0\r\n        - zope.globalrequest: 1.5\r\n        - zope.hookable:     5.1.0\r\n        - zope.i18n:         4.9.0\r\n        - zope.i18nmessageid: 5.0.1\r\n        - zope.interface:    5.4.0\r\n        - zope.lifecycleevent: 4.4\r\n        - zope.location:     4.2\r\n        - zope.pagetemplate: 4.6.0\r\n        - zope.processlifetime: 2.3.0\r\n        - zope.proxy:        4.5.0\r\n        - zope.ptresource:   4.3.0\r\n        - zope.publisher:    6.1.0\r\n        - zope.schema:       6.2.0\r\n        - zope.security:     5.3\r\n        - zope.sequencesort: 4.2\r\n        - zope.site:         4.5.0\r\n        - zope.size:         4.3\r\n        - zope.structuredtext: 4.4\r\n        - zope.tal:          4.5\r\n        - zope.tales:        5.1\r\n        - zope.testbrowser:  5.6.1\r\n        - zope.testing:      4.10\r\n        - zope.traversing:   4.4.1\r\n        - zope.viewlet:      4.3\r\n        - zstandard:         0.17.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         \r\n        - python:            3.10.4\r\n        - version:           #1 SMP Tue Nov 8 15:48:59 UTC 2022\r\n```\r\n\r\n</details>\r\n\r\n\r\n### More info\r\n\r\n_No response_\n\ncc @carmocca @mauvilsa",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/16970/comments",
    "author": "leopold-franz",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2023-03-15T00:28:58Z",
        "body": "Hey, I think the problem is that these keys in the config.yaml are not allowed:\r\n\r\n```\r\nseed_everything_default: null\r\nlog_dir: /cluster/dir/to/log\r\n```\r\n\r\nThey don't match anything in the Trainer. \r\n\r\nPerhaps it should be \r\n```\r\nseed_everything: false\r\ntrainer:\r\n    default_root_dir:  \"/cluster/dir/to/log\"\r\n    ...\r\n```\n\n---\n\nHi\r\n\r\nI tried to help here, did you find what the problem was? Please let me know."
      },
      {
        "user": "stale[bot]",
        "created_at": "2023-04-14T06:23:47Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions - the Lightning Team!\n"
      },
      {
        "user": "leopold-franz",
        "created_at": "2023-04-14T09:10:22Z",
        "body": "Yes sorry I forgot to answer. I somehow messed up a lot of the key settings, so you were right. Thank you for your help"
      },
      {
        "user": "awaelchli",
        "created_at": "2023-04-14T09:13:00Z",
        "body": "Thanks for confirming that it worked. Happy this was helpful."
      }
    ],
    "satisfaction_conditions": [
      "Correct configuration of PyTorch Lightning CLI parameters in YAML file",
      "Proper handling of PyTorch Lightning trainer configuration",
      "Clear validation of YAML configuration structure"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:59:24"
    }
  },
  {
    "number": 15973,
    "title": "access to the last training epoch that triggered early stopping",
    "created_at": "2022-12-09T01:07:08Z",
    "closed_at": "2022-12-14T02:50:04Z",
    "labels": [
      "question",
      "callback: early stopping"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/15973",
    "body": "### Description & Motivation\r\n\r\nI wish there was a method to access when the last training epoch number was. The max_training_epoch differs from the last training epoch resulting from the early stopping.\n\ncc @borda @carmocca @awaelchli",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/15973/comments",
    "author": "jaeho3690",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-12-13T04:38:37Z",
        "body": "Hi @jaeho3690 \r\nIs this what you are looking for?\r\n\r\n```py\r\nprint(trainer.early_stopping_callback.stopped_epoch)\r\n\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Provides access to the exact epoch number where early stopping was triggered",
      "Works with PyTorch Lightning's Trainer and EarlyStopping callback system",
      "Does not require modifying early stopping logic itself",
      "Retrieves the value programmatically for potential logging/analysis"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-04 23:59:50"
    }
  },
  {
    "number": 13360,
    "title": "Using Dataloader in pytorch_lightning when using DDP training?",
    "created_at": "2022-06-22T06:51:21Z",
    "closed_at": "2022-08-01T14:32:41Z",
    "labels": [
      "question",
      "won't fix",
      "distributed",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/13360",
    "body": "My computer has 2 gpus and I have some problems:\r\n1. First, I used Pytorch\r\nI have 10 images, I created distributed dataloader (using sampler) follow Pytorch instruction, batchsize = 4, gpu=2.\r\n=> So with each gpu, length of batch0 is 4 and length of batch1 is 4. Sampler added 2 to batch1 to make batchsize = 4.\r\nI trained with ddp_spawn with pytorch code and everything is ok.\r\n\r\n2. Next, I used Pytorch Lightning\r\nI also use 10 images, I created dataloader (Normal Dataloader) follow Pytorch Instruction, batchsize =4, gpu=2\r\n=> so with each gpu, length of batch0 is 4 and length of batch1 is 1.\r\n\r\nNow, I want to use pytorch lightning but I want batchsize=4 same distributed sampler when working with pytorchlightning. How should I do?\r\n\r\nThanks\n\ncc @awaelchli @rohitgr7 @akihironitta @justusschock @ninginthecloud @otaj",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/13360/comments",
    "author": "NguyenDuyDuc1491995",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2022-06-22T10:22:49Z",
        "body": "> So with each gpu, length of batch0 is 4 and length of batch1 is 4. Sampler added 2 to batch1 to make batchsize = 4.\r\n\r\nthat makes total batches to be processed = 16, which seems incorrect.\r\nPL uses distributed sampler internally so it should be `batch0=4` and `batch1=1` on each GPU."
      },
      {
        "user": "NguyenDuyDuc1491995",
        "created_at": "2022-06-22T16:50:56Z",
        "body": "The problem is when I train semantic segmentation DeepLabV3 by pytorchlightning\r\nBatch0 =4 is ok, but batch1=1 is error. Because the batch is 1 so there is a problem with batchnorm.\r\n\r\nBut I used distributed dataloader with sampler in pytorch and I saw it will create batch0=4 and batch1=4 ( it will take 3 more images) so I wonder if I can create a distributed dataloader as same as above in pytorch lightning?"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-06-22T17:21:49Z",
        "body": "The distributed sampler has nothing to do with batching. It only ensures that each GPU gets the same amount of samples and since your dataset of 10 images is divisible by 2, that's never a problem. \r\n\r\nTo avoid uneven batch sizes, just set `drop_last=True` in the dataloader and then you are guaranteed to get the batch size 4 for each batch. This is the same in PyTorch and PL, there should be no difference. "
      },
      {
        "user": "NguyenDuyDuc1491995",
        "created_at": "2022-06-24T07:05:37Z",
        "body": "ok, thanks"
      },
      {
        "user": "stale[bot]",
        "created_at": "2022-07-31T20:25:28Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, PyTorch Lightning Team!\n"
      },
      {
        "user": "awaelchli",
        "created_at": "2022-08-01T14:32:41Z",
        "body": "I hope the question was answered and the issues could be solved. Let us know if there is anything else causing trouble."
      }
    ],
    "satisfaction_conditions": [
      "Ensures consistent batch sizes across GPUs when using DDP in PyTorch Lightning",
      "Handles dataset sizes not perfectly divisible by (batch_size * num_gpus)",
      "Maintains compatibility with PyTorch Lightning's automatic distributed training setup",
      "Avoids BatchNorm layer errors caused by small batch sizes"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:00:01"
    }
  },
  {
    "number": 8559,
    "title": "Wandblogger not logging train loss after every step",
    "created_at": "2021-07-26T13:30:21Z",
    "closed_at": "2021-07-26T14:30:23Z",
    "labels": [
      "help wanted",
      "question",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8559",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI am using wandb with Pytorch Lightning. I am logging train/loss, val/loss, train/metric, val/metric. Everything is logged properly to wandb dashboard except the **train/loss** (after every step).\r\n\r\nHere's the main lightning module:\r\n\r\n`class ImageClassification(pl.LightningModule):\r\n    def __init__(self, model):\r\n        super().__init__()\r\n        self.model = model\r\n        self.criterion = nn.BCEWithLogitsLoss()\r\n        self.lr = CFG['lr']\r\n    \r\n    def forward(self, x):\r\n        output = self.model(x)\r\n        return output\r\n    \r\n    def configure_optimizers(self):\r\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=CFG['lr'])\r\n        return self.optimizer\r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        images, targets = batch['image'], batch['target']\r\n        output = self.model(images)\r\n        loss = self.criterion(output.view(-1), targets)\r\n        \r\n        self.log('train/loss', loss, logger=True)  # the thing that is not being logged\r\n\r\n        try:\r\n            auc = roc_auc_score(targets.detach().cpu(), output.sigmoid().detach().cpu())\r\n            self.log(\"train/auc\", auc, prog_bar=True, logger=True)\r\n        except:\r\n            pass\r\n        \r\n        return {\r\n            \"loss\": loss,\r\n            \"predictions\": output,\r\n            \"targets\" : targets\r\n        }\r\n    \r\n    def training_epoch_end(self, outputs):\r\n\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets)\r\n        preds = torch.stack(preds)\r\n\r\n        train_auc = roc_auc_score(targets.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log(\"train/auc_epoch\", train_auc,logger=True)\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        images, targets = batch['image'], batch['target']\r\n        output = self.model(images)\r\n        loss = self.criterion(output.view(-1), targets)\r\n\r\n        self.log('val/loss', loss,prog_bar=True, logger=True)\r\n\r\n        return {\r\n            \"predictions\": output,\r\n            \"targets\": targets\r\n        }\r\n    \r\n    def validation_epoch_end(self, outputs):\r\n\r\n        preds = []\r\n        targets = []\r\n\r\n        for output in outputs:\r\n            preds += output['predictions']\r\n            targets += output['targets']\r\n        \r\n        targets = torch.stack(targets)\r\n        preds = torch.stack(preds)\r\n    \r\n        val_auc = roc_auc_score(targets.detach().cpu(), preds.sigmoid().detach().cpu())\r\n        self.log(\"val/auc_epoch\", val_auc,prog_bar=True,logger=True)\r\n    \r\n    def test_step(self, batch, batch_idx):\r\n        images = batch['image']\r\n        output = self.model(images)\r\n        return output`",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8559/comments",
    "author": "Gladiator07",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-07-26T14:23:47Z",
        "body": "is the logging interval small enough? I.e., `Trainer(log_every_n_step=n)` where n must be smaller than `len(dataloader)`. \n\n---\n\n```python\r\ntry:\r\n        auc = roc_auc_score(targets.detach().cpu(), output.sigmoid().detach().cpu())\r\n        self.log(\"train/auc\", auc, prog_bar=True, logger=True)\r\n    except:\r\n        pass\r\n```\r\n\r\nIf an exception raises here, it will be silently ignored and nothing gets logged. Please try to remove the try-except block here and see if it raises an exception."
      },
      {
        "user": "Gladiator07",
        "created_at": "2021-07-26T14:30:23Z",
        "body": "Thanks, changing n to 1 in `Trainer(log_every_n_step=n)` solved my problem. I was testing my code with smaller data so it couldn't log as you said. I am closing this issue."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-07-26T14:32:52Z",
        "body": "Happy to help. Glad you found the issue. Lightning should also print a warning in that case, you may have missed it or you are not on the latest version. \r\ncheers"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how logging frequency settings interact with dataset size",
      "Verification of logging configuration compatibility with training setup"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:00:52"
    }
  },
  {
    "number": 6104,
    "title": "Early stopping on custom metric without validation_step",
    "created_at": "2021-02-20T16:19:44Z",
    "closed_at": "2021-02-23T18:18:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6104",
    "body": "#### What is your question?\r\n\r\nI have a metric that I can only define using every predictions on the validation split, so I cannot use `validation_step` since it only operates on batches of data.\r\nI have a callback that computes and log this metric in `on_train_epoch_end`. \r\nI am not executing the validation loop because it's useless in my case.\r\nMy question is: How can I properly use the EarlyStopping callback ? (Same question for ModelCheckpoint)\r\n\r\n#### What have you tried?\r\nI have tried manually calling `pl_module.on_validation_epoch_end()` in my callback but it doesn't seem to work because EarlyStopping never stops the model even though the patience should have dropped to 0.\r\n\r\n#### What's your environment?\r\n\r\n - OS: Kubuntu 20.04\r\n - Packaging: pip\r\n - Version: 1.1.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6104/comments",
    "author": "Inspirateur",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2021-02-21T00:37:08Z",
        "body": "If I understood you correctly, you just need to make sure to create your instance as:\r\n\r\n`EarlyStopping(monitor=\"your_metric\")`\r\n\r\nAnd then, in your LightningModule's `on_validation_epoch_end` do `self.log(\"your_metric\", value)`"
      },
      {
        "user": "Inspirateur",
        "created_at": "2021-02-23T18:18:41Z",
        "body": "Seems like that works, thank you.\r\nI just had to define an empty `validation_step` method in my lightning module so the fake validation would be quick."
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to log custom metrics during training without executing full validation loop",
      "Integration with PyTorch Lightning's callback system for monitoring",
      "Validation phase triggering without actual validation computations",
      "Metric availability for epoch-level decision making"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:01:21"
    }
  },
  {
    "number": 5636,
    "title": "Understanding accumulate_grad_batches  parameter? ",
    "created_at": "2021-01-24T10:10:44Z",
    "closed_at": "2021-01-24T18:19:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5636",
    "body": "I am very new to PL. As far as I understand **accumulate_grad_batches** works similar to  **'gradient_accumulation_steps'** , where the main task is to increase the effective batch size.  But I do not see any change in training epoch step count when increasing the  **accumulate_grad_batches** parameters.\r\n\r\nLet's say, I have a dataset of 1000 examples and my batch_size is one and I only use a single GPU. So in this case, if I use the value 2  for the **accumulate_grad_batches**,  the number of steps for an epoch should be shown as 500 (logger). But I still see 1000.\r\n\r\nIs it a bug or PL doesn't divide the number of steps when showing in the log?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5636/comments",
    "author": "shamanez",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-24T13:16:39Z",
        "body": "total step count will remain the same since it refers to total batches, but internally `optimizer/scheduler.step` is updated accordingly. You can check `self.global_step` with and without accumulation."
      },
      {
        "user": "tchaton",
        "created_at": "2021-01-24T18:19:18Z",
        "body": "Hey @shamanez,\r\n\r\nI believe @rohitgr7 properly answered your question.\r\nI will close this issue. Feel free to re-open it if something is missing.\r\n\r\nBest,\r\nT.C\r\n"
      },
      {
        "user": "shamanez",
        "created_at": "2021-01-24T20:35:29Z",
        "body": "Thanks a lot.sorted!"
      }
    ],
    "satisfaction_conditions": [
      "Clarify the relationship between gradient accumulation and logged step counts",
      "Explain how gradient accumulation impacts optimizer updates vs batch processing",
      "Provide a method to verify gradient accumulation is working as intended"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:01:28"
    }
  },
  {
    "number": 5597,
    "title": "outputs of training_epoch_end for different configure_optimizers conditions ",
    "created_at": "2021-01-21T04:54:50Z",
    "closed_at": "2021-01-22T03:26:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5597",
    "body": "> **Condition One:**\r\n> when I write optimizer as follows:\r\n\r\n```\r\ndef configure_optimizers(self):\r\n  return [disOptim,genOptim],[] \r\n```\r\n\r\n> I can simply write the training_epoch_end as follows: \r\n\r\n```\r\ndef training_epoch_end(self,outputs):\r\n  sum_loss_D_real=torch.stack([x['D_loss_real'] for x in outputs[0]]).sum()\r\n```\r\n\r\n> **Condition Two:**\r\n> However when I write the optimizer as follows:\r\n\r\n```\r\ndef configure_optimizers(self):\r\n  return ({'optimizer':disOptim,'frequency':2},{'optimizer':genOptim,'frequency':1})\r\n```\r\n\r\n> I need to write the training_epoch_end as follows:\r\n\r\n```\r\ndef training_epoch_end(self,outputs):\r\n  mean_d_loss=torch.stack([outputs[i]['d_loss'] for i in range(len(outputs)) if ((i+1)%(self.hparams.n_critic+1))]).mean()\r\n```\r\n\r\n> Is there any way that I can write the optimizer the same as condition two and the training_epoch_end as condition one?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5597/comments",
    "author": "Bajo1994",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-01-21T17:52:34Z",
        "body": "I don't think it's possible since in both the cases the behaviour of `training_step` will be different since you are passing optimizer frequencies in the second case, so expected outputs will be in different format.\r\n\r\nbut you can still simply it a bit. Try:\r\n```python\r\nx=torch.stack([outputs[i].get('d_loss', torch.tensor(float('nan'))) for i in range(len(outputs))])\r\nmean_d_loss = x[x==x].mean()\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how optimizer frequency configuration affects outputs structure",
      "Method to access optimizer-specific outputs without manual index calculations",
      "Consistent output structure across different optimizer configurations",
      "Pattern for aggregating losses from multiple optimizers with different execution frequencies"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:01:35"
    }
  },
  {
    "number": 5552,
    "title": "How to iterate over training set AGAIN on training epoch end?",
    "created_at": "2021-01-18T07:57:20Z",
    "closed_at": "2021-01-19T11:09:43Z",
    "labels": [
      "question",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5552",
    "body": "## \u2753 Questions and Help\r\n\r\n<!-- If you still can't find what you need: -->\r\n\r\n#### What is your question?\r\n\r\nHow to iterate over training set AGAIN on training epoch end or on validation epoch start?\r\nI have a model that works as expected on MNIST, but for my unique data, val_loss<train_loss for all samples.\r\nI have no idea what causes this, and it is too suspicious to allow me to go on.\r\n\r\n*I want to do a validation step on training data, in eval mode*\r\n\r\nI hope it will ease my mind.\r\n\r\n#### Code\r\nWell, if I had code for how to correctly do this I wouldn't ask :)\r\n\r\n#### What have you tried?\r\nThis doesn't sound like a standard use case, not even sure that's supported.\r\n\r\n#### What's your environment?\r\n\r\n - OS: [e.g. iOS, Linux, Win] Win\r\n - Packaging [e.g. pip, conda] Pip\r\n - Version [e.g. 0.5.2.1] 1.1.4\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5552/comments",
    "author": "noamzilo",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-18T08:33:00Z",
        "body": "Hey @noamzilo,\r\n\r\nIf you have a `val_dataloader`, the entire dataset of validation will be used once the latest batch of your train_dataset would be reached.\r\n\r\nTherefore, you can compare your metrics at this point in epoch_end hooks.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "awaelchli",
        "created_at": "2021-01-18T08:37:18Z",
        "body": "It is not an accident that PyTorch is in the name of PyTorchLightning :)) \r\nYou can just do it as you would in PyTorch:\r\n\r\n```python\r\nself.eval()\r\nfor idx, batch in enumerate(self.train_dataloader()):\r\n    # do what you have to do\r\n```\r\n\r\n"
      },
      {
        "user": "noamzilo",
        "created_at": "2021-01-19T11:09:43Z",
        "body": "> \r\n> \r\n> It is not an accident that PyTorch is in the name of PyTorchLightning :))\r\n> You can just do it as you would in PyTorch:\r\n> \r\n> ```python\r\n> self.eval()\r\n> for idx, batch in enumerate(self.train_dataloader()):\r\n>     # do what you have to do\r\n> ```\r\n\r\nThanks :) \r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Ability to perform validation-style evaluation on training data within the existing training workflow",
      "Mechanism to access training data loader after training epoch completion",
      "Support for manual model mode switching (train/eval)",
      "Integration with PyTorch Lightning's training loop structure"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:01:45"
    }
  },
  {
    "number": 5550,
    "title": "on_train_epoch_end vs training_epoch_end",
    "created_at": "2021-01-17T21:37:32Z",
    "closed_at": "2021-01-18T16:55:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5550",
    "body": "## \u2753 Questions and Help\r\n\r\nWhat is the difference between on_train_epoch_end and training_epoch_end? For what applications should we use each?\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5550/comments",
    "author": "Bajo1994",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-01-18T08:45:53Z",
        "body": "Hey @Bajo1994,\r\n\r\n* `training_epoch_end` will be used for the user to aggregate the outputs from training_step at the end of an epoch.\r\n\r\nExample.\r\n```\r\n    def training_step(self, batch, batch_idx):\r\n        output = self.layer(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n```\r\n\r\n* on_train_epoch_end is a hook. It would be used to add extra logic to control the behaviour of the model.\r\nBut it is left to the user to choice how he wants to use it :)\r\n\r\nI hope it helps !\r\n\r\nBest,\r\nT.C \r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Clear distinction between the purpose of training_epoch_end vs on_train_epoch_end",
      "Explanation of when to use each method based on their intended functionality",
      "Description of training_epoch_end's role in processing training_step outputs",
      "Clarification that on_train_epoch_end is a customizable hook for extended behavior",
      "Differentiation between output processing vs general epoch-end logic"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:01:49"
    }
  },
  {
    "number": 5166,
    "title": "How to compute metric in each step without reseting the metric",
    "created_at": "2020-12-17T08:47:01Z",
    "closed_at": "2021-01-13T09:05:03Z",
    "labels": [
      "feature",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5166",
    "body": "\r\nI want to know a metric during the process of an epoch. That is, I want to know the metric value until now. However, if I call `metric.compute()` in each step, it will reset my metric, which is not expected. I read the source code and didn't find a clever way to do that. So could you tell me how to do that?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5166/comments",
    "author": "Adoni",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2021-01-13T09:05:01Z",
        "body": "Fixed by #5409. Closing this now."
      },
      {
        "user": "Adoni",
        "created_at": "2021-01-13T10:00:11Z",
        "body": "> Fixed by #5409. Closing this now.\r\n\r\nThanks"
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to track metric values incrementally during training steps without resetting accumulated state",
      "Preservation of metric state between compute calls within an epoch",
      "Access to intermediate metric values during epoch progression"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:01:56"
    }
  },
  {
    "number": 5129,
    "title": "How to forbid save optimizer's state \uff1f",
    "created_at": "2020-12-14T15:12:33Z",
    "closed_at": "2020-12-15T02:59:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5129",
    "body": "Sometime's it's time-consuming.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5129/comments",
    "author": "Maybewuss",
    "comments": [
      {
        "user": "DuinoDu",
        "created_at": "2020-12-14T15:47:55Z",
        "body": "Set `save_weights_only=True` in ModelCheckpoint."
      },
      {
        "user": "Maybewuss",
        "created_at": "2020-12-15T02:59:44Z",
        "body": "> Set `save_weights_only=True` in ModelCheckpoint.\r\n\r\nThanks !"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to save model checkpoints without persisting the optimizer's state",
      "Maintains core model preservation functionality while removing non-essential components"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:02:05"
    }
  },
  {
    "number": 4940,
    "title": "typeError unexpected closure",
    "created_at": "2020-12-01T21:24:10Z",
    "closed_at": "2020-12-02T11:21:45Z",
    "labels": [
      "question",
      "waiting on author"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4940",
    "body": "## \u2753 Questions and Help\r\n\r\nI am using pytorch 1.6 and pytorch lightning 1.0.8.\r\n\r\nBefore I upgrade my pytorch-lighing from 1.0.2 to 1.0.8, everything works fine, today I upgrade to 1.0.8 to try some new metrics, but got this error. \r\n\r\n    main()\r\n  File \"main.py\", line 68, in main\r\n    trainer.fit(model, trainloader, evalloader)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 444, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 63, in train\r\n    results = self.train_or_test()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 74, in train_or_test\r\n    results = self.trainer.train()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 493, in train\r\n    self.train_loop.run_training_epoch()\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 561, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 728, in run_training_batch\r\n    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 470, in optimizer_step\r\n    optimizer, batch_idx, opt_idx, train_step_and_backward_closure\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 130, in optimizer_step\r\n    using_lbfgs=is_lbfgs\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1270, in optimizer_step\r\n    optimizer.step(closure=optimizer_closure)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\r\n    return func(*args, **kwargs)\r\nTypeError: step() got an unexpected keyword argument 'closure'\r\n(py3) yikuan@deepmedicine:~/project/version_control/HiBEHRT-BYOL$ TypeError: step() got an unexpected keyword argument 'closure'",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4940/comments",
    "author": "yikuanli",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-01T21:25:02Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-12-02T08:48:19Z",
        "body": "Which optimizer and scheduler are you using? If you are using a custom optimizer, you need to update the code of the ```step()``` function to take in a closure. PyTorch optimizer class implements the step method with the closure parameter in the latest version."
      },
      {
        "user": "yikuanli",
        "created_at": "2020-12-02T11:17:52Z",
        "body": "thanks, I now upgrade it to the latest version and seems the latest bolts solve the problem."
      },
      {
        "user": "ananyahjha93",
        "created_at": "2020-12-02T11:19:44Z",
        "body": "Oh, if you were using the LARS wrapper form bolts, then the latest version has the step method taking in a closure. That should solve it.\r\n\r\nShould I close this issue then?"
      },
      {
        "user": "yikuanli",
        "created_at": "2020-12-02T11:21:16Z",
        "body": "yes, thanks a lot"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the 'closure' parameter is required in the optimizer's step() method after upgrading PyTorch Lightning",
      "Guidance on ensuring custom optimizers/schedulers comply with PyTorch Lightning's expected interface",
      "Identification of version compatibility between PyTorch Lightning and optimizer dependencies",
      "Clarification on how PyTorch Lightning handles optimizer closures internally"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:02:16"
    }
  },
  {
    "number": 4874,
    "title": "Metric Reset",
    "created_at": "2020-11-26T17:37:30Z",
    "closed_at": "2020-11-28T22:40:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4874",
    "body": "How can I manually reset a metric? \r\nOr metric states are reset to default values after calling the `compute()` method?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4874/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-11-27T06:12:39Z",
        "body": "`metric.compute()` resets the state. This is usually done after an epoch."
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-11-28T22:40:39Z",
        "body": "> `metric.compute()` resets the state. This is usually done after an epoch.\r\n\r\nOk, thank you."
      }
    ],
    "satisfaction_conditions": [
      "Clarifies whether `compute()` automatically resets metric states",
      "Explains the relationship between metric computation and state management"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:02:23"
    }
  },
  {
    "number": 4711,
    "title": "How to monitor more than one quantity?",
    "created_at": "2020-11-17T12:41:15Z",
    "closed_at": "2020-11-18T00:18:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4711",
    "body": "What i do if i want to monitor more than one quantity?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4711/comments",
    "author": "zhhao1",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-11-17T13:59:23Z",
        "body": "You can pass multiple `ModelCheckpoint` callbacks to the trainer callback list\r\n\r\n```python\r\nTrainer(callbacks=[ModelCheckpoint(monitor=\"a\"), ModelCheckpoint(monitor=\"b\")])\r\n```\r\n\r\nhowever, this is not fully supported and the saved checkpoints will contain the state for only one of the `ModelCheckpoint`s\r\n\r\nDuplicate of #2908"
      }
    ],
    "satisfaction_conditions": [
      "Supports monitoring multiple distinct metrics/quantities simultaneously",
      "Preserves checkpoint data for all monitored metrics",
      "Works within the Trainer callback architecture",
      "Avoids conflicting checkpoint states"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:02:31"
    }
  },
  {
    "number": 4607,
    "title": "How to change the Datamodule during training with a callback?",
    "created_at": "2020-11-10T15:59:21Z",
    "closed_at": "2020-12-13T17:55:10Z",
    "labels": [
      "question",
      "won't fix",
      "data handling"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4607",
    "body": "#### What is your question?\r\nHow to change the Datamodule during training with a callback?\r\nMore details:\r\nI am looking for a way to reinitialized my Datamodule with different parameter, I am currently sending the height of my images as argument to my datamodule and I want to change this height at some point during training, the simple way is to call trainer.fit multiple times with different datamodules, but I am wondering is there a way to do this on callback, in the same way as you do when you change the optimizer or lr_scheduler?\r\n\r\n\r\nSomething similar to this:\r\n```\r\ndef on_train_epoch_start(self, trainer, pl_module):\r\n            sch = optim.lr_scheduler.StepLR(optimizer, 1, 0.96)\r\n            scheduler = {\r\n                'scheduler': sch,\r\n                'interval': interval,  # or 'step'\r\n                'monitor': 'train_loss',\r\n                'reduce_on_plateau': False,\r\n                'frequency': 1,\r\n            }\r\n            trainer.lr_schedulers = trainer.configure_schedulers([scheduler])\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4607/comments",
    "author": "MohammedAljahdali",
    "comments": [
      {
        "user": "stale[bot]",
        "created_at": "2020-12-10T23:18:51Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "MohammedAljahdali",
        "created_at": "2020-12-13T17:55:10Z",
        "body": "Answer from @teddykoker:\r\n \r\n\r\n> I have done this using a callback:\r\n> ```\r\n> class Scheduler(pl.Callback):\r\n>     def _prepare_epoch(self, trainer, model, epoch):\r\n>         phase = ... \r\n>         trainer.datamodule.set_phase(phase)\r\n> \r\n>     def on_epoch_end(self, trainer, model):\r\n>         self._prepare_epoch(trainer, model, trainer.current_epoch + 1)\r\n> \r\n> class Data(pl.LightningDataModule):\r\n>     def set_phase(self, phase: dict):\r\n>         self.size = phase.get(\"size\", self.size)\r\n>         train_transforms = T.Compose(\r\n>             [\r\n>                 T.RandomResizedCrop(self.size, scale=(self.min_scale, 1.0)),\r\n>                 T.RandomHorizontalFlip(),\r\n>                 T.ToTensor(),\r\n>                 normalize,\r\n>             ]\r\n>         )\r\n>         self.train_ds = ImageFolder(self.train_dir, transform=train_transforms)\r\n> \r\n>        \r\n>     def train_dataloader(self):\r\n>         train_dl = DataLoader(\r\n>             self.train_ds,\r\n>             batch_size=self.batch_size,\r\n>             shuffle=True,\r\n>             num_workers=self.num_workers,\r\n>             pin_memory=True,\r\n>         )\r\n>         return train_dl\r\n> ```\r\n> Its important to note:\r\n> \r\n> 1. You can access your datamodule from a callback using trainer.datamodule\r\n> 2. In order to have train_dataloader(), val_dataloader() called every epoch, you must set reload_dataloaders_every_epoch=True in your trainer.\r\n\r\nThank you @teddykoker for the help. "
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates dynamic modification of DataModule parameters during training",
      "Integrates with PyTorch Lightning's callback system",
      "Ensures dataloaders refresh with new parameters",
      "Maintains continuous training session without multiple fit() calls",
      "Provides access to modify DataModule from within training workflow"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:03:05"
    }
  },
  {
    "number": 4604,
    "title": "How to load to from a checkpoint to same device when pretrained encoder was used",
    "created_at": "2020-11-10T14:35:57Z",
    "closed_at": "2020-11-12T12:17:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4604",
    "body": "## \u2753 Questions and Help \r\n\r\nI implemented a `ClassificationNet` (see below) that's using a pretrained encoder. After training, I'm trying to load it to CPU using `ClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\")`, but since `map_location` in `get_encoder` is `None`, the encoder tries to load to GPU. How can I inform `get_encoder` to load to the same `map_location`? \r\nSince I just started using Lightning, I guess there's a much smarter way to circumvent this situation altogether -- I look forward to your suggestions :) Thanks!\r\n\r\n#### Code\r\n``` python\r\nclass ClassificationNet(LightningModule):\r\n    ...\r\n    self.encoder = get_encoder(pretrained=True)\r\n\r\nget_encoder(pretrained=False, map_location=None):\r\n    model = FancyModel()\r\n    if pretrained:\r\n        ckpt_data = torch.utils.model_zoo.load_url(url, map_location=map_location)\r\n    ....\r\n```\r\n\r\n - OS: Manjaro Linux\r\n - Version 1.0.5",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4604/comments",
    "author": "hakanyi",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-11-10T14:36:47Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-11-11T20:50:04Z",
        "body": "one solution can be having an additional argument for it.\r\n```python\r\nclass ClassificationNet(LightningModule):\r\n    def __init__(self, encoder_map_location, ...):\r\n        self.encoder = get_encoder(pretrained=True, map_location=encoder_map_location)\r\n```\r\n```python\r\nClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\"), encoder_map_location=torch.device(\"cpu\"))\r\n```\r\n\r\nor maybe without an additional argument. Not sure about this one though. But should work.\r\n```python\r\nclass ClassificationNet(LightningModule):\r\n    def __init__(self, ...):\r\n        ....\r\n    \r\n    def setup(self, stage):\r\n        self.encoder = get_encoder(pretrained=True, map_location=self.device)\r\n```\r\n```python\r\nClassificationNet.load_from_checkpoint(pth, map_location=torch.device(\"cpu\"))\r\n```"
      },
      {
        "user": "hakanyi",
        "created_at": "2020-11-12T12:17:41Z",
        "body": "Thanks @rohitgr7! The second solution works indeed and does the job just perfectly :)  "
      }
    ],
    "satisfaction_conditions": [
      "Solution must synchronize encoder's device mapping with the main model's map_location during checkpoint loading",
      "Must leverage PyTorch Lightning's native device management capabilities",
      "Avoid requiring additional parameters for device mapping at load time",
      "Ensure compatibility with standard checkpoint loading patterns"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:03:14"
    }
  },
  {
    "number": 4465,
    "title": "How to save the latest and best checkpoint?",
    "created_at": "2020-11-01T10:25:10Z",
    "closed_at": "2020-11-01T12:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4465",
    "body": "I can set a checkpoing callback to save best model, but I also want it save the latest model, so that i can `resume_from_checkpoint` from latest checkpoint. how to do this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4465/comments",
    "author": "xiadingZ",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-11-01T11:56:07Z",
        "body": "@xiadingZ  There is `save_last` parameter in ModelCheckpoint that saves the last epoch"
      }
    ],
    "satisfaction_conditions": [
      "Supports simultaneous saving of both the best-performing and most recent checkpoints",
      "Ensures latest checkpoint is compatible with resume_from_checkpoint functionality"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:03:19"
    }
  },
  {
    "number": 3803,
    "title": "Access metrics in custom callbacks",
    "created_at": "2020-10-02T19:05:49Z",
    "closed_at": "2020-10-04T02:43:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3803",
    "body": "## \u2753 Questions and Help\r\n\r\nI have found it useful/helpful to sometimes access metrics in custom callbacks. In v0.9.0 this works using something like this:\r\n\r\n```\r\ndef training_step(self, batch, batch_idx):\r\n    return {\"loss\": self._step(batch)}\r\n\r\ndef validation_step(self, batch, batch_idx):\r\n    return {\"val_loss\": self._step(batch)}\r\n\r\ndef training_epoch_end(self, outputs):\r\n    # ...\r\n    return {\"interesting_key_train\": interesting_value}\r\n\r\ndef validation_epoch_end(self, outputs):\r\n    # ...\r\n    return {\"interesting_key_val\": interesting_value}\r\n```\r\n\r\nThe setup allows for the values returned in the `_epoch_end` methods to be accessed via `trainer.callback_metrics`. As such, a callback could use these values, e.g.\r\n\r\n```\r\nclass CustomCallback(Callback):\r\n\r\n    def on_validation_end(self, trainer, pl_module):\r\n        metrics = trainer.callback_metrics\r\n        interesting_value = metrics[\"interesting_key_train\"]\r\n```\r\n\r\nWhen using the current master branch, the above approach is possible for values returned in `validation_epoch_end` but no longer possible for `training_epoch_end` as setting a return value in `training_epoch_end` raises the exception,\r\n\r\n```\r\nMisconfigurationException: training_epoch_end expects a return of None. HINT: remove the return statement in training_epoch_end\r\n```\r\n\r\nAdditionally the values stored in `trainer.callback_metrics` have changed. Using the example above, in v0.9.0, it is `{\"loss\": ..., \"interesting_key_train\": ..., \"interesting_key_val\": ...}` and on master it is simply `{\"interesting_key_val\": ...}`.\r\n\r\nWhat is the intended way to access metrics (in particular from the training loop) in callbacks?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3803/comments",
    "author": "pbmstrk",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2020-10-03T22:32:34Z",
        "body": "> When using the current master branch, the above approach is possible for values returned in validation_epoch_end but no longer possible for training_epoch_end as setting a return value in training_epoch_end raises the exception,\r\n\r\nCan you use `self.log(\"interesting_key_train\", interesting_value)`? \r\n\r\nThough there does seem to be an issue with accessing metrics on epoch end on master @williamFalcon "
      },
      {
        "user": "pbmstrk",
        "created_at": "2020-10-03T23:32:48Z",
        "body": "`self.log` works on master, I had tried it earlier today and run into issues but these seem to be resolved now."
      }
    ],
    "satisfaction_conditions": [
      "Clear explanation of how to expose training loop metrics for callback access",
      "Method to log both training and validation metrics without relying on epoch_end returns",
      "Consistent metric availability in callback_metrics for both training and validation phases",
      "Guidance on proper logging practices aligned with framework updates"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:03:24"
    }
  },
  {
    "number": 3698,
    "title": "How to keep some LightningModule's parameters on cpu when using CUDA devices for training",
    "created_at": "2020-09-28T11:46:05Z",
    "closed_at": "2020-10-18T08:56:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3698",
    "body": "## \u2753 Questions and Help\r\n\r\n\r\n#### What is your question?\r\nI tried to transform my code into Lightning yesterday, but the CUDA OOM error occurred. My model has a very large parameter ```nn.Embedding(24000000, 128)``` (more than 22GB), which obviously exceeds the memory of my CUDA device. I implemented two classes to sovle this problem in my torch_version code, the pseudo code is as follows:\r\n\r\n#### PyTorch Code\r\n```python\r\nclass Emb(nn.Module):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.emb = nn.Emebdding(24000000, 128)\r\n        def forward(self, idx):\r\n                return self.emb(idx)\r\n\r\nclass MyModule(nn.Module):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n        def forward(self, input):\r\n                out = self.calculation(input)\r\n                return out\r\n\r\n# train part:\r\nget_emb = Emb()\r\nmodel = MyModule()\r\nmodel = model.cuda()\r\noptimizer = some_optimizer([{\"params\": e.parameters}, {\"params\": model.parametersba}], lr=1e-3)\r\nloss_metric = some_loss()\r\nfor epo in epoch:\r\n        for x, y in dataloader:\r\n                embs = get_emb(x.cpu()).cuda()\r\n                out = model(embs)\r\n                loss = loss_metric(out, y)\r\n                optimizer.zero_grad()\r\n                loss.backward()\r\n                optimizer.step()\r\n```\r\nThe torch_version code above keeps the nn.Embedding on cpu and ensures that the optimization of training is completed on CUDA devices. But I don't know how to achieve this via pytorch_lightning, because the entire 'training' part is encapsulated in training_step. The PL code  is as follows:\r\n\r\n#### PL Code\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = nn.Embedding(24000000, 128)\r\n                self.loss_metric = some_loss()\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb(x)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n# train part\r\nmodel = MyModule()\r\ntrainer = pl.Trainer(gpus=-1)\r\ntrainer.fit(model, dataloader)\r\n```\r\nSo, is there any recommended way to keep a part of the LightningModule's parameters on cpu when using CUDA devices for training? \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu 16.04.6 LTS\r\n - CUDA: version 10.2, 2080Ti\r\n - Version 0.9.0\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3698/comments",
    "author": "David-AJ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-28T11:46:44Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-28T20:35:35Z",
        "body": "if you do this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = [nn.Embedding(24000000, 128)]\r\n                self.loss_metric = some_loss()\r\n        def forward(self, input):\r\n                x, y = input\r\n                embs = self.emb[0](x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n```\r\nit should work I guess. Can't think of a better solution than this :sweat_smile: "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-09-29T03:39:41Z",
        "body": "> if you do this:\r\n> \r\n> ```python\r\n> class MyModule(pl.LightningModule):\r\n>         def __init__(self):\r\n>                 xxxxxx # some init operations\r\n>                 self.calculation = some_calculation()\r\n>                 self.emb = [nn.Embedding(24000000, 128)]\r\n>                 self.loss_metric = some_loss()\r\n>         def forward(self, input):\r\n>                 x, y = input\r\n>                 embs = self.emb[0](x.cpu()).to(self.device)\r\n>                 out = self.calculation(embs)\r\n>                 return {\"loss\": self.loss_metric(out, y)}\r\n> ```\r\n> \r\n> it should work I guess. Can't think of a better solution than this \ud83d\ude05\r\n\r\n@rohitgr7 Really?! In this case, will the self.emb be saved in ckpt along with other parameters of ```MyModule```? Sorry, I just noticed that I had a typo in the PL Code: ```forward(self, input) -> training_step(self, batch, batch_idx)```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-29T07:48:51Z",
        "body": "Yeah, won't save. Didnt think of that. If any module in the lightning has a `.to` method then it will be moved to device. Somehow need to think of a way to override this `.to` method for embeddings."
      },
      {
        "user": "David-AJ",
        "created_at": "2020-09-30T03:23:40Z",
        "body": "@rohitgr7 I tried to use ```self.emb = nn.Embedding(24000000, 128).cpu()``` in lightning code, but it failed. Actually, it is very common in recommendation system to use this kind of large-scale embedding as the trainable weight of the model. For example, the sparse features of User Id (more than 24000000) can be represented by a dense embedding matrix. So is there any possible to implement this operation in Pytorch-Lightning? "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T20:35:33Z",
        "body": "Looked at PyTorch source code. Found something. Can you try this?  @David-AJ\r\n```python\r\n\r\nclass SpecialEmbedding(nn.Module):\r\n        def __init__(self, fin, fout):\r\n                self.emb = nn.Embedding(fin, fout)\r\n\r\n        def _apply(self, fn):\r\n                return self\r\n\r\n        def forward(self, x):\r\n                return self.emb(x)\r\n\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = SpecialEmbedding(24000000, 128)\r\n                self.loss_metric = some_loss()\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb(x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n# train part\r\nmodel = MyModule()\r\ntrainer = pl.Trainer(gpus=-1)\r\ntrainer.fit(model, dataloader)\r\n```"
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-09T06:50:50Z",
        "body": "@rohitgr7 Thanks for your kindly help! But this ```SpecialEmbedding ```code failed again \ud83d\ude05\r\nthe error message is as follows:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"debug.py\", line 742, in <module>\r\n    trainer.fit(model)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1064, in fit\r\n    results = self.accelerator_backend.train()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/accelerators/dp_backend.py\", line 97, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\r\n    batch_output = self.run_training_batch(batch, batch_idx)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 844, in run_training_batch\r\n    self.hiddens\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1015, in optimizer_closure\r\n    hiddens)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 1197, in training_forward\r\n    output = self.model(*args)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/envs/rapids/lib/python3.6/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 70, in forward\r\n    \"them on device: {}\".format(self.src_device_obj, t.device))\r\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu\r\n```\r\nIt seems that pytorch_lightning forces the parameters of a module to be set on the same device?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-09T19:58:46Z",
        "body": "not 100% sure why is this `RuntimeError` should be raised. @awaelchli any suggestions on how to make this work/?\r\n\r\nActually now I also want to know if this is the right way or not or there is another way around since it seems super useful.\r\n\r\n@David-AJ is it working on a single GPU device with no distributed backend?"
      },
      {
        "user": "chiragraman",
        "created_at": "2020-10-09T20:03:35Z",
        "body": "@rohitgr7 did you mean to tag @David-AJ ?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-10-09T20:09:02Z",
        "body": "oops, my bad, was looking at your issue too #3998 side by side :sweat_smile: "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-10T03:35:22Z",
        "body": "> not 100% sure why is this `RuntimeError` should be raised. @awaelchli any suggestions on how to make this work/?\r\n> \r\n> Actually now I also want to know if this is the right way or not or there is another way around since it seems super useful.\r\n> \r\n> @David-AJ is it working on a single GPU device with no distributed backend?\r\n\r\nHi @rohitgr7, I tried to run this code on a single GPU and the ```RuntimeError``` was raised again."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-11T16:15:24Z",
        "body": "do you want to train the embedding layer or is it pretrained?\r\nIf you want to train it, I'm afraid you can't have it on cpu while also using DP. The error you got above is because DataParallel detects that. "
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-12T02:58:01Z",
        "body": "@awaelchli Yes, I want to train it, how about using DDP or any other distributed backend? Actually @rohitgr7\u2018s first solution ```self.emb = [nn.Embedding(24000000, 128)]``` could make ```self.emb``` on CPU while training with DP, but in that case the ```self.emb``` won't be saved in the ckpt, nor can be loaded using load_from_checkpoint. Could this problem be solved by overriding the ```on_save_checkpoint``` and ```on_load_checkpoint```?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-12T03:01:40Z",
        "body": "> `self.emb = [nn.Embedding(24000000, 128)]`\r\n\r\n\ud83e\udd23  this is a funny trick. Very creative. Yeah, this makes torch unaware of this module, and keeps it on the cpu. \r\n\r\n> Could this problem be solved by overriding the on_save_checkpoint and on_load_checkpoint?\r\n\r\nYes, I think that would do the trick!\r\nBut will this this embedding layer not be a huge bottleneck? You will need to transfer all outputs to the GPU and this blocks execution."
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-12T03:18:03Z",
        "body": "@awaelchli \r\n\r\n> But will this this embedding layer not be a huge bottleneck? You will need to transfer all outputs to the GPU and this blocks execution.\r\n\r\nLooks like I have no choice\ud83e\udd23 otherwise I have to train all the module on the cpu, don't know which one could be faster. The application scenario is in the recommendation system, and in fact the number of users and items far exceeds 24 million, all these ID sparse features should be represented by the embedding layer and trained in the module. Do you have any other suggestions on how to make it faster?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-10-17T14:22:54Z",
        "body": "> Could this problem be solved by overriding the on_save_checkpoint and on_load_checkpoint?\r\n\r\nyes. I would do it this way. grab the state dict of the embedding layer and add it to the checkpoint dict. when loading, you do the opposite and read the state dict. \r\n\r\n> Do you have any other suggestions on how to make it faster?\r\n\r\nsorry, nothing comes to my mind :("
      },
      {
        "user": "David-AJ",
        "created_at": "2020-10-18T08:56:00Z",
        "body": "I modified my code like this:\r\n```python\r\nclass MyModule(pl.LightningModule):\r\n        def __init__(self):\r\n                xxxxxx # some init operations\r\n                self.calculation = some_calculation()\r\n                self.emb = [nn.Embedding(24000000, 128)]\r\n                self.loss_metric = some_loss()\r\n\r\n        def training_step(self, batch, batch_idx):\r\n                x, y = batch\r\n                embs = self.emb[0](x.cpu()).to(self.device)\r\n                out = self.calculation(embs)\r\n                return {\"loss\": self.loss_metric(out, y)}\r\n\r\n        def on_save_checkpoint(self, checkpoint):\r\n                checkpoint[\"emb\"] = self.emb\r\n\r\n        def on_load_checkpoint(self, checkpoint):\r\n                self.emb = checkpoint[\"emb\"]\r\n```\r\nIt works! Thank you @rohitgr7 and @awaelchli!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must allow keeping specific model parameters (like large embeddings) on CPU while training other parts on GPU",
      "Must maintain checkpoint compatibility for CPU-based parameters",
      "Should work with PyTorch Lightning's device management system",
      "Must avoid creating significant performance bottlenecks",
      "Solution should support parameter updates for CPU-based layers"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:03:36"
    }
  },
  {
    "number": 2984,
    "title": "CrossEntropyLoss with weights",
    "created_at": "2020-08-15T02:46:24Z",
    "closed_at": "2020-08-15T09:49:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2984",
    "body": "I need weights in CrossEntropyLoss (actually multiple, but the same issue).  The documentation talks about tensors copied from other tensors, but there is no tensor to copy from in the init.  So I'm stuck.\r\nTo make the weights unquestionably simple, I use ones.\r\n\r\n```\r\nclass JJG_Transformer(pl.LightningModule):\r\n\r\n    def __init__(self, alphanet_plus_2, letter_weights_per_position):\r\n        super(JJG_Transformer, self).__init__()\r\n        self.criterions = []\r\n        for weight in self.letter_weights_per_position:\r\n            weight = torch.ones((94))\r\n            self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n    def validation_step(self, batch, batch_idx):\r\n        batch_im, batch_true_value_NT, batch_letter_transformer_input = batch\r\n        out_NTA = self(batch_im, batch_letter_transformer_input)\r\n        loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n        loss1 = self.criterions[1](out_NTA[:,1,:], batch_true_value_NT[:,1])\r\n        loss = loss0 + loss1\r\n        tensorboard_logs = {'val_loss': loss, 'val_loss0': loss0, 'val_loss1':loss1}\r\n        return {'val_loss': loss, 'log': tensorboard_logs}\r\n\r\n```\r\n\r\n  ```\r\nFile \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/Aug2020_simple_transformer/src/kiss_transformer.py\", line 254, in <module>\r\n    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 34, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1017, in fit\r\n    self.accelerator_backend.train(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 56, in train\r\n    self.ddp_train(process_idx=self.task_idx, mp_queue=None, model=model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 219, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1196, in run_pretrain_routine\r\n    self._run_sanity_check(ref_model, model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1229, in _run_sanity_check\r\n    eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 325, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 609, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 160, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/Aug2020_simple_transformer/src/kiss_transformer.py\", line 128, in validation_step\r\n    loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\", line 948, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2422, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2218, in nll_loss\r\n    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward\r\nTraceback (most recent call last):\r\n  File \"kiss_transformer.py\", line 254, in <module>\r\n    trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 34, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1030, in fit\r\n    results = self.accelerator_backend.spawn_ddp_children(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 118, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, mp_queue=None, model=model, is_master=True)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/ddp_backend.py\", line 219, in ddp_train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1196, in run_pretrain_routine\r\n    self._run_sanity_check(ref_model, model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1229, in _run_sanity_check\r\n    eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 325, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 609, in evaluation_forward\r\n    output = model(*args)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 160, in forward\r\n    output = self.module.validation_step(*inputs[0], **kwargs[0])\r\n  File \"kiss_transformer.py\", line 128, in validation_step\r\n    loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\", line 948, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2422, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2218, in nll_loss\r\n    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward\r\n```\r\n\r\n```\r\ntrainer = pl.Trainer( gpus=[0, 1],  \r\n                accumulate_grad_batches=16, \r\n                max_epochs=500, \r\n                check_val_every_n_epoch=1, \r\n                distributed_backend='ddp', \r\n```\r\n\r\npl__version__ 0.9.0rc12\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2984/comments",
    "author": "johngrabner",
    "comments": [
      {
        "user": "sykrn",
        "created_at": "2020-08-15T06:57:00Z",
        "body": "I think you just need to use `.cuda()` for the weight or criterion."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-15T09:49:13Z",
        "body": "@sykrn is on the right track. \r\nThe problem is that your criterion is in a list. nn.Module does not recognize submodules in lists. \r\nYou solve your problem by using ModuleList: \r\n\r\n```python \r\n\r\nself.criterions = nn.ModuleList()  # this is the fix\r\n\r\nfor weight in self.letter_weights_per_position:\r\n    weight = torch.ones((94))\r\n    self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n```\r\nnow your criterions (and tensors within it) will be automatically moved to the right device!"
      },
      {
        "user": "johngrabner",
        "created_at": "2020-08-15T15:26:55Z",
        "body": "Thank you. Made the example more explicit for future references.\r\n\r\n```\r\n        # fake fixed tensor weights \r\n        weights_per_position = []\r\n        for i in range(26):\r\n            t = torch.ones((94), dtype=torch.float32)\r\n            weights_per_position.append(t)\r\n\r\nclass JJG_Transformer(pl.LightningModule):\r\n\r\n    def __init__(self, weights_per_position):\r\n\r\n        # the list to hold the criterion\r\n        self.criterions = torch.nn.ModuleList() \r\n\r\n        for weight in self.weights_per_position:\r\n            self.criterions.append( torch.nn.CrossEntropyLoss(weight=weight) )\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss0 = self.criterions[0](out_NTA[:,0,:], batch_true_value_NT[:,0])\r\n        loss1 = self.criterions[1](out_NTA[:,1,:], batch_true_value_NT[:,1])\r\n        loss = loss0 + loss1\r\n```\r\n\r\nWhat does \"properly registered\" mean in?\r\n`ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.`"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-15T22:31:32Z",
        "body": "It means:\r\n\r\n```python\r\nclass Wrong(nn.Module):\r\n\r\n    def __init__(self)\r\n        super().init()\r\n        self.list = [CrossEntropyLoss()]\r\n\r\nw = Wrong()\r\nw.children() # no children\r\n```\r\nvs.\r\n```python\r\nclass Right(nn.Module):\r\n\r\n    def __init__(self)\r\n        super().init()\r\n        self.list = nn.ModuleList([CrossEntropyLoss()])\r\n\r\nr = Right()\r\nr.children() # correctly returns the cross entropy module\r\n```\r\n\r\nOperations like model.to('cuda') will also recursively operate on all submodules (children and their children etc.). But a Python list for example is not considered a submodule, because it's not an instance of nn.Module"
      },
      {
        "user": "johngrabner",
        "created_at": "2020-08-15T22:43:28Z",
        "body": "Thank you."
      }
    ],
    "satisfaction_conditions": [
      "Ensures loss function weights are on the same device as the model",
      "Explains proper module registration for automatic device management",
      "Maintains multiple loss functions with different weights",
      "Works with PyTorch Lightning's device management system"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:03:45"
    }
  },
  {
    "number": 2928,
    "title": "is limit_train_batches shuffle or random",
    "created_at": "2020-08-12T08:13:07Z",
    "closed_at": "2020-08-13T10:30:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2928",
    "body": "hi, I am using limit_train_batches . If it is set, is it means a subdataset of whole train dataset ? similar with torch.utils.data.random_split",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2928/comments",
    "author": "qmpzzpmq",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-12T08:13:53Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-08-12T09:09:10Z",
        "body": "Yes, it is a subset of the train dataset\r\nBut, it doesn't similar with `random_split`"
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2020-08-12T10:08:17Z",
        "body": "@ydcjeff I mean, is it random?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-08-12T10:30:59Z",
        "body": "I think it is not random. It is the first `limit_train_batches` of the train dataset."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-12T15:20:47Z",
        "body": "Yes exactly, @ydcjeff is right. It will fetch batches from the dataloader until it reaches that amount, so your dataset and dataloader settings regarding shuffling will be respected. "
      },
      {
        "user": "qmpzzpmq",
        "created_at": "2020-08-13T08:27:02Z",
        "body": "@awaelchli @ydcjeff thx\r\n"
      },
      {
        "user": "adosar",
        "created_at": "2024-03-24T20:10:31Z",
        "body": "> @awaelchli @ydcjeff thx\r\n\r\nWhat if the dataloader uses `shuffle == True`?"
      }
    ],
    "satisfaction_conditions": [
      "Clarify whether `limit_train_batches` selects a random subset or a sequential subset of the training data",
      "Explain how `limit_train_batches` interacts with the DataLoader's shuffle parameter",
      "Differentiate `limit_train_batches` behavior from `random_split` functionality",
      "Specify whether the subset preserves original data order when shuffle is disabled"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:03:56"
    }
  },
  {
    "number": 2888,
    "title": "Understanding the Progress Bar",
    "created_at": "2020-08-08T14:01:20Z",
    "closed_at": "2020-08-08T14:42:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2888",
    "body": "I train on MNIST with data loaders defined below (full train / test sets with `batch_size=128`).  \r\n`'val_check_interval': 0.1`, so per training epoch, I have 10 validation runs.  \r\n\r\nNow:\r\n- 10000 (test) images / 128 (batch_size) = 78.125, so steps such as 54/79 do make sense.  \r\n- 60000 (train) images / 128 (batch_size) = 468.75, so I'd expect something like 120/469.  \r\n\r\nWhat is the \"1259\" representing in the progress bar? I can observe in tensorboard, that the epoch number goes up at exactly 459.\r\n```\r\nValidating:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 54/79 [00:08<00:03,  6.57it/s]\r\nEpoch 4:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 976/1259 [04:01<01:09,  4.05it/s, loss=19279.273, v_num=0]\r\n```\r\n\r\n#### Code\r\n##### Data Loaders\r\n```python\r\n    def train_dataloader(self) -> DataLoader:\r\n        \"\"\"Pytorch-lightning function.\"\"\"\r\n        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\r\n                                                    torchvision.transforms.ToTensor()])\r\n        train_set = torchvision.datasets.MNIST(root=DATA_DIR_PATH / 'mnist_data',\r\n                                               train=True,\r\n                                               download=True,\r\n                                               transform=transform)\r\n        return DataLoader(train_set,\r\n                          batch_size=128,\r\n                          shuffle=True,\r\n                          num_workers=0)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        \"\"\"Pytorch-lightning function.\"\"\"\r\n        transform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\r\n                                                    torchvision.transforms.ToTensor()])\r\n        val_set = torchvision.datasets.MNIST(root=DATA_DIR_PATH / 'mnist_data',\r\n                                             train=False,\r\n                                             download=True,\r\n                                             transform=transform)\r\n        return DataLoader(val_set,\r\n                          batch_size=128,\r\n                          shuffle=False,\r\n                          num_workers=0)\r\n```\r\n#### What's your environment?\r\n - OS: Ubuntu 20.04\r\n - Packaging: pipenv\r\n - Lightning Version: 0.8.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2888/comments",
    "author": "matthaeusheer",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-08T14:02:00Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-08T14:16:28Z",
        "body": "It's 79*10 (10 times validation per epoch) + 469(train batches) = 1259. The progress bar contains both train and validation steps."
      },
      {
        "user": "matthaeusheer",
        "created_at": "2020-08-08T14:42:38Z",
        "body": "I see, thanks @rohitgr7 :+1: "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how validation frequency affects total step count in progress bar",
      "Clarification of progress bar composition in terms of training + validation batches",
      "Relationship between val_check_interval configuration and progress tracking"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:04:06"
    }
  },
  {
    "number": 2666,
    "title": "Plotting learning rate from a lr_scheduler via a Callback",
    "created_at": "2020-07-21T23:00:50Z",
    "closed_at": "2020-08-05T07:35:41Z",
    "labels": [
      "feature",
      "good first issue",
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2666",
    "body": "I think the title explains a lot. But let me elaborate, I have a LightningModule which has a configure_optimizers method returns an optimizer and a scheduler. Later in a Callback I have a `on_batch_end` function in which I try to log the learning rate.\r\n\r\nOf course if the scheduler was accessible as a class member, we could `self.scheduler.get_lr()` on it and use the value to plot. Since this is not how it has been implemented, I am wondering how to do this?\r\n\r\nWould appreciate any pointers.\r\nPytorchLightning - 0.8.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2666/comments",
    "author": "bhashithe",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-21T23:01:49Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "s-rog",
        "created_at": "2020-07-22T00:29:35Z",
        "body": "If you have the same lr throughout the network (single param group) you can get it from:\r\n`self.trainer.optimizers[0].param_groups[0]['lr']`\r\nchange the indexing based on your optimizer and param configuration."
      },
      {
        "user": "bhashithe",
        "created_at": "2020-07-22T01:04:56Z",
        "body": "That worked, even if i have multiple groups does it work the same if I do something like this?\r\n\r\n`{f'lr_group{i}': param['lr'] for i, param in enumerate(self.trainer.optimizers[0].param_groups}`"
      },
      {
        "user": "s-rog",
        "created_at": "2020-07-22T01:11:41Z",
        "body": "should work!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-22T04:49:59Z",
        "body": "There is a `LearningRateLogger` callback in lightning."
      },
      {
        "user": "Borda",
        "created_at": "2020-08-04T21:03:22Z",
        "body": "@SkafteNicki mind have a look?"
      },
      {
        "user": "SkafteNicki",
        "created_at": "2020-08-05T05:58:02Z",
        "body": "As @rohitgr7 mention, the `LearningRateLogger` which can be imported as `from pytorch_lightning.callbacks import LearningRateLogger` should be able to do what you ask for."
      }
    ],
    "satisfaction_conditions": [
      "Access learning rate values from the optimizer/scheduler within a Callback",
      "Support multiple parameter groups with different learning rates",
      "Work with PyTorch Lightning's architecture without modifying scheduler ownership",
      "Provide logging capability during batch processing",
      "Leverage existing PyTorch Lightning utilities when available"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:04:18"
    }
  },
  {
    "number": 2656,
    "title": "How to reload partial weights from the trained checkpoint?",
    "created_at": "2020-07-21T10:40:17Z",
    "closed_at": "2020-07-22T01:47:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2656",
    "body": "#### What is your question?\r\nNow I want to reload partial weights from trained checkpoint and let the remaining parameters trained from scratch. But I didn't find the api that allows me to reload partial parameters.\r\n\r\n#### Code\r\nmy code is like this, I find pl only support resume_from_checkpoint path to reload all weights from checkpoint.\r\n```\r\ntrainer = pl.Trainer(gpus=1, early_stop_callback=None, resume_from_checkpoint=resume_checkpoint, val_check_interval=1000)\r\n\r\ntrainer.fit(model) \r\n```\r\n\r\nWhat should I do to reload partial parameters?   Thank you!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2656/comments",
    "author": "guvcolie",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-21T10:41:11Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-07-21T19:49:33Z",
        "body": "It's all PyTorch! The checkpoints we save are fully compatible with torch.load, so you do your fancy reloading before fit and then give whatever weights you want trained to your optimizers :)"
      },
      {
        "user": "guvcolie",
        "created_at": "2020-07-22T01:47:43Z",
        "body": "@awaelchli  got it! thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates how to load only specific parameters from a checkpoint while leaving others uninitialized",
      "Shows integration with PyTorch Lightning's training workflow",
      "Uses standard PyTorch weight loading mechanisms",
      "Allows custom logic for parameter selection"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:04:23"
    }
  },
  {
    "number": 2451,
    "title": "how to use custom dataset in pytorch-lightning module as I am encountering an error \"AttributeError: 'str' object has no attribute 'size'\"",
    "created_at": "2020-07-01T13:42:42Z",
    "closed_at": "2020-07-30T21:51:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2451",
    "body": "## \u2753 Questions and Help\r\n\r\n\r\n\r\n#### how to use custom dataset in pytorch-lightning module as I am encountering an error \"AttributeError: 'str' object has no attribute 'size'\"?\r\n\r\n#### Code\r\n\r\n\r\nclass CustomDataset(Dataset):\r\n       def read_data_set(self):\r\n       all_img_files = []\r\n       all_labels = []\r\n\r\n    class_names = os.walk(self.data_set_path).__next__()[1]\r\n    \r\n    for index, class_name in enumerate(class_names):\r\n        label = index\r\n        img_dir = os.path.join(self.data_set_path, class_name)\r\n        img_files = os.walk(img_dir).__next__()[2]\r\n        \r\n        \r\n        for img_file in img_files:\r\n            img_file = os.path.join(img_dir, img_file)\r\n            img = Image.open(img_file)\r\n            if img is not None:\r\n                all_img_files.append(img_file)\r\n                all_labels.append(label)\r\n                \r\n    return all_img_files, all_labels, len(all_img_files), len(class_names)\r\n\r\ndef __init__(self, data_set_path, transforms=None):\r\n    self.data_set_path = data_set_path\r\n    self.image_files_path, self.labels, self.length, self.num_classes = self.read_data_set()\r\n    self.transforms = transforms\r\n    \r\ndef __getitem__(self, index):\r\n    image = Image.open(self.image_files_path[index])\r\n    image = image.convert(\"RGB\")\r\n    \r\n    if self.transforms is not None:\r\n        image = self.transforms(image)\r\n        \r\n    return {'image': image, 'label': self.labels[index]}\r\n\r\ndef __len__(self):\r\n    return self.length\r\nclass MNISTClassifier(LightningModule):\r\ndef init(self):\r\nsuper(MNISTClassifier, self).init()\r\nself.layer_1 = torch.nn.Linear(28*28, 128)\r\nself.layer_2 = torch.nn.Linear(128, 256)\r\nself.layer_3 = torch.nn.Linear(256, 10)\r\n\r\ndef forward(self, x):\r\n    batch_size, channels, width, height = x.size()\r\n    \r\n    #(b_s, 1, 28, 28)\r\n    x = x.view(batch_size, -1)\r\n    \r\n    #layer1\r\n    x = self.layer_1(x)\r\n    x = torch.relu(x)\r\n    \r\n    #layer2\r\n    x = self.layer_2(x)\r\n    x = torch.relu(x)\r\n    \r\n    #layer3\r\n    x = self.layer_3()\r\n    \r\n    #probability distribution over labels\r\n    x = torch.log_softmax(x, dim = 1)\r\n    \r\n    return x\r\n\r\n\r\ndef cross_entropy_loss(self, logits, labels):\r\n    return F.null_loss(logits, labels)\r\n\r\n\r\ndef training_step(self, train_batch, batch_idx):\r\n    x, y = train_batch\r\n    logits = self.forward(x.size())\r\n    loss = self.cross_entropy_loss(logits, y)\r\n    \r\n    logs = {'train_loss': loss}\r\n    return {'loss': loss, 'log':logs}\r\n\r\n\r\n\r\n\r\ndef test_step(self, test_batch, batch_idx):\r\n    x, y = test_batch\r\n    logits = self.forward(x.size())\r\n    loss = self.cross_entropy_loss(logits, y)\r\n    \r\n    logs = {'test_loss:': loss}\r\n    return {'val_loss': loss, 'log':logs}\r\n\r\n \r\ndef train_dataloader(self):\r\n    dataset = CustomDataset(data_set_path='./files/MNIST/mnist_png/mnist_png/training/', transforms=transforms.ToTensor())\r\n    train_loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\r\n    return train_loader\r\n\r\n\r\n\r\ndef test_dataloader(self):\r\n    dataset = CustomDataset(data_set_path='files/MNIST/mnist_png/mnist_png/testing/', transforms=transforms.ToTensor())\r\n    test_loader = DataLoader(dataset, batch_size=32, num_workers=4)\r\n    return test_loader\r\n                       \r\ndef configure_optimizers(self):\r\n    optimizer = torch.optim.Adam(self.parameters(),lr=1e-3)\r\n    return optimizer\r\nmodel = MNISTClassifier()\r\ntrainer = pl.Trainer(gpus=1, max_epochs=1)\r\ntrainer.fit(model)\r\n\r\n\r\n\r\n#### What have you tried?\r\nI am getting an error as follows:\r\nAttributeError Traceback (most recent call last)\r\nin ()\r\n8 model = MNISTClassifier()\r\n9 trainer = pl.Trainer(gpus=1, max_epochs=1)\r\n---> 10 trainer.fit(model)\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n763\r\n764 elif self.single_gpu:\r\n--> 765 self.single_gpu_train(model)\r\n766\r\n767 elif self.use_tpu: # pragma: no-cover\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\r\n490 self.optimizers = optimizers\r\n491\r\n--> 492 self.run_pretrain_routine(model)\r\n493\r\n494 def tpu_train(self, tpu_core_idx, model):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n911\r\n912 # CORE TRAINING LOOP\r\n--> 913 self.train()\r\n914\r\n915 def test(self, model: Optional[LightningModule] = None, test_dataloaders: Optional[DataLoader] = None):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)\r\n345 # RUN TNG EPOCH\r\n346 # -----------------\r\n--> 347 self.run_training_epoch()\r\n348\r\n349 # update LR schedulers\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n417 # RUN TRAIN STEP\r\n418 # ---------------\r\n--> 419 _outputs = self.run_training_batch(batch, batch_idx)\r\n420 batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n421\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\r\n594\r\n595 # calculate loss\r\n--> 596 loss, batch_output = optimizer_closure()\r\n597\r\n598 # check if loss or model weights are nan\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\r\n558 opt_idx, self.hiddens)\r\n559 else:\r\n--> 560 output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n561\r\n562 # format and reduce outputs accordingly\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)\r\n724 batch = self.transfer_batch_to_gpu(batch, gpu_id)\r\n725 args[0] = batch\r\n--> 726 output = self.model.training_step(*args)\r\n727\r\n728 # TPU support\r\n\r\nin training_step(self, train_batch, batch_idx)\r\n36 def training_step(self, train_batch, batch_idx):\r\n37 x, y = train_batch\r\n---> 38 logits = self.forward(x.size())\r\n39 loss = self.cross_entropy_loss(logits, y)\r\n40\r\n\r\nAttributeError: 'str' object has no attribute 'size'   \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu 18.04\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2451/comments",
    "author": "drone-vision",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-01T13:43:34Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T17:09:05Z",
        "body": "@tanubapun You are sending `x.size()` in `self.forward(x.size())`. It should be `self.forward(x)`"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T18:21:36Z",
        "body": "Thanks @rohitgr7 for your response. I have changed the code to  self.forward(x) still got the same error.\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-bb768fd558f0> in <module>()\r\n      8 model = MNISTClassifier()\r\n      9 trainer = pl.Trainer(gpus=1, max_epochs=1)\r\n---> 10 trainer.fit(model)\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\r\n    763 \r\n    764         elif self.single_gpu:\r\n--> 765             self.single_gpu_train(model)\r\n    766 \r\n    767         elif self.use_tpu:  # pragma: no-cover\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\r\n    490             self.optimizers = optimizers\r\n    491 \r\n--> 492         self.run_pretrain_routine(model)\r\n    493 \r\n    494     def tpu_train(self, tpu_core_idx, model):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\r\n    911 \r\n    912         # CORE TRAINING LOOP\r\n--> 913         self.train()\r\n    914 \r\n    915     def test(self, model: Optional[LightningModule] = None, test_dataloaders: Optional[DataLoader] = None):\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)\r\n    345                 # RUN TNG EPOCH\r\n    346                 # -----------------\r\n--> 347                 self.run_training_epoch()\r\n    348 \r\n    349                 # update LR schedulers\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    417             # RUN TRAIN STEP\r\n    418             # ---------------\r\n--> 419             _outputs = self.run_training_batch(batch, batch_idx)\r\n    420             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\r\n    421 \r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\r\n    594 \r\n    595                 # calculate loss\r\n--> 596                 loss, batch_output = optimizer_closure()\r\n    597 \r\n    598                 # check if loss or model weights are nan\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\r\n    558                                                                     opt_idx, self.hiddens)\r\n    559                         else:\r\n--> 560                             output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\r\n    561 \r\n    562                         # format and reduce outputs accordingly\r\n\r\n~/anaconda3/envs/DL_ALL2/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)\r\n    724             batch = self.transfer_batch_to_gpu(batch, gpu_id)\r\n    725             args[0] = batch\r\n--> 726             output = self.model.training_step(*args)\r\n    727 \r\n    728         # TPU support\r\n\r\n<ipython-input-7-375f2fcd6b89> in training_step(self, train_batch, batch_idx)\r\n     36     def training_step(self, train_batch, batch_idx):\r\n     37         x, y = train_batch\r\n---> 38         logits = self.forward(x)\r\n     39         loss = self.cross_entropy_loss(logits, y)\r\n     40 \r\n\r\n<ipython-input-7-375f2fcd6b89> in forward(self, x)\r\n      8 \r\n      9     def forward(self, x):\r\n---> 10         batch_size, channels, width, height = x.size()\r\n     11 \r\n     12         #(b_s, 1, 28, 28)\r\n\r\nAttributeError: 'str' object has no attribute 'size'"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T18:28:19Z",
        "body": "@tanubapun Can you share the whole code in a notebook or colab or just LightningModule you created again after you made changes?"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T18:50:10Z",
        "body": "Sure @rohitgr7 \r\nthis is the lightning module I am using\r\n\r\nclass MNISTClassifier(LightningModule):\r\n    def __init__(self):\r\n        super(MNISTClassifier, self).__init__()\r\n        self.layer_1 = torch.nn.Linear(28*28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n        \r\n        \r\n    def forward(self, x):\r\n        batch_size, channels, width, height = x.size()\r\n        \r\n        #(b_s, 1, 28, 28)\r\n        x = x.view(batch_size, -1)\r\n        \r\n        #layer1\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n        \r\n        #layer2\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n        \r\n        #layer3\r\n        x = self.layer_3()\r\n        \r\n        #probability distribution over labels\r\n        x = torch.log_softmax(x, dim = 1)\r\n        \r\n        return x\r\n    \r\n    \r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.null_loss(logits, labels)\r\n    \r\n    \r\n    def training_step(self, train_batch, batch_idx):\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        \r\n        logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log':logs}\r\n    \r\n    \r\n    \r\n    def test_step(self, test_batch, batch_idx):\r\n        x, y = test_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        \r\n        logs = {'test_loss:': loss}\r\n        return {'val_loss': loss, 'log':logs}\r\n    \r\n\r\n    def train_dataloader(self):\r\n        dataset = CustomDataset(data_set_path='./files/MNIST/mnist_png/mnist_png/training/', transforms=transforms.ToTensor())\r\n        train_loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)\r\n        return train_loader\r\n\r\n    \r\n    def test_dataloader(self):\r\n        dataset = CustomDataset(data_set_path='files/MNIST/mnist_png/mnist_png/testing/', transforms=transforms.ToTensor())\r\n        test_loader = DataLoader(dataset, batch_size=32, num_workers=4)\r\n        return test_loader\r\n                           \r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(),lr=1e-3)\r\n        return optimizer\r\n        "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-07-01T19:04:48Z",
        "body": "@tanubapun In `__getitem__` of your `CustomDataset` you are returning `{'image': image, 'label': self.labels[index]}` but you are using `x, y = train_batch`. Your `train_batch` is still a `dict` here just that pytorch `collate_fn` create a batch in the values of this dict. Either return `image, self.labels[index]` or use `x, y= train_batch['image'], train_batch['label']`. Also change in `validation_step` and `test_step` accordingly.\r\n"
      },
      {
        "user": "drone-vision",
        "created_at": "2020-07-01T19:17:55Z",
        "body": "Thank you very much @rohitgr7, it wipes out the concerned error for the code... "
      }
    ],
    "satisfaction_conditions": [
      "Dataset returns tensors in format expected by model",
      "Data loading matches model's input expectations",
      "Proper tensor shape handling in forward pass",
      "Correct loss function implementation",
      "Consistent data structure between dataset and model"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:04:48"
    }
  },
  {
    "number": 2404,
    "title": "Will load_from_checkpoint load Huggingface models as well? ",
    "created_at": "2020-06-28T20:12:31Z",
    "closed_at": "2020-07-01T09:13:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2404",
    "body": "#### What is your question?\r\n\r\nJust wanted to know will using the `load_from_checkpoint` for a `LightningModule` load the state_dict for the **HuggingFace** models as well?\r\n\r\nEg: for the given example in the docs, will state_dict be loaded for `BertModel.from_pretrained` thing as well? \r\nIdeally, `load_from_checkpoint` should load state_dict for Bert as well like `BertModel.from_pretrained(same_checkpoint)` would do.\r\n\r\n#### Code\r\n```\r\nclass BertMNLIFinetuner(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)\r\n        self.W = nn.Linear(bert.config.hidden_size, 3)\r\n        self.num_classes = 3\r\n\r\n\r\n    def forward(self, input_ids, attention_mask, token_type_ids):\r\n\r\n        h, _, attn = self.bert(input_ids=input_ids,\r\n                         attention_mask=attention_mask,\r\n                         token_type_ids=token_type_ids)\r\n\r\n        h_cls = h[:, 0]\r\n        logits = self.W(h_cls)\r\n        return logits, attn\r\n``` ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2404/comments",
    "author": "vibhavagarwal5",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-28T20:13:23Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "dscarmo",
        "created_at": "2020-06-28T21:26:32Z",
        "body": "When working with the Hugging Face library i just store the model string (in this case best-base-cased) as an hparam."
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-28T22:00:54Z",
        "body": "Sure, that is fine. I think you misunderstood my question. Will the state_dict from the checkpoint (using `load_from_checkpoint`) load the bert model as well or not? Or will I have to load it explicitly using `model.from_pretrained(checkpoint_path)` ?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-28T22:17:10Z",
        "body": "I don't know this Bert stuff , but if it helps:\r\n`Model.load_from_checkpoint()` will init your model with the args and kwargs from the checkpoint and then call `model.load_state_dict` to load the model weights as you would do in pure PyTorch.\r\nSo, if your` self.bert` is a `nn.Module` , then that will have the parameters loaded as well. "
      },
      {
        "user": "dscarmo",
        "created_at": "2020-06-28T22:20:43Z",
        "body": "Oh now i understand. The updated weights of your whole trained lightning module (including bert since it is a nn.Module) will be loaded. Now I am just wondering if init's `from_pretrained`  will overwrite your trained weights? \r\n\r\nI hope PL will only load the trained weights after init runs."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-28T22:33:56Z",
        "body": "yes, so the from_pretrained will run first and load your pretrained weights (since it's in the init). and then the load_state dict will overwrite them again with the weights from the checkpoint. \r\n\r\nI encourage you to make a sanity check and not blindly believe me :), for example, you could load your checkpoint manually and replace all weights with ones and then save it again. Then load the checkpoint again using pytorch lightning and print the weights of the loaded model."
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T04:56:17Z",
        "body": "My guess is the same that after from_pretrained weights are loaded, checkpoints weights override them.  Which makes sense. \n\n---\n\nAnother doubt, do the checkpoints save all sorts of callbacks as well? Coz I don't see it in the dict keys when I do torch.load(chckpoint). "
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-29T05:04:13Z",
        "body": "we do that for early stopping and model checkpoint (see v0.8.2), but not for your custom callbacks. We recently discussed how to do that, here is an open discussion #2401 . "
      },
      {
        "user": "vibhavagarwal5",
        "created_at": "2020-06-29T05:17:51Z",
        "body": "v.0.8.2 is not released yet right? "
      }
    ],
    "satisfaction_conditions": [
      "Clarification of whether HuggingFace model parameters are included in LightningModule checkpoint loading",
      "Explanation of weight loading sequence between from_pretrained() and checkpoint restoration",
      "Confirmation that third-party modules (like BERT) are treated as standard nn.Modules during loading",
      "Verification mechanism for checkpoint content coverage"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:04:55"
    }
  },
  {
    "number": 2370,
    "title": "Access the logging directory through LightningModule or Trainer",
    "created_at": "2020-06-26T09:25:06Z",
    "closed_at": "2020-06-27T12:26:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2370",
    "body": "Is there a way to access the current logging directory (e.g., lightning_logs/version_x)? I've searched the documentation and the source code but haven't found a solution yet.\r\n\r\nI want to save some intermediate raw tensors to that directory.\r\n\r\nThanks,\r\nDavid",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2370/comments",
    "author": "DavidRuhe",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-26T09:25:48Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-26T20:35:57Z",
        "body": "For tensorboard I think you can use `self.logger.log_dir`, not sure about others. I think this property should be present for all the available loggers if possible."
      },
      {
        "user": "DavidRuhe",
        "created_at": "2020-06-27T12:26:34Z",
        "body": "Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to access the current logging directory path through official PyTorch Lightning APIs",
      "Works with different logger implementations (not just TensorBoard)",
      "Doesn't require accessing private/internal attributes or unstable workarounds",
      "Allows saving raw tensor data to the logging directory during training"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:05:02"
    }
  },
  {
    "number": 2310,
    "title": "how to train a network that doesn't require any training data",
    "created_at": "2020-06-21T22:46:58Z",
    "closed_at": "2020-06-23T18:46:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2310",
    "body": "The Wake-Sleep algorithm doesn't require any data during the sleep phase (effectively it generates it's own data). pytorch-lightning, however, appears to require a `train_dataloader()` method.\r\n\r\nThe only way I have to make pytorch-lightning run at all (for this admitted unusual case) is to specify some dummy dataset in `train_dataloader`, and then to ignore the data that gets passed to `training_step`. But I don't like that cycles are spent iterating through irrelevant data then. Is there a more elegant way?\r\n\r\nI considered defining my own custom `DataLoader` that returns the simulated data that the sleep phase uses, but this started seeming like even more of a hack than the previous solution. After all, my \"dataloader\" doesn't load any data; it effectively generates new data every \"epoch\". It's seems unnatural to split the sleep phase updates in this way.\r\n\r\nIs there a more straightforward way in lightning to train a network that doesn't require any data? Thanks!",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2310/comments",
    "author": "jeff-regier",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-21T22:47:54Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-23T17:00:36Z",
        "body": "Why is it not possible for you to write a Dataset class that generates the data? You mention you already have code for generating the data, so why not wrap it into a Dataset class?"
      },
      {
        "user": "jeff-regier",
        "created_at": "2020-06-23T18:46:56Z",
        "body": "I had been concerned that the Dataset class would shuffle the data unnecessarily, thus slowing down training. But an `IterableDataset` seems to work: the `__iter__` method is overridden to return an iterator that yields a whole batch at once. Maybe this is the best way."
      },
      {
        "user": "awaelchli",
        "created_at": "2020-06-23T18:53:43Z",
        "body": "No, that is simply not true. The Dataloader is doing the batching and shuffling, not the dataset. Besides, shuffling does not slow training down. If your dataset has an undefined or infinite length, use the IterableDataset, otherwise use the regular Dataset class. Once you have that, just pass it to the Dataloader. All of this is regular PyTorch :)"
      }
    ],
    "satisfaction_conditions": [
      "Supports dynamic data generation during training without pre-existing datasets",
      "Avoids computational overhead from unused data processing",
      "Integrates naturally with PyTorch Lightning's training loop requirements",
      "Enables per-epoch/batch data regeneration",
      "Avoids unnecessary data shuffling"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:05:12"
    }
  },
  {
    "number": 2308,
    "title": "How to make inference right",
    "created_at": "2020-06-21T19:30:23Z",
    "closed_at": "2020-06-24T22:31:40Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2308",
    "body": "Hello everyone. I'm new to pytorch-lightning, but already excited with this framework. It's very convenient to train my models using lightning. Now my usecase is: I have trained my model and want to do inference on my test data and get results (for example, in csv format). I'd like to do my inference pytorch-lightning-way. What is the best practice to do it?   \r\n\r\n### Before asking:   \r\n1. search the issues.   \r\n2. search the docs.    \r\n\r\n#### What's your environment?\r\n\r\n - OS: [Linux]\r\n - Packaging [pip]\r\n - Version [0.8.1]\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2308/comments",
    "author": "Podidiving",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-21T19:30:58Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "versatran01",
        "created_at": "2020-06-24T16:10:14Z",
        "body": "My understanding is that you just load the module and call freeze() on it and use it as any nn.Module"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T19:50:09Z",
        "body": "You don't need to freeze(). Just run with `torch.no_grad()` or set `torch.set_grad_enabled(False)`."
      },
      {
        "user": "Podidiving",
        "created_at": "2020-06-24T20:01:59Z",
        "body": "I think, one of the coolest features of lightning is that your don\u2019t need to specify your device (devices)\nIf you using LightningModule as plain nn.Module you should transfer your model and batches on devices manually, am I right? \nSo, I have my trained model. I\u2019d like to make inference on test data. I can define test_step and aggregate results in on_test_epoch_end, but I cannot run Trainer without train stage. Can I get around this somehow?"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T20:08:49Z",
        "body": "`test_step` and `test_epoch_end` are called with `trainer.test()`. AFAIK, these methods are used just to evaluate your test dataset and not return any logits. If you want to do just this evaluation, you don't have to do the transfer of model or batches. But I think if you want to get the logits from the model you need to do it manually just like a vanilla PyTorch model."
      },
      {
        "user": "Podidiving",
        "created_at": "2020-06-24T20:16:58Z",
        "body": "Ok, thanks, didn\u2019t know about `test` method! I think, if you want to make some kind of submission data, you can return logits from `test_step` method in dict object, and then aggregate and save them in `test_epoch_end`  "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-06-24T20:20:41Z",
        "body": "yeah, can do that too. Nice thought. :v:  "
      }
    ],
    "satisfaction_conditions": [
      "Integration with PyTorch-Lightning's native testing workflow",
      "Automatic device management without manual transfers",
      "Batch processing with result aggregation",
      "Inference without training phase overhead"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:05:20"
    }
  },
  {
    "number": 2263,
    "title": "Full batch training",
    "created_at": "2020-06-19T06:19:52Z",
    "closed_at": "2020-06-19T12:27:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2263",
    "body": "## \u2753 Questions and Help\r\n\r\nFor smaller datasets, it makes sense to do full batch training, not minibatch. How do you implement fullbatch training in pytorch lightning, given that train and validation might be different sizes?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2263/comments",
    "author": "turian",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-19T06:28:02Z",
        "body": "what do you mean by full batch training?\r\n\r\nThis is not lightning specific though... this is pytorch\r\n\r\n```\r\nDataloader(..., batch_size=1)\r\nDataloader(..., batch_size=32)\r\nDataloader(..., batch_size=len(dataset))\r\n```"
      },
      {
        "user": "turian",
        "created_at": "2020-06-19T12:27:00Z",
        "body": "Thank you, that is perfect."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to configure batch sizes to use the entire training and validation datasets",
      "Demonstration of PyTorch Lightning compatibility for full batch training"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:05:31"
    }
  },
  {
    "number": 2045,
    "title": "2 optimizers : skip updates for the second optim",
    "created_at": "2020-06-01T22:16:04Z",
    "closed_at": "2020-06-02T19:00:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2045",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nI have a model with 2 optimizers : one for the backbone weight and second for \"backbone support\" weights. However, 2nd optimizer should only accumulate grads through the whole epoch and perform one update at the epoch end. \r\n\r\nLightning keeps asking for an output for the 2nd optimizer, but there is nothing to output in addition to the first optimizer results. How can I bypass this ?\r\n\r\n#### Code\r\nHere are defined the training_step and optimizer_step functions to illustrate my issue.\r\n```\r\ndef training_step(self, batch, batch_idx, optimizer_idx):\r\n        if optimizer_idx == 0:\r\n            images, target = batch\r\n            output = self(images)\r\n            loss_val = F.cross_entropy(output, target)\r\n            acc1, acc5 = self.__accuracy(output, target, topk=(1, 5))\r\n            weight_cons, act_cons, weight_p_c_cons = self.normalized_consumption()\r\n\r\n            tqdm_dict = {'Loss/train_loss': loss_val, \r\n                        'Acc/acc1': acc1,\r\n                        'Acc/acc5': acc5,}\r\n            output = OrderedDict({\r\n                'loss': loss_val,\r\n                'Loss/loss': loss_val,\r\n                'Acc/acc1': acc1,\r\n                'Acc/acc5': acc5,\r\n                'progress_bar': tqdm_dict,\r\n                'log': tqdm_dict\r\n            })\r\n            return output\r\n     if optimizer_idx == 1:\r\n         # Do nothing ?\r\n\r\ndef optimizer_step(self, current_epoch, batch_nb, optimizer, optimizer_i, second_order_closure=None):\r\n        if optimizer_i == 0:\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n        # update 2nd optimizer at the end of the epoch\r\n        if optimizer_i == 1 and self.__nbbatch -1 <= batch_nb:\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n```\r\n<!-- Please paste a code snippet if your question requires it! -->   \r\n\r\n#### What have you tried?\r\nI tried to pass an empty output dict in training_step if the optimizer_idx == 1. However, Lightning complains that the dict is empty (`trainer/logging.py:106` -> `Nonetype has no attribute 'items'`)\r\n#### What's your environment?\r\n\r\n - OS: Linux\r\n - Packaging pip\r\n - Version 0.7.5\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2045/comments",
    "author": "sebastienwood",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-06-01T22:18:20Z",
        "body": "override the backward pass. \r\ncheck the current batch idx and don\u2019t apply the backward step until the end of the epoch"
      },
      {
        "user": "sebastienwood",
        "created_at": "2020-06-02T19:00:01Z",
        "body": "I managed to make it works thanks :) \r\nI had to also return a dummy `loss` for the 2nd optimizer that lightning really wanted to have (as usual in an ordered dict)\r\n\r\nAs a side note, a strange issue (that has probably nothing to do with Lightning) is that a custom function stopped receiving the backward call when using two optimizers. "
      }
    ],
    "satisfaction_conditions": [
      "Support for delayed optimizer updates at epoch end",
      "Compatibility with multiple optimizers requiring different update schedules",
      "Valid training step output handling for unused optimizers",
      "Gradient accumulation control for specific optimizers",
      "Compatibility with PyTorch Lightning 0.7.5 architecture"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:05:40"
    }
  },
  {
    "number": 1988,
    "title": "Stopping the code along with a graceful shutdown.",
    "created_at": "2020-05-28T16:12:51Z",
    "closed_at": "2020-05-29T07:27:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1988",
    "body": "##\r\nIs there a way to stop the training in the model when some criteria are satisfied. Something along the lines:\r\n```\r\nclass myCallback(Callback):\r\n    def __init__(self):\r\n        ...\r\n    def on_epoch_end(self, trainer, pl_module):\r\n        if criteria:\r\n            model.stop_training = True # stops the training; need help here\r\n```\r\nNote that I also want to have the early stopping feature where the 'val_loss' is monitored but want to stop running the code if some other criteria is satisfied. Also, is my method of having this feature in the callback module correct or should I inherit the early stopping criteria?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1988/comments",
    "author": "nsidn98",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-28T16:13:34Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:44:58Z",
        "body": "You could raise KeyboardInterrupt, this will lead to a graceful shutdown. There is some work in progress in this PR:\r\n#1631 \r\nBut it should already work if you raise it from within your code"
      },
      {
        "user": "nsidn98",
        "created_at": "2020-05-29T07:27:22Z",
        "body": "Got it by using `raise KeyboardInterrupt` in the code.\r\nClosing the issue"
      }
    ],
    "satisfaction_conditions": [
      "Supports programmatic training termination based on custom criteria",
      "Maintains compatibility with existing early stopping functionality",
      "Provides graceful shutdown mechanism",
      "Works within callback architecture"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:05:49"
    }
  },
  {
    "number": 1980,
    "title": "Collect all  losses into a list?",
    "created_at": "2020-05-28T10:09:45Z",
    "closed_at": "2020-05-29T23:08:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1980",
    "body": "What's the easiest way to return a list with all my losses in it? Note: I don't want to log it, but I want to have a list that I can plot in altair after training is done.\r\n\r\nCurrently, I'm maintaining a list outside the lightning module:\r\n\r\n```\r\nlosses = [] # <-- this one\r\n...\r\n\r\nclass MyModule(LightningModule):\r\n    def validation_epoch_end(self, outputs):\r\n            avg_loss = torch.stack([ x['val_loss'] for x in outputs ]).mean()\r\n            correct = sum([ x['correct'] for x in outputs])\r\n            accuracy = float(correct) / len(outputs)\r\n            losses.append(avg_loss)   # <--- append to outside var here\r\n            \r\n            return {'avg_loss' : avg_loss, 'accuracy': accuracy, 'log' : {'val_loss': avg_loss, 'accuracy': accuracy}}\r\n```\r\n\r\nIs there a \"lightning\" way of doing this?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1980/comments",
    "author": "drozzy",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T22:53:41Z",
        "body": "Why not save it into an attribute self.losses and then have it accessible via model.losses from outside?"
      }
    ],
    "satisfaction_conditions": [
      "Solution must store losses internally within the LightningModule",
      "Loss collection must be accessible post-training for visualization",
      "Implementation must avoid Lightning's logging system",
      "Solution should follow PyTorch Lightning conventions"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:05:57"
    }
  },
  {
    "number": 1978,
    "title": "Best practice: How to convert a tensor created in __init__() to the same device as the model",
    "created_at": "2020-05-28T07:23:57Z",
    "closed_at": "2020-05-28T21:28:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1978",
    "body": "#### Question\r\nSay I create a tensor in the `__init__()` of a module, what is the best practice for me to convert it to the same device as the module so that it works for cpu/gpu/distributed training environment?\r\n\r\n#### Things that I tried\r\nI tried to put this type check into the `prepare_data()` function, and run the module in GPU, but it seems like the module type is still in CPU when this function is called. \r\n\r\nI then tried to put in my `forward()` function, because this is eventually where the train/validation/test will end up with, but this looks ugly because we need to do this check at every step, but essentially we only need to call do it once. Is there a better solution? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1978/comments",
    "author": "DKandrew",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-05-28T21:28:43Z",
        "body": "This is a PyTorch question. Register it as a buffer.\r\nSee pytorch docs for register_buffer."
      },
      {
        "user": "DKandrew",
        "created_at": "2020-05-29T05:54:16Z",
        "body": "WOWW this is awesome!! Thank you!!!"
      }
    ],
    "satisfaction_conditions": [
      "Automatic device synchronization for tensors created in __init__",
      "One-time initialization that persists across training environments",
      "Compatibility with PyTorch's device management system",
      "Support for dynamic device changes",
      "Alignment with PyTorch module lifecycle"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:06:01"
    }
  },
  {
    "number": 1930,
    "title": "How to access training and validation losses from callbacks? ",
    "created_at": "2020-05-23T08:45:53Z",
    "closed_at": "2020-05-23T18:52:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1930",
    "body": "For example, if my validation_epoch_end in the trainer returns {'avg_loss':loss, 'log':logs}, how to get the loss value from a callback method like:def on_validation_end(trainer, pl_module)?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1930/comments",
    "author": "NagarajSMurthy",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-05-23T08:46:28Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "MaharshiYeluri01",
        "created_at": "2020-05-23T16:00:43Z",
        "body": "you can access the current epoch variables from trainer.callback_metrics\r\n/looks something like below\r\n{'epoch': 4,\r\n 'loss': tensor(0.4924, device='cuda:0'),\r\n 'train_loss': tensor(0.4924, device='cuda:0'),\r\n 'val_auc': tensor(0.7359, dtype=torch.float64),\r\n 'val_loss': tensor(0.7714, device='cuda:0')}"
      },
      {
        "user": "NagarajSMurthy",
        "created_at": "2020-05-23T18:52:35Z",
        "body": "Thank you @MaharshiYeluri01. That answers clearly. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to retrieve both training and validation loss values from the trainer's available metrics during callback execution",
      "Clarification of where epoch-specific metrics are stored in the trainer object",
      "Demonstration of accessing metrics without modifying existing validation_epoch_end implementation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:06:06"
    }
  },
  {
    "number": 1429,
    "title": "CUDA - numpy related copy error?",
    "created_at": "2020-04-09T14:32:21Z",
    "closed_at": "2020-04-09T16:36:05Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1429",
    "body": "```\r\nimport os\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nfrom sklearn.metrics import accuracy_score\r\nfrom pytorch_lightning import Trainer\r\nfrom argparse import ArgumentParser\r\n\r\nimport pytorch_lightning as pl\r\n\r\ninput_channel = 1\r\nout_channel = 64\r\nclass MNISTModel(pl.LightningModule):\r\n\r\n    def __init__(self,parameters):\r\n        super(MNISTModel, self).__init__()\r\n        # not the best model...\r\n        self.params = parameters\r\n        \r\n        self.conv1 = torch.nn.Conv2d(in_channels=1,out_channels=10,kernel_size=3)\r\n        self.pool1 = torch.nn.MaxPool2d(2)\r\n\r\n        self.conv2 = torch.nn.Conv2d(in_channels=10,out_channels=20,kernel_size=3)\r\n        self.pool2 = torch.nn.MaxPool2d(2)\r\n\r\n        self.conv3 = torch.nn.Conv2d(in_channels=20,out_channels=30,kernel_size=3)\r\n        self.dropout1 = torch.nn.Dropout2d(p=0.25) # probabilit 0.25\r\n\r\n        self.fc3 = torch.nn.Linear(30*3*3,270)\r\n        self.fc4 = torch.nn.Linear(270,26)\r\n\r\n        self.softmax = torch.nn.LogSoftmax(dim=1)\r\n\r\n\r\n    def forward(self, x):\r\n        # called with self(x)\r\n        x = F.relu(self.conv1(x))\r\n        x = self.pool1(x)\r\n\r\n        x = F.relu(self.conv2(x))\r\n        x = self.pool2(x)\r\n\r\n        x = F.relu(self.conv3(x))\r\n        x = self.dropout1(x)\r\n        x = x.view(-1, 30 * 3 * 3) \r\n        x = F.relu(self.fc3(x))\r\n        x = F.relu(self.fc4(x))\r\n        return self.softmax(x)\r\n\r\n\r\n\r\n    def training_step(self, batch, batch_nb):\r\n        # REQUIRED\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        tensorboard_logs = {'train_loss': loss}\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        # OPTIONAL\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        logits = F.log_softmax(y_hat, dim=1)\r\n        preds = torch.topk(logits, dim=1, k=1)[1].view(-1)\r\n        accuracy = accuracy_score(y,  preds)\r\n\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return {'val_loss': loss,\r\n                'accuracy': torch.tensor(accuracy)}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        # OPTIONAL\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        tensorboard_logs = {'val_loss': avg_loss}\r\n        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\r\n\r\n    def test_step(self, batch, batch_nb):\r\n        # OPTIONAL\r\n        return self.validation_step(batch, batch_nb)\r\n\r\n\r\n    def test_epoch_end(self, outputs):\r\n        # OPTIONAL\r\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        accuracy = torch.stack([x['accuracy'] for x in outputs]).mean()\r\n\r\n        logs = {'test_loss': avg_loss, 'test_acc': accuracy}\r\n        return {'avg_val_loss': avg_loss,\r\n                'progress_bar': logs,\r\n                'log': logs}\r\n\r\n\r\n    def configure_optimizers(self):\r\n        # REQUIRED\r\n        # can return multiple optimizers and learning_rate schedulers\r\n        # (LBFGS it is automatically supported, no need for closure function)\r\n        return torch.optim.Adadelta(self.parameters(), lr=self.params.lr)\r\n\r\n\r\n    def train_dataloader(self):\r\n        # REQUIRED\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32,num_workers=4)\r\n\r\n    def val_dataloader(self):\r\n        # OPTIONAL\r\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32,num_workers=4)\r\n\r\n    def test_dataloader(self):\r\n        # OPTIONAL\r\n        return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32,num_workers=4)\r\n\r\nif __name__ == '__main__':\r\n    args = ArgumentParser()\r\n    args.add_argument('--final_dim', type=int, default=128)\r\n    args.add_argument('--lr', type=float, default=0.02)\r\n    params = args.parse_args()\r\n   \r\n    model = MNISTModel(params)\r\n    trainer = Trainer(weights_save_path=os.getcwd(),gpus=1)\r\n    trainer.fit(model)\r\n\r\n```\r\n\r\nSorry its not exactly a snippet but not sure what seems to be the problem\r\nPyTorch version is 1.1.0\r\nTorchVision version is 0.3.0\r\nPytorch Lightning version is 0.7.2\r\nUbuntu 18.4 LTS\r\nPython version is 3.7.3\r\nCUDA Version is 10.2(Nvidia-smi , NVIDIA gt 840m 2GB VRAM)\r\n\r\nI can't find what seems to be problem\r\n\r\n\r\n```\r\nValidation sanity check:   0%|                                                                                                                                                        | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"mni.py\", line 124, in <module>\r\n    trainer.fit(model)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 705, in fit\r\n    self.single_gpu_train(model)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 476, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 844, in run_pretrain_routine\r\n    False)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 262, in _evaluate\r\n    output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 451, in evaluation_forward\r\n    output = model.validation_step(*args)\r\n  File \"mni.py\", line 69, in validation_step\r\n    accuracy = accuracy_score(y,  preds)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\", line 176, in accuracy_score\r\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\", line 72, in _check_targets\r\n    type_true = type_of_target(y_true)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\", line 247, in type_of_target\r\n    if is_multilabel(y):\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\", line 138, in is_multilabel\r\n    y = np.asarray(y)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 85, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n  File \"/home/paypaytr/anaconda3/lib/python3.7/site-packages/torch/tensor.py\", line 458, in __array__\r\n    return self.numpy()\r\nTypeError: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1429/comments",
    "author": "ugurkanates",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-09T14:33:04Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T16:36:40Z",
        "body": "accuracy_score is an SKLEARN function... it operates on numpy arrays. \r\nthe y in validation_step is a torch.Tensor...\r\n\r\nanything that goes in the accuracy_score function must be ONLY numpy arrays.\r\n\r\nbut this will slow your code a ton... instead calculate the accuracy using pure pytorch\n\n---\n\nwe are working on a metrics package to do this, but in the meantime you can do the implementation manually"
      },
      {
        "user": "ugurkanates",
        "created_at": "2020-04-09T17:20:21Z",
        "body": "Cool for now I'm turning data to CPU with \".cpu()\"\r\n\r\nBut thanks for follow up"
      },
      {
        "user": "williamFalcon",
        "created_at": "2020-04-09T17:22:44Z",
        "body": "yes but every .cpu will make your code VERY slow..."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why CUDA tensors cannot be directly converted to numpy arrays",
      "Solution maintains efficient GPU computation without unnecessary CPU transfers",
      "Demonstrates PyTorch-native accuracy calculation instead of sklearn",
      "Addresses tensor device consistency in validation metrics"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:06:14"
    }
  },
  {
    "number": 1002,
    "title": "Validation step isn't being ran ",
    "created_at": "2020-03-02T04:22:38Z",
    "closed_at": "2020-03-02T23:38:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1002",
    "body": "## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nI have been trying to get the trainer to call the validation_step function but it doesn't seem to ever get called.  I assume I am missing something obvious but have looking at the tutorials and docs I haven't been able to find the obvious.  The code for the model and trainer are below.  What might I be missing?  Thank you for the help!\r\n\r\n#### Code\r\n```\r\nclass SegModel(pl.LightningModule):\r\n    def __init__(self, batch_size, lr):\r\n        super(SegModel, self).__init__()\r\n        self.batch_size = batch_size\r\n        self.learning_rate = lr\r\n        self.net = UNet(num_classes=1)\r\n        self.transform = transforms.Compose([\r\n            transforms.ToTensor()\r\n        ])\r\n        self.trainset = Stacker(input_images, truth_images, transform=self.transform)\r\n        self.validset = Stacker(input_images, truth_images, transform=self.transform)\r\n        self.testset = Stacker(input_images, truth_images, transform=self.transform)\r\n    \r\n    def forward(self, x):\r\n        return self.net(x)\r\n    \r\n    def training_step(self, batch, batch_nb):\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'loss': loss_val, 'log': {'train_loss': loss_val}}\r\n    \r\n    def validation_step(self, batch, batch_nb):\r\n        print(\"RUNNING VALIDATION\")\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'val_loss': loss_val, \r\n                'val_dice': dice(out, mask),\r\n                'val_iou': IoU(out, mask)\r\n               }\r\n    \r\n    def test_step(self, batch, batch_nb):\r\n        img, mask = batch\r\n        img = img.float()\r\n        mask = mask.long()\r\n        out = self.forward(img)\r\n        loss_val = dice_loss(mask, out)\r\n        return {'test_loss': loss_val, \r\n                'test_dice': dice(out, mask),\r\n                'test_iou': IoU(out, mask)\r\n               }\r\n    \r\n    def validation_end(self, outputs):\r\n        if len(outputs)==0: return {}\r\n        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\r\n        val_dice_mean = torch.stack([x['val_dice'] for x in outputs]).mean()\r\n        val_iou_mean = torch.stack([x['val_iou'] for x in outputs]).mean()\r\n        return {'val_loss': val_loss_mean,\r\n                'log': {\r\n                    'val_loss': val_loss_mean,\r\n                    'val_dice': val_dice_mean,\r\n                    'val_iou': val_iou_mean\r\n                }}\r\n\r\n    def test_end(self, outputs):\r\n        if len(outputs)==0: return {}\r\n        test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()\r\n        test_dice_mean = torch.stack([x['test_dice'] for x in outputs]).mean()\r\n        test_iou_mean = torch.stack([x['test_iou'] for x in outputs]).mean()\r\n        print(test_dice_mean, test_iou_mean)\r\n        return {'test_loss': test_loss_mean,\r\n                'log': {\r\n                    'test_loss': test_loss_mean,\r\n                    'test_dice': test_dice_mean,\r\n                    'test_iou': test_iou_mean\r\n                }}\r\n    \r\n    def configure_optimizers(self):\r\n        opt = torch.optim.Adam(self.net.parameters(), lr=self.learning_rate)\r\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\r\n        return [opt], [sch]\r\n\r\n    @pl.data_loader\r\n    def train_dataloader(self):\r\n        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True)\r\n\r\n    @pl.data_loader\r\n    def valid_dataloader(self):\r\n        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=False)\r\n      \r\n    @pl.data_loader\r\n    def test_dataloader(self):\r\n        return DataLoader(self.testset, batch_size=self.batch_size, shuffle=False)\r\n\r\nmodel = SegModel(1, 0.001)\r\n\r\ntrainer = pl.Trainer(\r\n    gpus=[0], \r\n    early_stop_callback=None, \r\n    max_epochs=40,\r\n    check_val_every_n_epoch=1,\r\n)\r\n\r\ntrainer.fit(model)\r\n```\r\n\r\n#### What's your environment?\r\n\r\n - OS: Windows\r\n - Packaging: conda\r\n - Version: 0.6.1\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1002/comments",
    "author": "RyMo95",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-03-02T04:23:15Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-02T22:04:51Z",
        "body": "Hi @RyMo95 , I believe the issue is that your method is not named correctly. \r\n\r\n`valid_dataloader` -> `val_dataloader` should resolve this, please let me know if that doesn\u2019t work!"
      },
      {
        "user": "RyMo95",
        "created_at": "2020-03-02T23:38:10Z",
        "body": "@jeremyjordan Ah that was it, so careless! Appreciate the help :)"
      },
      {
        "user": "jeremyjordan",
        "created_at": "2020-03-03T01:11:13Z",
        "body": "no problem! :) "
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct method naming convention for validation data loaders in PyTorch Lightning",
      "Ensures validation data loader is properly recognized by the training framework",
      "Explains how validation workflow is triggered in the framework",
      "Addresses framework-specific requirements for hook methods"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:06:35"
    }
  },
  {
    "number": 877,
    "title": "How do I test before any training?",
    "created_at": "2020-02-17T06:34:46Z",
    "closed_at": "2020-02-17T09:40:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/877",
    "body": "## \u2753 Questions and Help\r\n\r\nI am now migrating some of my previous works into lightning. I wish to see if it is able to reproduce my previous results or not. But the doc implies that all the testing has to be performed after training or after loading the previous lightning training state, which I do not have either.\r\n\r\nSo How can I test before training?\r\n\r\n#### Code\r\n\r\n```python\r\n\r\n    trainer = Trainer(logger=logger, max_epochs=5, gpus=[3], distributed_backend=None)\r\n    hparams = HParams(fold=fold, model=model_name, batch_size=8, num_workers=16)\r\n    system = MySYS(hparams, trainer)\r\n\r\n    system.model.load_state_dict(torch.load(state_dict))\r\n    trainer.test()\r\n```\r\nIt cannot work since the trainer does not initialize at all.\r\n\r\n#### What have you tried?\r\nI found inside the code for testing:\r\n```python\r\n\r\n    def test(self, model=None):\r\n        r\"\"\"\r\n\r\n        Separates from fit to make sure you never run on your test set until you want to.\r\n\r\n        Args:\r\n            model (LightningModule): The model to test.\r\n\r\n        Example::\r\n\r\n            # Option 1\r\n            # run test after fitting\r\n            trainer = Trainer()\r\n            model = LightningModule()\r\n\r\n            trainer.fit()\r\n            trainer.test()\r\n\r\n            # Option 2\r\n            # run test from a loaded model\r\n            model = LightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\r\n            trainer = Trainer()\r\n            trainer.test(model)\r\n        \"\"\"\r\n        self.testing = True\r\n        if model is not None:\r\n            self.fit(model)\r\n        else:\r\n            self.run_evaluation(test=True)\r\n```\r\nWhich requires to fit the model that I do not understand at all. Why a fitting is required inside training code? If for the purpose of initialization, can't we just put some init code here?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/877/comments",
    "author": "shijianjian",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-02-17T06:35:28Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "festeh",
        "created_at": "2020-02-17T07:18:03Z",
        "body": "If you dig a bit deeper into the sources, you'll find that `fit` actually calls `evaluate` and do not fit a model if `testing` flag is True. So you can run `trainer.test(system)`, it should probably work."
      },
      {
        "user": "shijianjian",
        "created_at": "2020-02-17T09:40:20Z",
        "body": "Yes, it worked. Sorry for the dummy question."
      }
    ],
    "satisfaction_conditions": [
      "Demonstrate how to perform testing without requiring prior training or checkpoint loading",
      "Explain how to properly initialize the trainer for standalone testing",
      "Clarify the relationship between model initialization and testing workflow",
      "Show how to bypass the training phase while maintaining required system initialization"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:06:41"
    }
  },
  {
    "number": 690,
    "title": "How to make test_end() return metrics ",
    "created_at": "2020-01-15T17:47:23Z",
    "closed_at": "2020-01-21T12:31:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/690",
    "body": "I have searched through the docs / Google as well as looked through the source code.\r\n\r\nIt seems like test_end() returns nothing (it has no `return` in the function). I was wondering if I was missing something really obvious. \r\n\r\nI would simply like to return the metrics of the test end.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/690/comments",
    "author": "Laksh1997",
    "comments": [
      {
        "user": "matthew-z",
        "created_at": "2020-01-19T14:09:06Z",
        "body": "You may try this:\r\n\r\n```py\r\nclass MyModel(pl.LightningModule):\r\n  def __init__(self, ...):\r\n    self.test_result = None\r\n  def test_end(self, outputs):\r\n    self.test_result = get_eval_metrics(outputs)\r\n\r\nmodel = MyModel()\r\ntrainer.test(model)\r\nprint(model.test_result)\r\n```\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to access test metrics after test phase completion",
      "Integration with PyTorch Lightning's test workflow",
      "Aggregation of batch-level outputs into final metrics",
      "Persistence of results beyond test_end() execution"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:06:46"
    }
  },
  {
    "number": 12177,
    "title": "[RFC] Where to save checkpoints and profiler output in case of multiple loggers?",
    "created_at": "2022-03-02T07:34:55Z",
    "closed_at": "2022-08-26T17:23:55Z",
    "labels": [
      "help wanted",
      "discussion",
      "logger",
      "checkpointing",
      "profiler"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/12177",
    "body": "### Background \r\nCurrently, in the case that the user does not specify a `dirpath` to `ModelCheckpoint` or a `dirpath` to the `Profiler`, we write our output to the Logger, when there is exactly one Logger. However, in the case of multiple loggers we just write to `default_root_dir`.\r\n\r\nThe exact priority followed for one logger:\r\n1. if a `dirpath` is provided, write to it\r\n2. if not, write to the Logger\r\n\r\nThe exact priority followed for multiple loggers (or no loggers):\r\n1. if a `dirpath` is provided, write to it\r\n2. if not, write to `default_root_dir`\r\n\r\nThe purpose of this issue is to make sure everyone is aware of this behavior, and to see if there is a way to improve it.\r\n\r\n### Discussion Questions\r\nI'd like to discuss two questions here.\r\n1. Can we confirm that this is the behavior we want for when we have one logger?\r\n2. What behavior do we want for when we have multiple loggers? I see a few options:\r\n    A. Write to `default_root_dir` (keep current behavior)\r\n    B. Write to first logger in the list of loggers\r\n    C. Write to all loggers in the list of loggers \r\n\r\n\r\n\n\ncc @borda @awaelchli @edward-io @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy @ninginthecloud @otaj @carmocca @kaushikb11 @nbcsm @guotuofeng",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/12177/comments",
    "author": "daniellepintz",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2022-03-12T02:34:30Z",
        "body": "> Can we confirm that this is the behavior we want for when we have one logger?\r\n\r\nI can confirm. From my side it is clear this is a good default behavior and would prefer to keep it this way.\r\n\r\n\r\n> What behavior do we want for when we have multiple loggers? I see a few option\r\n\r\n**I vote for option B.** This gives the benefit of versioning. The default root dir does not get polluted and you don't have to \"invent\" a name for the folder. Option C is not practical IMO due to the duplication of large checkpoint files."
      },
      {
        "user": "daniellepintz",
        "created_at": "2022-03-12T03:46:28Z",
        "body": "Thanks @awaelchli for your input!! \r\n\r\nI like option B as well! I agree option C seems impractical.\n\n---\n\nWould be great to get some more opinions @PyTorchLightning/core-lightning"
      },
      {
        "user": "tchaton",
        "created_at": "2022-03-14T19:30:52Z",
        "body": "Same here, I would vote for B. Let's make sure this gets documented."
      },
      {
        "user": "daniellepintz",
        "created_at": "2022-03-15T13:51:44Z",
        "body": "I think we can go with option B, since two people are in favor and most other people probably don't have an opinion"
      }
    ],
    "satisfaction_conditions": [
      "Solution must prioritize versioned logger directories over default_root_dir when multiple loggers exist",
      "Solution must prevent checkpoint/profiler output duplication across multiple loggers",
      "Solution must maintain deterministic behavior for file storage locations",
      "Solution must preserve existing single-logger behavior while enhancing multi-logger handling",
      "Solution must include clear documentation of storage location priorities"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:07:01"
    }
  },
  {
    "number": 10285,
    "title": "UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop",
    "created_at": "2021-11-01T05:06:39Z",
    "closed_at": "2021-12-08T11:31:38Z",
    "labels": [
      "bug",
      "help wanted",
      "won't fix"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/10285",
    "body": "## \ud83d\udc1b Bug\r\n\r\nUsing pytorch-lightning 1.5.0rc1, I will get UserWarning:\r\n```\r\npytorch_lightning/trainer/configuration_validator.py:156: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\r\n  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\r\n```\r\nBut with pytorch-lightning 1.4.9, there is no such warning.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom collections import OrderedDict\r\nimport pytorch_lightning as pl\r\n\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\n\r\nclass TestLrModule(pl.LightningModule):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(TestLrModule, self).__init__()\r\n        self._fc = OrderedDict([\r\n            ('fc0', nn.Linear(input_size, hidden_size)),\r\n            ('tan0', nn.ReLU()),\r\n            ('fc1', nn.Linear(hidden_size, 1)),\r\n        ])\r\n        self.fc = nn.Sequential(self._fc)\r\n        self._loss_fn = nn.MSELoss()\r\n\r\n    def forward(self, x):\r\n        y = self.fc(x)\r\n        return y.squeeze(dim=1)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return self._loss_fn(y_hat, y)\r\n\r\n    def training_epoch_end(self, outputs):\r\n        loss = torch.mean(torch.stack([x['loss'] for x in outputs]))\r\n        self.log('train_loss', loss, on_epoch=True)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        return self._loss_fn(y_hat, y)\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        loss = torch.mean(torch.stack(outputs))\r\n        self.log('val_loss', loss, on_epoch=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=2e-3, weight_decay=1e-4)\r\n\r\n\r\nsample, feature = 4000, 10\r\nrx, ry = torch.rand(sample, feature), torch.rand(sample)\r\ntest_sample = int(sample * 0.2)\r\ntest_rx, test_ry = torch.rand(test_sample, feature), torch.rand(test_sample)\r\n\r\ntrain_data = DataLoader(TensorDataset(rx, ry), batch_size=32, num_workers=2)\r\nvalid_data = DataLoader(TensorDataset(test_rx, test_ry), batch_size=32, num_workers=2)\r\n\r\nm = TestLrModule(rx.shape[1], 16)\r\ntrainer = pl.Trainer(max_epochs=20)\r\ntrainer.fit(m, train_data, valid_data)\r\n```\r\n\r\n### Environment\r\n\r\n- PyTorch Version  1.8.0\r\n- Python version: 3.8.5\r\n- OS (e.g., Linux): linux\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/10285/comments",
    "author": "7starsea",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-11-01T09:14:50Z",
        "body": "it was fixed recently. Can you try master?"
      },
      {
        "user": "7starsea",
        "created_at": "2021-11-01T13:33:53Z",
        "body": "Thanks. With the master version, the ```UserWarning: you defined a validation_step but have no val_dataloader``` disappears. \r\n\r\nBy the way, I am not sure should I take care of the following ```UserWarning```:\r\n```configuration_validator.py:102: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).```\r\n\r\nThe sample code is like\r\n```\r\nclass TestLrModule(pl.LightningModule):\r\n    # standard training/validation_step here\r\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\r\n        optimizer.zero_grad(set_to_none=True)\r\n\r\ntrainer = pl.Trainer(max_epochs=max_epochs, callbacks=[early_stop_callback],\r\n                          check_val_every_n_epoch=4, accumulate_grad_batches=6)\r\n```\r\n\r\nThanks."
      },
      {
        "user": "stale[bot]",
        "created_at": "2021-12-08T03:34:02Z",
        "body": "This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-12-08T11:31:38Z",
        "body": "@7starsea apologies for the late reply. While accumulating, optimization doesn't happen at every step thus optimizer_step won't be called right after every training batch but only when the accumulation interval is complete. This is just a warning for the user to make sure they write their own custom logic within the `optimizer_step` taking the accumulation flag, set inside Trainer, into consideration."
      },
      {
        "user": "gezabohus",
        "created_at": "2024-07-14T11:02:21Z",
        "body": "This is still happening, pytorch-lightning 2.3.3, python 3.10."
      },
      {
        "user": "llctrautmann",
        "created_at": "2024-08-02T21:06:30Z",
        "body": "I am having the same issue with pytorch-lightning 2.3.3, python 3.11. Happy to share my training loop if that helps. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why validation_step triggers a warning despite having val_dataloader defined",
      "Clarification of version-specific behavior changes between 1.4.9 and 1.5.0rc1",
      "Guidance on proper validation loop configuration in newer PyTorch Lightning versions",
      "Resolution of false positive warnings about missing val_dataloader"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:07:13"
    }
  },
  {
    "number": 9862,
    "title": "AttributeError: module 'tqdm' has no attribute 'auto' on PL import",
    "created_at": "2021-10-07T18:12:56Z",
    "closed_at": "2021-10-08T14:17:09Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9862",
    "body": "## \ud83d\udc1b Bug\r\nWhen I import pl\r\n`import pytorch_lightning as pl`\r\n\r\nI get the following error:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\nC:\\Users\\S2F5D~1.RED\\AppData\\Local\\Temp/ipykernel_2984/1918236736.py in <module>\r\n      7 \r\n      8 import torch\r\n----> 9 from pytorch_lightning import  LightningModule\r\n     10 \r\n     11 get_ipython().run_line_magic('matplotlib', 'inline')\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\__init__.py in <module>\r\n     18 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT)\r\n     19 \r\n---> 20 from pytorch_lightning import metrics  # noqa: E402\r\n     21 from pytorch_lightning.callbacks import Callback  # noqa: E402\r\n     22 from pytorch_lightning.core import LightningDataModule, LightningModule  # noqa: E402\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\metrics\\__init__.py in <module>\r\n     13 # limitations under the License.\r\n     14 \r\n---> 15 from pytorch_lightning.metrics.classification import (  # noqa: F401\r\n     16     Accuracy,\r\n     17     AUC,\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\metrics\\classification\\__init__.py in <module>\r\n     12 # See the License for the specific language governing permissions and\r\n     13 # limitations under the License.\r\n---> 14 from pytorch_lightning.metrics.classification.accuracy import Accuracy  # noqa: F401\r\n     15 from pytorch_lightning.metrics.classification.auc import AUC  # noqa: F401\r\n     16 from pytorch_lightning.metrics.classification.auroc import AUROC  # noqa: F401\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\pytorch_lightning\\metrics\\classification\\accuracy.py in <module>\r\n     14 from typing import Any, Callable, Optional\r\n     15 \r\n---> 16 from torchmetrics import Accuracy as _Accuracy\r\n     17 \r\n     18 from pytorch_lightning.metrics.utils import deprecated_metrics, void\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\torchmetrics\\__init__.py in <module>\r\n     12 _PROJECT_ROOT = os.path.dirname(_PACKAGE_ROOT)\r\n     13 \r\n---> 14 from torchmetrics import functional  # noqa: E402\r\n     15 from torchmetrics.audio import PIT, SI_SDR, SI_SNR, SNR  # noqa: E402\r\n     16 from torchmetrics.average import AverageMeter  # noqa: E402\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\torchmetrics\\functional\\__init__.py in <module>\r\n     58 from torchmetrics.functional.retrieval.reciprocal_rank import retrieval_reciprocal_rank\r\n     59 from torchmetrics.functional.self_supervised import embedding_similarity\r\n---> 60 from torchmetrics.functional.text.bert import bert_score\r\n     61 from torchmetrics.functional.text.bleu import bleu_score\r\n     62 from torchmetrics.functional.text.rouge import rouge_score\r\n\r\n~\\Anaconda3\\envs\\da\\lib\\site-packages\\torchmetrics\\functional\\text\\bert.py in <module>\r\n    245 \r\n    246 \r\n--> 247 def _get_progress_bar(dataloader: DataLoader, verbose: bool = False) -> Union[DataLoader, tqdm.auto.tqdm]:\r\n    248     \"\"\"Helper function returning either the dataloader itself when `verbose = False`, or it wraps the dataloader with\r\n    249     `tqdm.auto.tqdm`, when `verbose = True` to display a progress bar during the embbeddings calculation.\"\"\"\r\n\r\nAttributeError: module 'tqdm' has no attribute 'auto'\r\n```\r\n\r\n### Expected behavior\r\nSuccessful import\r\n\r\n### Environment\r\nLibraries installed using conda\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.21.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.1 \r\n        - pytorch-lightning: 1.4.9\r\n        - tqdm:              4.62.2\r\n* System:\r\n        - OS:                Windows\r\n        - architecture:\r\n                - 64bit\r\n                - WindowsPE\r\n        - processor:         Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\r\n        - python:            3.9.7\r\n        - version:           10.0.19042\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9862/comments",
    "author": "sidwa",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "created_at": "2021-10-08T09:57:54Z",
        "body": "hi,\r\n\r\nCan you try downgrading tqdm or Pytorch lightning?\r\n\r\nRegards"
      },
      {
        "user": "sidwa",
        "created_at": "2021-10-08T14:17:03Z",
        "body": "do you have a last working version in mind?\n\n---\n\nImport works with tqdm version 4.50.0 (arbitrarily chosen older version)."
      },
      {
        "user": "Programmer-RD-AI",
        "created_at": "2021-10-08T14:38:51Z",
        "body": "> do you have a last working version in mind?\r\n\r\nI am new in the PyTorch-Community so I don't know\r\n\r\nRegards.\r\n\r\n\n\n---\n\nyou can try 1.4.8 or 1.4.7"
      },
      {
        "user": "sidwa",
        "created_at": "2021-10-08T14:58:08Z",
        "body": "@Programmer-RD-AI  Problem's fixed, I downgraded the tqdm package. Besides, 1.4.x isn't supported in the latest version of lightning, it needs >1.6 (environment.yml). Thank you for the suggestions"
      },
      {
        "user": "Programmer-RD-AI",
        "created_at": "2021-10-08T14:59:10Z",
        "body": "ok no problem \r\n\r\n"
      },
      {
        "user": "Rustemhak",
        "created_at": "2023-05-29T21:29:14Z",
        "body": "> @Programmer-RD-AI Problem's fixed, I downgraded the tqdm package. Besides, 1.4.x isn't supported in the latest version of lightning, it needs >1.6 (environment.yml). Thank you for the suggestions\r\n\r\n\r\nCan you please tell the version of what > 1.6? And I don\u2019t understand, what does environment.yml have to do with it?\r\nIt is desirable that you can please share the final versions, if you still have it.\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Resolve version incompatibility between tqdm and PyTorch Lightning/torchmetrics",
      "Ensure tqdm's API includes the 'auto' attribute required by torchmetrics",
      "Provide clear version compatibility guidance without breaking other dependencies"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:07:21"
    }
  },
  {
    "number": 9488,
    "title": "Getting error with Pytorch lightning when passing model checkpoint",
    "created_at": "2021-09-13T14:43:28Z",
    "closed_at": "2021-09-13T17:52:04Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9488",
    "body": "I am training a multi-label classification problem using Hugging face models.\r\n\r\nI am using `Pytorch lightning` to train the model.\r\n \r\n\r\nHere is the code:\r\n\r\nAnd early stopping triggers when the loss hasn't improved for the last \r\n\r\n    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\r\n\r\nWe can start the training process:\r\n\r\n\r\n    checkpoint_callback = ModelCheckpoint(\r\n      dirpath=\"checkpoints\",\r\n      filename=\"best-checkpoint\",\r\n      save_top_k=1,\r\n      verbose=True,\r\n      monitor=\"val_loss\",\r\n      mode=\"min\"\r\n    )\r\n\r\n\r\n    trainer = pl.Trainer(\r\n      logger=logger,\r\n      callbacks=[early_stopping_callback],\r\n      max_epochs=N_EPOCHS,\r\n     checkpoint_callback=checkpoint_callback,\r\n      gpus=1,\r\n      progress_bar_refresh_rate=30\r\n    )\r\n    # checkpoint_callback=checkpoint_callback,\r\n\r\nAs soon as I run this, I get error:\r\n\r\n\r\n    ~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py in _configure_checkpoint_callbacks(self, checkpoint_callback)\r\n         75             if isinstance(checkpoint_callback, Callback):\r\n         76                 error_msg += \" Pass callback instances to the `callbacks` argument in the Trainer constructor instead.\"\r\n    ---> 77             raise MisconfigurationException(error_msg)\r\n         78         if self._trainer_has_checkpoint_callbacks() and checkpoint_callback is False:\r\n         79             raise MisconfigurationException(\r\n    \r\n    MisconfigurationException: Invalid type provided for checkpoint_callback: Expected bool but received <class 'pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint'>. Pass callback instances to the `callbacks` argument in the Trainer constructor instead.\r\n\r\n\r\n**How can I fix this issue?**",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9488/comments",
    "author": "pratikchhapolika",
    "comments": [
      {
        "user": "rohitgr7",
        "created_at": "2021-09-13T14:53:17Z",
        "body": "`checkpoint_callback` supports only a bool value. If set to True, it will create a model checkpoint instance internally, but if you want to assign your own custom instance then pass it within callbacks:\r\n```py\r\ntrainer = Trainer(callbacks=[checkpoint_callback, early_stopping_callback], ...)\r\n```"
      },
      {
        "user": "pratikchhapolika",
        "created_at": "2021-09-13T17:52:04Z",
        "body": "> `checkpoint_callback` supports only a bool value. If set to True, it will create a model checkpoint instance internally, but if you want to assign your own custom instance then pass it within callbacks:\r\n> \r\n> ```python\r\n> trainer = Trainer(callbacks=[checkpoint_callback, early_stopping_callback], ...)\r\n> ```\r\n\r\nThanks. Closing."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to properly pass custom callback instances in PyTorch Lightning's Trainer",
      "Clarification about boolean vs callback instance parameters in PyTorch Lightning",
      "Compatibility with the user's PyTorch Lightning version requirements",
      "Preservation of both early stopping and checkpointing functionality"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:07:36"
    }
  },
  {
    "number": 9176,
    "title": "on_save_checkoint never called",
    "created_at": "2021-08-28T15:11:45Z",
    "closed_at": "2021-08-29T00:01:00Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9176",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI wrote a `Callback` class and found `on_save_checkpoint` had never been called\r\n\r\n### To Reproduce\r\n\r\nMy callback class:\r\n```\r\nfrom pytorch_lightning.callbacks import Callback\r\nfrom os.path import join\r\nimport torch\r\nimport os\r\nimport pytorch_lightning as pl\r\nfrom typing import Dict, Any, Optional\r\n\r\n\r\nclass JitSave(Callback):\r\n\r\n    def __init__(self):\r\n        self.outputs = None\r\n        self.n_dataloaders = None\r\n\r\n    def on_save_checkpoint(\r\n        self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', checkpoint: Dict[\r\n                str, Any]\r\n    ) -> dict:\r\n        # Torch.jit.save\r\n        jit_model_dir = join(\r\n            join(os.getcwd(), \"checkpoints\"), f\"jit_{self.logger[0].version}\" + \"{}.pt\"\r\n        )\r\n        torch.jit.save(self.model.cpu().to_torchscript(), jit_model_dir.format(\"cpu\"))\r\n        torch.jit.save(self.model.to_torchscript(), jit_model_dir.format(\"gpu\"))\r\n        print(f\"torch.jit.save path :\\n{jit_model_dir}\")\r\n        # return {\"jitsave_path\": jit_model_dir}\r\n        return checkpoint\r\n\r\n    def setup(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', stage: Optional[str] = None) -> None:\r\n        self.n_dataloaders = len(pl_module.val_dataloader())\r\n\r\n    def _reset(self):\r\n        self.outputs = [[] for _ in range(self.n_dataloaders)]\r\n\r\n    def on_epoch_start(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:\r\n        self._reset()\r\n\r\n    def on_validation_epoch_end(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:\r\n        for dataloader_idx, output in enumerate(self.outputs):\r\n            pass\r\n```\r\n`on_validation_epoch_end` works but on_save_checkpoint not.\r\n\r\nThis is my `ModelCheckpoint`:\r\n\r\n```\r\nmodel_checkpoint:\r\n  _target_: pytorch_lightning.callbacks.ModelCheckpoint\r\n  monitor: \"val/f1\" # name of the logged metric which determines when model isimproving\r\n  mode: \"max\" # can be \"max\" or \"min\"\r\n  save_top_k: 1 # save k best models (determined by above metric)\r\n  save_last: False # additionaly always save model from last epoch\r\n  verbose: False\r\n  dirpath: \"checkpoints/\"\r\n  filename: \"epoch_{epoch:03d}\"\r\n  auto_insert_metric_name: False\r\n  save_weights_only: True\r\n```\r\n\r\nCallbacks are passed to the trainer:\r\n\r\n```\r\ncallbacks: List[Callback] = []\r\n    if \"callbacks\" in config:\r\n        for _, cb_conf in config.callbacks.items():\r\n            if \"_target_\" in cb_conf:\r\n                log.info(f\"Instantiating callback <{cb_conf._target_}>\")\r\n                callbacks.append(hydra.utils.instantiate(cb_conf))\r\n```\r\n\r\n```\r\ntrainer: Trainer = hydra.utils.instantiate(\r\n        config.trainer, callbacks=callbacks, logger=logger, _convert_=\"partial\"\r\n    )\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n`on_save_checkpoint` should be called.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.5\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.9.0+cu102\r\n        - pytorch-lightning: 1.4.2\r\n        - tqdm:              4.62.1\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.10\r\n        - version:           #60~20.04.1-Ubuntu SMP Thu May 6 09:52:46 UTC 2021",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9176/comments",
    "author": "zhiyuanpeng",
    "comments": [
      {
        "user": "ananthsub",
        "created_at": "2021-08-28T17:14:14Z",
        "body": "you're specifying `save_weights_only=True` so no callback states are added to the checkpoint. this means the callback's `on_save_checkpoint` is never called"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how ModelCheckpoint's save_weights_only parameter affects callback checkpointing",
      "Clarification of requirements for triggering on_save_checkpoint in custom callbacks",
      "Identification of configuration settings that control callback state preservation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:07:47"
    }
  },
  {
    "number": 9155,
    "title": "AttributeError: Can't pickle local object when attempting multi-GPU training",
    "created_at": "2021-08-27T00:24:54Z",
    "closed_at": "2021-08-27T18:42:58Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 1"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/9155",
    "body": "## \ud83d\udc1b Bug\r\n\r\nRunning the provided script with multiple GPUs causes the following error:\r\n```\r\n$ python pickle_test.py\r\n.../lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:746: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `accelerator=\"ddp_spawn\"` for you.\r\n  rank_zero_warn(\r\nGPU available: True, used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\n.../lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:99: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping val loop\r\n  rank_zero_warn(f\"you passed in a {loader_name} but have no {step_name}. Skipping {stage} loop\")\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\nTraceback (most recent call last):\r\n  File \"pickle_test.py\", line 81, in <module>\r\n    test_x(tmpdir)\r\n  File \"pickle_test.py\", line 77, in test_x\r\n    trainer.fit(model=model, datamodule=dm)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\r\n    self._run(model)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\r\n    self._dispatch()\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\r\n    self.accelerator.start_training(self)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\r\n    self.training_type_plugin.start_training(trainer)\r\n  File \".../lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 158, in start_training\r\n    mp.spawn(self.new_process, **self.mp_spawn_kwargs)\r\n  File \".../lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \".../lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \".../lib/python3.8/multiprocessing/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \".../lib/python3.8/multiprocessing/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \".../lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \".../lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'LightningDataModule.from_datasets.<locals>.train_dataloader'\r\n```\r\n\r\n### To Reproduce\r\n\r\nThe following script causes the bug:\r\n```python\r\nimport pytorch_lightning as pl\r\nfrom pytorch_lightning import LightningDataModule\r\nfrom torch.nn import functional as F\r\nfrom torchvision import transforms\r\nfrom torchvision.datasets import MNIST\r\n\r\ntmpdir = '../../data'\r\n\r\n\r\ndef mnist(root: str, normalize: bool = False):\r\n    tlist = [transforms.ToTensor()]\r\n\r\n    if normalize:\r\n        tlist.append(transforms.Normalize((0.5,), (0.5,)))\r\n\r\n    transform = transforms.Compose(tlist)\r\n\r\n    trainset = MNIST(root=root, train=True, download=True, transform=transform)\r\n    testset = MNIST(root=root, train=False, download=True, transform=transform)\r\n    return trainset, testset\r\n\r\n\r\ndef mnist_datamodule(data_path: str, batch_size: int, num_workers: int):\r\n    train, val = mnist(data_path, normalize=True)\r\n    return LightningDataModule.from_datasets(train, val, None, batch_size=batch_size, num_workers=num_workers)\r\n\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc1 = torch.nn.Linear(28 * 28, 32)\r\n        self.fc2 = torch.nn.Linear(32, 10)\r\n\r\n    def forward(self, x):\r\n        x = torch.flatten(x, 1)\r\n        x = F.sigmoid(self.fc1(x))\r\n        x = F.softmax(self.fc2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n\r\ndef test_x(tmpdir):\r\n    # init model\r\n    model = BoringModel()\r\n\r\n    # Initialize a trainer\r\n    trainer = pl.Trainer(\r\n        max_epochs=1,\r\n        progress_bar_refresh_rate=20,\r\n        gpus=2\r\n    )\r\n\r\n    dm = mnist_datamodule(tmpdir, 16, 1)\r\n\r\n    # Train the model \u26a1\r\n    trainer.fit(model=model, datamodule=dm)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_x(tmpdir)\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\nRunning on a SLURM cluster\r\n- PyTorch Lightning Version (e.g., 1.3.0): 1.4.1\r\n- PyTorch Version (e.g., 1.8): 1.9.0\r\n- Python version: 3.8.0\r\n- OS (e.g., Linux): Linux HPCC\r\n- CUDA/cuDNN version: 10.1\r\n- GPU models and configuration: 2x 2080\r\n- How you installed PyTorch (`conda`, `pip`, source): conda\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/9155/comments",
    "author": "import-antigravity",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-08-27T11:26:21Z",
        "body": "@import-antigravity this is because `LightningModule.from_datasets` patches out the data loader methods. \r\nYou will have to select `accelerator=\"ddp\"` as a workaround."
      },
      {
        "user": "import-antigravity",
        "created_at": "2021-08-27T15:28:24Z",
        "body": "> @import-antigravity this is because `LightningModule.from_datasets` patches out the data loader methods.\r\n> You will have to select `accelerator=\"ddp\"` as a workaround.\r\n\r\nThat solved it, thanks"
      },
      {
        "user": "tchaton",
        "created_at": "2021-08-27T18:42:55Z",
        "body": "Dear @import-antigravity,\r\n\r\nClosing this issue as it is expected behaviour with the current design and there is an alternative.\r\n\r\nBest,\r\nT.C"
      }
    ],
    "satisfaction_conditions": [
      "Solution must resolve pickling limitations with local functions in multi-process training",
      "Must support multi-GPU training without requiring modification of core model architecture",
      "Should explain the relationship between parallelization strategy and serialization requirements",
      "Must maintain compatibility with PyTorch Lightning's DataModule paradigm"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:07:55"
    }
  },
  {
    "number": 8302,
    "title": "TypeError: training_step_end() missing 1 required positional argument: 'batch_idx'",
    "created_at": "2021-07-06T06:39:00Z",
    "closed_at": "2021-07-06T07:13:44Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/8302",
    "body": "My first time using lightning. Basically, I am trying to convert the following code into lightning format:\r\n```\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\nmodel.to(device)\r\n\r\navg_loss = 0.\r\navg_output_std = 0.\r\nfor e in range(epochs):\r\n\r\n    for (x0, x1), _, _ in dataloader_train_simsiam:\r\n\r\n        # move images to the gpu\r\n        x0 = x0.to(device)\r\n        x1 = x1.to(device)\r\n\r\n        # run the model on both transforms of the images\r\n        # the output of the simsiam model is a y containing the predictions\r\n        # and projections for each input x\r\n        y0, y1 = model(x0, x1)\r\n\r\n        # backpropagation\r\n        loss = criterion(y0, y1)\r\n        loss.backward()\r\n\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n\r\n        # calculate the per-dimension standard deviation of the outputs\r\n        # we can use this later to check whether the embeddings are collapsing\r\n        output, _ = y0\r\n        output = output.detach()\r\n        output = torch.nn.functional.normalize(output, dim=1)\r\n\r\n        output_std = torch.std(output, 0)\r\n        output_std = output_std.mean()\r\n\r\n        # use moving averages to track the loss and standard deviation\r\n        w = 0.9\r\n        avg_loss = w * avg_loss + (1 - w) * loss.item()\r\n        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\r\n\r\n    # the level of collapse is large if the standard deviation of the l2\r\n    # normalized output is much smaller than 1 / sqrt(dim)\r\n    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\r\n    # print intermediate results\r\n    print(f'[Epoch {e:3d}] '\r\n        f'Loss = {avg_loss:.2f} | '\r\n        f'Collapse Level: {collapse_level:.2f} / 1.00')\r\n```\r\nWhat I have done so far is this:\r\n```\r\nclass SimSiamModel(pl.LightningModule):\r\n    def __init__(self, backbone, num_ftrs, pred_hidden_dim, out_dim, num_mlp_layers):\r\n        super().__init__()\r\n\r\n        # create a moco based on ResNet\r\n        self.resnet_simsiam = \\\r\n            lightly.models.SimSiam(\r\n                            backbone,\r\n                            num_ftrs=num_ftrs,\r\n                            proj_hidden_dim=pred_hidden_dim,\r\n                            pred_hidden_dim=pred_hidden_dim,\r\n                            out_dim=out_dim,\r\n                            num_mlp_layers=num_mlp_layers\r\n                        )\r\n\r\n        # create our loss with the optional memory bank\r\n        self.criterion = lightly.loss.SymNegCosineSimilarityLoss()\r\n\r\n    def forward(self, x):\r\n        self.resnet_simsiam(x)\r\n\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        (x0, x1), _, _ = batch\r\n        y0, y1 = self.resnet_simsiam(x0, x1)\r\n        loss = self.criterion(y0, y1)\r\n        self.log('train_loss_ss', loss)\r\n        return loss, y0\r\n    \r\n    def training_step_end(self, batch, batch_idx):\r\n        loss, y0 = self.training_step(self, batch, batch_idx)\r\n        output, _ = y0\r\n        output = output.detach()\r\n        output = torch.nn.functional.normalize(output, dim=1)\r\n\r\n        output_std = torch.std(output, 0)\r\n        output_std = output_std.mean()\r\n\r\n        # use moving averages to track the loss and standard deviation\r\n        w = 0.9\r\n        avg_loss = 0.\r\n        avg_output_std = 0.\r\n        avg_loss = w * avg_loss + (1 - w) * loss.item()\r\n        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\r\n        \r\n        return avg_loss, avg_output_std\r\n        \r\n    def training_epoch_end(self, batch, batch_idx, ):\r\n        avg_loss, avg_output_std = self.training_step_end(self, batch, batch_idx)\r\n        collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\r\n        \r\n        self.log('loss', round(avg_loss,2), prog_bar=True)\r\n        self.log('Collapse Level', round(collapse_evel,2), prog_bar=True)\r\n        \r\n    def configure_optimizers(self):\r\n        lr = 0.05 * batch_size / 256\r\n        optimizer = AdamP(self.resnet_simsiam.parameters(), lr=lr,\r\n                                weight_decay=1e-4)\r\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\r\n        return [optimizer], [scheduler]\r\n```\r\nWhen I run the code, I get the error:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-eb464d96d0bd> in <module>\r\n     18     trainer.fit(\r\n     19         model,\r\n---> 20         train_loader_simsiam\r\n     21         )\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    458         )\r\n    459 \r\n--> 460         self._run(model)\r\n    461 \r\n    462         assert self.state.stopped\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)\r\n    756 \r\n    757         # dispatch `start_training` or `start_evaluating` or `start_predicting`\r\n--> 758         self.dispatch()\r\n    759 \r\n    760         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)\r\n    797             self.accelerator.start_predicting(self)\r\n    798         else:\r\n--> 799             self.accelerator.start_training(self)\r\n    800 \r\n    801     def run_stage(self):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)\r\n     94 \r\n     95     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n---> 96         self.training_type_plugin.start_training(trainer)\r\n     97 \r\n     98     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)\r\n    142     def start_training(self, trainer: 'pl.Trainer') -> None:\r\n    143         # double dispatch to initiate the training loop\r\n--> 144         self._results = trainer.run_stage()\r\n    145 \r\n    146     def start_evaluating(self, trainer: 'pl.Trainer') -> None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)\r\n    807         if self.predicting:\r\n    808             return self.run_predict()\r\n--> 809         return self.run_train()\r\n    810 \r\n    811     def _pre_training_routine(self):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)\r\n    869                 with self.profiler.profile(\"run_training_epoch\"):\r\n    870                     # run train epoch\r\n--> 871                     self.train_loop.run_training_epoch()\r\n    872 \r\n    873                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\r\n    497             # ------------------------------------\r\n    498             with self.trainer.profiler.profile(\"run_training_batch\"):\r\n--> 499                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    500 \r\n    501             # when returning -1 from train_step, we end epoch early\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    736 \r\n    737                         # optimizer step\r\n--> 738                         self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    739                         if len(self.trainer.optimizers) > 1:\r\n    740                             # revert back to previous state\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\r\n    440             on_tpu=self.trainer._device_type == DeviceType.TPU and _TPU_AVAILABLE,\r\n    441             using_native_amp=using_native_amp,\r\n--> 442             using_lbfgs=is_lbfgs,\r\n    443         )\r\n    444 \r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py in optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\r\n   1401 \r\n   1402         \"\"\"\r\n-> 1403         optimizer.step(closure=optimizer_closure)\r\n   1404 \r\n   1405     def optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: Optimizer, optimizer_idx: int):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py in step(self, closure, *args, **kwargs)\r\n    212             profiler_name = f\"optimizer_step_and_closure_{self._optimizer_idx}\"\r\n    213 \r\n--> 214         self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\r\n    215         self._total_optimizer_step_calls += 1\r\n    216 \r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/core/optimizer.py in __optimizer_step(self, closure, profiler_name, **kwargs)\r\n    132 \r\n    133         with trainer.profiler.profile(profiler_name):\r\n--> 134             trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\r\n    135 \r\n    136     def step(self, *args, closure: Optional[Callable] = None, **kwargs):\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py in optimizer_step(self, optimizer, opt_idx, lambda_closure, **kwargs)\r\n    327         )\r\n    328         if make_optimizer_step:\r\n--> 329             self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\r\n    330         self.precision_plugin.post_optimizer_step(optimizer, opt_idx)\r\n    331         self.training_type_plugin.post_optimizer_step(optimizer, opt_idx, **kwargs)\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py in run_optimizer_step(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\r\n    334         self, optimizer: Optimizer, optimizer_idx: int, lambda_closure: Callable, **kwargs: Any\r\n    335     ) -> None:\r\n--> 336         self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\r\n    337 \r\n    338     def optimizer_zero_grad(self, current_epoch: int, batch_idx: int, optimizer: Optimizer, opt_idx: int) -> None:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in optimizer_step(self, optimizer, lambda_closure, **kwargs)\r\n    191 \r\n    192     def optimizer_step(self, optimizer: torch.optim.Optimizer, lambda_closure: Callable, **kwargs):\r\n--> 193         optimizer.step(closure=lambda_closure, **kwargs)\r\n    194 \r\n    195     @property\r\n\r\n~/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py in wrapper(*args, **kwargs)\r\n     63                 instance._step_count += 1\r\n     64                 wrapped = func.__get__(instance, cls)\r\n---> 65                 return wrapped(*args, **kwargs)\r\n     66 \r\n     67             # Note that the returned function here is no longer a bound method,\r\n\r\n~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py in wrapper(*args, **kwargs)\r\n     86                 profile_name = \"Optimizer.step#{}.step\".format(obj.__class__.__name__)\r\n     87                 with torch.autograd.profiler.record_function(profile_name):\r\n---> 88                     return func(*args, **kwargs)\r\n     89             return wrapper\r\n     90 \r\n\r\n~/.local/lib/python3.6/site-packages/timm/optim/adamp.py in step(self, closure)\r\n     56         loss = None\r\n     57         if closure is not None:\r\n---> 58             loss = closure()\r\n     59 \r\n     60         for group in self.param_groups:\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in train_step_and_backward_closure()\r\n    731                         def train_step_and_backward_closure():\r\n    732                             result = self.training_step_and_backward(\r\n--> 733                                 split_batch, batch_idx, opt_idx, optimizer, self.trainer.hiddens\r\n    734                             )\r\n    735                             return None if result is None else result.loss\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    821         with self.trainer.profiler.profile(\"training_step_and_backward\"):\r\n    822             # lightning module hook\r\n--> 823             result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    824             self._curr_step_result = result\r\n    825 \r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    293             self.trainer.logger_connector.cache_logged_metrics()\r\n    294 \r\n--> 295             training_step_output = self.trainer.call_hook(\"training_step_end\", training_step_output)\r\n    296 \r\n    297             self._check_training_step_output(training_step_output)\r\n\r\n~/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in call_hook(self, hook_name, *args, **kwargs)\r\n   1233             if is_overridden(hook_name, model_ref):\r\n   1234                 hook_fx = getattr(model_ref, hook_name)\r\n-> 1235                 output = hook_fx(*args, **kwargs)\r\n   1236 \r\n   1237             # if the PL module doesn't have the hook then call the accelerator\r\n\r\nTypeError: training_step_end() missing 1 required positional argument: 'batch_idx'\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/8302/comments",
    "author": "etetteh",
    "comments": [
      {
        "user": "tchaton",
        "created_at": "2021-07-06T07:13:44Z",
        "body": "Dear @etetteh,\r\n\r\nHere is the correct signature for `training_step_end`. It just takes outputs from the `training_step` for you to make more processing on them. `batch_idx` shouldn't be there.\r\n\r\n```\r\n    def training_step_end(self, outputs):\r\n        # only use when  on dp\r\n        outputs = torch.cat(outputs, dim=1)\r\n        softmax = softmax(outputs, dim=1)\r\n        out = softmax.mean()\r\n        return out\r\n```\r\n\r\nSmall advice. When implementing model with Lightning, use auto-completion from your IDE. It will automatically add the right function signature.\r\n\r\nBest,\r\nT.C"
      },
      {
        "user": "etetteh",
        "created_at": "2021-07-06T09:01:01Z",
        "body": "Thanks for the advice on the IDE. The idx issue got resolved, but the training_step_end function is not performing the expected job. My goal is to get the loss and y0 from the training_step, to perform the operations under the training_step_end\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Correct implementation of Lightning's training_step_end method signature",
      "Proper handling of outputs from training_step for batch-wise processing",
      "Maintain moving averages of loss and output standard deviation across batches",
      "Correct aggregation of epoch-level metrics for logging"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:08:28"
    }
  },
  {
    "number": 7544,
    "title": "Training fails at the end of the epoch when returning None in the training step",
    "created_at": "2021-05-14T09:17:48Z",
    "closed_at": "2021-05-14T13:32:46Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/7544",
    "body": "## \ud83d\udc1b Bug\r\n\r\nSometimes my training loss in a batch is nan. Hence, I return None as loss so that the model will not backpropagate through it as suggested here: #4956. It works fine during the epoch; however, the code fails at the end of the epoch in the function reduce_across_time (line 532).\r\n\r\n```python\r\n           if isinstance(value, list):\r\n                value = torch.tensor(value)\r\n```\r\n\r\nIn case of None, value will be equal to [None] and torch cannot create a proper tensor out of it (*** RuntimeError: Could not infer dtype of NoneType)\r\n\r\nIs it me doing something wrong, or is it a bug in Lightning? Is there any workaround?\r\n\r\nPytorch Version \r\npytorch-lightning-1.3.1\r\ntorch 1.8.1+cu11\r\npython 3.7.9",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/7544/comments",
    "author": "TommasoBendinelli",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-05-14T10:32:10Z",
        "body": "Thanks for reporting this. Can you simulate it with our bug report model please? Would help me alot thanks!"
      },
      {
        "user": "TommasoBendinelli",
        "created_at": "2021-05-14T10:41:40Z",
        "body": "Sure, this reproduce the bug\r\n```python\r\nimport os\r\nimport random\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        if batch_idx == 2:\r\n            loss = None\r\n        self.log(\"train_loss\", loss)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=5,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=10,\r\n        weights_summary=None,\r\n    )\r\n    trainer.fit(model, train_dataloader=train_data, val_dataloaders=val_data)\r\n    trainer.test(model, test_dataloaders=test_data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```"
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-05-14T10:53:36Z",
        "body": "I think its because of this\r\n```python\r\nif batch_idx == 2:\r\n    loss = None\r\nself.log(\"train_loss\", loss)\r\n```\r\n`None` values are being logged and stored here which are then accumulated at epoch end which is then throwing this error.\r\nThis should work\r\n```python\r\nif batch_idx == 2:\r\n    loss = None\r\nelse:\r\n    self.log(\"train_loss\", loss)\r\n```\r\nor lightning should handle this internally?"
      },
      {
        "user": "TommasoBendinelli",
        "created_at": "2021-05-14T11:05:04Z",
        "body": "Ahh, I see, it makes sense.  When averaging the loss across multiple batches, how does lightning handles the fact that a batch was skipped due to the loss being None? Does it simply not include it in the average? \n\n---\n\nPerfect thank you."
      },
      {
        "user": "awaelchli",
        "created_at": "2021-05-14T11:08:32Z",
        "body": "Sorry, had to delete my answer and double check but yes, it averages only over the metrics logged, not over all training_steps. "
      },
      {
        "user": "rohitgr7",
        "created_at": "2021-05-14T11:13:16Z",
        "body": "to be specific it does weighted average by default using batch_size. In your case, it hasn't reached up till that point because this error is thrown while converting the logs list to PyTorch tensor and since it contains NaN values, it is throwing the error. Ideally, if a batch is skipped then it shouldn't contribute while aggregating the results so you can have an else statement there which will just work fine."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to properly skip a batch loss without causing aggregation errors",
      "Clarification on Lightning's handling of skipped batches in epoch-level metrics",
      "Solution that prevents None values from being stored in metric collections",
      "Guidance on supported patterns for conditional loss computation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:08:48"
    }
  },
  {
    "number": 6421,
    "title": "trainer.test is breaking when a model is not passed",
    "created_at": "2021-03-08T21:56:10Z",
    "closed_at": "2021-03-25T16:23:02Z",
    "labels": [
      "bug",
      "help wanted",
      "priority: 0"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/6421",
    "body": "From the docs:\r\n\r\n```\r\n# (1) load the best checkpoint automatically (lightning tracks this for you)\r\ntrainer.test()\r\n```\r\n\r\nTrainer.test should use the best checkpoint when a model isn't provided, and currently, that doesn't work.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/6421/comments",
    "author": "edenlightning",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-03-08T23:27:30Z",
        "body": "Here is an example that shows that it works:\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch.nn import functional as F\r\n\r\nimport pytorch_lightning as pl\r\nfrom pl_examples.basic_examples.mnist_datamodule import MNISTDataModule\r\nfrom pytorch_lightning import Trainer\r\n\r\n\r\nclass LitClassifier(pl.LightningModule):\r\n\r\n    def __init__(self, hidden_dim=128, learning_rate=1e-3):\r\n        super().__init__()\r\n        self.save_hyperparameters()\r\n\r\n        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\r\n        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        x = torch.relu(self.l1(x))\r\n        x = torch.relu(self.l2(x))\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return loss\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('valid_loss', loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        self.log('test_loss', loss)\r\n\r\n    def on_test_start(self):\r\n        checkpoint = torch.load(self.trainer.checkpoint_callback.best_model_path)\r\n        assert torch.allclose(checkpoint[\"state_dict\"][\"l1.weight\"], self.l1.weight)\r\n        assert torch.abs(self.l1.weight).sum().item() > 0\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument('--hidden_dim', type=int, default=128)\r\n        parser.add_argument('--learning_rate', type=float, default=0.0001)\r\n        return parser\r\n\r\n\r\ndef cli_main():\r\n    pl.seed_everything(1234)\r\n    parser = ArgumentParser()\r\n    parser = pl.Trainer.add_argparse_args(parser)\r\n    parser = LitClassifier.add_model_specific_args(parser)\r\n    parser = MNISTDataModule.add_argparse_args(parser)\r\n    args = parser.parse_args()\r\n\r\n    dm = MNISTDataModule.from_argparse_args(args)\r\n\r\n    model = LitClassifier(args.hidden_dim, args.learning_rate)\r\n    trainer = Trainer(\r\n        max_epochs=2,\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n    )\r\n    trainer.fit(model, datamodule=dm)\r\n\r\n    # erase model weight\r\n    torch.fill_(model.l1.weight.data, 0)\r\n    assert torch.abs(model.l1.weight).sum().item() == 0\r\n    trainer.test()\r\n    assert torch.abs(model.l1.weight).sum().item() > 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    cli_main()\r\n\r\n```\r\n\r\nIf you look at the assertion there in on_test_start, the weights are correctly loaded.\r\nPlease let me know under what circumstances it doesn't work. A reproducible example would be very much appreciated. Feel free to take my code and modify it."
      },
      {
        "user": "edenlightning",
        "created_at": "2021-03-25T16:23:02Z",
        "body": "Closing for now, feel free to open with a code example!"
      }
    ],
    "satisfaction_conditions": [
      "Demonstration that trainer.test() correctly loads the best checkpoint when no model is provided",
      "Clear verification method for checkpoint loading"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:09:00"
    }
  },
  {
    "number": 5641,
    "title": "Log fails: \"Tensors must be CUDA and dense\" with multi-GPUs using ddp",
    "created_at": "2021-01-24T21:26:14Z",
    "closed_at": "2021-01-25T18:30:45Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5641",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI'm encountering the following error when running my code (see below) with multi-GPUs (single GPU and CPU works fine). `accelerator` used is `ddp`.\r\n```\r\nline 117, in test_epoch_end\r\nwork = _default_pg.allreduce([tensor], opts)\r\nRuntimeError: self.log(\"avg_test_acc\", avg_test_acc, sync_dist=True)Tensors must be CUDA and dense\r\n```\r\nHowever, when I remove the `sync_dist=True` all goes well.\r\n\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\nThe code, at it's core, looks like this:\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom torchvision import datasets, transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.autograd import Variable\r\nfrom argparse import ArgumentParser\r\nfrom pytorch_lightning.metrics.functional import accuracy\r\nfrom torch.nn import functional as F\r\nfrom argparse import ArgumentParser\r\nimport mlflow\r\nfrom data_loading.data_loader import MNISTDataModule\r\nfrom model.model import LightningMNISTClassifier\r\nimport os\r\n\r\nclass MNISTDataModule(pl.LightningDataModule):\r\n    def __init__(self, **kwargs):\r\n        super(MNISTDataModule, self).__init__()\r\n        self.df_train = None\r\n        self.df_test = None\r\n        self.train_data_loader = None\r\n        self.test_data_loader = None\r\n        self.args = kwargs\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    def setup(self, stage=None):\r\n        self.df_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        self.df_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(\r\n            self.df_train, batch_size=self.args['training_batch_size'], num_workers=self.args[\"num_workers\"], shuffle=True\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(\r\n            self.df_test, batch_size=self.args['test_batch_size'], num_workers=self.args[\"num_workers\"], shuffle=False\r\n        )\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self, len_test_set: int, **kwargs):\r\n        super(LightningMNISTClassifier, self).__init__()\r\n        self.optimizer = None\r\n        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\r\n        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\r\n        self.dropout1 = torch.nn.Dropout2d(0.25)\r\n        self.fc1 = torch.nn.Linear(9216, 128)\r\n        self.dropout2 = torch.nn.Dropout2d(0.25)\r\n        self.fc2 = torch.nn.Linear(128, 10)\r\n        self.args = kwargs\r\n        self.len_test_set = len_test_set\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\"--num_workers\", type=int, default=3, metavar=\"N\", help=\"number of workers (default: 3)\")\r\n        parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate (default: 0.01)\")\r\n        parser.add_argument('--training-batch-size', type=int, default=64, help='Input batch size for training')\r\n        parser.add_argument('--test-batch-size', type=int, default=1000, help='Input batch size for testing')\r\n\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.max_pool2d(x, 2)\r\n        x = torch.flatten(self.dropout1(x), 1)\r\n        x = F.relu(self.fc1(x))\r\n        x = self.dropout2(x)\r\n        x = self.fc2(x)\r\n        output = F.log_softmax(x, dim=1)\r\n\r\n        return output\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def training_epoch_end(self, training_step_outputs):\r\n        train_avg_loss = torch.stack([train_output[\"loss\"] for train_output in training_step_outputs]).mean()\r\n        self.log(\"train_loss\", train_avg_loss)\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Predicts on the test dataset to compute the current accuracy of the model.\r\n\r\n        :param test_batch: Batch data\r\n        :param batch_idx: Batch indices\r\n\r\n        :return: output - Testing accuracy\r\n        \"\"\"\r\n\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        _, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy(y_hat.cpu(), y.cpu())\r\n        # sum up batch loss\r\n        data, target = Variable(x), Variable(y)\r\n        test_loss = F.nll_loss(output, target, reduction='sum').data.item()\r\n        # get the index of the max log-probability\r\n        pred = output.data.max(1)[1]\r\n        correct = pred.eq(target.data).cpu().sum().item()\r\n        return {\"test_acc\": test_acc, \"test_loss\": test_loss, \"correct\": correct}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n\r\n        :param outputs: outputs after every epoch end\r\n\r\n        :return: output - average test loss\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([test_output[\"test_acc\"] for test_output in outputs]).mean()\r\n        avg_test_loss = sum([test_output[\"test_loss\"] for test_output in outputs])/self.len_test_set\r\n        test_correct = sum([test_output[\"correct\"] for test_output in outputs])\r\n        self.log(\"avg_test_acc\", avg_test_acc, sync_dist=True)\r\n        self.log(\"avg_test_loss\", avg_test_loss, sync_dist=True)\r\n        self.log(\"test_correct\", test_correct, sync_dist=True)\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Prepares the data for training and prediction\r\n        \"\"\"\r\n        return {}\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Initializes the optimizer and learning rate scheduler\r\n\r\n        :return: output - Initialized optimizer and scheduler\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters())\r\n        return [self.optimizer]\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    parser = ArgumentParser(description=\"PyTorch Autolog Mnist Example\")\r\n    use_cuda = torch.cuda.is_available()\r\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n\r\n    parser = pl.Trainer.add_argparse_args(parent_parser=parser)\r\n    parser = LightningMNISTClassifier.add_model_specific_args(parent_parser=parser)\r\n\r\n    mlflow.pytorch.autolog()\r\n    # parse cli arguments\r\n    args = parser.parse_args()\r\n    dict_args = vars(args)\r\n\r\n    set_general_random_seeds(dict_args['general_seed'])\r\n    set_pytorch_random_seeds(dict_args['pytorch_seed'], True)\r\n\r\n    if \"accelerator\" in dict_args and dict_args[\"accelerator\"] == \"None\":\r\n        dict_args[\"accelerator\"] = None\r\n\r\n    dm = MNISTDataModule(**dict_args)\r\n\r\n    dm.prepare_data()\r\n    dm.setup(stage=\"fit\")\r\n    model = LightningMNISTClassifier(len_test_set=len(dm.df_test), **dict_args)\r\n    trainer = pl.Trainer.from_argparse_args(args)\r\n     \r\n\r\n    trainer.deterministic = True\r\n    trainer.benchmark = False\r\n    trainer.fit(model, dm)\r\n    trainer.test()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nTrain and test successfully without mentioned error above when using multiple GPUs (like it runs successfully on single GPU and CPU).\r\n\r\n### Environment\r\n\r\n* CUDA \r\n\t* GPU:\r\n\t\t* NVIDIA [Tesla V100 PCIe 32GB] \r\n\t* available: True\r\n\t* Version 11.2\r\n* Packages\r\n\t* cudatoolkit=10.1\r\n\t* numpy                     =1.19.1\r\n\t* torchvision             =  0.7.0 \r\n\t* pytorch-lightning=1.1.5\r\n\t* pycuda=2019.1.2\r\n\t* python=3.8.2\r\n\t* pytorch=1.6.0\r\n\t\r\n\r\n - OS: Linux Ubuntu Ubuntu 18.04.3 LTS\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5641/comments",
    "author": "Imipenem",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2021-01-25T01:40:28Z",
        "body": "Remove the .cpu() call \r\n`correct = pred.eq(target.data).cpu().sum().item()`\r\nshould be \r\n`correct = pred.eq(target.data).sum()`\n\n---\n\nAnd if you want to compute accuracy on multi gpu correctly, I recommend directly using the Accuracy metric (from pytorch_lightning.metrics)"
      },
      {
        "user": "Imipenem",
        "created_at": "2021-01-25T18:30:45Z",
        "body": "many thanks for the prompt answer, will use PyTorch lightning metrics ;)"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why tensors become non-CUDA/dense when using sync_dist=True with DDP",
      "Solution maintains proper tensor device/type consistency across GPUs",
      "Proper handling of metric aggregation in distributed training context",
      "Avoidance of premature tensor movement to CPU during metric calculation",
      "Compatibility with PyTorch Lightning's distributed training patterns"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:09:47"
    }
  },
  {
    "number": 5028,
    "title": "\"Shuffle\" in validation dataloader: is it really best practices?",
    "created_at": "2020-12-08T20:15:36Z",
    "closed_at": "2020-12-09T13:40:38Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/5028",
    "body": "## \ud83d\udc1b Bug\r\n\r\nIn my `LightningModule`'s `val_dataloader` method, I have this dataloader: \r\n\r\n```python\r\ndataloader = DataLoader(self.datasets[split], batch_size=batch_size,\r\n                                shuffle=True, num_workers=self.hparams.compute.num_workers,\r\n                                pin_memory=torch.cuda.is_available(), drop_last=False)\r\nreturn dataloader\r\n```\r\n\r\nI receive this warning: \r\n```\r\n.../pytorch_lightning/utilities/distributed.py:45: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\r\n  warnings.warn(*args, **kwargs)\r\n```\r\nHowever, it's quite important for me to shuffle my validation batches. For example, I visualize the first few batches in my validation to get an idea of random model performance on my images-- without shuffling, I'd only be able to inspect the same images every epoch. \r\n\r\n### Expected behavior\r\n\r\nNo warning\r\n\r\n### Additional information\r\n\r\nThis is more of a discussion than a bug report, but it didn't neatly fit into any categories. Do we really think it's important enough to warn the user when using shuffle in validation? I've tried suppressing it, but I can't figure out where exactly it's called. ",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/5028/comments",
    "author": "jbohnslav",
    "comments": [
      {
        "user": "carmocca",
        "created_at": "2020-12-09T01:34:04Z",
        "body": "Wouldn't you want to always inspect the same images to properly assess the model performance?\r\nIf you keep looking at different samples each time, it is harder to know if the change in performance is caused by the model improvement or by how well the model generalizes to the samples in particular.\r\n\r\n> I've tried suppressing it, but I can't figure out where exactly it's called.\r\n\r\nYou shouldn't need to, see this example of using `filterwarnings`\r\n\r\n```python\r\nimport warnings\r\n\r\ndef test():\r\n     warnings.warn(\"this is a test\", UserWarning)\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"this is a test\")\r\ntest() # No warning!\r\n```"
      },
      {
        "user": "jbohnslav",
        "created_at": "2020-12-09T13:38:26Z",
        "body": "> Wouldn't you want to always inspect the same images to properly assess the model performance?\r\n\r\nI'm working with video data, so the first N batches in an unshuffled dataset would be the first ~minute of the first video. This isn't very informative-- it's much better to get a random sample. \r\n\r\n```warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"this is a test\")```\r\nThanks for the sample! I thought one had to use it as a context manager. This solves the problem. "
      },
      {
        "user": "bjourne",
        "created_at": "2025-02-17T16:13:05Z",
        "body": "But the issue remains. WHY does PL \"strongly recommend\" that you don't shuffle validation data? Afaict, the warning is completely pointless."
      }
    ],
    "satisfaction_conditions": [
      "Justification for overriding shuffle warnings in validation data",
      "Warning suppression mechanism that persists beyond context managers",
      "Clarification of best practice rationale for validation data",
      "Validation of domain-specific shuffle requirements"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:09:54"
    }
  },
  {
    "number": 4653,
    "title": "accumulate_grad_batches ignores last batches in epoch if number of steps is not divisible by accumulate_grad_batches?",
    "created_at": "2020-11-13T09:39:10Z",
    "closed_at": "2020-11-13T11:00:50Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4653",
    "body": "Suppose I have accumulate_grad_batches=256 and number of steps in my epoch is 260. Loss is updated only on step number 256 every epoch. I suppose it means that the last 4 batches grads are ignored. Is that correct?",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4653/comments",
    "author": "Vozf",
    "comments": [
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:31:27Z",
        "body": "I suppose we do not ignore for last batches. Can you share a minimal example if it's not working?"
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T10:32:54Z",
        "body": "So what is done with last 6 batches? Is it aggreagated over 6 batches instead of asked 256?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:36:18Z",
        "body": "We call `.backward` and `optimizer.step` `optimizer.zero_grad()` for the last 4 batches."
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T10:44:52Z",
        "body": "So first you accumulate 256 batches and call backward and then you accumulate 4 batches and call backward, correct?"
      },
      {
        "user": "ydcjeff",
        "created_at": "2020-11-13T10:59:08Z",
        "body": "Yep we accumulate 256 if possible and accumulate the rest of batches if it's not divisible by 256"
      },
      {
        "user": "Vozf",
        "created_at": "2020-11-13T11:00:49Z",
        "body": "Ok, thanks for clarification"
      }
    ],
    "satisfaction_conditions": [
      "Clarify how gradient accumulation handles batches when total steps aren't divisible by accumulate_grad_batches",
      "Confirm that all batches contribute to gradient updates regardless of accumulation count",
      "Explain the framework's behavior for partial accumulation groups"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:10:12"
    }
  },
  {
    "number": 4079,
    "title": "ModelCheckpoint save_function() not set?",
    "created_at": "2020-10-11T15:29:35Z",
    "closed_at": "2020-10-11T15:37:26Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/4079",
    "body": "I am training a PL model using the following code snippet:\r\n\r\n```python\r\n    # logger\r\n    tb_logger = pl_loggers.TensorBoardLogger(cfg.logs.path, name='rnn_exp')\r\n\r\n    # checkpoint callback\r\n    checkpoint_callback = ModelCheckpoint(\r\n        filepath=cfg.checkpoint.path + \"encoder_rnn{epoch:02d}\",\r\n        save_top_k=1,\r\n        mode=\"min\" # monitor is defined in val_step: EvalResult(checkpoint_on=val_loss)\r\n    )\r\n\r\n    # early stopping callback\r\n    early_stopping_callback = EarlyStopping(\r\n        monitor=\"val_loss\",\r\n        patience=cfg.val.patience,\r\n        mode=\"min\"\r\n    )\r\n\r\n    tokenizer = ...\r\n    dm = MyDataModule(cfg, tokenizer)\r\n\r\n    model = RNNEncoder(cfg)\r\n\r\n    trainer = Trainer(\r\n        fast_dev_run=False,\r\n        max_epochs=cfg.train.max_epochs,\r\n        gpus=1,\r\n        logger=tb_logger,\r\n        callbacks=[checkpoint_callback, early_stopping_callback]\r\n    )\r\n\r\n    # training\r\n    dm.setup('fit')\r\n    trainer.fit(model, datamodule=dm)\r\n```\r\n\r\nHowever, after the first epoch, the model presents the following error, probably when calling the model checkpoint callback:\r\n\r\n```python\r\n    trainer.fit(model, datamodule=dm)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\r\n    result = fn(self, *args, **kwargs)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1073, in fit\r\n    results = self.accelerator_backend.train(model)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_backend.py\", line 51, in train\r\n    results = self.trainer.run_pretrain_routine(model)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\r\n    self.train()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\r\n    self.run_training_epoch()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 516, in run_training_epoch\r\n    self.run_evaluation(test_mode=False)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 603, in run_evaluation\r\n    self.on_validation_end()\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 176, in on_validation_end\r\n    callback.on_validation_end(self, self.get_model())\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 27, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 380, in on_validation_end\r\n    self._do_check_save(filepath, current, epoch, trainer, pl_module)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 421, in _do_check_save\r\n    self._save_model(filepath, trainer, pl_module)\r\n  File \"/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 212, in _save_model\r\n    raise ValueError(\".save_function() not set\")\r\nValueError: .save_function() not set\r\n\r\n```\r\nCould you tell me if I forgot to configure something?\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/4079/comments",
    "author": "celsofranssa",
    "comments": [
      {
        "user": "awaelchli",
        "created_at": "2020-10-11T15:33:26Z",
        "body": "currently you need to set the ModelCheckpoint via `Trainer(checkpoint_callback=...)`\r\n#3990 will enable passing it to callbacks"
      },
      {
        "user": "celsofranssa",
        "created_at": "2020-10-11T15:37:26Z",
        "body": "Thanks, @awaelchli, \r\n\r\nI've just thought that. Thanks a lot for the help."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to properly attach ModelCheckpoint callback to Trainer"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:10:23"
    }
  },
  {
    "number": 3752,
    "title": "Default reduction always applied by `Metric`, even when requesting `'none'` reduction",
    "created_at": "2020-09-30T19:46:37Z",
    "closed_at": "2020-10-05T14:13:24Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3752",
    "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nMetric reduction doesn't behave the same between the functional and class API when using `reduction='none'`. The functional API applies no reduction as expected, but the class API seems to apply the default reduction regardless.\r\n\r\nI haven't investigated the code yet to find the specific cause of the bug, so I'm not sure how widespread this bug is, but I've encountered it using both the `DiceCoefficient` and my own implementation of the differentiable dice, inheriting from `TensorMetric`.\r\n\r\n### To Reproduce\r\n\r\nGiven a pair of `pred` and `target`, I get the following behavior with 3 class + background segmentation data:\r\n```python\r\n>>> from pytorch_lightning.metrics import DiceCoefficient\r\n>>> from pytorch_lightning.metrics.functional import dice_score\r\n>>> DiceCoefficient(reduction=\"none\")(pred, target)\r\ntensor(0.0800)\r\n>>> dice_score(pred, target, reduction=\"none\")\r\ntensor([0.0876, 0.0937, 0.0586], device='cuda:0')\r\n```\r\nwhere I would have expected both version to give the same result.\r\n\r\nThe class API seems to apply the default reduction of `'elementwise_mean'` even though I requested `'none'`, since:\r\n```python\r\n>>> dice_score(x_hat, x, reduction=\"none\").mean()\r\ntensor(0.0800, device='cuda:0')\r\n```\r\n\r\n### Expected behavior\r\nReduction behavior should be consistent between class and functional API, and to behave like the current functional API.\r\n\r\n### Environment\r\nI just now installed Lightning from Git to ensure that it's not a bug that's already been solved since the last release.\r\n\r\n* CUDA:\r\n        - GPU: TITAN Xp\r\n        - available:         True\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.19.2\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.6.0\r\n        - pytorch-lightning: 0.9.1rc4\r\n        - tqdm:              4.49.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture: 64bit, ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.5\r\n        - version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3752/comments",
    "author": "nathanpainchaud",
    "comments": [
      {
        "user": "SkafteNicki",
        "created_at": "2020-10-05T13:44:51Z",
        "body": "Hi @nathanpainchaud, running your code example on master produces the correct result (your issue was probably solved by PR #3517). Could you please try upgrading?"
      },
      {
        "user": "nathanpainchaud",
        "created_at": "2020-10-05T14:13:24Z",
        "body": "Hi @SkafteNicki. I can confirm that master now runs fine on my end as well. Thanks for the follow up!"
      }
    ],
    "satisfaction_conditions": [
      "Ensures class API respects 'none' reduction parameter like functional API",
      "Eliminates default reduction being applied when 'none' is explicitly requested",
      "Maintains backward compatibility with existing metric implementations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:10:32"
    }
  },
  {
    "number": 3738,
    "title": "RuntimeError: Input and hidden tensors are not at the same device, found",
    "created_at": "2020-09-30T08:05:07Z",
    "closed_at": "2020-09-30T12:34:03Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3738",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI train LSTM for character level  text generation. At first I initialize hidden and cell with zeros using `torch.zeros`. Unfortunately this tensors are defaultly assigned to the cpu so I get the following error while training\r\n\r\n```python\r\nRuntimeError: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu\r\n```\r\n\r\n### To Reproduce\r\n\r\n#### Model\r\n\r\n```python\r\nclass RNN(pl.LightningModule):\r\n    lr = 0.0005\r\n\r\n    def __init__(self, input_size, hidden_size, embeding_size, n_categories, n_layers, output_size, p):\r\n        super().__init__()\r\n\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        \r\n        self.n_layers = n_layers\r\n        self.hidden_size = hidden_size\r\n        \r\n        \r\n        self.embeding = nn.Embedding(input_size+n_categories, embeding_size)\r\n        self.lstm = nn.LSTM(embeding_size+n_categories, hidden_size, n_layers, dropout=p)\r\n        self.out_fc = nn.Linear(hidden_size, output_size)\r\n        \r\n        self.dropout = nn.Dropout(p)\r\n        \r\n\r\n    def forward(self, batch_of_category, batch_of_letter, hidden, cell):\r\n        ## letter level operations\r\n        \r\n        embeding = self.dropout(self.embeding(batch_of_letter))\r\n        category_plus_letter = torch.cat((batch_of_category, embeding), 1)\r\n\r\n        #sequence_length = 1\r\n        category_plus_letter = category_plus_letter.unsqueeze(1)\r\n        \r\n        out, (hidden, cell) = self.lstm(category_plus_letter, (hidden, cell))\r\n        out = self.out_fc(out)\r\n        out = out.squeeze(1)\r\n        \r\n        return out, (hidden, cell)\r\n        \r\n\r\n    def configure_optimizers(self):\r\n        optimizer = Adam(self.parameters(), self.lr)\r\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\r\n\r\n        return [optimizer], [scheduler]\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        item_dict = batch\r\n        loss = 0\r\n        batch_of_category = item_dict[\"category_tensors\"]\r\n\r\n        #we loop over letters, single batch at the time \r\n        \r\n        hidden = torch.zeros(self.n_layers, 1, self.hidden_size).cuda()\r\n        cell = torcAh.zeros(self.n_layers, 1, self.hidden_size).cuda()\r\n        \r\n        for t in range(item_dict[\"input_tensors\"].size(1)):\r\n            batch_of_letter = item_dict[\"input_tensors\"][:, t]\r\n            \r\n            output, (hidden, cell) = self(batch_of_category, batch_of_letter, hidden, cell)\r\n            \r\n            loss += criterion(output, item_dict[\"target_tensors\"][:, t])\r\n\r\n        loss = loss/(t+1)\r\n\r\n\r\n        tensorboard_logs = {'train_loss': loss}\r\n\r\n        return {'loss': loss, 'log': tensorboard_logs}\r\n    \r\n    \r\n    def init_hidden(self, batch_size):\r\n        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\r\n        cell = torch.zeros(self.n_layers, batch_size, self.hidden_size)\r\n        \r\n        return hidden, cell\r\n```\r\n\r\n#### Batch\r\n\r\n```\r\n(['Russian', 'English', 'Russian', 'English'],\r\n ['Piskarenkov', 'Clarkson', 'Pochkaev', 'Woods'],\r\n tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]]),\r\n tensor([[42,  9, 19, 11,  1, 18,  5, 14, 11, 15, 22],\r\n         [29, 12,  1, 18, 11, 19, 15, 14,  0,  0,  0],\r\n         [42, 15,  3,  8, 11,  1,  5, 22,  0,  0,  0],\r\n         [49, 15, 15,  4, 19,  0,  0,  0,  0,  0,  0]]),\r\n tensor([[ 9, 19, 11,  1, 18,  5, 14, 11, 15, 22, 59],\r\n         [12,  1, 18, 11, 19, 15, 14, 59,  0,  0,  0],\r\n         [15,  3,  8, 11,  1,  5, 22, 59,  0,  0,  0],\r\n         [15, 15,  4, 19, 59,  0,  0,  0,  0,  0,  0]]))\r\n```\r\n\r\n#### Trainer \r\n\r\n```python\r\ndm = NamesDatamodule(1)\r\n\r\nrnn_model = RNN(input_size=ds.n_tokens,\r\n            hidden_size=256,\r\n            embeding_size = 128, \r\n            n_layers=2,    \r\n            n_categories=ds.n_categories,\r\n            output_size=ds.n_tokens,\r\n            p=0.3)\r\n\r\n\r\ntrainer = Trainer(max_epochs=3, \r\n                  logger=None,\r\n                  gpus=1,\r\n                  early_stop_callback=False,\r\n                  checkpoint_callback=False,\r\n                  )\r\n\r\ntrainer.fit(rnn_model, dm)\r\n```\r\n\r\n### Expected behavior\r\n\r\nHidden values should automatically be assigned to the `device`\r\n\r\n### Environment\r\n\r\nGoogle Colab\r\n\r\n - Pytroch 1.6.0+cu101\r\n - Lightning 0.9.1rc3\r\n - Python version:\r\n - GPU models and configuration: single colab GPU\r\n\r\n### Additional context\r\n\r\nProblem can be solved by adding `.cuda()` to the variables but it is not a solution that I think should be necessary \r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3738/comments",
    "author": "tugot17",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-30T08:05:49Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T10:00:04Z",
        "body": "Try changing to\r\n```python\r\nhidden = torch.zeros(self.n_layers, 1, self.hidden_size)..to(self.device)\r\ncell = torch.zeros(self.n_layers, 1, self.hidden_size).to(self.device)\r\n```"
      },
      {
        "user": "tugot17",
        "created_at": "2020-09-30T11:55:25Z",
        "body": "@rohitgr7 Yeah, this fixes the problem, however I'm not entirely sure it will also work in case if I used more then a single machine to train the model "
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-30T12:02:18Z",
        "body": "`self.device` will always give you the device for the current process(ddp) or current batch(dp) being executed."
      },
      {
        "user": "tugot17",
        "created_at": "2020-09-30T12:34:03Z",
        "body": "Ok, I can close the issue. Thanks for your help :)  "
      }
    ],
    "satisfaction_conditions": [
      "Ensures hidden/cell tensors are automatically created on the same device as the model",
      "Solution works in multi-device/multi-machine training scenarios",
      "Uses PyTorch Lightning's native device management capabilities",
      "Avoids hardcoding device-specific methods like .cuda()"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:10:42"
    }
  },
  {
    "number": 3426,
    "title": "Model checkpoint not saving hyperparameters correctly",
    "created_at": "2020-09-09T17:13:40Z",
    "closed_at": "2020-09-09T20:44:07Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3426",
    "body": "When using the ModelCheckpoint, my hyperparameters are not being saved with the checkpoints.  So I get an AttributeError when attempting to load from checkpoints.\r\n\r\nTo reproduce:\r\n```\r\nimport pytorch_lightning as pl\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nfrom torchvision import transforms\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport argparse\r\nfrom bunch import Bunch\r\n\r\nimport pytorch_lightning as pl\r\nclass LitModel(pl.LightningModule):\r\n\r\n    def __init__(self, args):\r\n        super().__init__()\r\n        self.l1 = torch.nn.Linear(28 * 28, 10)\r\n        print('args:', args)\r\n        print(args.to_print)\r\n\r\n    def forward(self, x):\r\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        return pl.TrainResult(loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=0.02)\r\n\r\n    \r\ntrain_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\r\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\r\n        os.path.join(os.getcwd(), 'chkpts'),\r\n        save_top_k=1,\r\n        verbose=True,\r\n        monitor='loss',\r\n        mode='min'\r\n    )\r\ntrainer = pl.Trainer(checkpoint_callback=checkpoint_callback,\r\n                    train_percent_check=0.1,\r\n                    val_percent_check=0,\r\n                    max_epochs=1)\r\n\r\nhparams = argparse.Namespace()\r\nhparams.to_print = 'foo'\r\nmodel = LitModel(hparams)\r\n\r\ntrainer.fit(model, train_loader)\r\n\r\nmod = LitModel.load_from_checkpoint(ckpt_path)\r\n```\r\n\r\nProduces the following Error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-38-868d05212321> in <module>\r\n     49 trainer.fit(model, train_loader)\r\n     50 \r\n---> 51 mod = LitModel.load_from_checkpoint(ckpt_path)\r\n\r\n~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, *args, **kwargs)\r\n    151         checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)\r\n    152 \r\n--> 153         model = cls._load_model_state(checkpoint, *args, strict=strict, **kwargs)\r\n    154         return model\r\n    155 \r\n\r\n~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in _load_model_state(cls, checkpoint, strict, *cls_args, **cls_kwargs)\r\n    188             cls_args, cls_kwargs = [], {}\r\n    189 \r\n--> 190         model = cls(*cls_args, **cls_kwargs)\r\n    191         # load the state_dict on the model automatically\r\n    192         model.load_state_dict(checkpoint['state_dict'], strict=strict)\r\n\r\n<ipython-input-38-868d05212321> in __init__(self, args)\r\n     15         self.l1 = torch.nn.Linear(28 * 28, 10)\r\n     16         print('args:', args)\r\n---> 17         print(args.to_print)\r\n     18 \r\n     19     def forward(self, x):\r\n\r\nAttributeError: 'dict' object has no attribute 'to_print'\r\n```\r\n\r\nThe print statements indicate that `args` is an empty dict when attempting to load from checkpoint. \r\n\r\nWhen inspecting the checkpoint\r\n```\r\nckpt_path = os.path.join(os.getcwd(), '_ckpt_epoch_0.ckpt')\r\nckpt = torch.load(ckpt_path)\r\nprint(ckpt.keys())\r\n```\r\n\r\nI get the following:\r\n```\r\ndict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'checkpoint_callback_best_model_score', 'checkpoint_callback_best_model_path', 'optimizer_states', 'lr_schedulers', 'state_dict'])\r\n```\r\n\r\nMy understanding is there should be a `hyper_parameters` in the checkpoint.\r\n\r\n\r\nSystem:\r\n- PyTorch Version 1.3.1\r\n- pytorch-lightning: 0.9.0 installed conda\r\n- OS: Ubuntu 18.04\r\n- Python 3.6",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3426/comments",
    "author": "davidwhealey",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-09-09T17:14:23Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-09-09T19:29:10Z",
        "body": "you need to call `self.save_hyperparameters()` in `__init__` to make it work."
      },
      {
        "user": "davidwhealey",
        "created_at": "2020-09-09T20:44:04Z",
        "body": "Worked, thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to properly persist hyperparameters in PyTorch Lightning checkpoints",
      "Clarification of PyTorch Lightning's hyperparameter serialization requirements",
      "Solution ensuring loaded checkpoints contain properly typed hyperparameter objects",
      "Mechanism to preserve custom attributes like 'to_print' during checkpointing"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:11:05"
    }
  },
  {
    "number": 3113,
    "title": "TypeError in closure_loss = closure_loss / self.accumulate_grad_batches for Cross_entropy loss",
    "created_at": "2020-08-23T14:55:43Z",
    "closed_at": "2020-08-24T18:43:38Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/3113",
    "body": "## \ud83d\udc1b Bug\r\n\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n   1055         # (if accumulate_grad_batches = 1 no effect)\r\n   1056         closure_loss = training_step_output.minimize if is_result_obj else training_step_output.batch_loss\r\n-> 1057         closure_loss = closure_loss / self.accumulate_grad_batches\r\n   1058 \r\n   1059         # the loss will get scaled for amp. avoid any modifications to it\r\n\r\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\r\n```\r\n#### Code sample\r\n\r\n```\r\nclass CustomModel(pl.LightningModule):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.cnn_model = nn.Sequential(\r\n            nn.Conv2d(1, 6, kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2),\r\n            nn.Conv2d(6, 16, kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2),\r\n            nn.Conv2d(16,32,kernel_size = 5),\r\n            nn.ReLU(),\r\n            nn.AvgPool2d(2, stride = 2))\r\n\r\n        self.fc_model = nn.Sequential(\r\n            nn.Linear(2592, 1024), # (N, 2592) -> (N, 1024)\r\n            nn.ReLU(),\r\n            nn.Linear(1024, 30))  # (N, 1024)  -> (N, 30)) #30 classes\r\n\r\n    def forward(self, x):\r\n        x = self.cnn_model(x)\r\n        # print(x.shape) \r\n        x = x.view(x.size(0), -1)\r\n        # print(x.shape)    \r\n        x = self.fc_model(x)\r\n        # print(x.shape)\r\n        return x\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        # acc = FM.accuracy(y_hat, y)\r\n        result = pl.TrainResult()\r\n        print('f')\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        y_hat = self.forward(x)\r\n        loss = F.cross_entropy(y_hat, y)\r\n        acc = FM.accuracy(y_hat, y)\r\n        result = pl.EvalResult(checkpoint_on=loss)\r\n        result.log('val_loss', loss, prog_bar=True)\r\n        result.log('val_acc', acc, prog_bar=True)\r\n        print('f')\r\n        return result\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\r\n        return optimizer\r\n\r\n    def train_dataloader(self):\r\n        train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=32)\r\n        # print(\"Length of the train_loader:\", len(train_loader))\r\n        return train_loader\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(dataset=val_dataset, shuffle=False, batch_size=32)\r\n```\r\n\r\nThe error occurs when I am fitting the model to train. Using lightning 0.9.0 on colab. I am loading dataset by mounting drive and using torchvision datasets.ImageFolder function.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/3113/comments",
    "author": "srijansingh53",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-08-23T14:56:22Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "rohitgr7",
        "created_at": "2020-08-23T15:47:18Z",
        "body": "changing `result = pl.TrainResult()` to `result = pl.TrainResult(minimize=loss)` is all you need."
      },
      {
        "user": "srijansingh53",
        "created_at": "2020-08-24T18:43:38Z",
        "body": "yes, it worked. Thank you @rohitgr7 . Closing the issue"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to properly associate the loss value with the training result object",
      "Clarification on PyTorch Lightning's expectation for loss tracking in training steps"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:11:19"
    }
  },
  {
    "number": 2939,
    "title": "mlflow checkpoints in the wrong location ",
    "created_at": "2020-08-12T22:58:48Z",
    "closed_at": "2020-08-15T10:54:07Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2939",
    "body": "I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2939/comments",
    "author": "david-waterworth",
    "comments": [
      {
        "user": "Borda",
        "created_at": "2020-08-13T06:30:27Z",
        "body": "@david-waterworth mind try the latest 0.9rc12?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-14T06:15:19Z",
        "body": "It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n"
      },
      {
        "user": "david-waterworth",
        "created_at": "2020-08-14T06:19:01Z",
        "body": "Thanks @awaelchli  yes that's what I want - thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Checkpoints are stored in a run-specific subdirectory under the MLflow tracking structure",
      "Solution maintains MLflow's native artifact organization pattern",
      "Checkpoint location matches MLflow's expected directory hierarchy"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:11:52"
    }
  },
  {
    "number": 2679,
    "title": "Default checkpoint location problematic when using docker ",
    "created_at": "2020-07-23T17:53:41Z",
    "closed_at": "2020-08-11T14:11:44Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2679",
    "body": "The default behavior of `ModelCheckpoint` is to use `os.getcwd()`. Outside my docker container, this ended up being the same directory where my tensorboard logs were saved (e.g. `/my/dir/tb_logs/default/version_0/checkpoints/`).  But inside the docker container, it saved to the internal working directory (e.g. `/home/default/version_0/checkpoints/`). Since this location disappeared along with the container, the checkpoint was gone, and there was no warning raised to explain why.\r\n\r\nRequiring a checkpoint directory isn't desirable, but I'd like to help others avoid this grief in the future. Is there a better way to infer a default location than `os.getcwd()`? Something as simple as a print statement with the checkpoint location would have saved me a lot of time troubleshooting.\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2679/comments",
    "author": "drStacky",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-07-23T17:54:41Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-08-08T05:21:33Z",
        "body": "You can set the `default_root_dir` arg in the Trainer. Is that what you want? Otherwise there is an option verbose in the ModelCheckpoint callback which, when turned on, should print the file path everytime it saves."
      },
      {
        "user": "drStacky",
        "created_at": "2020-08-11T14:11:44Z",
        "body": "Somehow I misread the explanation of `default_root_dir`. I thought it only changed the name of the `default` directory, not the whole path. This is exactly what I needed. Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Provides a way to specify a persistent checkpoint directory outside the container's ephemeral filesystem",
      "Offers visibility/feedback about where checkpoints are being saved",
      "Avoids reliance on container's working directory for critical file storage",
      "Maintains optional configuration rather than mandatory directory specification"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:12:01"
    }
  },
  {
    "number": 2282,
    "title": "optimizer got an empty parameter list",
    "created_at": "2020-06-19T20:39:51Z",
    "closed_at": "2020-06-20T23:04:53Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/2282",
    "body": " Hi,\r\nGot the following error:\r\nValueError: optimizer got an empty parameter list with both options below:\r\n\r\ndef configure_optimizers(self):\r\n        # option1 optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n        # option 2\r\n        optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)\r\n        return optimizer\r\n\r\nclass Autoencoder(pl.LightningModule):\r\n    \r\n    def __init__(self, hparams: argparse.Namespace):\r\n        super(Autoencoder,self).__init__() \r\n        self.hparams = hparams\r\n           \r\n        self_layer_e_1 = nn.Conv1d(hparams.in_channels, hparams.out_channels, hparams.kernel_size)\r\n        self_layer_e_2 = nn.Conv1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        self_layer_d_1 = nn.ConvTranspose1d(hparams.in_channels,hparams.out_channels,hparams.kernel_size)\r\n        self_layer_d_2 = nn.ConvTranspose1d(hparams.out_channels,hparams.in_channels,hparams.kernel_size)\r\n        \r\n    \r\n    def forward(self,x):\r\n        x = self_layer_e_1(x)\r\n        x = nn.ReLu(x)\r\n        x = self_layer_e_2(x)\r\n        encoded = nn.ReLU(x)\r\n        x = self_layer_d_1(encoded)\r\n        x = nn.ReLU(x)\r\n        decoded = self_layer_d_2(x)\r\n        decoded = self.decoder(encoded)\r\n        return self.decoded, self.encoded\r\n    \r\n    \r\n    def training_step(self, batch, batch_idx):\r\n        x, _ = batch\r\n        decoded, encoded = self.forward(x)\r\n        loss = MSE(x, decoded)\r\n        return loss\r\n    \r\n    def validation_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'val')\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        return self._shared_eval(batch, batch_idx, 'test')\r\n        \r\n    def _shared_eval(self, batch, batch_idx, prefix):\r\n        x, y = batch\r\n        decoded, encoded = self.forward(x)\r\n        loss = F.nll_loss(x, decoded)\r\n        return {f'{prefix}_loss': loss}\r\n    \r\n    def train_dataloader(self):\r\n        return DataLoader(self.CarrierDataset, batch_size=self.hparams.batch_size)\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(self.CarrierDataset, batch_size=hparams.batch_size)\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(self,CarrierDataset, batch_size=hparams.batch_size)\r\n\r\n    def configure_optimizers(self):\r\n        #optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\r\n        optimizer = torch.optim.Adam(params = list(self.parameters()), lr=self.hparams.lr)\r\n        return optimizer",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/2282/comments",
    "author": "soulhi-vz",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-06-19T20:40:31Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "versatran01",
        "created_at": "2020-06-19T21:56:35Z",
        "body": "You need to use `self.xxx = nn.Conv2d(a,b,c)`  instead of `self_xxx = nn.Conv2d(a,b,c)`  for `nn.Module` to register them as parameters, otherwise your module has no paramters, thuse the optimizer gets nothing."
      },
      {
        "user": "soulhi-vz",
        "created_at": "2020-06-20T23:04:53Z",
        "body": "It works now. Thanks for the catch !!!!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies why model parameters are not being registered",
      "Explains proper parameter registration requirements",
      "Addresses layer definition syntax errors",
      "Ensures model contains trainable parameters"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:12:09"
    }
  },
  {
    "number": 1869,
    "title": "0.7.6 breaks model checkpoint",
    "created_at": "2020-05-18T01:00:26Z",
    "closed_at": "2020-05-18T06:07:31Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1869",
    "body": "## \ud83d\udc1b Bug\r\ntraining crashes when model checkpoint is triggered\r\n\r\n### To Reproduce\r\nIdentical code works in 0.7.5, hparam types used are `int`, `float`, `str`, `bool` \r\nhparams is generated via test tube and saved in the module as `self.hparams`\r\n```\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 389, in ddp_train\r\n    self.run_pretrain_routine(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1015, in run_pretrain_routine\r\n    self.train()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 347, in train\r\n    self.run_training_epoch()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch\r\n    self.call_checkpoint_callback()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 790, in call_checkpoint_callback\r\n    self.checkpoint_callback.on_validation_end(self, self.get_model())\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 241, in on_validation_end\r\n    self._do_check_save(filepath, current, epoch)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 275, in _do_check_save\r\n    self._save_model(filepath)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 142, in _save_model\r\n    self.save_function(filepath)\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py\", line 260, in save_checkpoint\r\n    checkpoint = self.dump_checkpoint()\r\n  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py\", line 355, in dump_checkpoint\r\n    f' not {checkpoint[\"hparams_type\"]}'\r\nValueError: ('The acceptable hparams type is dict or argparse.Namespace,', ' not TTNamespace')\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1869/comments",
    "author": "s-rog",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2020-05-18T01:27:32Z",
        "body": "try master?"
      },
      {
        "user": "s-rog",
        "created_at": "2020-05-18T06:07:31Z",
        "body": "yep 0.7.7 dev fixed it, cheers."
      }
    ],
    "satisfaction_conditions": [
      "Resolves compatibility between model checkpointing and TTNamespace hparams type",
      "Maintains checkpoint functionality that worked in 0.7.5",
      "Handles hparams serialization/deserialization transparently"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:13:09"
    }
  },
  {
    "number": 1665,
    "title": "Trainer add args doesn't add default root dir",
    "created_at": "2020-04-29T15:59:49Z",
    "closed_at": "2020-05-12T12:53:27Z",
    "labels": [
      "bug",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1665",
    "body": "## \ud83d\udc1b Bug\r\n1. When using `parser = Trainer.add_argparse_args(parser)`, it's supposed to put all Trainer's arguments in the argparse with default values. Though currently it doesn't add `default_root_dir` and you get the error:\r\n\r\n```\r\n'Namespace' object has no attribute 'default_root_dir'\r\n```\r\nIt does add `default_save_path` which is deprecated.\r\n\r\n\r\n### To Reproduce\r\n#### Code Sample\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = Trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.default_root_dir)\r\n```\r\n\r\nA similar unit test could also be made, if not there already.\r\n\r\n### Environment\r\n\r\n```\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.3\r\n        - tensorboard:       2.2.0\r\n        - tqdm:              4.45.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.6.7\r\n        - version:           #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019\r\n```",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1665/comments",
    "author": "tshrjn",
    "comments": [
      {
        "user": "olineumann",
        "created_at": "2020-04-30T11:46:49Z",
        "body": "Did you tried to update to 0.7.5. Maybe it is already solved."
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-04T07:15:14Z",
        "body": "Hi @olineumann, yes updating did resolve this. However, the `profiler` arg is now broken. The same demo code above with `profiler` gives the same error `'Namespace' object has no attribute 'profiler'`."
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-04T08:57:26Z",
        "body": "What do you mean 'with profiler'? Do you mean Trainer(..., profiler=True)? But you don't initialize a Trainer.\r\n\r\nRunning your code or this below didn't crash with any error on my machine.\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\nparser = argparse.ArgumentParser(description='demo')\r\ntrainer = Trainer(profiler=True)\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.default_root_dir)\r\n```\r\n\r\nMaybe you could post the complete error message from the python interpreter. "
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-12T03:12:23Z",
        "body": "`add_argparse_args ` is supposed to add the args from trainer to parser. But it doesn't do that for a few args. In this case `profiler`, previously the issue was for `default_root_dir`.\r\n\r\nTry the following code by running:\r\n`python demo.py --profiler True` or  other possibly accepted way `python demo.py --profiler`  with the following code:\r\n\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\n\r\ntrainer = Trainer()\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\n\r\nprint(args.profiler)\r\n\r\n```\r\n\r\n\n\n---\n\nAny update?"
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-12T10:43:03Z",
        "body": "I just created a PR. After looking at the code I found out that add_argparse_args is checking the argument types and is only adding attributes of type str, float, int or bool. The profiler attribute could be of type bool so it should be a bug.\r\n\r\nI saw that get_init_arguments_and_types() is returning profiler as argument but only of type BaseProfiler. After updating typing annotation of profiler argument it worked. Should be available in the next version.\r\n\r\nSee PR #1794 "
      },
      {
        "user": "tshrjn",
        "created_at": "2020-05-12T21:06:28Z",
        "body": "A similar issue is with the pickling of the profiler when it's a `Profile` object & the trainer tries to save the `hparams`.\r\n\r\n```python\r\nTypeError: can't pickle Profile objects\r\n```\r\n\r\n\r\nExample code:\r\n\r\n```python\r\nimport argparse\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning import profiler\r\nfrom pl_bolts.models.gans import BasicGAN\r\n\r\ntrainer = Trainer()\r\nparser = argparse.ArgumentParser(description='demo')\r\nparser = trainer.add_argparse_args(parser)\r\nargs = parser.parse_args()\r\nmodel = BasicGAN()\r\n\r\ntrainer = Trainer.from_argparse_args(\r\n        args, profiler=profiler.AdvancedProfiler())\r\ntrainer.fit(model)\r\n\r\n```\r\n"
      },
      {
        "user": "olineumann",
        "created_at": "2020-05-13T08:02:45Z",
        "body": "Can't reproduce your issue with pl version 0.7.6rc1. On my machine your code runs and saves checkpoints without crashing. Also this wouldn't belong to the topic of this issue imo. This would be a bug in the saving routine. "
      }
    ],
    "satisfaction_conditions": [
      "All Trainer arguments including default_root_dir and profiler must be automatically added by add_argparse_args",
      "Argument type validation must support non-primitive types like BaseProfiler",
      "Deprecated arguments like default_save_path must be properly handled in migration",
      "Added arguments must be compatible with serialization/checkpointing workflows",
      "Solution must prevent similar omissions for other Trainer parameters in the future"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:13:15"
    }
  },
  {
    "number": 1487,
    "title": "ImportError: cannot import name 'WandbLogger' from 'pytorch_lightning.loggers'",
    "created_at": "2020-04-14T13:19:28Z",
    "closed_at": "2020-04-15T07:15:24Z",
    "labels": [
      "bug",
      "help wanted",
      "logger"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/1487",
    "body": "## \ud83d\udc1b Bug\r\n\r\nI tried importing wandb logger from pytorch_lightning.loggers and got import error.\r\n\r\n#### Code sample\r\n```python\r\nfrom pytorch_lightning.loggers import WandbLogger\r\n```\r\n### Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.7.7\r\n - CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.105\r\n - GPU models and configuration: 1050ti",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/1487/comments",
    "author": "braindotai",
    "comments": [
      {
        "user": "github-actions[bot]",
        "created_at": "2020-04-14T13:20:10Z",
        "body": "Hi! thanks for your contribution!, great first issue!"
      },
      {
        "user": "olineumann",
        "created_at": "2020-04-14T13:54:36Z",
        "body": "What is the exact error message? Which lightning version is installed?"
      },
      {
        "user": "awaelchli",
        "created_at": "2020-04-14T16:55:09Z",
        "body": "`from pytorch_lightning.loggers.wandb import WandbLogger` also doesn't work?"
      },
      {
        "user": "lezwon",
        "created_at": "2020-04-14T17:16:00Z",
        "body": "You need to install wandb package first. \r\n`pip install  wandb`"
      },
      {
        "user": "Borda",
        "created_at": "2020-04-14T17:23:04Z",
        "body": "You may also install all extra libs `pip install -r requirements-extra.txt`\r\n@braindotai feel free to re-open if needed :rabbit: "
      },
      {
        "user": "lezwon",
        "created_at": "2020-04-15T06:13:08Z",
        "body": "Guess it would be helpful if there was a error displayed suggesting to install the logger package while importing. I had this same issue while using comet logger and wasn't sure what was wrong."
      },
      {
        "user": "braindotai",
        "created_at": "2020-04-15T07:01:20Z",
        "body": ">  I had this same issue while using comet logger and wasn't sure what was wrong.\r\n\r\nActually I opened my code in vs code, and after that, I installed wandb and did the logging. It needed a restart to the editor to setup wandb properly with my editor command line, and after the restart, the error was gone indeed. I guess that's what happened with you as well.\r\n\r\nThe issue is solved. Thanks to everyone for the quick response. "
      }
    ],
    "satisfaction_conditions": [
      "Identifies missing dependencies required for WandbLogger functionality",
      "Explains environment configuration requirements for logger integration",
      "Includes validation of proper package installation verification methods"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:13:21"
    }
  },
  {
    "number": 185,
    "title": "Log metric row after validation passes",
    "created_at": "2019-08-31T21:14:40Z",
    "closed_at": "2019-09-01T11:31:21Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/185",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nmetrics.csv file, used with test-tube Experiment, is pretty nice way to report some simple results. Unfortunately, logging gets invoked only after training steps. As a result, I can't get the last validation pass results in this file. \r\n\r\n**Describe the solution you'd like**\r\nOption to write metrics from validation_end in a file, similar to metrics.csv, as a Trainer parameter.\r\n\r\n**Describe alternatives you've considered**\r\nLogging directly in \"validation_end\" method, but I haven't found a way to get the context (Experiment, Trainer state) to enrich the information.\r\nPerhaps some access to active Trainer from LightningModule functions, so we can control logging without global variables or duplicating this functionality.",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/185/comments",
    "author": "IvanLazarevsky",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-09-01T11:31:21Z",
        "body": "1. \r\nin any part of the LightningModule you can access self.experiment and self.trainer.  This includes validation_end. \r\n\r\n2. \r\nmetrics.csv logs whatever you add to self.experiment.log(...). In addition, tensorboard metrics can be downloaded from any graph by clicking download as csv. \r\n\r\nis that what you mean?"
      },
      {
        "user": "IvanLazarevsky",
        "created_at": "2019-09-01T11:47:40Z",
        "body": "1. Exactly. Feeling embarrassed not to check that."
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-09-01T13:05:10Z",
        "body": "no need to be embarrassed haha. maybe the docs can be made more clear regarding this?"
      },
      {
        "user": "expectopatronum",
        "created_at": "2019-09-10T11:11:28Z",
        "body": "> metrics.csv logs whatever you add to self.trainer.log(...). In addition, tensorboard metrics can be downloaded from any graph by clicking download as csv.\r\n\r\nI'm confused - I can't find a log function in the trainer class - am I missing something?"
      },
      {
        "user": "williamFalcon",
        "created_at": "2019-09-10T11:22:56Z",
        "body": "self.experiment.log (typo before)"
      }
    ],
    "satisfaction_conditions": [
      "Access to experiment and trainer context during validation_end",
      "Consistent logging mechanism for validation metrics",
      "Avoidance of global variables or code duplication"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:14:29"
    }
  },
  {
    "number": 167,
    "title": "Adding Support for Torchtext iterators",
    "created_at": "2019-08-26T01:35:55Z",
    "closed_at": "2019-08-26T23:04:12Z",
    "labels": [
      "feature",
      "help wanted"
    ],
    "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/167",
    "body": "I recently came across pytorch lightning and i am absolutely loving it  until now. Not having to worry about my training cycle and making it super efficient and fast. It has increased the amount of experiments i can pull off and good results have come out from it. \r\n\r\nRight now, i have been using torchtext with its dataset classes and its custom iterators. But when i tried to use the iterators option from torchtext such as Iterator or BucketIterator instead of Dataloader i get the following error:\r\n\r\n``` TypeError: embedding(): argument 'indices' (position 2) must be Tensor, not NoneType```\r\n\r\nThe problem is that instead of getting a Tensor im getting a NoneType. And i dont know why that is.\r\n\r\nNow, i tried to load the Dataset classes from torchtext with the DataLoader itself and i find the next error:\r\n\r\n```TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torchtext.data.example.Example'> ```\r\n\r\nSo, ideally i would really like to have the torchtext iterators supported with pytorch-lighting. But i dont know if there is a way around this issue that i havent found, still using the torchtext Dataset classes. Could anybody help me out with this?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/167/comments",
    "author": "dehoyosb",
    "comments": [
      {
        "user": "williamFalcon",
        "created_at": "2019-08-26T01:39:01Z",
        "body": "thanks for bringing this up. \r\nCan you try with the latest version? i think we fixed this. \r\n\r\notherwise, can you post a code snippet that generates this error so we can add a patch?\r\n"
      },
      {
        "user": "dehoyosb",
        "created_at": "2019-08-26T22:57:59Z",
        "body": "Yeah! Thank you, i checked and i had the previous version of the package. With the latest one i can use torchtext iterators with no problem. Thank you very much."
      }
    ],
    "satisfaction_conditions": [
      "Compatibility with Torchtext iterators (Iterator/BucketIterator)",
      "Proper handling of Torchtext Dataset classes",
      "Error resolution for type mismatches (Tensor vs NoneType)",
      "Seamless integration without custom workarounds"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:15:03"
    }
  }
]