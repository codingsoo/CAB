{
  "number": 754,
  "title": "Allow a flag into trainer to save checkpoints at partial epochs",
  "created_at": "2020-01-26T13:20:57Z",
  "closed_at": "2020-01-26T14:44:53Z",
  "labels": [
    "feature",
    "help wanted"
  ],
  "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/754",
  "body": "## \ud83d\ude80 Feature\r\nAllow a flag into trainer to save checkpoints at partial epochs\r\n\r\n### Motivation\r\n\r\nWhen you have a large dataset that takes tens of hours per epoch, it's important to have checkpoints along the way. Right now we only get a checkpoint on_epoch_end.\r\n\r\n### Workaround\r\nAlso interested to see if there is a good workaround. I guess I can set a reference to trainer inside my model and manually call on_epoch_end, but that feels like a hack and won't work without changing lightning code because of \r\n\r\nif self.epochs_since_last_check >= self.period:\r\n\r\ninside on_epoch_end.\r\n\r\n### Other ideas?\r\nAlso interested if there are better ways to solve this. Also thought about taking samples out of the dataset to make 'mini epochs,' but this breaks epochs naming convention. (eg an epoch implies all data has been run through once)\r\n\r\nThank you!\r\n\r\n",
  "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/754/comments",
  "author": "thisisjeffchen",
  "comments": [
    {
      "user": "williamFalcon",
      "created_at": "2020-01-26T14:44:53Z",
      "body": "@thisisjeffchen val_check_interval does this. \r\n\r\n```\r\nTrainer(val_check_interval=0.1)\r\n```\r\n\r\nThis will check every 10% of the epoch and save checkpoints\r\n"
    },
    {
      "user": "thisisjeffchen",
      "created_at": "2020-01-26T15:34:09Z",
      "body": "Ah, thank you. Didn't realize it saves over the last checkpoint, so only one file per epoch despite many saves :)"
    }
  ],
  "satisfaction_conditions": [
    "Supports checkpointing at intervals within a single epoch",
    "Does not require modifying internal framework code",
    "Maintains standard epoch semantics (full dataset iteration)",
    "Provides configurable checkpoint frequency",
    "Manages checkpoint storage efficiently during partial saves"
  ],
  "_classification": {
    "category": "Can be dockerized without any issue",
    "timestamp": "2025-04-05 00:14:14"
  },
  "git_commit_info": {
    "sha": "b35c472bb17d170102fd0b987655462b7e3304d3",
    "date": "2020-01-24T23:18:51Z",
    "message": "early stopping check_val_every_n_epoch fix (#743)",
    "author": "Vadim Bereznyuk"
  },
  "repository_info": {
    "structure_summary": ".\n./.git\n./.git/branches\n./.git/description\n./.git/hooks\n./.git/hooks/applypatch-msg.sample\n./.git/hooks/commit-msg.sample\n./.git/hooks/post-update.sample\n./.git/hooks/pre-applypatch.sample\n./.git/hooks/pre-commit.sample\n./.git/hooks/pre-merge-commit.sample\n./.git/hooks/pre-push.sample\n./.git/hooks/pre-receive.sample\n./.git/hooks/push-to-checkout.sample\n./.git/hooks/update.sample\n./.git/hooks/fsmonitor-watchman.sample\n./.git/hooks/pre-rebase.sample\n./.git/hooks/prepare-commit-msg.sample\n./.git/hooks/sendemail-validate.sample\n./.git/info\n./.git/info/exclude\n./.git/config\n./.git/objects\n./.git/objects/pack\n./.git/objects/pack/pack-f2a8a2935a679ac6163e35c17a70fb5698a278b9.pack\n./.git/objects/pack/pack-f2a8a2935a679ac6163e35c17a70fb5698a278b9.rev\n./.git/objects/pack/pack-f2a8a2935a679ac6163e35c17a70fb5698a278b9.idx\n./.git/objects/info\n./.git/HEAD\n./.git/refs\n./.git/refs/heads\n./.git/refs/heads/master\n./.git/refs/tags\n./.git/refs/remotes\n./.git/refs/remotes/origin\n./.git/refs/remotes/origin/HEAD\n./.git/packed-refs\n./.git/logs\n./.git/logs/refs\n./.git/logs/refs/remotes\n./.git/logs/refs/remotes/origin\n./.git/logs/refs/remotes/origin/HEAD\n./.git/logs/refs/heads\n./.git/logs/refs/heads/master\n./.git/logs/HEAD\n./.git/index\n./.github\n./.github/BECOMING_A_CORE_CONTRIBUTOR.md\n./.github/CODE_OF_CONDUCT.md\n./.github/CONTRIBUTING.md\n./.github/ISSUE_TEMPLATE\n./.github/ISSUE_TEMPLATE/bug_report.md\n./.github/ISSUE_TEMPLATE/documentation.md\n./.github/ISSUE_TEMPLATE/feature_request.md\n./.github/ISSUE_TEMPLATE/how-to-question.md\n./.github/PULL_REQUEST_TEMPLATE.md\n./tests\n./tests/README.md\n./tests/__init__.py\n./tests/conftest.py\n./tests/debug.py\n./tests/requirements.txt\n./tests/test_amp.py\n./tests/test_cpu_models.py\n./tests/test_gpu_models.py\n./tests/test_logging.py\n./tests/test_restore_models.py\n./tests/test_trainer.py\n./tests/utils.py\n./.circleci\n./.circleci/config.yml\n./.codecov.yml\n./.gitignore\n./.readthedocs.yml\n./.run_local_tests.sh\n./.travis.yml\n./LICENSE\n./MANIFEST.in\n./README.md\n./appveyor.yml\n./docs\n./docs/Makefile\n./docs/make.bat\n./docs/requirements.txt\n./docs/source\n./docs/source/BECOMING_A_CORE_CONTRIBUTOR.md\n./docs/source/CODE_OF_CONDUCT.md\n./docs/source/CONTRIBUTING.md\n./docs/source/PULL_REQUEST_TEMPLATE.md\n./docs/source/_static\n./docs/source/_static/images\n./docs/source/_static/images/coverage.svg\n./docs/source/_static/images/lightning_icon.svg\n./docs/source/_static/images/lightning_logo-large.svg\n./docs/source/_static/images/lightning_logo-name.svg\n./docs/source/_static/images/lightning_logo.png\n./docs/source/_static/images/lightning_logo.svg\n./docs/source/_static/images/overview_flat.jpg\n./docs/source/_static/images/pl.gif\n./docs/source/_static/images/tf_loss.png\n./docs/source/_static/images/tf_tags.png\n./docs/source/_templates\n./docs/source/_templates/theme_variables.jinja\n./docs/source/callbacks.rst\n./docs/source/common-cases.rst\n./docs/source/conf.py\n./docs/source/documentation.rst\n./docs/source/examples.rst\n./docs/source/governance.md\n./docs/source/index.rst\n./docs/source/lightning-module.rst\n./docs/source/logging.rst\n./docs/source/modules.rst\n./docs/source/new-project.rst\n./docs/source/trainer.rst\n./docs/source/tutorials.rst\n./pl_examples\n./pl_examples/README.md\n./pl_examples/__init__.py\n./pl_examples/basic_examples\n./pl_examples/basic_examples/README.md\n./pl_examples/basic_examples/__init__.py\n./pl_examples/basic_examples/cpu_template.py\n./pl_examples/basic_examples/gpu_template.py\n./pl_examples/basic_examples/lightning_module_template.py\n./pl_examples/domain_templates\n./pl_examples/domain_templates/__init__.py\n./pl_examples/domain_templates/gan.py\n./pl_examples/full_examples\n./pl_examples/full_examples/imagenet\n./pl_examples/full_examples/imagenet/imagenet_example.py\n./pl_examples/multi_node_examples\n./pl_examples/multi_node_examples/README.md\n./pl_examples/multi_node_examples/__init__.py\n./pl_examples/multi_node_examples/ddp2_job_submit.sh\n./pl_examples/multi_node_examples/ddp_job_submit.sh\n./pl_examples/multi_node_examples/multi_node_ddp2_demo.py\n./pl_examples/multi_node_examples/multi_node_ddp_demo.py\n./pyproject.toml\n./pytorch_lightning\n./pytorch_lightning/__init__.py\n./pytorch_lightning/callbacks\n./pytorch_lightning/callbacks/__init__.py\n./pytorch_lightning/callbacks/pt_callbacks.py\n./pytorch_lightning/core\n./pytorch_lightning/core/__init__.py\n./pytorch_lightning/core/decorators.py\n./pytorch_lightning/core/grads.py\n./pytorch_lightning/core/hooks.py\n./pytorch_lightning/core/lightning.py\n./pytorch_lightning/core/memory.py\n./pytorch_lightning/core/model_saving.py\n./pytorch_lightning/core/root_module.py\n./pytorch_lightning/core/saving.py\n./pytorch_lightning/logging\n./pytorch_lightning/logging/__init__.py\n./pytorch_lightning/logging/base.py\n./pytorch_lightning/logging/comet.py\n./pytorch_lightning/logging/comet_logger.py\n./pytorch_lightning/logging/mlflow.py\n./pytorch_lightning/logging/mlflow_logger.py\n./pytorch_lightning/logging/neptune.py\n./pytorch_lightning/logging/tensorboard.py\n./pytorch_lightning/logging/test_tube.py\n./pytorch_lightning/logging/test_tube_logger.py\n./pytorch_lightning/logging/wandb.py\n./pytorch_lightning/overrides\n./pytorch_lightning/overrides/__init__.py\n./pytorch_lightning/overrides/data_parallel.py\n./pytorch_lightning/overrides/override_data_parallel.py\n./pytorch_lightning/pt_overrides\n./pytorch_lightning/pt_overrides/__init__.py\n./pytorch_lightning/root_module\n./pytorch_lightning/root_module/__init__.py\n./pytorch_lightning/testing\n./pytorch_lightning/testing/__init__.py\n./pytorch_lightning/testing/model.py\n./pytorch_lightning/testing/model_base.py\n./pytorch_lightning/testing/model_mixins.py\n./pytorch_lightning/trainer\n./pytorch_lightning/trainer/__init__.py\n./pytorch_lightning/trainer/auto_mix_precision.py\n./pytorch_lightning/trainer/callback_config.py\n./pytorch_lightning/trainer/data_loading.py\n./pytorch_lightning/trainer/distrib_data_parallel.py\n./pytorch_lightning/trainer/distrib_parts.py\n./pytorch_lightning/trainer/evaluation_loop.py\n./pytorch_lightning/trainer/ignored_warnings.py\n./pytorch_lightning/trainer/logging.py\n./pytorch_lightning/trainer/model_hooks.py\n./pytorch_lightning/trainer/trainer.py\n./pytorch_lightning/trainer/training_io.py\n./pytorch_lightning/trainer/training_loop.py\n./pytorch_lightning/trainer/training_tricks.py\n./pytorch_lightning/utilities\n./pytorch_lightning/utilities/__init__.py\n./pytorch_lightning/utilities/arg_parse.py\n./pytorch_lightning/utilities/debugging.py\n./requirements.txt\n./setup.cfg\n./setup.py\n./tox.ini\n./update.sh\n",
    "readme": "\n--- ./tests/README.md ---\n# PyTorch-Lightning Tests\nMost PL tests train a full MNIST model under various trainer conditions (ddp, ddp2+amp, etc...).\nThis provides testing for most combinations of important settings.\nThe tests expect the model to perform to a reasonable degree of testing accuracy to pass.\n\n## Running tests\nThe automatic travis tests ONLY run CPU-based tests. Although these cover most of the use cases,\nrun on a 2-GPU machine to validate the full test-suite.\n\n\nTo run all tests do the following:\n```bash\ngit clone https://github.com/PyTorchLightning/pytorch-lightning\ncd pytorch-lightning\n\n# install module locally\npip install -e .\n\n# install dev deps\npip install -r requirements.txt\n\n# run tests\npy.test -v\n```\n\nTo test models that require GPU make sure to run the above command on a GPU machine.\nThe GPU machine must have:\n1. At least 2 GPUs.\n2. [NVIDIA-apex](https://github.com/NVIDIA/apex#linux) installed.\n\n\n## Running Coverage   \nMake sure to run coverage on a GPU machine with at least 2 GPUs and NVIDIA apex installed. \n\n```bash\ncd pytorch-lightning\n\n# generate coverage \npip install coverage\ncoverage run --source pytorch_lightning -m py.test pytorch_lightning tests examples -v --doctest-modules\n\n# print coverage stats\ncoverage report -m\n\n# exporting resulys\ncoverage xml\ncodecov -t 17327163-8cca-4a5d-86c8-ca5f2ef700bc  -v\n```\n\n\n\n\n\n--- ./README.md ---\n<div align=\"center\">\n\n<img src=\"docs/source/_static/images/lightning_logo.png\" width=\"50\" height=\"50\">\n\n# PyTorch Lightning\n\n**The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.**\n\n\n[![PyPI Status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)\n[![PyPI Status](https://pepy.tech/badge/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)\n[![Build Status](https://travis-ci.org/PytorchLightning/pytorch-lightning.svg?branch=master)](https://travis-ci.org/PytorchLightning/pytorch-lightning)\n[![Build status](https://ci.appveyor.com/api/projects/status/NEW-PROJECT-ID?svg=true)](https://ci.appveyor.com/project/PytorchLightning/pytorch-lightning)\n[![Coverage](docs/source/_static/images/coverage.svg)](https://github.com/PytorchLightning/pytorch-lightning/tree/master/tests#running-coverage)\n[![CodeFactor](https://www.codefactor.io/repository/github/borda/pytorch-lightning/badge)](https://www.codefactor.io/repository/github/borda/pytorch-lightning)    \n\n[![ReadTheDocs](https://readthedocs.org/projects/pytorch-lightning/badge/?version=0.6.0)](https://pytorch-lightning.readthedocs.io/en/0.6.0/)\n[![Slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://join.slack.com/t/pytorch-lightning/shared_invite/enQtODU5ODIyNTUzODQwLTFkMDg5Mzc1MDBmNjEzMDgxOTVmYTdhYjA1MDdmODUyOTg2OGQ1ZWZkYTQzODhhNzdhZDA3YmNhMDhlMDY4YzQ)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/PytorchLightning/pytorch-lightning/blob/master/LICENSE)\n[![Next Release](https://img.shields.io/badge/Next%20Release-Mar%2021-<COLOR>.svg)](https://shields.io/)\n\n<!-- \nremoved until codecov badge isn't empy. likely a config error showing nothing on master.\n[![codecov](https://codecov.io/gh/Borda/pytorch-lightning/branch/master/graph/badge.svg)](https://codecov.io/gh/Borda/pytorch-lightning)\n-->\n\n</div>\n\nSimple installation from PyPI\n```bash\npip install pytorch-lightning  \n```\n\n## Docs   \n- [master](https://pytorch-lightning.readthedocs.io/en/latest)   \n- [0.6.0](https://pytorch-lightning.readthedocs.io/en/0.6.0/)\n- [0.5.3.2](https://pytorch-lightning.readthedocs.io/en/0.5.3.2/)\n\n\n## Demo  \n[Copy and run this COLAB!](https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=HOk9c4_35FKg)\n\n## What is it?  \nLightning is a very lightweight wrapper on PyTorch that decouples the science code from the engineering code. It's more of a style-guide than a framework. By refactoring your code, we can automate most of the non-research code.  \n\nTo use Lightning, simply refactor your research code into the [LightningModule](https://github.com/PytorchLightning/pytorch-lightning#how-do-i-do-use-it) format (the science) and Lightning will automate the rest (the engineering). Lightning guarantees tested, correct, modern best practices for the automated parts.\n\n- If you are a researcher, Lightning is infinitely flexible, you can modify everything down to the way .backward is called or distributed is set up. \n- If you are a scientist or production team, lightning is very simple to use with best practice defaults.\n\n## What does lightning control for me?   \n\nEverything in Blue!   \nThis is how lightning separates the science (red) from the engineering (blue).\n\n![Overview](docs/source/_static/images/pl.gif)\n\n## How much effort is it to convert?\nYou're probably tired of switching frameworks at this point. But it is a very quick process to refactor into the Lightning format (ie: hours). [Check out this tutorial](https://towardsdatascience.com/how-to-refactor-your-pytorch-code-to-get-these-42-benefits-of-pytorch-lighting-6fdd0dc97538)\n\n## Starting a new project?   \n[Use our seed-project aimed at reproducibility!](https://github.com/PytorchLightning/pytorch-lightning-conference-seed)     \n\n## Why do I want to use lightning?\nEvery research project starts the same, a model, a training loop, validation loop, etc. As your research advances, you're likely to need distributed training, 16-bit precision, checkpointing, gradient accumulation, etc.   \n\nLightning sets up all the boilerplate state-of-the-art training for you so you can focus on the research.   \n\n---\n \n## README Table of Contents        \n- [How do I use it](https://github.com/PytorchLightning/pytorch-lightning#how-do-i-do-use-it)     \n- [What lightning automates](https://github.com/PytorchLightning/pytorch-lightning#what-does-lightning-control-for-me)    \n- [Tensorboard integration](https://github.com/PytorchLightning/pytorch-lightning#tensorboard)    \n- [Lightning features](https://github.com/PytorchLightning/pytorch-lightning#lightning-automates-all-of-the-following-each-is-also-configurable)    \n- [Examples](https://github.com/PytorchLightning/pytorch-lightning#examples)    \n- [Tutorials](https://github.com/PytorchLightning/pytorch-lightning#tutorials)\n- [Contributing](https://github.com/PytorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md)\n- [Bleeding edge install](https://github.com/PytorchLightning/pytorch-lightning#bleeding-edge)   \n- [Lightning Design Principles](https://github.com/PytorchLightning/pytorch-lightning#lightning-design-principles)   \n- [Asking for help](https://github.com/PytorchLightning/pytorch-lightning#asking-for-help)\n- [FAQ](https://github.com/PytorchLightning/pytorch-lightning#faq)    \n\n---\n\n## How do I do use it?   \nThink about Lightning as refactoring your research code instead of using a new framework. The research code goes into a [LightningModule](https://pytorch-lightning.rtfd.io/en/latest/LightningModule/RequiredTrainerInterface/) which you fit using a Trainer.\n\nThe LightningModule defines a *system* such as seq-2-seq, GAN, etc... It can ALSO define a simple classifier such as the example below.     \n\nTo use lightning do 2 things:  \n1. [Define a LightningModule](https://pytorch-lightning.rtfd.io/en/latest/LightningModule/RequiredTrainerInterface/)\n**WARNING:** This syntax is for version 0.5.0+ where abbreviations were removed.\n    ```python\n    import os\n    \n    import torch\n    from torch.nn import functional as F\n    from torch.utils.data import DataLoader\n    from torchvision.datasets import MNIST\n    from torchvision import transforms\n    \n    import pytorch_lightning as pl\n    \n    class CoolSystem(pl.LightningModule):\n    \n        def __init__(self):\n            super(CoolSystem, self).__init__()\n            # not the best model...\n            self.l1 = torch.nn.Linear(28 * 28, 10)\n    \n        def forward(self, x):\n            return torch.relu(self.l1(x.view(x.size(0), -1)))\n    \n        def training_step(self, batch, batch_idx):\n            # REQUIRED\n            x, y = batch\n            y_hat = self.forward(x)\n            loss = F.cross_entropy(y_hat, y)\n            tensorboard_logs = {'train_loss': loss}\n            return {'loss': loss, 'log': tensorboard_logs}\n    \n        def validation_step(self, batch, batch_idx):\n            # OPTIONAL\n            x, y = batch\n            y_hat = self.forward(x)\n            return {'val_loss': F.cross_entropy(y_hat, y)}\n    \n        def validation_end(self, outputs):\n            # OPTIONAL\n            avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n            tensorboard_logs = {'val_loss': avg_loss}\n            return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n            \n        def test_step(self, batch, batch_idx):\n            # OPTIONAL\n            x, y = batch\n            y_hat = self.forward(x)\n            return {'test_loss': F.cross_entropy(y_hat, y)}\n    \n        def test_end(self, outputs):\n            # OPTIONAL\n            avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n            tensorboard_logs = {'test_loss': avg_loss}\n            return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n    \n        def configure_optimizers(self):\n            # REQUIRED\n            # can return multiple optimizers and learning_rate schedulers\n            # (LBFGS it is automatically supported, no need for closure function)\n            return torch.optim.Adam(self.parameters(), lr=0.02)\n    \n        @pl.data_loader\n        def train_dataloader(self):\n            # REQUIRED\n            return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n    \n        @pl.data_loader\n        def val_dataloader(self):\n            # OPTIONAL\n            return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n    \n        @pl.data_loader\n        def test_dataloader(self):\n            # OPTIONAL\n            return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)\n    ```\n2. Fit with a [trainer](https://pytorch-lightning.rtfd.io/en/latest/Trainer/)    \n    ```python\n    from pytorch_lightning import Trainer\n    \n    model = CoolSystem()\n    \n    # most basic trainer, uses good defaults\n    trainer = Trainer()    \n    trainer.fit(model)   \n    ```\n\nTrainer sets up a tensorboard logger, early stopping and checkpointing by default (you can modify all of them or\nuse something other than tensorboard).   \n\nHere are more advanced examples\n```python   \n# train on cpu using only 10% of the data (for demo purposes)\ntrainer = Trainer(max_epochs=1, train_percent_check=0.1)\n\n# train on 4 gpus (lightning chooses GPUs for you)\n# trainer = Trainer(max_epochs=1, gpus=4, distributed_backend='ddp')  \n\n# train on 4 gpus (you choose GPUs)\n# trainer = Trainer(max_epochs=1, gpus=[0, 1, 3, 7], distributed_backend='ddp')   \n\n# train on 32 gpus across 4 nodes (make sure to submit appropriate SLURM job)\n# trainer = Trainer(max_epochs=1, gpus=8, num_gpu_nodes=4, distributed_backend='ddp')\n\n# train (1 epoch only here for demo)\ntrainer.fit(model)\n\n# view tensorboard logs \nlogging.info(f'View tensorboard logs by running\\ntensorboard --logdir {os.getcwd()}')\nlogging.info('and going to http://localhost:6006 on your browser')\n```\n\nWhen you're all done you can even run the test set separately.   \n```python\ntrainer.test()\n```\n\n**Could be as complex as seq-2-seq + attention**    \n\n```python\n# define what happens for training here\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    \n    # define your own forward and loss calculation\n    hidden_states = self.encoder(x)\n     \n    # even as complex as a seq-2-seq + attn model\n    # (this is just a toy, non-working example to illustrate)\n    start_token = '<SOS>'\n    last_hidden = torch.zeros(...)\n    loss = 0\n    for step in range(max_seq_len):\n        attn_context = self.attention_nn(hidden_states, start_token)\n        pred = self.decoder(start_token, attn_context, last_hidden) \n        last_hidden = pred\n        pred = self.predict_nn(pred)\n        loss += self.loss(last_hidden, y[step])\n        \n    #toy example as well\n    loss = loss / max_seq_len\n    return {'loss': loss} \n```\n\n**Or as basic as CNN image classification**      \n\n```python\n# define what happens for validation here\ndef validation_step(self, batch, batch_idx):    \n    x, y = batch\n    \n    # or as basic as a CNN classification\n    out = self.forward(x)\n    loss = my_loss(out, y)\n    return {'loss': loss} \n```\n\n**And you also decide how to collate the output of all validation steps**    \n\n```python\ndef validation_end(self, outputs):\n    \"\"\"\n    Called at the end of validation to aggregate outputs\n    :param outputs: list of individual outputs of each validation step\n    :return:\n    \"\"\"\n    val_loss_mean = 0\n    val_acc_mean = 0\n    for output in outputs:\n        val_loss_mean += output['val_loss']\n        val_acc_mean += output['val_acc']\n\n    val_loss_mean /= len(outputs)\n    val_acc_mean /= len(outputs)\n    logs = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}\n    result = {'log': logs}\n    return result\n```\n   \n## Tensorboard    \nLightning is fully integrated with tensorboard, MLFlow and supports any logging module.   \n\n![tensorboard-support](docs/source/_static/images/tf_loss.png)\n\nLightning also adds a text column with all the hyperparameters for this experiment.      \n\n![tensorboard-support](docs/source/_static/images/tf_tags.png)\n\n## Lightning automates all of the following ([each is also configurable](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.trainer.html)):\n\n\n- [Running grid search on a cluster](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.trainer.distrib_data_parallel.html)  \n- [Fast dev run](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.utilities.debugging.html)\n- [Logging](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.logging.html)\n- [Implement Your Own Distributed (DDP) training](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_ddp)\n- [Multi-GPU & Multi-node](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.trainer.distrib_parts.html)\n- [Training loop](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.trainer.training_loop.html)\n- [Hooks](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.core.hooks.html)\n- [Configure optimizers](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers)\n- [Validations](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.trainer.evaluation_loop.html)\n- [Model saving & Restoring training session](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.trainer.training_io.html)\n  \n\n## Examples   \n- [GAN](https://github.com/PytorchLightning/pytorch-lightning/tree/master/pl_examples/domain_templates/gan.py)    \n- [MNIST](https://github.com/PytorchLightning/pytorch-lightning/tree/master/pl_examples/basic_examples)      \n- [Other projects using Lightning](https://github.com/PytorchLightning/pytorch-lightning/network/dependents?package_id=UGFja2FnZS0zNzE3NDU4OTM%3D)    \n- [Multi-node](https://github.com/PytorchLightning/pytorch-lightning/tree/master/pl_examples/multi_node_examples)   \n\n## Tutorials   \n- [Basic Lightning use](https://towardsdatascience.com/supercharge-your-ai-research-with-pytorch-lightning-337948a99eec)    \n- [9 key speed features in Pytorch-Lightning](https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565)    \n- [SLURM, multi-node training with Lightning](https://towardsdatascience.com/trivial-multi-node-training-with-pytorch-lightning-ff75dfb809bd)     \n\n---\n\n## Asking for help    \nWelcome to the Lightning community!   \n\nIf you have any questions, feel free to:   \n1. [read the docs](https://pytorch-lightning.rtfd.io/en/latest/).     \n2. [Search through the issues](https://github.com/PytorchLightning/pytorch-lightning/issues?utf8=%E2%9C%93&q=my++question).      \n3. [Ask on stackoverflow](https://stackoverflow.com/questions/ask?guided=false) with the tag pytorch-lightning.   \n\nIf no one replies to you quickly enough, feel free to post the stackoverflow link to our Gitter chat!   \n\nTo chat with the rest of us visit our [gitter channel](https://gitter.im/PyTorch-Lightning/community)!     \n\n---   \n## FAQ    \n**How do I use Lightning for rapid research?**   \n[Here's a walk-through](https://pytorch-lightning.rtfd.io/en/latest/)  \n\n**Why was Lightning created?**     \nLightning has 3 goals in mind:\n1. Maximal flexibility while abstracting out the common boilerplate across research projects.   \n2. Reproducibility. If all projects use the LightningModule template, it will be much much easier to understand what's going on and where to look! It will also mean every implementation follows a standard format.   \n3. Democratizing PyTorch power user features. Distributed training? 16-bit? know you need them but don't want to take the time to implement? All good... these come built into Lightning.    \n\n**How does Lightning compare with Ignite and fast.ai?**     \n[Here's a thorough comparison](https://medium.com/@_willfalcon/pytorch-lightning-vs-pytorch-ignite-vs-fast-ai-61dc7480ad8a).    \n\n**Is this another library I have to learn?**    \nNope! We use pure Pytorch everywhere and don't add unecessary abstractions!   \n\n**Are there plans to support Python 2?**          \nNope.     \n\n**Are there plans to support virtualenv?**    \nNope. Please use anaconda or miniconda.    \n\n**Which PyTorch versions do you support?**    \n- **PyTorch 1.1.0**       \n    ```bash    \n    # install pytorch 1.1.0 using the official instructions   \n    \n    # install test-tube 0.6.7.6 which supports 1.1.0   \n    pip install test-tube==0.6.7.6   \n    \n    # install latest Lightning version without upgrading deps    \n    pip install -U --no-deps pytorch-lightning\n    ```     \n- **PyTorch 1.2.0, 1.3.0,**\n    Install via pip as normal   \n\n## Custom installation\n\n### Bleeding edge\n\nIf you can't wait for the next release, install the most up to date code with:\n* using GIT (locally clone whole repo with full history)\n    ```bash\n    pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade\n    ```\n* using instant zip (last state of the repo without git history)\n    ```bash\n    pip install https://github.com/PytorchLightning/pytorch-lightning/archive/master.zip --upgrade\n    ```\n\n### Any release installation\n\nYou can also install any past release `0.X.Y` from this repository:\n```bash\npip install https://github.com/PytorchLightning/pytorch-lightning/archive/0.X.Y.zip --upgrade\n```\n\n## Bibtex\nIf you want to cite the framework feel free to use this (but only if you loved it \ud83d\ude0a):\n```\n@misc{Falcon2019,\n  author = {Falcon, W.A. et al.},\n  title = {PyTorch Lightning},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/PytorchLightning/pytorch-lightning}}\n}\n```\n\n\n\n--- ./pl_examples/README.md ---\n# Examples   \nThis folder has 3 sections:   \n\n### Domain templates   \nThese are templates to show common approaches such as GANs and RL.   \n\n### Basic examples   \nThese show the most common use of Lightning for either CPU or GPU training.   \n\n### Multi-node examples   \nThese show how to run jobs on a GPU cluster using lightning.\n\n",
    "readme_filenames": [
      "./tests/README.md",
      "./README.md",
      "./pl_examples/README.md"
    ]
  },
  "llm_calls_before_build": 80,
  "dockerfile_attempt_1": 1,
  "dockerfile": "FROM pytorch/pytorch:1.4-cuda10.1-cudnn7-runtime\n\n# Install system packages required for git\nRUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n\n# Clone repository and checkout specific commit with the fix\nWORKDIR /workspace\nRUN git clone https://github.com/Lightning-AI/pytorch-lightning.git\nWORKDIR /workspace/pytorch-lightning\nRUN git checkout b35c472bb17d170102fd0b987655462b7e3304d3\n\n# Install Python dependencies\nRUN pip install --upgrade pip\nRUN pip install -r requirements.txt\nRUN pip install -r tests/requirements.txt\n\n# Install project in development mode\nRUN pip install -e .\n\n# Set default working directory to project root\nWORKDIR /workspace/pytorch-lightning",
  "dockerfile_build_success": true,
  "successful_candidate_index": 1,
  "successful_candidate_attempt": 1,
  "successful_candidate_improvement_attempt": 0,
  "llm_calls_total": 85
}