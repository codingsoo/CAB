{
  "number": 4853,
  "title": "Jax saves forward-pass intermediate values under lax.stop_gradient",
  "created_at": "2020-11-10T08:48:04Z",
  "closed_at": "2020-11-11T05:51:28Z",
  "labels": [
    "question"
  ],
  "url": "https://github.com/jax-ml/jax/issues/4853",
  "body": "The following code illustrates the problem:\r\n\r\n```\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import jit, grad\r\n\r\nnum_iters = 10_000\r\ndim = 1_000\r\n\r\ndef long_scan(X):\r\n  def scan_inner(carry, _):\r\n    return carry @ X, None\r\n  \r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n\r\n@jit\r\ndef outer(x):\r\n  scan_out = long_scan(x)\r\n  scan_out = jax.lax.stop_gradient(scan_out)\r\n  return jnp.sum(x @ scan_out)\r\n\r\ninput_matrix = jax.random.normal(jax.random.PRNGKey(0), shape=(dim, dim))\r\nouter(input_matrix).block_until_ready()\r\nprint('Does forward pass OK')\r\ngrad(outer)(input_matrix).block_until_ready()\r\n```\r\n\r\nWhen run on the colab GPU we get `RuntimeError: Resource exhausted: Out of memory while trying to allocate 40004000128 bytes.` More generally, the memory usage scales with the length of the scan. As far as I understand, normally that makes sense--the intermediate values have to be saved for the reverse pass of the grad. But here, those intermediate values are never used because of the `stop gradient`. \r\n\r\nI think we can avoid the memory growth by using `remat(scan_inner)` instead of `scan_inner` inside the scan (like in #3186), but it would be great if jax could automatically do this, since we should never need the intermediate values. \r\n\r\nThe actual use-case is adversarial training, where the `long_scan` computes adversarial inputs for a model but we don't take the gradient wrt the model parameters through the process of computing those inputs. ",
  "comments_url": "https://api.github.com/repos/jax-ml/jax/issues/4853/comments",
  "author": "C-J-Cundy",
  "comments": [
    {
      "user": "shoyer",
      "created_at": "2020-11-10T17:00:39Z",
      "body": "Have you tried `long_scan(stop_gradient(x))` instead?\r\n\r\n`stop_gradient()` actually get applied during the JVP calculation from the forward pass"
    },
    {
      "user": "C-J-Cundy",
      "created_at": "2020-11-10T17:46:16Z",
      "body": "~`long_scan(stop_gradient(x))` also runs out of memory.~ (not true, see below)\r\nI can get it to not save intermediate values by using a version of `long_scan` with `scan_inner` stopping the gradient in each iteration:\r\n\r\n```\r\ndef long_scan_stopped(X):\r\n  def scan_inner(carry, _):\r\n    return jax.lax.stop_gradient(carry @ X), jax.lax.stop_gradient(None)\r\n  \r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n```\r\n\r\nIt would be nice if jax could do this automatically though, since it seems like a bug if it's storing intermediate values that we know are never used. "
    },
    {
      "user": "mattjj",
      "created_at": "2020-11-10T21:59:17Z",
      "body": "Are you willing to put a `jit` on the outside, as in `jit(grad(outer))(input_matrix)`? That way XLA will do the memory pruning for you.\n\n---\n\nIt's really surprising to me that @shoyer's suggestion didn't work!\r\n\r\nHere's a look at the forward and backward passes of the original code as jaxprs (I tweaked the jaxpr pretty-printing to show us shapes of jaxpr invars and outvars):\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import jit, grad\r\n\r\nnum_iters = 10_000\r\ndim = 1_000\r\n\r\ndef long_scan(X):\r\n  def scan_inner(carry, _):\r\n    return carry @ X, None\r\n  \r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n\r\n@jit\r\ndef outer(x):\r\n  scan_out = long_scan(x)\r\n  scan_out = jax.lax.stop_gradient(scan_out)\r\n  return jnp.sum(x @ scan_out)\r\n\r\ninput_matrix = jax.random.normal(jax.random.PRNGKey(0), shape=(dim, dim))\r\nouter(input_matrix).block_until_ready()\r\nprint('Does forward pass OK')\r\ngrad(outer)(input_matrix).block_until_ready()\r\n```\r\n\r\n```\r\n=== forward pass ===\r\n{ lambda  ; a:float32[1000,1000].\r\n  let b _ c = xla_call[ backend=None\r\n                        call_jaxpr={ lambda  ; a:float32[1000,1000] b:*.\r\n                                     let c _ _ _ =\r\n                                           scan[ jaxpr={ lambda  ; e:float32[1000,1000] a:* b:* c:float32[1000,1000] d:*.\r\n                                                         let f = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                                              precision=None ] c e\r\n                                                         in (f:float32[1000,1000] *:* *:* c:float32[1000,1000]) }\r\n                                                 length=10000\r\n                                                 linear=(False, True, True, False, True)\r\n                                                 num_carry=2\r\n                                                 num_consts=3\r\n                                                 reverse=False\r\n                                                 unroll=1 ] a * * a *\r\n                                         d = stop_gradient c\r\n                                         e = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                          precision=None ] a d\r\n                                         f = reduce_sum[ axes=(0, 1) ] e\r\n                                     in (f:float32[] *:* d:float32[1000,1000]) }\r\n                        device=None\r\n                        donated_invars=(False, False)\r\n                        name=jvp(outer) ] a *\r\n  in (b:float32[] c:float32[1000,1000]) }\r\n\r\n=== backward pass ===\r\n{ lambda a ; b:float32[].\r\n  let c = xla_call[ backend=None\r\n                    call_jaxpr={ lambda  ; a:float32[1000,1000] b:float32[].\r\n                                 let c = broadcast_in_dim[ broadcast_dimensions=(  )\r\n                                                           shape=(1000, 1000) ] b\r\n                                     d = dot_general[ dimension_numbers=(((1,), (1,)), ((), ()))\r\n                                                      precision=None ] c a\r\n                                 in (d:float32[1000,1000]) }\r\n                    device=None\r\n                    donated_invars=(False, False)\r\n                    name=transpose(jvp(outer)) ] a b\r\n  in (c:float32[1000,1000]) }\r\n```\r\n\r\nIt's a bit subtle to read, but the fourth `scan` output is going to be of shape `(10000, 1000, 1000)` here. It's unused in the outer jaxpr (which is why it is assigned to an underscore) but it'll still be computed in the forward pass.\r\n\r\nApplying @shoyer's suggestion:\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax import jit, grad\r\n\r\nnum_iters = 10_000\r\ndim = 1_000\r\n\r\ndef long_scan(X):\r\n  def scan_inner(carry, _):\r\n    return carry @ X, None\r\n\r\n  carry, _ = jax.lax.scan(scan_inner, X, None, length=num_iters)\r\n  return carry\r\n\r\n@jit\r\ndef outer(x):\r\n  scan_out = long_scan(jax.lax.stop_gradient(x))\r\n  return jnp.sum(x @ scan_out)\r\n\r\ninput_matrix = jax.random.normal(jax.random.PRNGKey(0), shape=(dim, dim))\r\n\r\nfwd_jaxpr = jax.make_jaxpr(lambda x: jax.vjp(outer, x))(input_matrix)\r\nprint('=== forward pass ===')\r\nprint(fwd_jaxpr)\r\n\r\noutput, outer_vjp = jax.vjp(outer, input_matrix)\r\nbwd_jaxpr = jax.make_jaxpr(outer_vjp)(output)\r\nprint('=== backward pass ===')\r\nprint(bwd_jaxpr)\r\n```\r\n\r\n```\r\n=== forward pass ===\r\n{ lambda  ; a:float32[1000,1000].\r\n  let b _ c = xla_call[ backend=None\r\n                        call_jaxpr={ lambda  ; a:float32[1000,1000] b:*.\r\n                                     let c = stop_gradient a\r\n                                         d = scan[ jaxpr={ lambda  ; a:float32[1000,1000] b:float32[1000,1000].\r\n                                                           let c = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                                                precision=None ] b a\r\n                                                           in (c:float32[1000,1000]) }\r\n                                                   length=10000\r\n                                                   linear=(False, False)\r\n                                                   num_carry=1\r\n                                                   num_consts=1\r\n                                                   reverse=False\r\n                                                   unroll=1 ] c c\r\n                                         e = dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\r\n                                                          precision=None ] a d\r\n                                         f = reduce_sum[ axes=(0, 1) ] e\r\n                                     in (f:float32[] *:* d:float32[1000,1000]) }\r\n                        device=None\r\n                        donated_invars=(False, False)\r\n                        name=jvp(outer) ] a *\r\n  in (b:float32[] c:float32[1000,1000]) }\r\n\r\n=== backward pass ===\r\n{ lambda a ; b:float32[].\r\n  let c = xla_call[ backend=None\r\n                    call_jaxpr={ lambda  ; a:float32[1000,1000] b:float32[].\r\n                                 let c = broadcast_in_dim[ broadcast_dimensions=(  )\r\n                                                           shape=(1000, 1000) ] b\r\n                                     d = dot_general[ dimension_numbers=(((1,), (1,)), ((), ()))\r\n                                                      precision=None ] c a\r\n                                 in (d:float32[1000,1000]) }\r\n                    device=None\r\n                    donated_invars=(False, False)\r\n                    name=transpose(jvp(outer)) ] a b\r\n  in (c:float32[1000,1000]) }\r\n```\r\n\r\nIt sure looks to me like the issue is gone: the scan has no scanned-over outputs whatsoever now, and only outputs the final value of the carry.\r\n\r\n@C-J-Cundy maybe the OOM issue with `long_scan(stop_gradient(x))` has some other cause, rather than this scan? Is it worth double-checking?"
    },
    {
      "user": "C-J-Cundy",
      "created_at": "2020-11-10T22:39:27Z",
      "body": "@mattjj, you're completely right, @shoyer's suggestion did work. \r\nI misread the suggestion as ` scan_out = jax.lax.stop_gradient(long_scan(x))` (which didn't work) instead of \r\n`long_scan(jax.lax.stop_gradient(x))`. My mistake! \ud83e\udd26\u200d\u2640\ufe0f\r\n\r\nInterestingly, it seems like the memory pruning doesn't get done at the XLA level with jit-of-grad.\r\nIf I take the initial example and change the last line to \r\n`jit(grad(outer))(input_matrix).block_until_ready()` (and remove the @jit on outer) then I still get an OOM error. \r\n\r\n"
    },
    {
      "user": "mattjj",
      "created_at": "2020-11-11T05:51:28Z",
      "body": "Hrm interesting, I wonder if somehow XLA is missing the optimization.\r\n\r\nGlad to hear that putting stop_gradient earlier fixes things! I think that's the best solution; to notice this optimization automatically is tricky in the grad-of-jit situation, basically because grad thinks it's operating eagerly (i.e. it lives in a \"dynamic graph\" world and doesn't do any compiler-y optimizations). When doing jit-of-grad (or jit-of-grad-of-jit) I'd expect XLA to take care of this optimization for us, but it sounds like it's missing it, at least on the backend you're using.\r\n\r\nIn general it seems it's a good idea to put stop_gradient as early as possible.\r\n\r\nIf it's alright with you, I'll close this issue, but let me know if we should reopen it, and don't hesitate to open new issues!"
    }
  ],
  "satisfaction_conditions": [
    "Prevent memory exhaustion caused by saving unnecessary intermediate values during gradient computation when using stop_gradient",
    "Explain the relationship between stop_gradient placement and XLA's memory optimization behavior",
    "Provide a reliable method to avoid saving scan intermediates when gradients are not required through the scan path",
    "Clarify when JAX/XLA can automatically prune unnecessary intermediate values versus requiring manual intervention"
  ],
  "_classification": {
    "category": "Can be dockerized without any issue",
    "timestamp": "2025-04-05 00:18:16"
  },
  "git_commit_info": {
    "sha": "692386001b8808296260dbeadc4015aef1f98b80",
    "date": "2019-10-31T23:58:04Z",
    "message": "Merge pull request #1605 from google/memleaks\n\nfix a leak where compiled results lived too long",
    "author": "Matthew Johnson"
  },
  "repository_info": {
    "structure_summary": ".\n./.git\n./.git/branches\n./.git/description\n./.git/hooks\n./.git/hooks/applypatch-msg.sample\n./.git/hooks/commit-msg.sample\n./.git/hooks/post-update.sample\n./.git/hooks/pre-applypatch.sample\n./.git/hooks/pre-commit.sample\n./.git/hooks/pre-merge-commit.sample\n./.git/hooks/pre-push.sample\n./.git/hooks/pre-receive.sample\n./.git/hooks/push-to-checkout.sample\n./.git/hooks/update.sample\n./.git/hooks/fsmonitor-watchman.sample\n./.git/hooks/pre-rebase.sample\n./.git/hooks/prepare-commit-msg.sample\n./.git/hooks/sendemail-validate.sample\n./.git/info\n./.git/info/exclude\n./.git/config\n./.git/objects\n./.git/objects/pack\n./.git/objects/pack/pack-db4ef859df0caf20fe3080fc7bb5d6ebfa1f6239.pack\n./.git/objects/pack/pack-db4ef859df0caf20fe3080fc7bb5d6ebfa1f6239.rev\n./.git/objects/pack/pack-db4ef859df0caf20fe3080fc7bb5d6ebfa1f6239.idx\n./.git/objects/info\n./.git/HEAD\n./.git/refs\n./.git/refs/heads\n./.git/refs/heads/main\n./.git/refs/tags\n./.git/refs/remotes\n./.git/refs/remotes/origin\n./.git/refs/remotes/origin/HEAD\n./.git/packed-refs\n./.git/logs\n./.git/logs/refs\n./.git/logs/refs/remotes\n./.git/logs/refs/remotes/origin\n./.git/logs/refs/remotes/origin/HEAD\n./.git/logs/refs/heads\n./.git/logs/refs/heads/main\n./.git/logs/HEAD\n./.git/index\n./LICENSE\n./build\n./build/BUILD.bazel\n./build/Dockerfile\n./build/build.py\n./build/build_jaxlib_wheels.sh\n./build/build_jaxlib_wheels_macos.sh\n./build/build_wheel_docker_entrypoint.sh\n./build/install_xla_in_source_tree.sh\n./build/jaxlib\n./build/jaxlib/__init__.py\n./build/setup.py\n./docs\n./docs/README.md\n./docs/_static\n./docs/_static/jax_logo_250px.png\n./docs/_static/style.css\n./docs/_templates\n./docs/_templates/layout.html\n./docs/conf.py\n./docs/developer.rst\n./docs/gpu_memory_allocation.rst\n./docs/index.rst\n./docs/jax.experimental.optimizers.rst\n./docs/async_dispatch.rst\n./docs/concurrency.rst\n./docs/jax.experimental.rst\n./docs/jax.experimental.stax.rst\n./docs/jax.experimental.vectorize.rst\n./docs/jax.lax.rst\n./docs/jax.nn.initializers.rst\n./docs/jax.nn.rst\n./docs/jax.numpy.rst\n./docs/jax.ops.rst\n./docs/jax.random.rst\n./docs/jax.rst\n./docs/jax.scipy.rst\n./docs/jax.tree_util.rst\n./docs/modules.rst\n./docs/profiling.rst\n./docs/rank_promotion_warning.rst\n./docs/requirements.txt\n./docs/notebooks\n./docs/notebooks/Common_Gotchas_in_JAX.ipynb\n./docs/notebooks/How_JAX_primitives_work.ipynb\n./docs/notebooks/Neural_Network_and_Data_Loading.ipynb\n./docs/notebooks/XLA_in_Python.ipynb\n./docs/notebooks/autodiff_cookbook.ipynb\n./docs/notebooks/maml.ipynb\n./docs/notebooks/neural_network_with_tfds_data.ipynb\n./docs/notebooks/quickstart.ipynb\n./docs/notebooks/score_matching.ipynb\n./docs/notebooks/vmapped_log_probs.ipynb\n./examples\n./examples/__init__.py\n./examples/advi.py\n./examples/datasets.py\n./examples/differentially_private_sgd.py\n./examples/examples_test.py\n./examples/gaussian_process_regression.py\n./examples/kernel_lsq.py\n./examples/mnist_classifier.py\n./examples/mnist_classifier_fromscratch.py\n./examples/mnist_vae.py\n./examples/onnx2xla.py\n./examples/resnet50.py\n./examples/spmd_mnist_classifier_fromscratch.py\n./images\n./images/jax_logo.png\n./images/jax_logo_250px.png\n./images/jax_logo_500px.png\n./images/lifecycle.png\n./jax\n./jax/experimental\n./jax/experimental/__init__.py\n./jax/experimental/ode.py\n./jax/experimental/optimizers.py\n./jax/experimental/stax.py\n./jax/experimental/vectorize.py\n./jax/interpreters\n./jax/interpreters/__init__.py\n./jax/interpreters/ad.py\n./jax/interpreters/batching.py\n./jax/interpreters/masking.py\n./jax/interpreters/parallel.py\n./jax/interpreters/partial_eval.py\n./jax/interpreters/pxla.py\n./jax/interpreters/xla.py\n./jax/lax\n./jax/lax/__init__.py\n./jax/lax/lax.py\n./jax/lax/lax_control_flow.py\n./jax/lax/lax_fft.py\n./jax/lax/lax_parallel.py\n./jax/lib\n./jax/lib/__init__.py\n./jax/lib/xla_bridge.py\n./jax/nn\n./jax/nn/__init__.py\n./jax/nn/functions.py\n./jax/nn/initializers.py\n./jax/numpy\n./jax/numpy/__init__.py\n./jax/numpy/fft.py\n./jax/numpy/lax_numpy.py\n./jax/numpy/linalg.py\n./jax/ops\n./jax/ops/__init__.py\n./jax/ops/scatter.py\n./jax/scipy\n./jax/scipy/stats\n./jax/scipy/stats/__init__.py\n./jax/scipy/stats/bernoulli.py\n./jax/scipy/stats/beta.py\n./jax/scipy/stats/cauchy.py\n./jax/scipy/stats/dirichlet.py\n./jax/scipy/stats/expon.py\n./jax/scipy/stats/gamma.py\n./jax/scipy/stats/laplace.py\n./jax/scipy/stats/multivariate_normal.py\n./jax/scipy/stats/norm.py\n./jax/scipy/stats/pareto.py\n./jax/scipy/stats/poisson.py\n./jax/scipy/stats/t.py\n./jax/scipy/stats/uniform.py\n./jax/scipy/__init__.py\n./jax/scipy/linalg.py\n./jax/scipy/special.py\n./jax/tools\n./jax/tools/BUILD\n./jax/tools/__init__.py\n./jax/tools/build_defs.bzl\n./jax/tools/jax_to_hlo.py\n./jax/BUILD\n./jax/__init__.py\n./jax/abstract_arrays.py\n./jax/ad_util.py\n./jax/api.py\n./jax/api_util.py\n./jax/config.py\n./jax/core.py\n./jax/flatten_util.py\n./jax/lax_linalg.py\n./jax/lax_reference.py\n./jax/linear_util.py\n./jax/pprint_util.py\n./jax/random.py\n./jax/test_util.py\n./jax/tree_util.py\n./jax/util.py\n./jax/version.py\n./jaxlib\n./jaxlib/BUILD\n./jaxlib/cublas.cc\n./jaxlib/cusolver.cc\n./jaxlib/cusolver.py\n./jaxlib/lapack.pyx\n./jaxlib/pytree.cc\n./jaxlib/version.py\n./tests\n./tests/batching_test.py\n./tests/core_test.py\n./tests/debug_nans_test.py\n./tests/fft_test.py\n./tests/api_test.py\n./tests/generated_fun_test.py\n./tests/infeed_test.py\n./tests/jax_to_hlo_test.py\n./tests/lax_control_flow_test.py\n./tests/lax_numpy_einsum_test.py\n./tests/lax_numpy_indexing_test.py\n./tests/lax_numpy_test.py\n./tests/lax_scipy_test.py\n./tests/lax_test.py\n./tests/linalg_test.py\n./tests/masking_test.py\n./tests/multibackend_test.py\n./tests/nn_test.py\n./tests/optimizers_test.py\n./tests/parallel_test.py\n./tests/pmap_test.py\n./tests/random_test.py\n./tests/scipy_stats_test.py\n./tests/stax_test.py\n./tests/tree_util_tests.py\n./tests/vectorize_test.py\n./.gitignore\n./.readthedocs.yml\n./.travis.yml\n./CONTRIBUTING.md\n./LICENSE_SHORT\n./README.md\n./WORKSPACE\n./design_notes\n./design_notes/prng.md\n./pylintrc\n./setup.py\n",
    "readme": "\n--- ./docs/README.md ---\nTo rebuild the documentation, \nsee [Update Documentation](https://jax.readthedocs.io/en/latest/developer.html#update-documentation).\n\n\n\n--- ./README.md ---\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png\" alt=\"logo\"></img>\n</div>\n\n# JAX: Autograd and XLA [![Test status](https://travis-ci.org/google/jax.svg?branch=master)](https://travis-ci.org/google/jax)\n\n[**Reference docs**](https://jax.readthedocs.io/en/latest/)\n| [**Install guide**](#installation)\n| [**Quickstart**](#quickstart-colab-in-the-cloud)\n\nJAX is [Autograd](https://github.com/hips/autograd) and\n[XLA](https://www.tensorflow.org/xla),\nbrought together for high-performance machine learning research.\n\nWith its updated version of [Autograd](https://github.com/hips/autograd),\nJAX can automatically differentiate native\nPython and NumPy functions. It can differentiate through loops, branches,\nrecursion, and closures, and it can take derivatives of derivatives of\nderivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)\nvia [`grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,\nand the two can be composed arbitrarily to any order.\n\nWhat\u2019s new is that JAX uses\n[XLA](https://www.tensorflow.org/xla)\nto compile and run your NumPy programs on GPUs and TPUs. Compilation happens\nunder the hood by default, with library calls getting just-in-time compiled and\nexecuted. But JAX also lets you just-in-time compile your own Python functions\ninto XLA-optimized kernels using a one-function API,\n[`jit`](#compilation-with-jit). Compilation and automatic differentiation can be\ncomposed arbitrarily, so you can express sophisticated algorithms and get\nmaximal performance without leaving Python.\n\nDig a little deeper, and you'll see that JAX is really an extensible system for\n[composable function transformations](#transformations). Both\n[`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)\nare instances of such transformations. Another is [`vmap`](#auto-vectorization-with-vmap)\nfor automatic vectorization, with more to come.\n\nThis is a research project, not an official Google product. Expect bugs and\n[sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nPlease help by trying it out, [reporting\nbugs](https://github.com/google/jax/issues), and letting us know what you\nthink!\n\n```python\nimport jax.numpy as np\nfrom jax import grad, jit, vmap\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = np.dot(inputs, W) + b\n    inputs = np.tanh(outputs)\n  return outputs\n\ndef logprob_fun(params, inputs, targets):\n  preds = predict(params, inputs)\n  return np.sum((preds - targets)**2)\n\ngrad_fun = jit(grad(logprob_fun))  # compiled gradient evaluation function\nperex_grads = jit(vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads\n```\n\nJAX started as a research project by [Matt Johnson](https://github.com/mattjj),\n[Roy Frostig](https://github.com/froystig), [Dougal\nMaclaurin](https://github.com/dougalm), and [Chris\nLeary](https://github.com/learyg), and is now developed [in the\nopen](https://github.com/google/jax) by a growing number of\n[contributors](#contributors).\n\n### Contents\n* [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)\n* [Installation](#installation)\n* [Reference documentation](#reference-documentation)\n* [A brief tour](#a-brief-tour)\n* [What's supported](#whats-supported)\n* [Transformations](#transformations)\n* [Random numbers are different](#random-numbers-are-different)\n* [Mini-libraries](#mini-libraries)\n* [How it works](#how-it-works)\n* [What we're working on](#what-were-working-on)\n* [Current gotchas](#current-gotchas)\n* [Citing JAX](#citing-jax)\n\n## Quickstart: Colab in the Cloud\nJump right in using a notebook in your browser, connected to a Google Cloud GPU. Here are some starter notebooks:\n- [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n- [Training a Simple Neural Network, with PyTorch Data Loading](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/Neural_Network_and_Data_Loading.ipynb)\n- [Training a Simple Neural Network, with TensorFlow Dataset Data Loading](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/neural_network_with_tfds_data.ipynb)\n\nAnd for a deeper dive into JAX:\n- [Common gotchas and sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n- [The Autodiff Cookbook, Part 1: easy and powerful automatic differentiation in JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\n- [Directly using XLA in Python](https://jax.readthedocs.io/en/latest/notebooks/XLA_in_Python.html)\n- [How JAX primitives work](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html)\n- [MAML Tutorial with JAX](https://jax.readthedocs.io/en/latest/notebooks/maml.html)\n- [Generative Modeling by Estimating Gradients of Data Distribution in JAX](https://jax.readthedocs.io/en/latest/notebooks/score_matching.html).\n\n\n## Installation\nJAX is written in pure Python, but it depends on XLA, which needs to be compiled\nand installed as the `jaxlib` package. Use the following instructions to\ninstall a binary package with `pip`, or to build JAX from source.\n\nWe support installing or building `jaxlib` on Linux (Ubuntu 16.04 or later) and\nmacOS (10.12 or later) platforms, but not yet Windows. We're not currently\nworking on Windows support, but contributions are welcome\n(see [#438](https://github.com/google/jax/issues/438)). Some users have reported\nsuccess with building a CPU-only `jaxlib` from source using the Windows Subsytem\nfor Linux.\n\n### pip installation\n\nTo install a CPU-only version, which might be useful for doing local\ndevelopment on a laptop, you can run\n\n```bash\npip install --upgrade pip\npip install --upgrade jax jaxlib  # CPU-only version\n```\n\nOn Linux, it is often necessary to first update `pip` to a version that supports\n`manylinux2010` wheels.\n\nIf you want to install JAX with both CPU and GPU support, using existing CUDA\nand CUDNN7 installations on your machine (for example, preinstalled on your\ncloud VM), you can run\n\n```bash\n# install jaxlib\nPYTHON_VERSION=cp37  # alternatives: cp27, cp35, cp36, cp37\nCUDA_VERSION=cuda92  # alternatives: cuda90, cuda92, cuda100, cuda101\nPLATFORM=linux_x86_64  # alternatives: linux_x86_64\nBASE_URL='https://storage.googleapis.com/jax-releases'\npip install --upgrade $BASE_URL/$CUDA_VERSION/jaxlib-0.1.31-$PYTHON_VERSION-none-$PLATFORM.whl\n\npip install --upgrade jax  # install jax\n```\n\nThe library package name must correspond to the version of the existing CUDA\ninstallation you want to use, with `cuda101` for CUDA 10.1, `cuda100` for CUDA\n10.0, `cuda92` for CUDA 9.2, and `cuda90` for CUDA 9.0. To find your CUDA and\nCUDNN versions, you can run commands like these, depending on your CUDNN install\npath:\n\n```bash\nnvcc --version\ngrep CUDNN_MAJOR -A 2 /usr/local/cuda/include/cudnn.h  # might need different path\n```\n\nThe Python version must match your Python interpreter. There are prebuilt wheels\nfor Python 2.7, 3.5, 3.6, and 3.7; for anything else, you must build from\nsource.\n\nPlease let us know on [the issue tracker](https://github.com/google/jax/issues)\nif you run into any errors or problems with the prebuilt wheels.\n\n### Building JAX from source\nSee [Building JAX from source](https://jax.readthedocs.io/en/latest/developer.html#building-from-source).\n\n\n## Reference documentation\n\nFor details about the JAX API, see the\n[reference documentation](https://jax.readthedocs.io/).\n\n## Developer documentation\n\nFor getting started as a JAX developer, see the\n[developer documentation](https://jax.readthedocs.io/en/latest/developer.html).\n\n## A brief tour\n\n```python\nIn [1]: import jax.numpy as np\n\nIn [2]: from jax import random\n\nIn [3]: key = random.PRNGKey(0)\n\nIn [4]: x = random.normal(key, (5000, 5000))\n\nIn [5]: print(np.dot(x, x.T) / 2)  # fast!\n[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...\n\nIn [6]: print(np.dot(x, x.T) / 2)  # even faster!\n# JIT-compiled code is cached and reused in the 2nd call\n[[  2.52727051e+03   8.15895557e+00  -8.53276134e-01 ...,  # ...\n```\n\nWhat\u2019s happening behind-the-scenes is that JAX is using XLA to just-in-time\n(JIT) compile and execute these individual operations on the GPU. First the\n`random.normal` call is compiled and the array referred to by `x` is generated\non the GPU. Next, each function called on `x` (namely `transpose`, `dot`, and\n`divide`) is individually JIT-compiled and executed, each keeping its results on\nthe device.\nIt\u2019s only when a value needs to be printed, plotted, saved, or passed into a raw\nNumPy function that a read-only copy of the value is brought back to the host as\nan ndarray and cached. The second call to `dot` is faster because the\nJIT-compiled code is cached and reused, saving the compilation time.\n\nThe fun really starts when you use `grad` for automatic differentiation and\n`jit` to compile your own functions end-to-end. Here\u2019s a more complete toy\nexample:\n\n```python\nfrom jax import grad, jit\nimport jax.numpy as np\n\ndef sigmoid(x):\n    return 0.5 * (np.tanh(x / 2.) + 1)\n\n# Outputs probability of a label being true according to logistic model.\ndef logistic_predictions(weights, inputs):\n    return sigmoid(np.dot(inputs, weights))\n\n# Training loss is the negative log-likelihood of the training labels.\ndef loss(weights, inputs, targets):\n    preds = logistic_predictions(weights, inputs)\n    label_logprobs = np.log(preds) * targets + np.log(1 - preds) * (1 - targets)\n    return -np.sum(label_logprobs)\n\n# Build a toy dataset.\ninputs = np.array([[0.52, 1.12,  0.77],\n                   [0.88, -1.08, 0.15],\n                   [0.52, 0.06, -1.30],\n                   [0.74, -2.49, 1.39]])\ntargets = np.array([True, True, False, True])\n\n# Define a compiled function that returns gradients of the training loss\ntraining_gradient_fun = jit(grad(loss))\n\n# Optimize weights using gradient descent.\nweights = np.array([0.0, 0.0, 0.0])\nprint(\"Initial loss: {:0.2f}\".format(loss(weights, inputs, targets)))\nfor i in range(100):\n    weights -= 0.1 * training_gradient_fun(weights, inputs, targets)\n\nprint(\"Trained loss: {:0.2f}\".format(loss(weights, inputs, targets)))\n```\n\nTo see more, check out the [quickstart\nnotebook](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html),\na [simple MNIST classifier\nexample](https://github.com/google/jax/blob/master/examples/mnist_classifier.py)\nand the rest of the [JAX\nexamples](https://github.com/google/jax/blob/master/examples/).\n\n## What's supported\n\nIf you\u2019re using JAX just as an accelerator-backed NumPy, without using `grad` or\n`jit` in your code, then in principle there are no constraints, though some\nNumPy functions haven\u2019t been implemented yet. A list of supported functions can\nbe found in the [reference documentation](https://jax.readthedocs.io/).\n\nGenerally using `np.dot(A, B)` is\nbetter than `A.dot(B)` because the former gives us more opportunities to run the\ncomputation on the device. NumPy also does a lot of work to cast any array-like\nfunction arguments to arrays, as in `np.sum([x, y])`, while `jax.numpy`\ntypically requires explicit casting of array arguments, like\n`np.sum(np.array([x, y]))`.\n\nFor automatic differentiation with `grad`, JAX has the same restrictions\nas [Autograd](https://github.com/hips/autograd). Specifically, differentiation\nworks with indexing (`x = A[i, j, :]`) but not indexed assignment (`A[i, j] =\nx`) or indexed in-place updating (`A[i] += b`) (use\n[`jax.ops.index_update`](https://jax.readthedocs.io/en/latest/_autosummary/jax.ops.index_update.html#jax.ops.index_update)\nor\n[`jax.ops.index_add`](https://jax.readthedocs.io/en/latest/_autosummary/jax.ops.index_add.html#jax.ops.index_add)\ninstead). You can use lists, tuples, and\ndicts freely: JAX doesn't even see them. Using `np.dot(A, B)` rather than\n`A.dot(B)` is required for automatic differentiation when `A` is a raw ndarray.\n\nFor compiling your own functions with `jit` there are a few more requirements.\nBecause `jit` aims to specialize Python functions only on shapes and dtypes\nduring tracing, rather than on concrete values, Python control flow that depends\non concrete values won\u2019t be able to execute and will instead raise an error. If\nyou want compiled control flow, use structured control flow primitives like\n`lax.cond` and `lax.while_loop`. Some indexing features, like slice-based\nindexing, e.g. `A[i:i+5]` for argument-dependent `i`, or boolean-based indexing,\ne.g. `A[bool_ind]` for argument-dependent `bool_ind`, produce abstract values of\nunknown shape and are thus unsupported in `jit` functions.\n\nIn general, JAX is intended to be used with a functional style of Python\nprogramming. Functions passed to transformations like `grad` and `jit` are\nexpected to be free of side-effects. You can write print statements for\ndebugging but they may only be executed once if they're under a `jit` decorator.\n\n> TLDR **Do use**\n>\n> *   Functional programming\n> *   [Many](https://jax.readthedocs.io/en/latest/jax.numpy.html) of NumPy\u2019s\n>     functions (help us add more!)\n> *   [Some](https://jax.readthedocs.io/en/latest/jax.scipy.html) SciPy functions\n> *   Indexing and slicing of arrays like `x = A[[5, 1, 7], :, 2:4]`\n> *   Explicit array creation from lists like `A = np.array([x, y])`\n>\n> **Don\u2019t use**\n>\n> *   Assignment into arrays like `A[0, 0] = x` (use\n>     [`jax.ops.index_update`](https://jax.readthedocs.io/en/latest/_autosummary/jax.ops.index_update.html#jax.ops.index_update)\n>     instead)\n> *   Implicit casting to arrays like `np.sum([x, y])` (use `np.sum(np.array([x,\n>     y])` instead)\n> *   `A.dot(B)` method syntax for functions of more than one argument (use\n>     `np.dot(A, B)` instead)\n> *   Side-effects like mutation of arguments or mutation of global variables\n> *   The `out` argument of NumPy functions\n> *   Dtype casting like `np.float64(x)` (use `x.astype('float64')` or\n>     `x.astype(np.float64)` instead).\n>\n> **For jit functions, also don\u2019t use**\n>\n> *   Control flow based on dynamic values `if x > 0: ...`. Control flow based\n>     on shapes is fine: `if x.shape[0] > 2: ...` and `for subarr in array`.\n> *   Slicing `A[i:i+5]` for dynamic index `i` (use `lax.dynamic_slice` instead)\n>     or boolean indexing `A[bool_ind]` for traced values `bool_ind`.\n\nYou should get loud errors if your code violates any of these.\n\n## Transformations\n\nAt its core, JAX is an extensible system for transforming numerical functions.\nWe currently expose three important transformations: `grad`, `jit`, and `vmap`.\n\n### Automatic differentiation with grad\n\nJAX has roughly the same API as [Autograd](https://github.com/hips/autograd).\nThe most popular function is `grad` for reverse-mode gradients:\n\n```python\nfrom jax import grad\nimport jax.numpy as np\n\ndef tanh(x):  # Define a function\n  y = np.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y)\n\ngrad_tanh = grad(tanh)  # Obtain its gradient function\nprint(grad_tanh(1.0))   # Evaluate it at x = 1.0\n# prints 0.41997434161402603\n```\n\nYou can differentiate to any order with `grad`.\n\nFor more advanced autodiff, you can use `jax.vjp` for reverse-mode\nvector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector\nproducts. The two can be composed arbitrarily with one another, and with other\nJAX transformations. Here's one way to compose\nthose to make a function that efficiently computes full Hessian matrices:\n\n```python\nfrom jax import jit, jacfwd, jacrev\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\n```\n\nAs with Autograd, you're free to use differentiation with Python control\nstructures:\n\n```python\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x\n\nabs_val_grad = grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n```\n\n### Compilation with jit\n\nYou can use XLA to compile your functions end-to-end with `jit`, used either as\nan `@jit` decorator or as a higher-order function.\n\n```python\nimport jax.numpy as np\nfrom jax import jit\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = np.ones((5000, 5000))\nfast_f = jit(slow_f)\n%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X\n%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)\n```\n\nYou can mix `jit` and `grad` and any other JAX transformation however you like.\n\n### Auto-vectorization with vmap\n\n`vmap` is the vectorizing map.\nIt has the familiar semantics of mapping a function along array axes, but\ninstead of keeping the loop on the outside, it pushes the loop down into a\nfunction\u2019s primitive operations for better performance.\n\nUsing `vmap` can save you from having to carry around batch dimensions in your\ncode. For example, consider this simple *unbatched* neural network prediction\nfunction:\n\n```python\ndef predict(params, input_vec):\n  assert input_vec.ndim == 1\n  for W, b in params:\n    output_vec = np.dot(W, input_vec) + b  # `input_vec` on the right-hand side!\n    input_vec = np.tanh(output_vec)\n  return output_vec\n```\n\nWe often instead write `np.dot(inputs, W)` to allow for a batch dimension on the\nleft side of `inputs`, but we\u2019ve written this particular prediction function to\napply only to single input vectors. If we wanted to apply this function to a\nbatch of inputs at once, semantically we could just write\n\n```python\nfrom functools import partial\npredictions = np.stack(list(map(partial(predict, params), input_batch)))\n```\n\nBut pushing one example through the network at a time would be slow! It\u2019s better\nto vectorize the computation, so that at every layer we\u2019re doing matrix-matrix\nmultiplies rather than matrix-vector multiplies.\n\nThe `vmap` function does that transformation for us. That is, if we write\n\n```python\nfrom jax import vmap\npredictions = vmap(partial(predict, params))(input_batch)\n# or, alternatively\npredictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n```\n\nthen the `vmap` function will push the outer loop inside the function, and our\nmachine will end up executing matrix-matrix multiplications exactly as if we\u2019d\ndone the batching by hand.\n\nIt\u2019s easy enough to manually batch a simple neural network without `vmap`, but\nin other cases manual vectorization can be impractical or impossible. Take the\nproblem of efficiently computing per-example gradients: that is, for a fixed set\nof parameters, we want to compute the gradient of our loss function evaluated\nseparately at each example in a batch. With `vmap`, it\u2019s easy:\n\n```python\nper_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)\n```\n\nOf course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other\nJAX transformation! We use `vmap` with both forward- and reverse-mode automatic\ndifferentiation for fast Jacobian and Hessian matrix calculations in\n`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.\n\n\n## Random numbers are different\n\nJAX needs a [functional pseudo-random number generator (PRNG) system](design_notes/prng.md) to provide\nreproducible results invariant to compilation boundaries and backends, while\nalso maximizing performance by enabling vectorized generation and\nparallelization across random calls. The `numpy.random` library doesn\u2019t have\nthose properties. The `jax.random` library meets those needs: it\u2019s functionally\npure, but it doesn\u2019t require you to pass stateful random objects back out of\nevery function.\n\nThe `jax.random` library uses\n[count-based PRNGs](http://www.thesalmons.org/john/random123/papers/random123sc11.pdf)\nand a functional array-oriented\n[splitting model](http://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf).\nTo generate random values, you call a function like `jax.random.normal` and give\nit a PRNG key:\n\n```python\nimport jax.random as random\n\nkey = random.PRNGKey(0)\nprint(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n```\n\nIf we make the same call again with the same key, we get the same values:\n\n```python\nprint(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n```\n\nThe key never gets updated. So how do we get fresh random values? We use\n`jax.random.split` to create new keys from existing ones. A common pattern is to\nsplit off a new key for every function call that needs random values:\n\n```python\nkey = random.PRNGKey(0)\n\nkey, subkey = random.split(key)\nprint(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]\n\nkey, subkey = random.split(key)\nprint(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]\n```\n\nBy splitting the PRNG key, not only do we avoid having to thread random states\nback out of every function call, but also we can generate multiple random arrays\nin parallel because we can avoid unnecessary sequential dependencies.\n\nThere's a gotcha here, which is that it's easy to unintentionally reuse a key\nwithout splitting. We intend to add a check for this (a sort of dynamic linear\ntyping) but for now it's something to be careful about.\n\nFor more detailed information on the design and the reasoning behind it, see the\n[PRNG design doc](design_notes/prng.md).\n\n\n## Mini-libraries\n\nJAX provides some small, experimental libraries for machine learning. These\nlibraries are in part about providing tools and in part about serving as\nexamples for how to build such libraries using JAX. Each one is only a few\nhundred lines of code, so take a look inside and adapt them as you need!\n\n### Neural-net building with Stax\n\n**Stax** is a functional neural network building library. The basic idea is that\na single layer or an entire network can be modeled as an `(init_fun, apply_fun)`\npair. The `init_fun` is used to initialize network parameters and the\n`apply_fun` takes parameters and inputs to produce outputs. There are\nconstructor functions for common basic pairs, like `Conv` and `Relu`, and these\npairs can be composed in series using `stax.serial` or in parallel using\n`stax.parallel`.\n\nHere\u2019s an example:\n\n```python\nimport jax.numpy as np\nfrom jax import random\nfrom jax.experimental import stax\nfrom jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax\n\n# Use stax to set up network initialization and evaluation functions\nnet_init, net_apply = stax.serial(\n    Conv(32, (3, 3), padding='SAME'), Relu,\n    Conv(64, (3, 3), padding='SAME'), Relu,\n    MaxPool((2, 2)), Flatten,\n    Dense(128), Relu,\n    Dense(10), LogSoftmax,\n)\n\n# Initialize parameters, not committing to a batch shape\nrng = random.PRNGKey(0)\nin_shape = (-1, 28, 28, 1)\nout_shape, net_params = net_init(rng, in_shape)\n\n# Apply network to dummy inputs\ninputs = np.zeros((128, 28, 28, 1))\npredictions = net_apply(net_params, inputs)\n```\n\n### First-order optimization\n\nJAX has a minimal optimization library focused on stochastic first-order\noptimizers. Every optimizer is modeled as an `(init_fun, update_fun,\nget_params)` triple of functions. The `init_fun` is used to initialize the\noptimizer state, which could include things like momentum variables, and the\n`update_fun` accepts a gradient and an optimizer state to produce a new\noptimizer state. The `get_params` function extracts the current iterate (i.e.\nthe current parameters) from the optimizer state. The parameters being optimized\ncan be ndarrays or arbitrarily-nested list/tuple/dict structures, so you can\nstore your parameters however you\u2019d like.\n\nHere\u2019s an example, using `jit` to compile the whole update end-to-end:\n\n```python\nfrom jax.experimental import optimizers\nfrom jax import jit, grad\n\n# Define a simple squared-error loss\ndef loss(params, batch):\n  inputs, targets = batch\n  predictions = net_apply(params, inputs)\n  return np.sum((predictions - targets)**2)\n\n# Use optimizers to set optimizer initialization and update functions\nopt_init, opt_update, get_params = optimizers.momentum(step_size=1e-3, mass=0.9)\n\n# Define a compiled update step\n@jit\ndef step(i, opt_state, batch):\n  params = get_params(opt_state)\n  g = grad(loss)(params, batch)\n  return opt_update(i, g, opt_state)\n\n# Dummy input data stream\ndata_generator = ((np.zeros((128, 28, 28, 1)), np.zeros((128, 10)))\n                  for _ in range(10))\n\n# Optimize parameters in a loop\nopt_state = opt_init(net_params)\nfor i in range(10):\n  opt_state = step(i, opt_state, next(data_generator))\nnet_params = get_params(opt_state)\n```\n\n## How it works\n\nProgramming in machine learning is about expressing and transforming functions.\nTransformations include automatic differentiation, compilation for accelerators,\nand automatic batching. High-level languages like Python are great for\nexpressing functions, but usually all we can do with them is apply them. We lose\naccess to their internal structure which would let us perform transformations.\n\nJAX is a tool for specializing and translating high-level Python+NumPy functions\ninto a representation that can be transformed and then lifted back into a Python\nfunction.\n\n![simplified-lifecycle](https://raw.githubusercontent.com/google/jax/master/images/lifecycle.png)\n\nJAX specializes Python functions by tracing. Tracing a function means monitoring\nall the basic operations that are applied to its input to produce its output,\nand recording these operations and the data-flow between them in a directed\nacyclic graph (DAG). To perform tracing, JAX wraps primitive operations, like\nbasic numerical kernels, so that when they\u2019re called they add themselves to a\nlist of operations performed along with their inputs and outputs. To keep track\nof how data flows between these primitives, values being tracked are wrapped in\ninstances of the `Tracer` class.\n\nWhen a Python function is provided to `grad` or `jit`, it\u2019s wrapped for tracing\nand returned. When the wrapped function is called, we abstract the concrete\narguments provided into instances of the `AbstractValue` class, box them for\ntracing in instances of the `Tracer` class, and call the function on them.\nAbstract arguments represent sets of possible values rather than specific\nvalues: for example, `jit` abstracts ndarray arguments to abstract values that\nrepresent all ndarrays with the same shape and dtype. In contrast, `grad`\nabstracts ndarray arguments to represent an infinitesimal neighborhood of the\nunderlying\nvalue. By tracing the Python function on these abstract values, we ensure that\nit\u2019s specialized enough so that it\u2019s tractable to transform, and that it\u2019s still\ngeneral enough so that the transformed result is useful, and possibly reusable.\nThese transformed functions are then lifted back into Python callables in a way\nthat allows them to be traced and transformed again as needed.\n\nThe primitive functions that JAX traces are mostly in 1:1 correspondence with\n[XLA HLO](https://www.tensorflow.org/xla/operation_semantics) and are defined\nin [lax.py](https://github.com/google/jax/blob/master/jax/lax/lax.py). This 1:1\ncorrespondence makes most of the translations to XLA essentially trivial, and\nensures we only have a small set of primitives to cover for other\ntransformations like automatic differentiation. The [`jax.numpy`\nlayer](https://github.com/google/jax/blob/master/jax/numpy/) is written in pure\nPython simply by expressing NumPy functions in terms of the LAX functions (and\nother NumPy functions we\u2019ve already written). That makes `jax.numpy` easy to\nextend.\n\nWhen you use `jax.numpy`, the underlying LAX primitives are `jit`-compiled\nbehind the scenes, allowing you to write unrestricted Python+Numpy code while\nstill executing each primitive operation on an accelerator.\n\nBut JAX can do more: instead of just compiling and dispatching to a fixed set of\nindividual primitives, you can use `jit` on larger and larger functions to be\nend-to-end compiled and optimized. For example, instead of just compiling and\ndispatching a convolution op, you can compile a whole network, or a whole\ngradient evaluation and optimizer update step.\n\nThe tradeoff is that `jit` functions have to satisfy some additional\nspecialization requirements: since we want to compile traces that are\nspecialized on shapes and dtypes, but not specialized all the way to concrete\nvalues, the Python code under a `jit` decorator must be applicable to abstract\nvalues. If we try to evaluate `x > 0` on an abstract `x`, the result is an\nabstract value representing the set `{True, False}`, and so a Python branch like\n`if x > 0` will raise an error: it doesn\u2019t know which way to go!\nSee [What\u2019s supported](#whats-supported) for more\ninformation about `jit` requirements.\n\nThe good news about this tradeoff is that `jit` is opt-in: JAX libraries use\n`jit` on individual operations and functions behind the scenes, allowing you to\nwrite unrestricted Python+Numpy and still make use of a hardware accelerator.\nBut when you want to maximize performance, you can often use `jit` in your own\ncode to compile and end-to-end optimize much bigger functions.\n\n## What we're working on\n1. Documentation!\n2. Cloud TPU support\n3. Multi-GPU and multi-TPU support\n4. Full NumPy coverage and some SciPy coverage\n5. Full coverage for vmap\n6. Make everything faster\n    * Lowering the XLA function dispatch overhead\n    * Linear algebra routines (MKL on CPU, MAGMA on GPU)\n7. `cond` and `while` primitives with efficient automatic differentiation\n\n## Current gotchas\n\nFor a survey of current gotchas, with examples and explanations, we highly\nrecommend reading the [Gotchas Notebook](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n\nSome stand-out gotchas that might surprise NumPy users:\n1. JAX enforces single-precision (32-bit, e.g. `float32`) values by default, and\n   to enable double-precision (64-bit, e.g. `float64`) one needs to set the\n   `jax_enable_x64` variable **at startup** (or set the environment variable\n   `JAX_ENABLE_X64=True`, see [the Gotchas Notebook](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#scrollTo=Double-(64bit)-precision))\n2. Some of NumPy's dtype promotion semantics involving a mix of Python scalars\n   and NumPy types aren't preserved, namely `np.add(1, np.array([2],\n   np.float32)).dtype` is `float64` rather than `float32`.\n3. In-place mutation of arrays isn't supported, though [there is an\n   alternative](https://jax.readthedocs.io/en/latest/jax.ops.html). Generally\n   JAX requires functional code.\n4. PRNGs are different and can be awkward, though for [good\n   reasons](https://github.com/google/jax/blob/master/design_notes/prng.md), and\n   non-reuse (linearity) is not yet checked.\n\nSee [the notebook](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) for much more information.\n\n## Citing JAX\n\nTo cite this repository:\n\n```\n@software{jax2018github,\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and Skye Wanderman-Milne},\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\n  url = {http://github.com/google/jax},\n  version = {0.1.46},\n  year = {2018},\n}\n```\n\nIn the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [jax/version.py](../blob/master/jax/version.py), and\nthe year corresponds to the project's open-source release.\n\nA nascent version of JAX, supporting only automatic differentiation and\ncompilation to XLA, was described in a [paper that appeared at SysML\n2018](https://www.sysml.cc/doc/2018/146.pdf). We're currently working on\ncovering JAX's ideas and capabilities in a more comprehensive and up-to-date\npaper.\n\n## Contributors\n\nSo far, JAX includes lots of help and [contributions](https://github.com/google/jax/graphs/contributors). In addition to the code contributions reflected on GitHub, JAX has benefitted substantially from the advice of\n[Jamie Townsend](https://github.com/j-towns),\n[Peter Hawkins](https://github.com/hawkinsp),\n[Jonathan Ragan-Kelley](https://people.eecs.berkeley.edu/~jrk/),\n[Alex Wiltschko](http://github.com/alexbw),\nGeorge Dahl,\n[Stephan Hoyer](http://stephanhoyer.com/),\nSam Schoenholz,\n[Eli Bendersky](https://github.com/eliben),\nZak Stone,\n[Alexey Radul](https://github.com/axch),\nMichael Isard,\nSkye Wanderman-Milne,\nand many others.\n\n\n\n--- ./CONTRIBUTING.md ---\n# How to Contribute\n\n## Contributor License Agreement\n\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to <https://cla.developers.google.com/> to see\nyour current agreements on file or to sign a new one.\n\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\n\n\n",
    "readme_filenames": [
      "./docs/README.md",
      "./README.md",
      "./CONTRIBUTING.md"
    ],
    "dockerfile": "\n--- ./build/Dockerfile ---\nFROM gcr.io/tensorflow-testing/nosla-cuda10.0-cudnn7-ubuntu16.04-manylinux2010\nLABEL maintainer \"Matt Johnson <mattjj@google.com>\"\n\nWORKDIR /\nRUN git clone --branch v1.2.14 https://github.com/pyenv/pyenv.git /pyenv\nENV PYENV_ROOT /pyenv\nRUN /pyenv/bin/pyenv install 2.7.15\nRUN /pyenv/bin/pyenv install 3.5.6\nRUN /pyenv/bin/pyenv install 3.6.8\nRUN /pyenv/bin/pyenv install 3.7.2\nRUN /pyenv/bin/pyenv install 3.8.0\n\n# Install build-dependencies of scipy.\n# TODO(phawkins): remove when there are scipy wheels for Python 3.8.\nRUN apt-get update && apt-get install -y libopenblas-dev gfortran\n\n# We pin numpy to a version < 1.16 to avoid version compatibility issues.\nRUN eval \"$(/pyenv/bin/pyenv init -)\" && /pyenv/bin/pyenv local 2.7.15 && pip install numpy==1.15.4 scipy cython setuptools wheel future six\nRUN eval \"$(/pyenv/bin/pyenv init -)\" && /pyenv/bin/pyenv local 3.5.6 && pip install numpy==1.15.4 scipy cython setuptools wheel six\nRUN eval \"$(/pyenv/bin/pyenv init -)\" && /pyenv/bin/pyenv local 3.6.8 && pip install numpy==1.15.4 scipy cython setuptools wheel six\nRUN eval \"$(/pyenv/bin/pyenv init -)\" && /pyenv/bin/pyenv local 3.7.2 && pip install numpy==1.15.4 scipy cython setuptools wheel six\nRUN eval \"$(/pyenv/bin/pyenv init -)\" && /pyenv/bin/pyenv local 3.8.0 && pip install numpy==1.17.3 scipy cython setuptools wheel six\n\n# Change the CUDA version if it doesn't match the installed version.\nARG JAX_CUDA_VERSION=10.0\nRUN /bin/bash -c 'if [[ ! \"$CUDA_VERSION\" =~ ^$JAX_CUDA_VERSION.*$ ]]; then \\\n  apt-get update && \\\n  apt-get remove -y --allow-change-held-packages cuda-license-10-0 libcudnn7 libnccl2 && \\\n  apt-get install -y --no-install-recommends \\\n      cuda-nvml-dev-$JAX_CUDA_VERSION \\\n      cuda-command-line-tools-$JAX_CUDA_VERSION \\\n      cuda-libraries-dev-$JAX_CUDA_VERSION \\\n      cuda-minimal-build-$JAX_CUDA_VERSION \\\n      libnccl2=$NCCL_VERSION-1+cuda$JAX_CUDA_VERSION \\\n      libnccl-dev=$NCCL_VERSION-1+cuda$JAX_CUDA_VERSION \\\n      libcudnn7=$CUDNN_VERSION-1+cuda$JAX_CUDA_VERSION \\\n      libcudnn7-dev=$CUDNN_VERSION-1+cuda$JAX_CUDA_VERSION && \\\n  rm -f /usr/local/cuda && \\\n  ln -s /usr/local/cuda-$JAX_CUDA_VERSION /usr/local/cuda; \\\n  fi'\n\n\nWORKDIR /\nCOPY build_wheel_docker_entrypoint.sh /build_wheel_docker_entrypoint.sh\nRUN chmod +x /build_wheel_docker_entrypoint.sh\n\nWORKDIR /build\nENV TEST_TMPDIR /build\nENTRYPOINT [\"/build_wheel_docker_entrypoint.sh\"]\n\n\n",
    "dockerfile_paths": [
      "./build/Dockerfile"
    ]
  },
  "llm_calls_before_build": 25,
  "dockerfile": "FROM nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04\n\n# Install system dependencies and Python 3.8\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    python3.8 \\\n    python3-pip \\\n    git \\\n    python3.8-dev \\\n    build-essential \\\n    libc6-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set Python 3.8 as default\nRUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n\n# Upgrade pip and install base Python dependencies\nRUN pip3 install --upgrade pip setuptools wheel && \\\n    pip3 install numpy scipy\n\n# Install CUDA-compatible jaxlib required for JAX\nRUN pip3 install jaxlib==0.4.7+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n# Clone JAX repository and checkout specific commit\nRUN git clone https://github.com/jax-ml/jax.git /jax\nWORKDIR /jax\nRUN git checkout 692386001b8808296260dbeadc4015aef1f98b80\n\n# Install JAX in editable mode from source with CUDA support\nRUN pip3 install -e .",
  "dockerfile_source": "Repository at Dockerfile",
  "dockerfile_attempt_1": 1,
  "dockerfile_build_success": true,
  "successful_candidate_index": 1,
  "successful_candidate_attempt": 1,
  "successful_candidate_improvement_attempt": 0,
  "llm_calls_total": 30
}