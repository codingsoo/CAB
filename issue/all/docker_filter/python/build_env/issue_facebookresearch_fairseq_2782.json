{
  "number": 2782,
  "title": "Error when trying to train with pipeline parallelism",
  "created_at": "2020-10-23T15:59:08Z",
  "closed_at": "2020-10-26T08:40:55Z",
  "labels": [
    "question",
    "needs triage"
  ],
  "url": "https://github.com/facebookresearch/fairseq/issues/2782",
  "body": "Hi guys,\r\n\r\nI was trying to train a transformer model with pipeline parallelism. Is this supposed to work already? \r\n\r\nThe command i tried (following the translation example):\r\n`fairseq-train     data-bin/iwslt14.tokenized.de-en     --arch transformer_iwslt_de_en_pipeline_parallel --share-decoder-input-output-embed     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0     --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000     --dropout 0.3 --weight-decay 0.0001     --criterion label_smoothed_cross_entropy --label-smoothing 0.1     --max-tokens 4096     --eval-bleu     --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}'     --eval-bleu-detok moses     --eval-bleu-remove-bpe     --eval-bleu-print-samples     --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --pipeline-model-parallel --pipeline-encoder-balance '[8]' --pipeline-encoder-devices '[0]' --pipeline-decoder-balance '[1,6,1]' --pipeline-decoder-devices '[0,1,0]' --pipeline-chunks 1 --distributed-world-size 2`\r\n\r\nerror:\r\n```\r\n2020-10-23 17:17:08 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types\r\n2020-10-23 17:17:08 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types\r\n2020-10-23 17:17:08 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de\r\n2020-10-23 17:17:08 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en\r\n2020-10-23 17:17:08 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples\r\nTraceback (most recent call last):\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/tertiary/thies/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/tertiary/thies/fairseq/fairseq/distributed_utils.py\", line 301, in call_main\r\n    cfg.distributed_training.distributed_world_size,\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 247, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 205, in start_processes\r\n    while not context.join():\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 166, in join\r\n    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\ntorch.multiprocessing.spawn.ProcessRaisedException: \r\n\r\n-- Process 0 terminated with the following error:\r\nTraceback (most recent call last):\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/tertiary/thies/fairseq/fairseq/distributed_utils.py\", line 283, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/tertiary/thies/fairseq/fairseq_cli/train.py\", line 74, in main\r\n    model = task.build_model(cfg.model)\r\n  File \"/tertiary/thies/fairseq/fairseq/tasks/translation.py\", line 327, in build_model\r\n    model = super().build_model(args)\r\n  File \"/tertiary/thies/fairseq/fairseq/tasks/fairseq_task.py\", line 548, in build_model\r\n    model = models.build_model(args, self)\r\n  File \"/tertiary/thies/fairseq/fairseq/models/__init__.py\", line 56, in build_model\r\n    return ARCH_MODEL_REGISTRY[cfg.arch].build_model(cfg, task)\r\n  File \"/tertiary/thies/fairseq/fairseq/model_parallel/models/pipeline_parallel_transformer/model.py\", line 277, in build_model\r\n    checkpoint=args.pipeline_checkpoint,\r\n  File \"/tertiary/thies/fairseq/fairseq/model_parallel/models/pipeline_parallel_transformer/model.py\", line 57, in __init__\r\n    + [encoder.final_layer_norm]\r\n  File \"/secondary/thies/.virtualenvs/pytorch-23102020/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 796, in __getattr__\r\n    type(self).__name__, name))\r\ntorch.nn.modules.module.ModuleAttributeError: 'TransformerEncoder' object has no attribute 'embedding_layer'\r\n```",
  "comments_url": "https://api.github.com/repos/facebookresearch/fairseq/issues/2782/comments",
  "author": "thies1006",
  "comments": [
    {
      "user": "shruti-bh",
      "created_at": "2020-10-23T17:28:40Z",
      "body": "For training, a single `Pipe()` module is created for the Transformer encoder-decoder model. So, you need to set `--pipeline-balance` and `--pipeline-devices` in the training command, instead of `--pipeline-encoder-balance`, `--pipeline-encoder-devices`, `--pipeline-decoder-balance`, `--pipeline-decoder-devices`.\r\nFor inference/generation, two `Pipe()` modules are created, one for the encoder and one for the decoder, since the encoder and decoder are called separately during generation. So, in that case, you need to set `--pipeline-encoder-balance`, `--pipeline-encoder-devices`, `--pipeline-decoder-balance`, `--pipeline-decoder-devices` instead."
    },
    {
      "user": "thies1006",
      "created_at": "2020-10-26T08:40:55Z",
      "body": "Awesome, works now.\r\nThank you very much."
    }
  ],
  "satisfaction_conditions": [
    "Clarifies the distinction between pipeline configuration parameters for training vs inference",
    "Identifies correct pipeline parameter requirements for training phase",
    "Explains how model architecture differs between training and generation phases"
  ],
  "_classification": {
    "category": "Can be dockerized without any issue",
    "timestamp": "2025-04-05 00:39:49"
  },
  "git_commit_info": {
    "sha": "85f097141d83d6aac378838b6c0c8f2a0f77154f",
    "date": "2020-10-13T19:32:58Z",
    "message": "Remove FileContentsAction for --langs\n\nSummary:\n# Facebook:\n\nRevert changes made in D24059296 (https://github.com/pytorch/fairseq/commit/0557ed8b0df90fe671bcb745f384ef7fd0386ab3) since it breaks normal --langs usage. To specify languages in a file, use --lang-dict instead\n\nAlso update integration test params so it can catch this\n\nReviewed By: tangyuq\n\nDifferential Revision: D24224622\n\nfbshipit-source-id: 292eeb86e02528128ced09f8165045be9c847c19",
    "author": "Wei Ho"
  },
  "repository_info": {
    "structure_summary": ".\n./.git\n./.git/branches\n./.git/description\n./.git/hooks\n./.git/hooks/applypatch-msg.sample\n./.git/hooks/commit-msg.sample\n./.git/hooks/post-update.sample\n./.git/hooks/pre-applypatch.sample\n./.git/hooks/pre-commit.sample\n./.git/hooks/pre-merge-commit.sample\n./.git/hooks/pre-push.sample\n./.git/hooks/pre-receive.sample\n./.git/hooks/push-to-checkout.sample\n./.git/hooks/update.sample\n./.git/hooks/fsmonitor-watchman.sample\n./.git/hooks/pre-rebase.sample\n./.git/hooks/prepare-commit-msg.sample\n./.git/hooks/sendemail-validate.sample\n./.git/info\n./.git/info/exclude\n./.git/config\n./.git/objects\n./.git/objects/pack\n./.git/objects/pack/pack-a8d486bff67a95566580ca03aef19204e6640583.pack\n./.git/objects/pack/pack-a8d486bff67a95566580ca03aef19204e6640583.rev\n./.git/objects/pack/pack-a8d486bff67a95566580ca03aef19204e6640583.idx\n./.git/objects/info\n./.git/HEAD\n./.git/refs\n./.git/refs/heads\n./.git/refs/heads/main\n./.git/refs/tags\n./.git/refs/remotes\n./.git/refs/remotes/origin\n./.git/refs/remotes/origin/HEAD\n./.git/packed-refs\n./.git/logs\n./.git/logs/refs\n./.git/logs/refs/remotes\n./.git/logs/refs/remotes/origin\n./.git/logs/refs/remotes/origin/HEAD\n./.git/logs/refs/heads\n./.git/logs/refs/heads/main\n./.git/logs/HEAD\n./.git/index\n./.github\n./.github/ISSUE_TEMPLATE.md\n./.github/ISSUE_TEMPLATE\n./.github/ISSUE_TEMPLATE/documentation.md\n./.github/ISSUE_TEMPLATE/feature_request.md\n./.github/ISSUE_TEMPLATE/bug_report.md\n./.github/ISSUE_TEMPLATE/how-to-question.md\n./.github/workflows\n./.github/workflows/build.yml\n./.github/PULL_REQUEST_TEMPLATE.md\n./CODE_OF_CONDUCT.md\n./LICENSE\n./docs\n./docs/Makefile\n./docs/command_line_tools.rst\n./docs/criterions.rst\n./docs/data.rst\n./docs/docutils.conf\n./docs/fairseq.gif\n./docs/fairseq_logo.png\n./docs/index.rst\n./docs/lr_scheduler.rst\n./docs/make.bat\n./docs/models.rst\n./docs/modules.rst\n./docs/optim.rst\n./docs/overview.rst\n./docs/tasks.rst\n./docs/tutorial_simple_lstm.rst\n./docs/_static\n./docs/_static/theme_overrides.css\n./docs/conf.py\n./docs/getting_started.rst\n./docs/hydra_integration.md\n./docs/requirements.txt\n./docs/tutorial_classifying_names.rst\n./examples\n./examples/.gitignore\n./examples/backtranslation\n./examples/backtranslation/README.md\n./examples/backtranslation/prepare-de-monolingual.sh\n./examples/backtranslation/prepare-wmt18en2de.sh\n./examples/backtranslation/sacrebleu.sh\n./examples/backtranslation/tokenized_bleu.sh\n./examples/backtranslation/deduplicate_lines.py\n./examples/backtranslation/extract_bt_data.py\n./examples/bart\n./examples/bart/README.glue.md\n./examples/bart/README.md\n./examples/bart/README.summarization.md\n./examples/byte_level_bpe\n./examples/byte_level_bpe/README.md\n./examples/byte_level_bpe/get_data.sh\n./examples/byte_level_bpe/get_bitext.py\n./examples/byte_level_bpe/gru_transformer.py\n./examples/camembert\n./examples/camembert/README.md\n./examples/constrained_decoding\n./examples/constrained_decoding/README.md\n./examples/constrained_decoding/normalize.py\n./examples/constrained_decoding/tok.py\n./examples/conv_seq2seq\n./examples/conv_seq2seq/README.md\n./examples/cross_lingual_language_model\n./examples/cross_lingual_language_model/README.md\n./examples/joint_alignment_translation\n./examples/joint_alignment_translation/README.md\n./examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh\n./examples/language_model\n./examples/language_model/prepare-wikitext-103.sh\n./examples/language_model/README.adaptive_inputs.md\n./examples/language_model/README.conv.md\n./examples/language_model/README.md\n./examples/layerdrop\n./examples/layerdrop/README.md\n./examples/linformer\n./examples/linformer/README.md\n./examples/linformer/src\n./examples/linformer/src/__init__.py\n./examples/linformer/src/models\n./examples/linformer/src/models/__init__.py\n./examples/linformer/src/models/linformer_roberta.py\n./examples/linformer/src/modules\n./examples/linformer/src/modules/__init__.py\n./examples/linformer/src/modules/linformer_sentence_encoder.py\n./examples/linformer/src/modules/linformer_sentence_encoder_layer.py\n./examples/linformer/src/modules/multihead_linear_attention.py\n./examples/mbart\n./examples/mbart/README.md\n./examples/megatron_11b\n./examples/megatron_11b/README.md\n./examples/megatron_11b/detok.py\n./examples/multilingual\n./examples/multilingual/README.md\n./examples/multilingual/finetune_multilingual_model.sh\n./examples/multilingual/multilingual_fairseq_gen.sh\n./examples/multilingual/train_multilingual_model.sh\n./examples/noisychannel\n./examples/noisychannel/README.md\n./examples/noisychannel/__init__.py\n./examples/noisychannel/rerank.py\n./examples/noisychannel/rerank_generate.py\n./examples/noisychannel/rerank_options.py\n./examples/noisychannel/rerank_score_bw.py\n./examples/noisychannel/rerank_score_lm.py\n./examples/noisychannel/rerank_tune.py\n./examples/noisychannel/rerank_utils.py\n./examples/nonautoregressive_translation\n./examples/nonautoregressive_translation/README.md\n./examples/nonautoregressive_translation/scripts.md\n./examples/paraphraser\n./examples/paraphraser/README.md\n./examples/paraphraser/paraphrase.py\n./examples/pay_less_attention_paper\n./examples/pay_less_attention_paper/README.md\n./examples/pointer_generator\n./examples/pointer_generator/README.md\n./examples/pointer_generator/README.xsum.md\n./examples/pointer_generator/postprocess.py\n./examples/pointer_generator/preprocess.py\n./examples/pointer_generator/src\n./examples/pointer_generator/src/__init__.py\n./examples/pointer_generator/src/transformer_pg.py\n./examples/quant_noise\n./examples/quant_noise/transformer_quantization_config.yaml\n./examples/quant_noise/README.md\n./examples/roberta\n./examples/roberta/README.custom_classification.md\n./examples/roberta/commonsense_qa\n./examples/roberta/commonsense_qa/__init__.py\n./examples/roberta/commonsense_qa/download_cqa_data.sh\n./examples/roberta/commonsense_qa/README.md\n./examples/roberta/commonsense_qa/commonsense_qa_task.py\n./examples/roberta/preprocess_GLUE_tasks.sh\n./examples/roberta/preprocess_RACE.sh\n./examples/roberta/wsc\n./examples/roberta/wsc/__init__.py\n./examples/roberta/wsc/README.md\n./examples/roberta/wsc/wsc_criterion.py\n./examples/roberta/wsc/wsc_task.py\n./examples/roberta/wsc/wsc_utils.py\n./examples/roberta/README.glue.md\n./examples/roberta/README.md\n./examples/roberta/README.pretraining.md\n./examples/roberta/README.race.md\n./examples/roberta/multiprocessing_bpe_encoder.py\n./examples/roberta/preprocess_RACE.py\n./examples/scaling_nmt\n./examples/scaling_nmt/README.md\n./examples/simultaneous_translation\n./examples/simultaneous_translation/models\n./examples/simultaneous_translation/models/__init__.py\n./examples/simultaneous_translation/models/transformer_monotonic_attention.py\n./examples/simultaneous_translation/modules\n./examples/simultaneous_translation/modules/__init__.py\n./examples/simultaneous_translation/modules/monotonic_multihead_attention.py\n./examples/simultaneous_translation/modules/monotonic_transformer_layer.py\n./examples/simultaneous_translation/utils\n./examples/simultaneous_translation/utils/__init__.py\n./examples/simultaneous_translation/utils/functions.py\n./examples/simultaneous_translation/utils/latency.py\n./examples/simultaneous_translation/README.md\n./examples/simultaneous_translation/__init__.py\n./examples/simultaneous_translation/criterions\n./examples/simultaneous_translation/criterions/__init__.py\n./examples/simultaneous_translation/criterions/label_smoothed_cross_entropy_latency_augmented.py\n./examples/simultaneous_translation/docs\n./examples/simultaneous_translation/docs/baseline.md\n./examples/simultaneous_translation/docs/evaluation.md\n./examples/simultaneous_translation/eval\n./examples/simultaneous_translation/eval/__init__.py\n./examples/simultaneous_translation/eval/agents\n./examples/simultaneous_translation/eval/agents/__init__.py\n./examples/simultaneous_translation/eval/agents/agent.py\n./examples/simultaneous_translation/eval/agents/simul_trans_agent.py\n./examples/simultaneous_translation/eval/agents/simul_trans_text_agent.py\n./examples/simultaneous_translation/eval/agents/word_splitter.py\n./examples/simultaneous_translation/eval/client.py\n./examples/simultaneous_translation/eval/eval_latency.py\n./examples/simultaneous_translation/eval/evaluate.py\n./examples/simultaneous_translation/eval/scorers\n./examples/simultaneous_translation/eval/scorers/__init__.py\n./examples/simultaneous_translation/eval/scorers/scorer.py\n./examples/simultaneous_translation/eval/scorers/text_scorer.py\n./examples/simultaneous_translation/eval/server.py\n./examples/speech_recognition\n./examples/speech_recognition/criterions\n./examples/speech_recognition/criterions/cross_entropy_acc.py\n./examples/speech_recognition/criterions/ASG_loss.py\n./examples/speech_recognition/criterions/__init__.py\n./examples/speech_recognition/data\n./examples/speech_recognition/data/data_utils.py\n./examples/speech_recognition/data/__init__.py\n./examples/speech_recognition/data/asr_dataset.py\n./examples/speech_recognition/data/collaters.py\n./examples/speech_recognition/data/replabels.py\n./examples/speech_recognition/datasets\n./examples/speech_recognition/datasets/prepare-librispeech.sh\n./examples/speech_recognition/datasets/asr_prep_json.py\n./examples/speech_recognition/models\n./examples/speech_recognition/models/__init__.py\n./examples/speech_recognition/models/vggtransformer.py\n./examples/speech_recognition/models/w2l_conv_glu_enc.py\n./examples/speech_recognition/tasks\n./examples/speech_recognition/tasks/__init__.py\n./examples/speech_recognition/tasks/speech_recognition.py\n./examples/speech_recognition/utils\n./examples/speech_recognition/utils/wer_utils.py\n./examples/speech_recognition/README.md\n./examples/speech_recognition/__init__.py\n./examples/speech_recognition/infer.py\n./examples/speech_recognition/w2l_decoder.py\n./examples/stories\n./examples/stories/README.md\n./examples/translation\n./examples/translation/prepare-iwslt17-multilingual.sh\n./examples/translation/prepare-wmt14en2de.sh\n./examples/translation/prepare-wmt14en2fr.sh\n./examples/translation/README.md\n./examples/translation/prepare-iwslt14.sh\n./examples/translation_moe\n./examples/translation_moe/README.md\n./examples/translation_moe/score.py\n./examples/translation_moe/src\n./examples/translation_moe/src/__init__.py\n./examples/translation_moe/src/logsumexp_moe.py\n./examples/translation_moe/src/mean_pool_gating_network.py\n./examples/translation_moe/src/translation_moe.py\n./examples/unsupervised_quality_estimation\n./examples/unsupervised_quality_estimation/README.md\n./examples/unsupervised_quality_estimation/aggregate_scores.py\n./examples/unsupervised_quality_estimation/meteor.py\n./examples/unsupervised_quality_estimation/repeat_lines.py\n./examples/wav2vec\n./examples/wav2vec/README.md\n./examples/wav2vec/libri_labels.py\n./examples/wav2vec/vq-wav2vec_featurize.py\n./examples/wav2vec/wav2vec_featurize.py\n./examples/wav2vec/wav2vec_manifest.py\n./examples/wmt19\n./examples/wmt19/README.md\n./examples/xlmr\n./examples/xlmr/README.md\n./examples/__init__.py\n./fairseq\n./fairseq/benchmark\n./fairseq/benchmark/__init__.py\n./fairseq/benchmark/dummy_lm.py\n./fairseq/benchmark/dummy_masked_lm.py\n./fairseq/benchmark/dummy_model.py\n./fairseq/benchmark/dummy_mt.py\n./fairseq/clib\n./fairseq/clib/libbleu\n./fairseq/clib/libbleu/libbleu.cpp\n./fairseq/clib/libbleu/module.cpp\n./fairseq/clib/libnat\n./fairseq/clib/libnat/edit_dist.cpp\n./fairseq/clib/libnat_cuda\n./fairseq/clib/libnat_cuda/binding.cpp\n./fairseq/clib/libnat_cuda/edit_dist.cu\n./fairseq/clib/libnat_cuda/edit_dist.h\n./fairseq/criterions\n./fairseq/criterions/__init__.py\n./fairseq/criterions/adaptive_loss.py\n./fairseq/criterions/composite_loss.py\n./fairseq/criterions/cross_entropy.py\n./fairseq/criterions/ctc.py\n./fairseq/criterions/fairseq_criterion.py\n./fairseq/criterions/label_smoothed_cross_entropy.py\n./fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py\n./fairseq/criterions/legacy_masked_lm.py\n./fairseq/criterions/masked_lm.py\n./fairseq/criterions/nat_loss.py\n./fairseq/criterions/sentence_prediction.py\n./fairseq/criterions/sentence_ranking.py\n./fairseq/criterions/wav2vec_criterion.py\n./fairseq/data\n./fairseq/data/audio\n./fairseq/data/audio/__init__.py\n./fairseq/data/audio/raw_audio_dataset.py\n./fairseq/data/offset_tokens_dataset.py\n./fairseq/data/plasma_utils.py\n./fairseq/data/raw_label_dataset.py\n./fairseq/data/resampling_dataset.py\n./fairseq/data/encoders\n./fairseq/data/encoders/__init__.py\n./fairseq/data/encoders/byte_bpe.py\n./fairseq/data/encoders/byte_utils.py\n./fairseq/data/encoders/bytes.py\n./fairseq/data/encoders/characters.py\n./fairseq/data/encoders/fastbpe.py\n./fairseq/data/encoders/gpt2_bpe.py\n./fairseq/data/encoders/gpt2_bpe_utils.py\n./fairseq/data/encoders/hf_bert_bpe.py\n./fairseq/data/encoders/hf_byte_bpe.py\n./fairseq/data/encoders/moses_tokenizer.py\n./fairseq/data/encoders/nltk_tokenizer.py\n./fairseq/data/encoders/sentencepiece_bpe.py\n./fairseq/data/encoders/space_tokenizer.py\n./fairseq/data/encoders/subword_nmt_bpe.py\n./fairseq/data/encoders/utils.py\n./fairseq/data/fasta_dataset.py\n./fairseq/data/legacy\n./fairseq/data/legacy/__init__.py\n./fairseq/data/legacy/block_pair_dataset.py\n./fairseq/data/legacy/masked_lm_dataset.py\n./fairseq/data/legacy/masked_lm_dictionary.py\n./fairseq/data/round_robin_zip_datasets.py\n./fairseq/data/multilingual\n./fairseq/data/multilingual/__init__.py\n./fairseq/data/multilingual/multilingual_utils.py\n./fairseq/data/multilingual/multilingual_data_manager.py\n./fairseq/data/multilingual/sampled_multi_dataset.py\n./fairseq/data/multilingual/sampled_multi_epoch_dataset.py\n./fairseq/data/multilingual/sampling_method.py\n./fairseq/data/roll_dataset.py\n./fairseq/data/prepend_dataset.py\n./fairseq/data/sort_dataset.py\n./fairseq/data/strip_token_dataset.py\n./fairseq/data/subsample_dataset.py\n./fairseq/data/token_block_dataset.py\n./fairseq/data/token_block_utils_fast.pyx\n./fairseq/data/transform_eos_dataset.py\n./fairseq/data/transform_eos_lang_pair_dataset.py\n./fairseq/data/__init__.py\n./fairseq/data/add_target_dataset.py\n./fairseq/data/append_token_dataset.py\n./fairseq/data/backtranslation_dataset.py\n./fairseq/data/base_wrapper_dataset.py\n./fairseq/data/bucket_pad_length_dataset.py\n./fairseq/data/colorize_dataset.py\n./fairseq/data/concat_dataset.py\n./fairseq/data/concat_sentences_dataset.py\n./fairseq/data/data_utils.py\n./fairseq/data/data_utils_fast.pyx\n./fairseq/data/denoising_dataset.py\n./fairseq/data/dictionary.py\n./fairseq/data/fairseq_dataset.py\n./fairseq/data/id_dataset.py\n./fairseq/data/indexed_dataset.py\n./fairseq/data/iterators.py\n./fairseq/data/language_pair_dataset.py\n./fairseq/data/list_dataset.py\n./fairseq/data/lm_context_window_dataset.py\n./fairseq/data/lru_cache_dataset.py\n./fairseq/data/mask_tokens_dataset.py\n./fairseq/data/monolingual_dataset.py\n./fairseq/data/multi_corpus_dataset.py\n./fairseq/data/multi_corpus_sampled_dataset.py\n./fairseq/data/nested_dictionary_dataset.py\n./fairseq/data/noising.py\n./fairseq/data/num_samples_dataset.py\n./fairseq/data/numel_dataset.py\n./fairseq/data/pad_dataset.py\n./fairseq/data/prepend_token_dataset.py\n./fairseq/data/replace_dataset.py\n./fairseq/data/shorten_dataset.py\n./fairseq/dataclass\n./fairseq/dataclass/__init__.py\n./fairseq/dataclass/constants.py\n./fairseq/dataclass/data_class.py\n./fairseq/dataclass/utils.py\n./fairseq/logging\n./fairseq/logging/__init__.py\n./fairseq/logging/meters.py\n./fairseq/logging/metrics.py\n./fairseq/logging/progress_bar.py\n./fairseq/model_parallel\n./fairseq/model_parallel/criterions\n./fairseq/model_parallel/criterions/__init__.py\n./fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py\n./fairseq/model_parallel/megatron\n./fairseq/model_parallel/models\n./fairseq/model_parallel/models/pipeline_parallel_transformer\n./fairseq/model_parallel/models/pipeline_parallel_transformer/__init__.py\n./fairseq/model_parallel/models/pipeline_parallel_transformer/layers.py\n./fairseq/model_parallel/models/pipeline_parallel_transformer/model.py\n./fairseq/model_parallel/models/roberta\n./fairseq/model_parallel/models/roberta/__init__.py\n./fairseq/model_parallel/models/roberta/model.py\n./fairseq/model_parallel/models/__init__.py\n./fairseq/model_parallel/models/transformer.py\n./fairseq/model_parallel/models/transformer_lm.py\n./fairseq/model_parallel/modules\n./fairseq/model_parallel/modules/__init__.py\n./fairseq/model_parallel/modules/multihead_attention.py\n./fairseq/model_parallel/modules/transformer_layer.py\n./fairseq/model_parallel/modules/transformer_sentence_encoder.py\n./fairseq/model_parallel/modules/transformer_sentence_encoder_layer.py\n./fairseq/model_parallel/__init__.py\n./fairseq/model_parallel/megatron_trainer.py\n./fairseq/models\n./fairseq/models/bart\n./fairseq/models/bart/__init__.py\n./fairseq/models/bart/hub_interface.py\n./fairseq/models/bart/model.py\n./fairseq/models/huggingface\n./fairseq/models/huggingface/__init__.py\n./fairseq/models/huggingface/hf_gpt2.py\n./fairseq/models/huggingface/transformers\n./fairseq/models/nat\n./fairseq/models/nat/__init__.py\n./fairseq/models/nat/cmlm_transformer.py\n./fairseq/models/nat/fairseq_nat_model.py\n./fairseq/models/nat/insertion_transformer.py\n./fairseq/models/nat/iterative_nonautoregressive_transformer.py\n./fairseq/models/nat/levenshtein_transformer.py\n./fairseq/models/nat/levenshtein_utils.py\n./fairseq/models/nat/nat_crf_transformer.py\n./fairseq/models/nat/nonautoregressive_ensembles.py\n./fairseq/models/nat/nonautoregressive_transformer.py\n./fairseq/models/roberta\n./fairseq/models/roberta/__init__.py\n./fairseq/models/roberta/alignment_utils.py\n./fairseq/models/roberta/hub_interface.py\n./fairseq/models/roberta/model.py\n./fairseq/models/roberta/model_camembert.py\n./fairseq/models/roberta/model_xlmr.py\n./fairseq/models/wav2vec\n./fairseq/models/wav2vec/__init__.py\n./fairseq/models/wav2vec/wav2vec.py\n./fairseq/models/wav2vec/wav2vec2.py\n./fairseq/models/wav2vec/wav2vec2_asr.py\n./fairseq/models/__init__.py\n./fairseq/models/composite_encoder.py\n./fairseq/models/distributed_fairseq_model.py\n./fairseq/models/fairseq_decoder.py\n./fairseq/models/fairseq_encoder.py\n./fairseq/models/fairseq_incremental_decoder.py\n./fairseq/models/fairseq_model.py\n./fairseq/models/fconv.py\n./fairseq/models/fconv_lm.py\n./fairseq/models/fconv_self_att.py\n./fairseq/models/lightconv.py\n./fairseq/models/lightconv_lm.py\n./fairseq/models/lstm.py\n./fairseq/models/lstm_lm.py\n./fairseq/models/masked_lm.py\n./fairseq/models/model_utils.py\n./fairseq/models/multilingual_transformer.py\n./fairseq/models/transformer.py\n./fairseq/models/transformer_align.py\n./fairseq/models/transformer_from_pretrained_xlm.py\n./fairseq/models/transformer_lm.py\n./fairseq/modules\n./fairseq/modules/dynamicconv_layer\n./fairseq/modules/dynamicconv_layer/__init__.py\n./fairseq/modules/dynamicconv_layer/cuda_function_gen.py\n./fairseq/modules/dynamicconv_layer/dynamicconv_cuda.cpp\n./fairseq/modules/dynamicconv_layer/dynamicconv_cuda.cuh\n./fairseq/modules/dynamicconv_layer/dynamicconv_cuda_kernel.cu\n./fairseq/modules/dynamicconv_layer/dynamicconv_layer.py\n./fairseq/modules/dynamicconv_layer/dynamiconv_cpu.cpp\n./fairseq/modules/dynamicconv_layer/setup.py\n./fairseq/modules/fp32_group_norm.py\n./fairseq/modules/gelu.py\n./fairseq/modules/grad_multiply.py\n./fairseq/modules/layer_drop.py\n./fairseq/modules/learned_positional_embedding.py\n./fairseq/modules/lightconv_layer\n./fairseq/modules/lightconv_layer/__init__.py\n./fairseq/modules/lightconv_layer/cuda_function_gen.py\n./fairseq/modules/lightconv_layer/lightconv_cuda.cpp\n./fairseq/modules/lightconv_layer/lightconv_cuda.cuh\n./fairseq/modules/lightconv_layer/lightconv_cuda_kernel.cu\n./fairseq/modules/lightconv_layer/lightconv_layer.py\n./fairseq/modules/lightconv_layer/setup.py\n./fairseq/modules/quantization\n./fairseq/modules/quantization/__init__.py\n./fairseq/modules/quantization/pq\n./fairseq/modules/quantization/pq/modules\n./fairseq/modules/quantization/pq/modules/qconv.py\n./fairseq/modules/quantization/pq/modules/qlinear.py\n./fairseq/modules/quantization/pq/modules/__init__.py\n./fairseq/modules/quantization/pq/modules/qemb.py\n./fairseq/modules/quantization/pq/pq.py\n./fairseq/modules/quantization/pq/__init__.py\n./fairseq/modules/quantization/pq/em.py\n./fairseq/modules/quantization/pq/utils.py\n./fairseq/modules/quantization/quantization_options.py\n./fairseq/modules/quantization/scalar\n./fairseq/modules/quantization/scalar/__init__.py\n./fairseq/modules/quantization/scalar/modules\n./fairseq/modules/quantization/scalar/modules/__init__.py\n./fairseq/modules/quantization/scalar/modules/qact.py\n./fairseq/modules/quantization/scalar/modules/qconv.py\n./fairseq/modules/quantization/scalar/modules/qemb.py\n./fairseq/modules/quantization/scalar/modules/qlinear.py\n./fairseq/modules/quantization/scalar/ops.py\n./fairseq/modules/quantization/scalar/utils.py\n./fairseq/modules/scalar_bias.py\n./fairseq/modules/vggblock.py\n./fairseq/modules/__init__.py\n./fairseq/modules/adaptive_input.py\n./fairseq/modules/adaptive_softmax.py\n./fairseq/modules/beamable_mm.py\n./fairseq/modules/character_token_embedder.py\n./fairseq/modules/conv_tbc.py\n./fairseq/modules/cross_entropy.py\n./fairseq/modules/cuda_utils.cu\n./fairseq/modules/downsampled_multihead_attention.py\n./fairseq/modules/dynamic_convolution.py\n./fairseq/modules/dynamic_crf_layer.py\n./fairseq/modules/fairseq_dropout.py\n./fairseq/modules/gumbel_vector_quantizer.py\n./fairseq/modules/kmeans_vector_quantizer.py\n./fairseq/modules/layer_norm.py\n./fairseq/modules/lightweight_convolution.py\n./fairseq/modules/linearized_convolution.py\n./fairseq/modules/multihead_attention.py\n./fairseq/modules/positional_embedding.py\n./fairseq/modules/quant_noise.py\n./fairseq/modules/same_pad.py\n./fairseq/modules/sinusoidal_positional_embedding.py\n./fairseq/modules/sparse_multihead_attention.py\n./fairseq/modules/sparse_transformer_sentence_encoder.py\n./fairseq/modules/sparse_transformer_sentence_encoder_layer.py\n./fairseq/modules/transformer_layer.py\n./fairseq/modules/transformer_sentence_encoder.py\n./fairseq/modules/transformer_sentence_encoder_layer.py\n./fairseq/modules/transpose_last.py\n./fairseq/modules/unfold.py\n./fairseq/optim\n./fairseq/optim/lr_scheduler\n./fairseq/optim/lr_scheduler/__init__.py\n./fairseq/optim/lr_scheduler/cosine_lr_scheduler.py\n./fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py\n./fairseq/optim/lr_scheduler/fixed_schedule.py\n./fairseq/optim/lr_scheduler/inverse_square_root_schedule.py\n./fairseq/optim/lr_scheduler/polynomial_decay_schedule.py\n./fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py\n./fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py\n./fairseq/optim/lr_scheduler/triangular_lr_scheduler.py\n./fairseq/optim/__init__.py\n./fairseq/optim/adadelta.py\n./fairseq/optim/adafactor.py\n./fairseq/optim/adagrad.py\n./fairseq/optim/adam.py\n./fairseq/optim/adamax.py\n./fairseq/optim/bmuf.py\n./fairseq/optim/dynamic_loss_scaler.py\n./fairseq/optim/fairseq_optimizer.py\n./fairseq/optim/fp16_optimizer.py\n./fairseq/optim/fused_adam.py\n./fairseq/optim/fused_lamb.py\n./fairseq/optim/nag.py\n./fairseq/optim/sgd.py\n./fairseq/optim/shard.py\n./fairseq/scoring\n./fairseq/scoring/__init__.py\n./fairseq/scoring/bleu.py\n./fairseq/scoring/wer.py\n./fairseq/tasks\n./fairseq/tasks/__init__.py\n./fairseq/tasks/audio_pretraining.py\n./fairseq/tasks/cross_lingual_lm.py\n./fairseq/tasks/denoising.py\n./fairseq/tasks/fairseq_task.py\n./fairseq/tasks/language_modeling.py\n./fairseq/tasks/legacy_masked_lm.py\n./fairseq/tasks/masked_lm.py\n./fairseq/tasks/multilingual_denoising.py\n./fairseq/tasks/multilingual_masked_lm.py\n./fairseq/tasks/multilingual_translation.py\n./fairseq/tasks/semisupervised_translation.py\n./fairseq/tasks/sentence_prediction.py\n./fairseq/tasks/sentence_ranking.py\n./fairseq/tasks/translation.py\n./fairseq/tasks/translation_from_pretrained_bart.py\n./fairseq/tasks/translation_from_pretrained_xlm.py\n./fairseq/tasks/translation_lev.py\n./fairseq/tasks/translation_multi_simple_epoch.py\n./fairseq/__init__.py\n./fairseq/binarizer.py\n./fairseq/checkpoint_utils.py\n./fairseq/distributed_utils.py\n./fairseq/file_io.py\n./fairseq/file_utils.py\n./fairseq/hub_utils.py\n./fairseq/incremental_decoding_utils.py\n./fairseq/iterative_refinement_generator.py\n./fairseq/legacy_distributed_data_parallel.py\n./fairseq/nan_detector.py\n./fairseq/options.py\n./fairseq/pdb.py\n./fairseq/quantization_utils.py\n./fairseq/registry.py\n./fairseq/search.py\n./fairseq/sequence_generator.py\n./fairseq/sequence_scorer.py\n./fairseq/token_generation_constraints.py\n./fairseq/tokenizer.py\n./fairseq/trainer.py\n./fairseq/utils.py\n./fairseq_cli\n./fairseq_cli/__init__.py\n./fairseq_cli/eval_lm.py\n./fairseq_cli/generate.py\n./fairseq_cli/interactive.py\n./fairseq_cli/preprocess.py\n./fairseq_cli/score.py\n./fairseq_cli/train.py\n./fairseq_cli/validate.py\n./scripts\n./scripts/__init__.py\n./scripts/compound_split_bleu.sh\n./scripts/constraints\n./scripts/constraints/extract.py\n./scripts/constraints/validate.py\n./scripts/convert_dictionary.lua\n./scripts/convert_model.lua\n./scripts/sacrebleu.sh\n./scripts/spm_train.py\n./scripts/average_checkpoints.py\n./scripts/build_sym_alignment.py\n./scripts/compare_namespaces.py\n./scripts/count_docs.py\n./scripts/read_binarized.py\n./scripts/rm_pt.py\n./scripts/shard_docs.py\n./scripts/split_train_valid_docs.py\n./scripts/spm_decode.py\n./scripts/spm_encode.py\n./tests\n./tests/__init__.py\n./tests/gpu\n./tests/gpu/__init__.py\n./tests/gpu/transformer_quantization_config.yaml\n./tests/gpu/test_binaries_gpu.py\n./tests/speech_recognition\n./tests/speech_recognition/__init__.py\n./tests/speech_recognition/test_collaters.py\n./tests/speech_recognition/test_vggtransformer.py\n./tests/speech_recognition/asr_test_base.py\n./tests/speech_recognition/test_cross_entropy.py\n./tests/speech_recognition/test_data_utils.py\n./tests/test_inference_dropout.py\n./tests/test_iterators.py\n./tests/test_label_smoothing.py\n./tests/test_lstm_jitable.py\n./tests/test_memory_efficient_fp16.py\n./tests/test_metrics.py\n./tests/test_multi_corpus_sampled_dataset.py\n./tests/test_multihead_attention.py\n./tests/test_noising.py\n./tests/test_reproducibility.py\n./tests/test_resampling_dataset.py\n./tests/test_sequence_generator.py\n./tests/test_sequence_scorer.py\n./tests/test_sparse_multihead_attention.py\n./tests/test_token_block_dataset.py\n./tests/test_train.py\n./tests/test_utils.py\n./tests/utils.py\n./tests/test_average_checkpoints.py\n./tests/test_backtranslation_dataset.py\n./tests/test_binaries.py\n./tests/test_bmuf.py\n./tests/test_character_token_embedder.py\n./tests/test_concat_dataset.py\n./tests/test_constraints.py\n./tests/test_convtbc.py\n./tests/test_dictionary.py\n./tests/test_export.py\n./tests/test_file_io.py\n./tests/test_fp16_optimizer.py\n./.gitignore\n./.gitmodules\n./CONTRIBUTING.md\n./README.md\n./config\n./config/config.yaml\n./config/config_eval_lm.yaml\n./config/criterion\n./config/criterion/adaptive_loss.yaml\n./config/criterion/cross_entropy.yaml\n./config/lr_scheduler\n./config/lr_scheduler/cosine.yaml\n./config/lr_scheduler/inverse_sqrt.yaml\n./config/model\n./config/model/transformer_lm.yaml\n./config/model/transformer_lm_baevski_gbw.yaml\n./config/model/transformer_lm_baevski_wiki103.yaml\n./config/model/transformer_lm_big.yaml\n./config/model/transformer_lm_gbw.yaml\n./config/model/transformer_lm_gpt.yaml\n./config/model/transformer_lm_gpt2_big.yaml\n./config/model/transformer_lm_gpt2_medium.yaml\n./config/model/transformer_lm_gpt2_small.yaml\n./config/model/transformer_lm_wiki103.yaml\n./config/optimizer\n./config/optimizer/adam.yaml\n./config/optimizer/nag.yaml\n./config/params\n./config/params/eval_lm_params.yaml\n./config/params/training_params.yaml\n./config/task\n./config/task/language_modeling.yaml\n./hubconf.py\n./pyproject.toml\n./setup.py\n./train.py\n",
    "readme": "\n--- ./examples/backtranslation/README.md ---\n# Understanding Back-Translation at Scale (Edunov et al., 2018)\n\nThis page includes pre-trained models from the paper [Understanding Back-Translation at Scale (Edunov et al., 2018)](https://arxiv.org/abs/1808.09381).\n\n## Pre-trained models\n\nModel | Description | Dataset | Download\n---|---|---|---\n`transformer.wmt18.en-de` | Transformer <br> ([Edunov et al., 2018](https://arxiv.org/abs/1808.09381)) <br> WMT'18 winner | [WMT'18 English-German](http://www.statmt.org/wmt18/translation-task.html) | [download (.tar.gz)](https://dl.fbaipublicfiles.com/fairseq/models/wmt18.en-de.ensemble.tar.gz) <br> See NOTE in the archive\n\n## Example usage (torch.hub)\n\nWe require a few additional Python dependencies for preprocessing:\n```bash\npip install subword_nmt sacremoses\n```\n\nThen to generate translations from the full model ensemble:\n```python\nimport torch\n\n# List available models\ntorch.hub.list('pytorch/fairseq')  # [..., 'transformer.wmt18.en-de', ... ]\n\n# Load the WMT'18 En-De ensemble\nen2de_ensemble = torch.hub.load(\n    'pytorch/fairseq', 'transformer.wmt18.en-de',\n    checkpoint_file='wmt18.model1.pt:wmt18.model2.pt:wmt18.model3.pt:wmt18.model4.pt:wmt18.model5.pt',\n    tokenizer='moses', bpe='subword_nmt')\n\n# The ensemble contains 5 models\nlen(en2de_ensemble.models)\n# 5\n\n# Translate\nen2de_ensemble.translate('Hello world!')\n# 'Hallo Welt!'\n```\n\n## Training your own model (WMT'18 English-German)\n\nThe following instructions can be adapted to reproduce the models from the paper.\n\n\n#### Step 1. Prepare parallel data and optionally train a baseline (English-German) model\n\nFirst download and preprocess the data:\n```bash\n# Download and prepare the data\ncd examples/backtranslation/\nbash prepare-wmt18en2de.sh\ncd ../..\n\n# Binarize the data\nTEXT=examples/backtranslation/wmt18_en_de\nfairseq-preprocess \\\n    --joined-dictionary \\\n    --source-lang en --target-lang de \\\n    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n    --destdir data-bin/wmt18_en_de --thresholdtgt 0 --thresholdsrc 0 \\\n    --workers 20\n\n# Copy the BPE code into the data-bin directory for future use\ncp examples/backtranslation/wmt18_en_de/code data-bin/wmt18_en_de/code\n```\n\n(Optionally) Train a baseline model (English-German) using just the parallel data:\n```bash\nCHECKPOINT_DIR=checkpoints_en_de_parallel\nfairseq-train --fp16 \\\n    data-bin/wmt18_en_de \\\n    --source-lang en --target-lang de \\\n    --arch transformer_wmt_en_de_big --share-all-embeddings \\\n    --dropout 0.3 --weight-decay 0.0 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --max-tokens 3584 --update-freq 16 \\\n    --max-update 30000 \\\n    --save-dir $CHECKPOINT_DIR\n# Note: the above command assumes 8 GPUs. Adjust `--update-freq` if you have a\n# different number of GPUs.\n```\n\nAverage the last 10 checkpoints:\n```bash\npython scripts/average_checkpoints.py \\\n    --inputs $CHECKPOINT_DIR \\\n    --num-epoch-checkpoints 10 \\\n    --output $CHECKPOINT_DIR/checkpoint.avg10.pt\n```\n\nEvaluate BLEU:\n```bash\n# tokenized BLEU on newstest2017:\nbash examples/backtranslation/tokenized_bleu.sh \\\n    wmt17 \\\n    en-de \\\n    data-bin/wmt18_en_de \\\n    data-bin/wmt18_en_de/code \\\n    $CHECKPOINT_DIR/checkpoint.avg10.pt\n# BLEU4 = 29.57, 60.9/35.4/22.9/15.5 (BP=1.000, ratio=1.014, syslen=63049, reflen=62152)\n# compare to 29.46 in Table 1, which is also for tokenized BLEU\n\n# generally it's better to report (detokenized) sacrebleu though:\nbash examples/backtranslation/sacrebleu.sh \\\n    wmt17 \\\n    en-de \\\n    data-bin/wmt18_en_de \\\n    data-bin/wmt18_en_de/code \\\n    $CHECKPOINT_DIR/checkpoint.avg10.pt\n# BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt17+tok.13a+version.1.4.3 = 29.0 60.6/34.7/22.4/14.9 (BP = 1.000 ratio = 1.013 hyp_len = 62099 ref_len = 61287)\n```\n\n\n#### Step 2. Back-translate monolingual German data\n\nTrain a reverse model (German-English) to do the back-translation:\n```bash\nCHECKPOINT_DIR=checkpoints_de_en_parallel\nfairseq-train --fp16 \\\n    data-bin/wmt18_en_de \\\n    --source-lang de --target-lang en \\\n    --arch transformer_wmt_en_de_big --share-all-embeddings \\\n    --dropout 0.3 --weight-decay 0.0 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --max-tokens 3584 --update-freq 16 \\\n    --max-update 30000 \\\n    --save-dir $CHECKPOINT_DIR\n# Note: the above command assumes 8 GPUs. Adjust `--update-freq` if you have a\n# different number of GPUs.\n```\n\nLet's evaluate the back-translation (BT) model to make sure it is well trained:\n```bash\nbash examples/backtranslation/sacrebleu.sh \\\n    wmt17 \\\n    de-en \\\n    data-bin/wmt18_en_de \\\n    data-bin/wmt18_en_de/code \\\n    $CHECKPOINT_DIR/checkpoint_best.py\n# BLEU+case.mixed+lang.de-en+numrefs.1+smooth.exp+test.wmt17+tok.13a+version.1.4.3 = 34.9 66.9/41.8/28.5/19.9 (BP = 0.983 ratio = 0.984 hyp_len = 63342 ref_len = 64399)\n# compare to the best system from WMT'17 which scored 35.1: http://matrix.statmt.org/matrix/systems_list/1868\n```\n\nNext prepare the monolingual data:\n```bash\n# Download and prepare the monolingual data\n# By default the script samples 25M monolingual sentences, which after\n# deduplication should be just over 24M sentences. These are split into 25\n# shards, each with 1M sentences (except for the last shard).\ncd examples/backtranslation/\nbash prepare-de-monolingual.sh\ncd ../..\n\n# Binarize each shard of the monolingual data\nTEXT=examples/backtranslation/wmt18_de_mono\nfor SHARD in $(seq -f \"%02g\" 0 24); do \\\n    fairseq-preprocess \\\n        --only-source \\\n        --source-lang de --target-lang en \\\n        --joined-dictionary \\\n        --srcdict data-bin/wmt18_en_de/dict.de.txt \\\n        --testpref $TEXT/bpe.monolingual.dedup.${SHARD} \\\n        --destdir data-bin/wmt18_de_mono/shard${SHARD} \\\n        --workers 20; \\\n    cp data-bin/wmt18_en_de/dict.en.txt data-bin/wmt18_de_mono/shard${SHARD}/; \\\ndone\n```\n\nNow we're ready to perform back-translation over the monolingual data. The\nfollowing command generates via sampling, but it's possible to use greedy\ndecoding (`--beam 1`), beam search (`--beam 5`),\ntop-k sampling (`--sampling --beam 1 --sampling-topk 10`), etc.:\n```bash\nmkdir backtranslation_output\nfor SHARD in $(seq -f \"%02g\" 0 24); do \\\n    fairseq-generate --fp16 \\\n        data-bin/wmt18_de_mono/shard${SHARD} \\\n        --path $CHECKPOINT_DIR/checkpoint_best.pt \\\n        --skip-invalid-size-inputs-valid-test \\\n        --max-tokens 4096 \\\n        --sampling --beam 1 \\\n    > backtranslation_output/sampling.shard${SHARD}.out; \\\ndone\n```\n\nAfter BT, use the `extract_bt_data.py` script to re-combine the shards, extract\nthe back-translations and apply length ratio filters:\n```bash\npython examples/backtranslation/extract_bt_data.py \\\n    --minlen 1 --maxlen 250 --ratio 1.5 \\\n    --output backtranslation_output/bt_data --srclang en --tgtlang de \\\n    backtranslation_output/sampling.shard*.out\n\n# Ensure lengths are the same:\n# wc -l backtranslation_output/bt_data.{en,de}\n#   21795614 backtranslation_output/bt_data.en\n#   21795614 backtranslation_output/bt_data.de\n#   43591228 total\n```\n\nBinarize the filtered BT data and combine it with the parallel data:\n```bash\nTEXT=backtranslation_output\nfairseq-preprocess \\\n    --source-lang en --target-lang de \\\n    --joined-dictionary \\\n    --srcdict data-bin/wmt18_en_de/dict.en.txt \\\n    --trainpref $TEXT/bt_data \\\n    --destdir data-bin/wmt18_en_de_bt \\\n    --workers 20\n\n# We want to train on the combined data, so we'll symlink the parallel + BT data\n# in the wmt18_en_de_para_plus_bt directory. We link the parallel data as \"train\"\n# and the BT data as \"train1\", so that fairseq will combine them automatically\n# and so that we can use the `--upsample-primary` option to upsample the\n# parallel data (if desired).\nPARA_DATA=$(readlink -f data-bin/wmt18_en_de)\nBT_DATA=$(readlink -f data-bin/wmt18_en_de_bt)\nCOMB_DATA=data-bin/wmt18_en_de_para_plus_bt\nmkdir -p $COMB_DATA\nfor LANG in en de; do \\\n    ln -s ${PARA_DATA}/dict.$LANG.txt ${COMB_DATA}/dict.$LANG.txt; \\\n    for EXT in bin idx; do \\\n        ln -s ${PARA_DATA}/train.en-de.$LANG.$EXT ${COMB_DATA}/train.en-de.$LANG.$EXT; \\\n        ln -s ${BT_DATA}/train.en-de.$LANG.$EXT ${COMB_DATA}/train1.en-de.$LANG.$EXT; \\\n        ln -s ${PARA_DATA}/valid.en-de.$LANG.$EXT ${COMB_DATA}/valid.en-de.$LANG.$EXT; \\\n        ln -s ${PARA_DATA}/test.en-de.$LANG.$EXT ${COMB_DATA}/test.en-de.$LANG.$EXT; \\\n    done; \\\ndone\n```\n\n\n#### 3. Train an English-German model over the combined parallel + BT data\n\nFinally we can train a model over the parallel + BT data:\n```bash\nCHECKPOINT_DIR=checkpoints_en_de_parallel_plus_bt\nfairseq-train --fp16 \\\n    data-bin/wmt18_en_de_para_plus_bt \\\n    --upsample-primary 16 \\\n    --source-lang en --target-lang de \\\n    --arch transformer_wmt_en_de_big --share-all-embeddings \\\n    --dropout 0.3 --weight-decay 0.0 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n    --lr 0.0007 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --max-tokens 3584 --update-freq 16 \\\n    --max-update 100000 \\\n    --save-dir $CHECKPOINT_DIR\n# Note: the above command assumes 8 GPUs. Adjust `--update-freq` if you have a\n# different number of GPUs.\n```\n\nAverage the last 10 checkpoints:\n```bash\npython scripts/average_checkpoints.py \\\n    --inputs $CHECKPOINT_DIR \\\n    --num-epoch-checkpoints 10 \\\n    --output $CHECKPOINT_DIR/checkpoint.avg10.pt\n```\n\nEvaluate BLEU:\n```bash\n# tokenized BLEU on newstest2017:\nbash examples/backtranslation/tokenized_bleu.sh \\\n    wmt17 \\\n    en-de \\\n    data-bin/wmt18_en_de \\\n    data-bin/wmt18_en_de/code \\\n    $CHECKPOINT_DIR/checkpoint.avg10.pt\n# BLEU4 = 32.35, 64.4/38.9/26.2/18.3 (BP=0.977, ratio=0.977, syslen=60729, reflen=62152)\n# compare to 32.35 in Table 1, which is also for tokenized BLEU\n\n# generally it's better to report (detokenized) sacrebleu:\nbash examples/backtranslation/sacrebleu.sh \\\n    wmt17 \\\n    en-de \\\n    data-bin/wmt18_en_de \\\n    data-bin/wmt18_en_de/code \\\n    $CHECKPOINT_DIR/checkpoint.avg10.pt\n# BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt17+tok.13a+version.1.4.3 = 31.5 64.3/38.2/25.6/17.6 (BP = 0.971 ratio = 0.971 hyp_len = 59515 ref_len = 61287)\n```\n\n\n## Citation\n```bibtex\n@inproceedings{edunov2018backtranslation,\n  title = {Understanding Back-Translation at Scale},\n  author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},\n  booktitle = {Conference of the Association for Computational Linguistics (ACL)},\n  year = 2018,\n}\n```\n\n\n\n--- ./examples/bart/README.md ---\n# BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n\n[https://arxiv.org/pdf/1910.13461.pdf]\n\n## Introduction\n\nBART is sequence-to-sequence model trained with denoising as pretraining objective. We show that this pretraining objective is more generic and show that we can match [RoBERTa](../roberta) results on SQuAD and GLUE and gain state-of-the-art results on summarization (XSum, CNN dataset), long form generative question answering (ELI5) and dialog response genration (ConvAI2). See the associated paper for more details.\n\n## Pre-trained models\n\nModel | Description | # params | Download\n---|---|---|---\n`bart.base` | BART model with 6 encoder and decoder layers | 140M | [bart.base.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/bart.base.tar.gz)\n`bart.large` | BART model with 12 encoder and decoder layers | 400M | [bart.large.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz)\n`bart.large.mnli` | `bart.large` finetuned on `MNLI` | 400M | [bart.large.mnli.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/bart.large.mnli.tar.gz)\n`bart.large.cnn` | `bart.large` finetuned on `CNN-DM` | 400M | [bart.large.cnn.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/bart.large.cnn.tar.gz)\n`bart.large.xsum` | `bart.large` finetuned on `Xsum` | 400M | [bart.large.xsum.tar.gz](https://dl.fbaipublicfiles.com/fairseq/models/bart.large.xsum.tar.gz)\n\n## Results\n\n**[GLUE (Wang et al., 2019)](https://gluebenchmark.com/)**\n_(dev set, single model, single-task finetuning)_\n\nModel | MNLI | QNLI | QQP | RTE | SST-2 | MRPC | CoLA | STS-B\n---|---|---|---|---|---|---|---|---\n`roberta.large` | 90.2 | 94.7 | 92.2 | 86.6 | 96.4 | 90.9 | 68.0 | 92.4\n`bart.large` | 89.9 | 94.9 | 92.5 | 87.0 | 96.6 | 90.4 | 62.8 | 91.2\n\n**[SQuAD (Rajpurkar et al., 2018)](https://rajpurkar.github.io/SQuAD-explorer/)**\n_(dev set, no additional data used)_\n\nModel | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1\n---|---|---\n`roberta.large` | 88.9/94.6 | 86.5/89.4\n`bart.large` | 88.8/94.6 | 86.1/89.2\n\n**[CNN/Daily Mail](http://nlpprogress.com/english/summarization.html)**\n_(test set, no additional data used)_\n\nModel | R1 | R2 | RL\n---|---|---|---\n`BERTSUMEXTABS` | 42.13 | 19.60 | 39.18\n`bart.large` | 44.16 | 21.28 | 40.90\n\n## Example usage\n\n##### Load BART from torch.hub (PyTorch >= 1.1):\n```python\nimport torch\nbart = torch.hub.load('pytorch/fairseq', 'bart.large')\nbart.eval()  # disable dropout (or leave in train mode to finetune)\n```\n\n##### Load BART (for PyTorch 1.0 or custom models):\n```python\n# Download bart.large model\nwget https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz\ntar -xzvf bart.large.tar.gz\n\n# Load the model in fairseq\nfrom fairseq.models.bart import BARTModel\nbart = BARTModel.from_pretrained('/path/to/bart.large', checkpoint_file='model.pt')\nbart.eval()  # disable dropout (or leave in train mode to finetune)\n```\n\n##### Apply Byte-Pair Encoding (BPE) to input text:\n```python\ntokens = bart.encode('Hello world!')\nassert tokens.tolist() == [0, 31414, 232, 328, 2]\nbart.decode(tokens)  # 'Hello world!'\n```\n\n##### Extract features from BART:\n```python\n# Extract the last layer's features\nlast_layer_features = bart.extract_features(tokens)\nassert last_layer_features.size() == torch.Size([1, 5, 1024])\n\n# Extract all layer's features from decoder (layer 0 is the embedding layer)\nall_layers = bart.extract_features(tokens, return_all_hiddens=True)\nassert len(all_layers) == 13\nassert torch.all(all_layers[-1] == last_layer_features)\n```\n\n##### Use BART for sentence-pair classification tasks:\n```python\n# Download BART already finetuned for MNLI\nbart = torch.hub.load('pytorch/fairseq', 'bart.large.mnli')\nbart.eval()  # disable dropout for evaluation\n\n# Encode a pair of sentences and make a prediction\ntokens = bart.encode('BART is a seq2seq model.', 'BART is not sequence to sequence.')\nbart.predict('mnli', tokens).argmax()  # 0: contradiction\n\n# Encode another pair of sentences\ntokens = bart.encode('BART is denoising autoencoder.', 'BART is version of autoencoder.')\nbart.predict('mnli', tokens).argmax()  # 2: entailment\n```\n\n##### Register a new (randomly initialized) classification head:\n```python\nbart.register_classification_head('new_task', num_classes=3)\nlogprobs = bart.predict('new_task', tokens)  \n```\n\n##### Batched prediction:\n```python\nimport torch\nfrom fairseq.data.data_utils import collate_tokens\n\nbart = torch.hub.load('pytorch/fairseq', 'bart.large.mnli')\nbart.eval()\n\nbatch_of_pairs = [\n    ['BART is a seq2seq model.', 'BART is not sequence to sequence.'],\n    ['BART is denoising autoencoder.', 'BART is version of autoencoder.'],\n]\n\nbatch = collate_tokens(\n    [bart.encode(pair[0], pair[1]) for pair in batch_of_pairs], pad_idx=1\n)\n\nlogprobs = bart.predict('mnli', batch)\nprint(logprobs.argmax(dim=1))\n# tensor([0, 2])\n```\n\n##### Using the GPU:\n```python\nbart.cuda()\nbart.predict('new_task', tokens)\n```\n\n#### Evaluating the `bart.large.mnli` model:\n\nExample python code snippet to evaluate accuracy on the MNLI `dev_matched` set.\n```python\nlabel_map = {0: 'contradiction', 1: 'neutral', 2: 'entailment'}\nncorrect, nsamples = 0, 0\nbart.cuda()\nbart.eval()\nwith open('glue_data/MNLI/dev_matched.tsv') as fin:\n    fin.readline()\n    for index, line in enumerate(fin):\n        tokens = line.strip().split('\\t')\n        sent1, sent2, target = tokens[8], tokens[9], tokens[-1]\n        tokens = bart.encode(sent1, sent2)\n        prediction = bart.predict('mnli', tokens).argmax().item()\n        prediction_label = label_map[prediction]\n        ncorrect += int(prediction_label == target)\n        nsamples += 1\n        print('| Accuracy: ', float(ncorrect)/float(nsamples))\n# Expected output: 0.9010\n```\n\n#### Evaluating the `bart.large.cnn` model:\nFollow instructions [here](https://github.com/abisee/cnn-dailymail) to download and process into data-files such that `test.source` and `test.target` has one line for each non-tokenized sample.\n\n```python\nbart = torch.hub.load('pytorch/fairseq', 'bart.large.cnn')\nbart.cuda()\nbart.eval()\nbart.half()\ncount = 1\nbsz = 32\nwith open('test.source') as source, open('test.hypo', 'w') as fout:\n    sline = source.readline().strip()\n    slines = [sline]\n    for sline in source:\n        if count % bsz == 0:\n            with torch.no_grad():\n                hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\n\n            for hypothesis in hypotheses_batch:\n                fout.write(hypothesis + '\\n')\n                fout.flush()\n            slines = []\n\n        slines.append(sline.strip())\n        count += 1\n    if slines != []:\n        hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\n        for hypothesis in hypotheses_batch:\n            fout.write(hypothesis + '\\n')\n            fout.flush()\n```\n\nInstall `files2rouge` from [here](https://github.com/pltrdy/files2rouge).\n\n```bash\nexport CLASSPATH=/path/to/stanford-corenlp-full-2016-10-31/stanford-corenlp-3.7.0.jar\n\n# Tokenize hypothesis and target files.\ncat test.hypo | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > test.hypo.tokenized\ncat test.target | java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines > test.hypo.target\nfiles2rouge test.hypo.tokenized test.hypo.target\n# Expected output: (ROUGE-2 Average_F: 0.21238)\n```\n\n\n## Finetuning\n\n- [Finetuning on GLUE](README.glue.md)\n- [Finetuning on CNN-DM](README.summarization.md)\n\n## Citation\n\n```bibtex\n@article{lewis2019bart,\n    title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural\nLanguage Generation, Translation, and Comprehension},\n    author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and\n              Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov\n              and Luke Zettlemoyer },\n    journal={arXiv preprint arXiv:1910.13461},\n    year = {2019},\n}\n```\n\n\n\n--- ./examples/byte_level_bpe/README.md ---\n# Neural Machine Translation with Byte-Level Subwords\n\nhttps://arxiv.org/abs/1909.03341\n\nWe provide an implementation of byte-level byte-pair encoding (BBPE), taking IWSLT 2017 Fr-En translation as\nexample.\n\n## Data\nGet data and generate fairseq binary dataset:\n```bash\nbash ./get_data.sh\n```\n\n## Model Training\nTrain Transformer model with Bi-GRU embedding contextualization (implemented in `gru_transformer.py`):\n```bash\n# VOCAB=bytes\n# VOCAB=chars\nVOCAB=bbpe2048\n# VOCAB=bpe2048\n# VOCAB=bbpe4096\n# VOCAB=bpe4096\n# VOCAB=bpe16384\n```\n```bash\nfairseq-train \"data/bin_${VOCAB}\" --task translation --user-dir examples/byte_level_bpe/gru_transformer \\\n    --arch gru_transformer --encoder-layers 2 --decoder-layers 2 --dropout 0.3 --share-all-embeddings \\\n    --optimizer adam --adam-betas '(0.9, 0.98)' \\\n    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n    --log-format 'simple' --log-interval 100 --save-dir \"checkpoints/${VOCAB}\" \\\n    --batch-size 100 --max-update 100000 --update-freq 2\n```\n\n## Generation\n`fairseq-generate` requires bytes (BBPE) decoder to convert byte-level representation back to characters:\n```bash\n# BPE=--bpe bytes\n# BPE=--bpe characters\nBPE=--bpe byte_bpe --sentencepiece-model-path data/spm_bbpe2048.model\n# BPE=--bpe sentencepiece --sentencepiece-model data/spm_bpe2048.model\n# BPE=--bpe byte_bpe --sentencepiece-model-path data/spm_bbpe4096.model\n# BPE=--bpe sentencepiece --sentencepiece-model data/spm_bpe4096.model\n# BPE=--bpe sentencepiece --sentencepiece-model data/spm_bpe16384.model\n```\n\n```bash\nfairseq-generate \"data/bin_${VOCAB}\" --task translation --user-dir examples/byte_level_bpe/gru_transformer \\\n    --source-lang fr --gen-subset test --sacrebleu --path \"checkpoints/${VOCAB}/checkpoint_last.pt\" \\\n    --tokenizer moses --moses-target-lang en ${BPE}\n```\nWhen using `fairseq-interactive`, bytes (BBPE) encoder/decoder is required to tokenize input data and detokenize model predictions:\n```bash\nfairseq-interactive \"data/bin_${VOCAB}\" --task translation --user-dir examples/byte_level_bpe/gru_transformer \\\n    --path \"checkpoints/${VOCAB}/checkpoint_last.pt\" --input data/test.fr --tokenizer moses --moses-source-lang fr \\\n    --moses-target-lang en ${BPE} --buffer-size 1000 --max-tokens 10000\n```\n\n## Results\n| Vocabulary    | Model  | BLEU |\n|:-------------:|:-------------:|:-------------:|\n| Joint BPE 16k ([Kudo, 2018](https://arxiv.org/abs/1804.10959)) | 512d LSTM 2+2 | 33.81 |\n| Joint BPE 16k | Transformer base 2+2 (w/ GRU) | 36.64 (36.72) |\n| Joint BPE 4k | Transformer base 2+2 (w/ GRU) | 35.49 (36.10) |\n| Joint BBPE 4k | Transformer base 2+2 (w/ GRU) | 35.61 (35.82) |\n| Joint BPE 2k | Transformer base 2+2 (w/ GRU) | 34.87 (36.13) |\n| Joint BBPE 2k | Transformer base 2+2 (w/ GRU) | 34.98 (35.43) |\n| Characters | Transformer base 2+2 (w/ GRU) | 31.78 (33.30) |\n| Bytes | Transformer base 2+2 (w/ GRU) | 31.57 (33.62) |\n\n\n## Citation\n```\n@misc{wang2019neural,\n    title={Neural Machine Translation with Byte-Level Subwords},\n    author={Changhan Wang and Kyunghyun Cho and Jiatao Gu},\n    year={2019},\n    eprint={1909.03341},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n\n## Contact\nChanghan Wang ([changhan@fb.com](mailto:changhan@fb.com)),\nKyunghyun Cho ([kyunghyuncho@fb.com](mailto:kyunghyuncho@fb.com)),\nJiatao Gu ([jgu@fb.com](mailto:jgu@fb.com))\n\n\n",
    "readme_filenames": [
      "./examples/backtranslation/README.md",
      "./examples/bart/README.md",
      "./examples/byte_level_bpe/README.md"
    ],
    "github_workflows": {
      ".github/workflows/build.yml": "name: build\n\non:\n  # Trigger the workflow on push to master or any pull request\n  push:\n    branches:\n      - master\n  pull_request:\n\njobs:\n  build:\n\n    strategy:\n      max-parallel: 4\n      matrix:\n        platform: [ubuntu-latest, macos-latest]\n        python-version: [3.6, 3.7]\n\n    runs-on: ${{ matrix.platform }}\n\n    steps:\n    - uses: actions/checkout@v1\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v1\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Conditionally install pytorch\n      if: matrix.platform == 'windows-latest'\n      run: pip3 install torch -f https://download.pytorch.org/whl/torch_stable.html\n    - name: Install locally\n      run: |\n        python -m pip install --upgrade pip\n        python setup.py build_ext --inplace\n        python -m pip install --editable .\n    - name: Lint with flake8\n      run: |\n        pip install flake8\n        # stop the build if there are Python syntax errors or undefined names\n        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n    - name: Run tests\n      run: |\n          python setup.py test\n"
    }
  },
  "llm_calls_before_build": 0,
  "github_workflows_found": [
    ".github/workflows/build.yml"
  ],
  "dockerfile_attempt_1": 1,
  "dockerfile": "FROM pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n\n# Install system dependencies required for building and data processing\nRUN apt-get update && apt-get install -y \\\n    git \\\n    wget \\\n    perl \\\n    cmake \\\n    pkg-config \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies from example requirements and error context\nRUN pip install --no-cache-dir \\\n    numpy \\\n    cython \\\n    subword-nmt \\\n    sacremoses \\\n    sentencepiece \\\n    sacrebleu \\\n    regex \\\n    pandas\n\n# Clone fairseq repository and checkout specific commit with pipeline parallel changes\nRUN git clone https://github.com/facebookresearch/fairseq.git /workspace/fairseq\nWORKDIR /workspace/fairseq\nRUN git checkout 85f097141d83d6aac378838b6c0c8f2a0f77154f\n\n# Install fairseq in editable mode with development dependencies\nRUN pip install --editable .[dev]\n\n# Set default working directory and maintain compatibility with example scripts\nWORKDIR /workspace\nENV PYTHONPATH=/workspace/fairseq:$PYTHONPATH",
  "dockerfile_build_success": true,
  "successful_candidate_index": 3,
  "successful_candidate_attempt": 1,
  "successful_candidate_improvement_attempt": 0,
  "llm_calls_total": 5
}