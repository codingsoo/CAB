{
  "number": 18170,
  "title": "Basic ProgressBar does not work",
  "created_at": "2023-07-26T21:40:27Z",
  "closed_at": "2023-07-27T11:20:36Z",
  "labels": [
    "question",
    "progress bar: tqdm",
    "ver: 2.0.x"
  ],
  "url": "https://github.com/Lightning-AI/pytorch-lightning/issues/18170",
  "body": "### Bug description\r\n\r\nVersion of `pytorch_lightning==2.0.6`, `tqdm==4.65.0`\r\nI want to display the training progress of my models and the basic ProgressBar from pytorch_lightning.callbacks does not work (nothing shows up).\r\nHowever, when I switched to RichProgressBar, the rich progress bar shows up.\r\n\r\n### What version are you seeing the problem on?\r\n\r\nv2.0\r\n\r\n### How to reproduce the bug\r\n\r\n`python\r\npytorch_lightning.callbacks import ProgressBar` does not show up anything.\r\n\r\n`from pytorch_lightning.callbacks import RichProgressBar` can show the training progress.\r\n\r\n\r\n### Error messages and logs\r\n\r\n\r\nNothing shows up for ```ProgressBar```.\r\n\r\n### Environment\r\n`pytorch_lightning==2.0.6`\r\n`tqdm==4.65.0`\r\n\r\n### More info\r\nRunning things in a Linux environment, with an A40 GPU.\n\ncc @awaelchli",
  "comments_url": "https://api.github.com/repos/Lightning-AI/pytorch-lightning/issues/18170/comments",
  "author": "dnaihao",
  "comments": [
    {
      "user": "awaelchli",
      "created_at": "2023-07-26T22:22:25Z",
      "body": "Hi @dnaihao \r\n\r\nYou don't need to import the progress bar if you want to use tqdm. It gets enabled by default if you just run the Trainer :)\r\nThe reason why you don't see anything is because you accidentally imported the base class, but you probably wanted to import `pytorch_lightning.callbacks import TQDMProgressBar`. (But again, it is the default, so it's not necessary technically). \r\n\r\nLet me know if that resolves your problem :)"
    },
    {
      "user": "dnaihao",
      "created_at": "2023-07-27T01:30:52Z",
      "body": "Thanks for the explanation. \r\nBut for the previous versions of Pytorch Lightening (probably around 1.6.X), I imported ProgressBar and it worked out perfectly fine.\r\nBut sure, I am using the TODMProgressBar now and it works fine."
    },
    {
      "user": "awaelchli",
      "created_at": "2023-07-27T11:20:36Z",
      "body": "Yes, you are right, in previous versions prior to 2.0, ProgressBar was the tqdm-version. Later, the name of the base class was changed from ProgressBarBase to just ProgressBar #17058. Apologies if this caused confusion!"
    }
  ],
  "satisfaction_conditions": [
    "Explanation of version-specific class name changes in PyTorch Lightning progress bars",
    "Identification of the correct default progress bar class in v2.0+",
    "Clarification that explicit progress bar imports are unnecessary for default behavior"
  ],
  "_classification": {
    "category": "Can be dockerized without any issue",
    "timestamp": "2025-04-04 23:58:52"
  },
  "git_commit_info": {
    "sha": "aca8c7e6f3a88f2083375133e33d5f9c5b9bb1c9",
    "date": "2020-03-31T16:41:24Z",
    "message": "Optimizer Frequencies logic, and new configure_optimizers (#1269)\n\n* init_optimizers accepts Dict, Sequence[Dict]\nand returns optimizer_frequencies.\noptimizer_frequencies was added as a member of Trainer.\n\n* Optimizer frequencies logic implemented in training_loop.\nDescription added to configure_optimizers in LightningModule\n\n* optimizer frequencies tests added to test_gpu\n\n* Fixed formatting for merging PR #1269\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\nCo-Authored-By: Asaf Manor <32155911+asafmanor@users.noreply.github.com>\n\n* Update trainer.py\n\n* Moving get_optimizers_iterable() outside.\n\n* Update note\n\n* Apply suggestions from code review\n\n* formatting\n\n* formatting\n\n* Update CHANGELOG.md\n\n* formatting\n\n* Update CHANGELOG.md\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>",
    "author": "Asaf Manor"
  },
  "repository_info": {
    "structure_summary": ".\n./.git\n./.git/branches\n./.git/description\n./.git/hooks\n./.git/hooks/applypatch-msg.sample\n./.git/hooks/commit-msg.sample\n./.git/hooks/post-update.sample\n./.git/hooks/pre-applypatch.sample\n./.git/hooks/pre-commit.sample\n./.git/hooks/pre-merge-commit.sample\n./.git/hooks/pre-push.sample\n./.git/hooks/pre-receive.sample\n./.git/hooks/push-to-checkout.sample\n./.git/hooks/update.sample\n./.git/hooks/fsmonitor-watchman.sample\n./.git/hooks/pre-rebase.sample\n./.git/hooks/prepare-commit-msg.sample\n./.git/hooks/sendemail-validate.sample\n./.git/info\n./.git/info/exclude\n./.git/config\n./.git/objects\n./.git/objects/pack\n./.git/objects/pack/pack-ff3db302a1e547e7ffcedd7feda81697ff365af1.pack\n./.git/objects/pack/pack-ff3db302a1e547e7ffcedd7feda81697ff365af1.rev\n./.git/objects/pack/pack-ff3db302a1e547e7ffcedd7feda81697ff365af1.idx\n./.git/objects/info\n./.git/HEAD\n./.git/refs\n./.git/refs/heads\n./.git/refs/heads/master\n./.git/refs/tags\n./.git/refs/remotes\n./.git/refs/remotes/origin\n./.git/refs/remotes/origin/HEAD\n./.git/packed-refs\n./.git/logs\n./.git/logs/refs\n./.git/logs/refs/remotes\n./.git/logs/refs/remotes/origin\n./.git/logs/refs/remotes/origin/HEAD\n./.git/logs/refs/heads\n./.git/logs/refs/heads/master\n./.git/logs/HEAD\n./.git/index\n./.github\n./.github/BECOMING_A_CORE_CONTRIBUTOR.md\n./.github/CODE_OF_CONDUCT.md\n./.github/CONTRIBUTING.md\n./.github/ISSUE_TEMPLATE\n./.github/ISSUE_TEMPLATE/bug_report.md\n./.github/ISSUE_TEMPLATE/documentation.md\n./.github/ISSUE_TEMPLATE/feature_request.md\n./.github/ISSUE_TEMPLATE/how-to-question.md\n./.github/PULL_REQUEST_TEMPLATE.md\n./.github/stale.yml\n./.github/workflows\n./.github/workflows/ci-testing.yml\n./.github/workflows/docs-check.yml\n./.github/workflows/greetings.yml\n./.github/workflows/rebase.yml\n./tests\n./tests/Dockerfile\n./tests/README.md\n./tests/__init__.py\n./tests/base\n./tests/base/__init__.py\n./tests/base/datasets.py\n./tests/base/debug.py\n./tests/base/mixins.py\n./tests/base/models.py\n./tests/base/utils.py\n./tests/collect_env_details.py\n./tests/conftest.py\n./tests/datasets\n./tests/datasets/__init__.py\n./tests/install_AMP.sh\n./tests/loggers\n./tests/loggers/__init__.py\n./tests/loggers/test_base.py\n./tests/loggers/test_comet.py\n./tests/loggers/test_mlflow.py\n./tests/loggers/test_neptune.py\n./tests/loggers/test_tensorboard.py\n./tests/loggers/test_test_tube.py\n./tests/loggers/test_trains.py\n./tests/loggers/test_wandb.py\n./tests/models\n./tests/models/__init__.py\n./tests/models/test_amp.py\n./tests/models/test_cpu.py\n./tests/models/test_gpu.py\n./tests/models/test_restore.py\n./tests/requirements.txt\n./tests/test_deprecated.py\n./tests/test_profiler.py\n./tests/trainer\n./tests/trainer/__init__.py\n./tests/trainer/test_callbacks.py\n./tests/trainer/test_dataloaders.py\n./tests/trainer/test_optimizers.py\n./tests/trainer/test_trainer.py\n./tests/trainer/test_trainer_cli.py\n./.circleci\n./.circleci/config.yml\n./.codecov.yml\n./.drone.yml\n./.gitignore\n./.mergify.yml\n./.pep8speaks.yml\n./.readthedocs.yml\n./.run_local_tests.sh\n./CHANGELOG.md\n./LICENSE\n./MANIFEST.in\n./README.md\n./benchmarks\n./benchmarks/__init__.py\n./benchmarks/test_trainer_parity.py\n./docs\n./docs/Makefile\n./docs/make.bat\n./docs/requirements.txt\n./docs/source\n./docs/source/_images\n./docs/source/_images/general\n./docs/source/_images/general/pl_overview.gif\n./docs/source/_images/general/pl_overview_flat.jpg\n./docs/source/_images/general/tf_loss.png\n./docs/source/_images/general/tf_tags.png\n./docs/source/_images/lightning_module\n./docs/source/_images/lightning_module/pt_to_pl.png\n./docs/source/_images/lightning_module/pt_trainer.png\n./docs/source/_images/logos\n./docs/source/_images/logos/lightning_icon.svg\n./docs/source/_images/logos/lightning_logo-large.svg\n./docs/source/_images/logos/lightning_logo-name.svg\n./docs/source/_images/logos/lightning_logo.png\n./docs/source/_images/logos/lightning_logo.svg\n./docs/source/_images/mnist_imgs\n./docs/source/_images/mnist_imgs/mnist_cpu_bar.png\n./docs/source/_images/mnist_imgs/mnist_gpu.png\n./docs/source/_images/mnist_imgs/mnist_tb.png\n./docs/source/_images/mnist_imgs/pt_to_pl.jpg\n./docs/source/_images/mnist_imgs/restart_runtime.png\n./docs/source/_images/mnist_imgs/runtime_tpu.png\n./docs/source/_images/mnist_imgs/tpu_fast.png\n./docs/source/_images/mnist_imgs/tpu_start.png\n./docs/source/_templates\n./docs/source/_templates/theme_variables.jinja\n./docs/source/apex.rst\n./docs/source/callbacks.rst\n./docs/source/child_modules.rst\n./docs/source/conf.py\n./docs/source/debugging.rst\n./docs/source/early_stopping.rst\n./docs/source/experiment_logging.rst\n./docs/source/experiment_reporting.rst\n./docs/source/fast_training.rst\n./docs/source/governance.rst\n./docs/source/hooks.rst\n./docs/source/hyperparameters.rst\n./docs/source/index.rst\n./docs/source/introduction_guide.rst\n./docs/source/lightning-module.rst\n./docs/source/loggers.rst\n./docs/source/modules.rst\n./docs/source/multi_gpu.rst\n./docs/source/new-project.rst\n./docs/source/optimizers.rst\n./docs/source/profiler.rst\n./docs/source/sequences.rst\n./docs/source/single_gpu.rst\n./docs/source/slurm.rst\n./docs/source/test_set.rst\n./docs/source/tpu.rst\n./docs/source/trainer.rst\n./docs/source/training_tricks.rst\n./docs/source/transfer_learning.rst\n./docs/source/weights_loading.rst\n./environment.yml\n./pl_examples\n./pl_examples/README.md\n./pl_examples/__init__.py\n./pl_examples/basic_examples\n./pl_examples/basic_examples/README.md\n./pl_examples/basic_examples/__init__.py\n./pl_examples/basic_examples/cpu_template.py\n./pl_examples/basic_examples/gpu_template.py\n./pl_examples/basic_examples/lightning_module_template.py\n./pl_examples/domain_templates\n./pl_examples/domain_templates/__init__.py\n./pl_examples/domain_templates/gan.py\n./pl_examples/domain_templates/reinforse_learn_Qnet.py\n./pl_examples/full_examples\n./pl_examples/full_examples/__init__.py\n./pl_examples/full_examples/imagenet\n./pl_examples/full_examples/imagenet/__init__.py\n./pl_examples/full_examples/imagenet/imagenet_example.py\n./pl_examples/full_examples/semantic_segmentation\n./pl_examples/full_examples/semantic_segmentation/models\n./pl_examples/full_examples/semantic_segmentation/models/unet\n./pl_examples/full_examples/semantic_segmentation/models/unet/model.py\n./pl_examples/full_examples/semantic_segmentation/models/unet/parts.py\n./pl_examples/full_examples/semantic_segmentation/semseg.py\n./pl_examples/multi_node_examples\n./pl_examples/multi_node_examples/README.md\n./pl_examples/multi_node_examples/__init__.py\n./pl_examples/multi_node_examples/ddp2_job_submit.sh\n./pl_examples/multi_node_examples/ddp_job_submit.sh\n./pl_examples/multi_node_examples/multi_node_ddp2_demo.py\n./pl_examples/multi_node_examples/multi_node_ddp_demo.py\n./pl_examples/requirements.txt\n./pyproject.toml\n./pytorch_lightning\n./pytorch_lightning/__init__.py\n./pytorch_lightning/callbacks\n./pytorch_lightning/callbacks/__init__.py\n./pytorch_lightning/callbacks/base.py\n./pytorch_lightning/callbacks/early_stopping.py\n./pytorch_lightning/callbacks/gradient_accumulation_scheduler.py\n./pytorch_lightning/callbacks/model_checkpoint.py\n./pytorch_lightning/core\n./pytorch_lightning/core/__init__.py\n./pytorch_lightning/core/decorators.py\n./pytorch_lightning/core/grads.py\n./pytorch_lightning/core/hooks.py\n./pytorch_lightning/core/lightning.py\n./pytorch_lightning/core/memory.py\n./pytorch_lightning/core/model_saving.py\n./pytorch_lightning/core/root_module.py\n./pytorch_lightning/core/saving.py\n./pytorch_lightning/loggers\n./pytorch_lightning/loggers/__init__.py\n./pytorch_lightning/loggers/base.py\n./pytorch_lightning/loggers/comet.py\n./pytorch_lightning/loggers/mlflow.py\n./pytorch_lightning/loggers/neptune.py\n./pytorch_lightning/loggers/tensorboard.py\n./pytorch_lightning/loggers/test_tube.py\n./pytorch_lightning/loggers/trains.py\n./pytorch_lightning/loggers/wandb.py\n./pytorch_lightning/logging\n./pytorch_lightning/logging/__init__.py\n./pytorch_lightning/logging/comet.py\n./pytorch_lightning/logging/comet_logger.py\n./pytorch_lightning/logging/mlflow.py\n./pytorch_lightning/logging/mlflow_logger.py\n./pytorch_lightning/logging/neptune.py\n./pytorch_lightning/logging/test_tube.py\n./pytorch_lightning/logging/test_tube_logger.py\n./pytorch_lightning/logging/wandb.py\n./pytorch_lightning/overrides\n./pytorch_lightning/overrides/__init__.py\n./pytorch_lightning/overrides/data_parallel.py\n./pytorch_lightning/overrides/override_data_parallel.py\n./pytorch_lightning/profiler\n./pytorch_lightning/profiler/__init__.py\n./pytorch_lightning/profiler/profilers.py\n./pytorch_lightning/pt_overrides\n./pytorch_lightning/pt_overrides/__init__.py\n./pytorch_lightning/pt_overrides/override_data_parallel.py\n./pytorch_lightning/root_module\n./pytorch_lightning/root_module/__init__.py\n./pytorch_lightning/root_module/decorators.py\n./pytorch_lightning/root_module/grads.py\n./pytorch_lightning/root_module/hooks.py\n./pytorch_lightning/root_module/memory.py\n./pytorch_lightning/root_module/model_saving.py\n./pytorch_lightning/root_module/root_module.py\n./pytorch_lightning/trainer\n./pytorch_lightning/trainer/__init__.py\n./pytorch_lightning/trainer/auto_mix_precision.py\n./pytorch_lightning/trainer/callback_config.py\n./pytorch_lightning/trainer/callback_hook.py\n./pytorch_lightning/trainer/data_loading.py\n./pytorch_lightning/trainer/deprecated_api.py\n./pytorch_lightning/trainer/distrib_data_parallel.py\n./pytorch_lightning/trainer/distrib_parts.py\n./pytorch_lightning/trainer/evaluation_loop.py\n./pytorch_lightning/trainer/ignored_warnings.py\n./pytorch_lightning/trainer/logging.py\n./pytorch_lightning/trainer/model_hooks.py\n./pytorch_lightning/trainer/supporters.py\n./pytorch_lightning/trainer/trainer.py\n./pytorch_lightning/trainer/training_io.py\n./pytorch_lightning/trainer/training_loop.py\n./pytorch_lightning/trainer/training_tricks.py\n./pytorch_lightning/utilities\n./pytorch_lightning/utilities/__init__.py\n./pytorch_lightning/utilities/exceptions.py\n./requirements-extra.txt\n./requirements.txt\n./setup.cfg\n./setup.py\n./update.sh\n",
    "readme": "\n--- ./tests/README.md ---\n# PyTorch-Lightning Tests\nMost PL tests train a full MNIST model under various trainer conditions (ddp, ddp2+amp, etc...).\nThis provides testing for most combinations of important settings.\nThe tests expect the model to perform to a reasonable degree of testing accuracy to pass.\n\n## Running tests\nThe automatic travis tests ONLY run CPU-based tests. Although these cover most of the use cases,\nrun on a 2-GPU machine to validate the full test-suite.\n\n\nTo run all tests do the following:\n```bash\ngit clone https://github.com/PyTorchLightning/pytorch-lightning\ncd pytorch-lightning\n\n# install AMP support\nbash tests/install_AMP.sh\n\n# install dev deps\npip install -r tests/requirements.txt\n\n# run tests\npy.test -v\n```\n\nTo test models that require GPU make sure to run the above command on a GPU machine.\nThe GPU machine must have:\n1. At least 2 GPUs.\n2. [NVIDIA-apex](https://github.com/NVIDIA/apex#linux) installed.\n\n\n## Running Coverage   \nMake sure to run coverage on a GPU machine with at least 2 GPUs and NVIDIA apex installed. \n\n```bash\ncd pytorch-lightning\n\n# generate coverage (coverage is also installed as part of dev dependencies under tests/requirements.txt)\ncoverage run --source pytorch_lightning -m py.test pytorch_lightning tests examples -v --doctest-modules\n\n# print coverage stats\ncoverage report -m\n\n# exporting results\ncoverage xml\n```\n\n\n\n\n\n--- ./README.md ---\n<div align=\"center\">\n\n![Logo](docs/source/_images/logos/lightning_logo.svg)\n\n# PyTorch Lightning\n\n**The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.**\n\n\n[![PyPI Status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)\n[![PyPI Status](https://pepy.tech/badge/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)\n[![codecov](https://codecov.io/gh/PyTorchLightning/pytorch-lightning/branch/master/graph/badge.svg)](https://codecov.io/gh/PyTorchLightning/pytorch-lightning)\n[![CodeFactor](https://www.codefactor.io/repository/github/pytorchlightning/pytorch-lightning/badge)](https://www.codefactor.io/repository/github/pytorchlightning/pytorch-lightning)\n\n[![ReadTheDocs](https://readthedocs.org/projects/pytorch-lightning/badge/?version=0.7.1)](https://pytorch-lightning.readthedocs.io/en/0.7.1/)\n[![Slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://join.slack.com/t/pytorch-lightning/shared_invite/enQtODU5ODIyNTUzODQwLTFkMDg5Mzc1MDBmNjEzMDgxOTVmYTdhYjA1MDdmODUyOTg2OGQ1ZWZkYTQzODhhNzdhZDA3YmNhMDhlMDY4YzQ)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/PytorchLightning/pytorch-lightning/blob/master/LICENSE)\n[![Next Release](https://img.shields.io/badge/Next%20Release-May%2006-<COLOR>.svg)](https://shields.io/)\n\n<!--\nremoved until codecov badge isn't empy. likely a config error showing nothing on master.\n[![codecov](https://codecov.io/gh/Borda/pytorch-lightning/branch/master/graph/badge.svg)](https://codecov.io/gh/Borda/pytorch-lightning)\n-->\n</div>\n\n---\n## Continuous Integration\n<center>\n\n| System / PyTorch ver. | 1.1 | 1.2 | 1.3 | 1.4 |\n| :---: | :---: | :---: | :---: | :---: |\n| Linux py3.6 [CPU] | [![CircleCI](https://circleci.com/gh/PyTorchLightning/pytorch-lightning.svg?style=svg)](https://circleci.com/gh/PyTorchLightning/pytorch-lightning) | [![CircleCI](https://circleci.com/gh/PyTorchLightning/pytorch-lightning.svg?style=svg)](https://circleci.com/gh/PyTorchLightning/pytorch-lightning) | [![CircleCI](https://circleci.com/gh/PyTorchLightning/pytorch-lightning.svg?style=svg)](https://circleci.com/gh/PyTorchLightning/pytorch-lightning) | [![CircleCI](https://circleci.com/gh/PyTorchLightning/pytorch-lightning.svg?style=svg)](https://circleci.com/gh/PyTorchLightning/pytorch-lightning) |\n| Linux py3.7 [GPU] | <center>\u2014</center> | <center>\u2014</center> | <center>\u2014</center> | [![Build Status](http://35.192.60.23/api/badges/PyTorchLightning/pytorch-lightning/status.svg)](http://35.192.60.23/PyTorchLightning/pytorch-lightning) |\n| Linux py3.6 / py3.7 | [![CI testing](https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20testing/badge.svg?event=push)](https://github.com/PyTorchLightning/pytorch-lightning/actions?query=workflow%3A%22CI+testing%22) | <center>\u2014</center> | <center>\u2014</center> | [![CI testing](https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20testing/badge.svg?event=push)](https://github.com/PyTorchLightning/pytorch-lightning/actions?query=workflow%3A%22CI+testing%22) |\n| OSX py3.6 / py3.7| [![CI testing](https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20testing/badge.svg?event=push)](https://github.com/PyTorchLightning/pytorch-lightning/actions?query=workflow%3A%22CI+testing%22) | <center>\u2014</center> | <center>\u2014</center> | [![CI testing](https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20testing/badge.svg?event=push)](https://github.com/PyTorchLightning/pytorch-lightning/actions?query=workflow%3A%22CI+testing%22) |\n| Windows py3.6 / py3.7 | [![CI testing](https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20testing/badge.svg?event=push)](https://github.com/PyTorchLightning/pytorch-lightning/actions?query=workflow%3A%22CI+testing%22) | <center>\u2014</center> | <center>\u2014</center> | [![CI testing](https://github.com/PyTorchLightning/pytorch-lightning/workflows/CI%20testing/badge.svg?event=push)](https://github.com/PyTorchLightning/pytorch-lightning/actions?query=workflow%3A%22CI+testing%22) |\n\n</center>\n\nSimple installation from PyPI\n```bash\npip install pytorch-lightning\n```\n\n## Docs\n- [master](https://pytorch-lightning.readthedocs.io/en/latest)\n- [0.7.1](https://pytorch-lightning.readthedocs.io/en/0.7.1/)\n- [0.6.0](https://pytorch-lightning.readthedocs.io/en/0.6.0/)\n- [0.5.3.2](https://pytorch-lightning.readthedocs.io/en/0.5.3.2/)\n\n## Demo\n[MNIST, GAN, BERT on COLAB!](https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=HOk9c4_35FKg)\n[MNIST on TPUs](https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3)\n\n## What is it?\nLightning is a way to organize your PyTorch code to decouple the science code from the engineering. It's more of a style-guide than a framework.\n\nTo use Lightning, first refactor your research code into a [LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html).\n\n![PT to PL](docs/source/_images/lightning_module/pt_to_pl.png)\n\nAnd Lightning automates the rest using the [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html)!\n![PT to PL](docs/source/_images/lightning_module/pt_trainer.png)\n\nLightning guarantees rigorously tested, correct, modern best practices for the automated parts.\n\n## How flexible is it?\nAs you see, you're just organizing your PyTorch code - there's no abstraction.\n\nAnd for the stuff that the Trainer abstracts out you can [override any part](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html#extensibility) you want to do things like implement your own distributed training, 16-bit precision, or even a custom backwards pass.\n\nFor anything else you might need, we have an extensive [callback system](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html#callbacks) you can use to add arbitrary functionality not implemented by our team in the Trainer.\n\n## Who is Lightning for?\n- Professional researchers\n- PhD students\n- Corporate production teams\n\nIf you're just getting into deep learning, we recommend you learn PyTorch first! Once you've implemented a few models, come back and use all the advanced features of Lightning :)\n\n## What does lightning control for me?\n\nEverything in Blue!\nThis is how lightning separates the science (red) from the engineering (blue).\n\n![Overview](docs/source/_images/general/pl_overview.gif)\n\n## How much effort is it to convert?\nIf your code is not a huge mess you should be able to organize it into a LightningModule in less than 1 hour.\nIf your code IS a mess, then you needed to clean up anyhow ;)\n\n[Check out this step-by-step guide](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09).\n\n\n## Starting a new project?\n[Use our seed-project aimed at reproducibility!](https://github.com/PytorchLightning/pytorch-lightning-conference-seed)\n\n## Why do I want to use lightning?\nAlthough your research/production project might start simple, once you add things like GPU AND TPU training, 16-bit precision, etc, you end up spending more time engineering than researching. Lightning automates AND rigorously tests those parts for you.\n\n## Support\n- [8 core contributors](https://pytorch-lightning.readthedocs.io/en/latest/governance.html) who are all a mix of professional engineers, Research Scientists, PhD students from top AI labs. \n- 100+ community contributors.\n\nLightning is also part of the [PyTorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\n---\n\n## README Table of Contents\n- [How do I use it](https://github.com/PytorchLightning/pytorch-lightning#how-do-i-do-use-it)\n- [What lightning automates](https://github.com/PytorchLightning/pytorch-lightning#what-does-lightning-control-for-me)\n- [Tensorboard integration](https://github.com/PytorchLightning/pytorch-lightning#tensorboard)\n- [Lightning features](https://github.com/PytorchLightning/pytorch-lightning#lightning-automates-all-of-the-following-each-is-also-configurable)\n- [Examples](https://github.com/PytorchLightning/pytorch-lightning#examples)\n- [Tutorials](https://github.com/PytorchLightning/pytorch-lightning#tutorials)\n- [Asking for help](https://github.com/PytorchLightning/pytorch-lightning#asking-for-help)\n- [Contributing](https://github.com/PytorchLightning/pytorch-lightning/blob/master/.github/CONTRIBUTING.md)\n- [Bleeding edge install](https://github.com/PytorchLightning/pytorch-lightning#bleeding-edge)\n- [Lightning Design Principles](https://github.com/PytorchLightning/pytorch-lightning#lightning-design-principles)\n- [Lightning team](https://github.com/PytorchLightning/pytorch-lightning#lightning-team)\n- [FAQ](https://github.com/PytorchLightning/pytorch-lightning#faq)\n\n---\n\n## Realistic example\nHere's how you would organize a realistic PyTorch project into Lightning.\n\n![PT to PL](docs/source/_images/mnist_imgs/pt_to_pl.jpg)\n\nThe LightningModule defines a *system* such as seq-2-seq, GAN, etc...\nIt can ALSO define a simple classifier.\n\nIn summary, you:\n\n1. Define a [LightningModule](https://pytorch-lightning.rtfd.io/en/latest/lightning-module.html)\n```python\n    class LitSystem(pl.LightningModule):\n\n        def __init__(self):\n            super().__init__()\n            # not the best model...\n            self.l1 = torch.nn.Linear(28 * 28, 10)\n\n        def forward(self, x):\n            return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n        def training_step(self, batch, batch_idx):\n            ...\n```\n\n2. Fit it with a [Trainer](https://pytorch-lightning.rtfd.io/en/latest/pytorch_lightning.trainer.html)\n ```python\n from pytorch_lightning import Trainer\n\n model = LitSystem()\n\n # most basic trainer, uses good defaults\n trainer = Trainer()\n trainer.fit(model)\n ```\n\n[Check out the COLAB demo here](https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=HOk9c4_35FKg)\n\n## What types of research works?\nAnything! Remember, that this is just organized PyTorch code.\nThe Training step defines the core complexity found in the training loop.\n\n#### Could be as complex as a seq2seq\n\n```python\n# define what happens for training here\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n\n    # define your own forward and loss calculation\n    hidden_states = self.encoder(x)\n\n    # even as complex as a seq-2-seq + attn model\n    # (this is just a toy, non-working example to illustrate)\n    start_token = '<SOS>'\n    last_hidden = torch.zeros(...)\n    loss = 0\n    for step in range(max_seq_len):\n        attn_context = self.attention_nn(hidden_states, start_token)\n        pred = self.decoder(start_token, attn_context, last_hidden)\n        last_hidden = pred\n        pred = self.predict_nn(pred)\n        loss += self.loss(last_hidden, y[step])\n\n    #toy example as well\n    loss = loss / max_seq_len\n    return {'loss': loss}\n```\n\n#### Or as basic as CNN image classification\n\n```python\n# define what happens for validation here\ndef validation_step(self, batch, batch_idx):\n    x, y = batch\n\n    # or as basic as a CNN classification\n    out = self(x)\n    loss = my_loss(out, y)\n    return {'loss': loss}\n```\n\nAnd without changing a single line of code, you could run on CPUs\n```python\ntrainer = Trainer(max_epochs=1)\n```\n\n\nOr GPUs\n```python\n# 8 GPUs\ntrainer = Trainer(max_epochs=1, gpus=8)\n\n# 256 GPUs\ntrainer = Trainer(max_epochs=1, gpus=8, num_nodes=32)\n```\n\nOr TPUs\n```python\ntrainer = Trainer(num_tpu_cores=8)\n```\n\nWhen you're done training, run the test accuracy\n```python\ntrainer.test()\n```\n\n## Visualization\nLightning has out-of-the-box integration with the popular logging/visualizing frameworks\n\n- [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html)\n- [MLFlow](https://mlflow.org/)\n- [Neptune.ai](https://neptune.ai/)\n- [Comet.ml](https://www.comet.ml/site/)\n- [Wandb](https://www.wandb.com/)\n- [Trains](https://github.com/allegroai/trains)\n- ...  \n\n![tensorboard-support](docs/source/_images/general/tf_loss.png)\n\n\n## Lightning automates 40+ parts of DL/ML research\n- GPU training\n- Distributed GPU (cluster) training\n- TPU training\n- EarlyStopping\n- Logging/Visualizing\n- Checkpointing\n- Experiment management\n- [Full list here](https://pytorch-lightning.readthedocs.io/en/latest/#common-use-cases)\n\n\n## Examples\nCheck out this awesome list of research papers and implementations done with Lightning.\n\n- [Contextual Emotion Detection (DoubleDistilBert)](https://github.com/PyTorchLightning/emotion_transformer)\n- [Generative Adversarial Network](https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=TyYOdg8g77P0)\n- [Hyperparameter optimization with Optuna](https://github.com/optuna/optuna/blob/master/examples/pytorch_lightning_simple.py)\n- [Image Inpainting using Partial Convolutions](https://github.com/ryanwongsa/Image-Inpainting)\n- [MNIST on TPU](https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3#scrollTo=BHBz1_AnamN_)\n- [NER (transformers, TPU, huggingface)](https://colab.research.google.com/drive/1dBN-wwYUngLYVt985wGs_OKPlK_ANB9D)\n- [NeuralTexture (CVPR)](https://github.com/PyTorchLightning/neuraltexture)\n- [Recurrent Attentive Neural Process](https://github.com/PyTorchLightning/attentive-neural-processes)\n- [Siamese Nets for One-shot Image Recognition](https://github.com/PyTorchLightning/Siamese-Neural-Networks)\n- [Speech Transformers](https://github.com/PyTorchLightning/speech-transformer-pytorch_lightning)\n- [Transformers transfer learning (Huggingface)](https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=yr7eaxkF-djf)\n- [Transformers text classification](https://github.com/ricardorei/lightning-text-classification)\n- [VAE Library of over 18+ VAE flavors](https://github.com/AntixK/PyTorch-VAE)\n\n## Tutorials\nCheck out our [introduction guide](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html) to get started.\nOr jump straight into [our tutorials](https://pytorch-lightning.readthedocs.io/en/latest/#tutorials).\n\n---\n\n## Asking for help\nWelcome to the Lightning community!\n\nIf you have any questions, feel free to:\n1. [read the docs](https://pytorch-lightning.rtfd.io/en/latest/).\n2. [Search through the issues](https://github.com/PytorchLightning/pytorch-lightning/issues?utf8=%E2%9C%93&q=my++question).\n3. [Ask on stackoverflow](https://stackoverflow.com/questions/ask?guided=false) with the tag pytorch-lightning.\n4. [Join our slack](https://join.slack.com/t/pytorch-lightning/shared_invite/enQtODU5ODIyNTUzODQwLTFkMDg5Mzc1MDBmNjEzMDgxOTVmYTdhYjA1MDdmODUyOTg2OGQ1ZWZkYTQzODhhNzdhZDA3YmNhMDhlMDY4YzQ).\n\n---\n## FAQ\n**How do I use Lightning for rapid research?**\n[Here's a walk-through](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html)\n\n**Why was Lightning created?**\nLightning has 3 goals in mind:\n\n1. Maximal flexibility while abstracting out the common boilerplate across research projects.\n2. Reproducibility. If all projects use the LightningModule template, it will be much much easier to understand what's going on and where to look! It will also mean every implementation follows a standard format.\n3. Democratizing PyTorch power user features. Distributed training? 16-bit? know you need them but don't want to take the time to implement? All good... these come built into Lightning.\n\n**How does Lightning compare with Ignite and fast.ai?**\n[Here's a thorough comparison](https://medium.com/@_willfalcon/pytorch-lightning-vs-pytorch-ignite-vs-fast-ai-61dc7480ad8a).\n\n**Is this another library I have to learn?**\nNope! We use pure Pytorch everywhere and don't add unnecessary abstractions!\n\n**Are there plans to support Python 2?**\nNope.\n\n**Are there plans to support virtualenv?**\nNope. Please use anaconda or miniconda.\n\n**Which PyTorch versions do you support?**\n- **PyTorch 1.1.0**\n    ```bash\n    # install pytorch 1.1.0 using the official instructions\n\n    # install test-tube 0.6.7.6 which supports 1.1.0\n    pip install test-tube==0.6.7.6\n\n    # install latest Lightning version without upgrading deps\n    pip install -U --no-deps pytorch-lightning\n    ```\n- **PyTorch 1.2.0, 1.3.0,**\n    Install via pip as normal\n\n## Custom installation\n\n### Bleeding edge\n\nIf you can't wait for the next release, install the most up to date code with:\n* using GIT (locally clone whole repo with full history)\n    ```bash\n    pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade\n    ```\n* using instant zip (last state of the repo without git history)\n    ```bash\n    pip install https://github.com/PytorchLightning/pytorch-lightning/archive/master.zip --upgrade\n    ```\n\n### Any release installation\n\nYou can also install any past release `0.X.Y` from this repository:\n```bash\npip install https://github.com/PytorchLightning/pytorch-lightning/archive/0.X.Y.zip --upgrade\n```\n\n### Lightning team\n\n#### Leads\n- William Falcon [(williamFalcon)](https://github.com/williamFalcon) (Lightning founder)\n- Jirka Borovec [(Borda)](https://github.com/Borda) (ghost :)\n- Ethan Harris [(ethanwharris)](https://github.com/ethanwharris) (Torchbearer founder)\n- Matthew Painter [(MattPainter01)](https://github.com/MattPainter01) (Torchbearer founder)\n- Justus Schock [(justusschock)](https://github.com/justusschock) (Former Core Member PyTorch Ignite)\n\n#### Core Maintainers\n\n- Nick Eggert [(neggert)](https://github.com/neggert)\n- Jeff Ling [(jeffling)](https://github.com/jeffling)\n- Jeremy Jordan [(jeremyjordan)](https://github.com/jeremyjordan)\n- Tullie Murrell [(tullie)](https://github.com/tullie)\n\n## Bibtex\nIf you want to cite the framework feel free to use this (but only if you loved it \ud83d\ude0a):\n```\n@misc{Falcon2019,\n  author = {Falcon, W.A. et al.},\n  title = {PyTorch Lightning},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/PytorchLightning/pytorch-lightning}}\n}\n```\n\n\n\n--- ./pl_examples/README.md ---\n# Examples   \nThis folder has 3 sections:   \n\n### Domain templates   \nThese are templates to show common approaches such as GANs and RL.   \n\n### Basic examples   \nThese show the most common use of Lightning for either CPU or GPU training.   \n\n### Multi-node examples   \nThese show how to run jobs on a GPU cluster using lightning.\n\n",
    "readme_filenames": [
      "./tests/README.md",
      "./README.md",
      "./pl_examples/README.md"
    ],
    "dockerfile": "\n--- ./tests/Dockerfile ---\nARG TORCH_VERSION=1.4\nARG CUDA_VERSION=10.1\n\nFROM pytorch/pytorch:${TORCH_VERSION}-cuda${CUDA_VERSION}-cudnn7-runtime\n\n# Install AMP\nRUN bash ./tests/install_AMP.sh\n\n\n",
    "dockerfile_paths": [
      "./tests/Dockerfile"
    ],
    "github_workflows": {
      ".github/workflows/ci-testing.yml": "name: CI testing\n\n# https://help.github.com/en/actions/reference/events-that-trigger-workflows\non:\n  # Trigger the workflow on push or pull request,\n  # but only for the master branch\n  push:\n    branches:\n      - master\n  pull_request:\n    branches:\n      - master\n\njobs:\n  build:\n\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      # max-parallel: 6\n      matrix:\n        os: [ubuntu-18.04, windows-2016, macOS-10.15]\n        python-version: [3.6, 3.7]\n        requires: ['minimal', 'latest']\n\n    # Timeout: https://stackoverflow.com/a/59076067/4521646\n    timeout-minutes: 20\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v1\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    # Github Actions: Run step on specific OS: https://stackoverflow.com/a/57948488/4521646\n    - name: Setup macOS\n      if: runner.os == 'macOS'\n      run: |\n        brew install libomp  # https://github.com/pytorch/pytorch/issues/20030\n\n    - name: Set min. dependencies\n      if: matrix.requires == 'minimal'\n      run: |\n        python -c \"req = open('requirements.txt').read().replace('>', '=') ; open('requirements.txt', 'w').write(req)\"\n        python -c \"req = open('requirements-extra.txt').read().replace('>', '=') ; open('requirements-extra.txt', 'w').write(req)\"\n\n    - name: Cache pip\n      uses: actions/cache@v1\n      with:\n        path: ~/.cache/pip # This path is specific to Ubuntu\n        # Look to see if there is a cache hit for the corresponding requirements file\n        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('requirements.txt') }}\n        restore-keys: |\n          ${{ runner.os }}-pip-${{ matrix.python-version }}-\n          ${{ runner.os }}-${{ matrix.python-version }}-\n\n    - name: Cache datasets\n      uses: actions/cache@v1\n      with:\n        path: tests/datasets # This path is specific to Ubuntu\n        # Look to see if there is a cache hit for the corresponding requirements file\n        key: mnist-dataset\n\n    - name: Install dependencies\n      run: |\n        # python -m pip install --upgrade --user pip\n        pip install -r requirements.txt -U -f https://download.pytorch.org/whl/torch_stable.html\n        pip install -r ./tests/requirements.txt\n        # pip install tox coverage\n        python --version\n        pip --version\n        pip list\n\n    - name: Tests\n      # env:\n      #   TOXENV: py${{ matrix.python-version }}\n      run: |\n        # tox --sitepackages\n        # flake8 .\n        coverage run --source pytorch_lightning -m py.test pytorch_lightning tests -v --doctest-modules --junitxml=junit/test-results-${{ runner.os }}-${{ matrix.python-version }}.xml\n        coverage report\n\n    - name: Upload pytest test results\n      uses: actions/upload-artifact@master\n      with:\n        name: pytest-results-${{ runner.os }}-${{ matrix.python-version }}\n        path: junit/test-results-${{ runner.os }}-${{ matrix.python-version }}.xml\n      # Use always() to always run this step to publish test results when there are test failures\n      if: always()\n\n    - name: Package Setup\n      run: |\n        check-manifest\n        python setup.py check --metadata --strict\n        python setup.py sdist\n        twine check dist/*\n\n    #- name: Try install package\n    #  if: ! startsWith(matrix.os, 'windows')\n    #  run: |\n    #    virtualenv vEnv ; source vEnv/bin/activate\n    #    pip install --editable . ; cd .. & python -c \"import pytorch_lightning ; print(pytorch_lightning.__version__)\"\n    #    deactivate ; rm -rf vEnv\n\n    - name: Statistics\n      if: success()\n      run: |\n         coverage report\n",
      ".github/workflows/docs-check.yml": "name: \"Docs check\"\n# https://github.com/marketplace/actions/sphinx-build\n\non:\n- pull_request\n\njobs:\n  docs:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - uses: ammaraskar/sphinx-action@master\n      with:\n        # git is requried to clone the docs theme\n        pre-build-command: \"apt-get update -y && apt-get install -y git\"\n        docs-folder: \"docs/\"\n        repo-token: \"${{ secrets.GITHUB_TOKEN }}\"\n",
      ".github/workflows/greetings.yml": "name: Greetings\n# https://github.com/marketplace/actions/first-interaction\n\non: [issues]  # pull_request\n\njobs:\n  greeting:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/first-interaction@v1\n      with:\n        repo-token: ${{ secrets.GITHUB_TOKEN }}\n        issue-message: 'Hi! thanks for your contribution!, great first issue!'\n        pr-message: 'Hey thanks for the input! Please give us a bit of time to review it!'\n"
    }
  },
  "llm_calls_before_build": 0,
  "github_workflows_found": [
    ".github/workflows/ci-testing.yml",
    ".github/workflows/docs-check.yml",
    ".github/workflows/greetings.yml"
  ],
  "dockerfile": "FROM pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime\n\n# Install system dependencies and git\nRUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n\n# Clone repository and checkout specific commit\nWORKDIR /workspace\nRUN git clone https://github.com/Lightning-AI/pytorch-lightning.git\nWORKDIR /workspace/pytorch-lightning\nRUN git checkout aca8c7e6f3a88f2083375133e33d5f9c5b9bb1c9\n\n# Install AMP support from project scripts\nRUN bash ./tests/install_AMP.sh\n\n# Install exact dependency versions from issue description\nRUN pip install tqdm==4.65.0\n\n# Install project requirements and test dependencies\nRUN pip install -r requirements.txt\nRUN pip install -r tests/requirements.txt\n\n# Install PyTorch Lightning in editable mode from checked-out commit\nRUN pip install -e .\n\n# Set default command to interactive shell\nCMD [\"/bin/bash\"]",
  "dockerfile_source": "Repository at Dockerfile",
  "dockerfile_attempt_1": 1,
  "dockerfile_build_success": true,
  "successful_candidate_index": 4,
  "successful_candidate_attempt": 1,
  "successful_candidate_improvement_attempt": 0,
  "llm_calls_total": 5
}