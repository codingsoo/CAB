[
  {
    "number": 28903,
    "title": "clickhouse server crashes after starts ",
    "created_at": "2021-09-11T18:16:54Z",
    "closed_at": "2021-09-11T21:51:45Z",
    "labels": [
      "invalid",
      "question",
      "alternative build"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28903",
    "body": "error message below\r\n```\r\n2021.09.12 01:23:44.437039 [ 18936 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.09.12 01:23:44.437087 [ 18936 ] {} <Fatal> BaseDaemon: (version 20.8.3.18, no build id) (from thread 18818) (no query) Received signal Segmentation fault (11)\r\n2021.09.12 01:23:44.437113 [ 18936 ] {} <Fatal> BaseDaemon: Address: 0x7f78fb378000 Access: write. Attempted access has violated the permissions assigned to the memory area.\r\n2021.09.12 01:23:44.437126 [ 18936 ] {} <Fatal> BaseDaemon: Stack trace: 0x14b99c2f 0x14b9e7ad 0x14b9fd36 0x14b98e78 0x14ba17ef 0x14ba1b33 0x9e7e7ea 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d 0x114dc197 0x10a7675d\r\n2021.09.12 01:23:44.437163 [ 18936 ] {} <Fatal> BaseDaemon: 3. ? @ 0x14b99c2f in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437171 [ 18936 ] {} <Fatal> BaseDaemon: 4. ? @ 0x14b9e7ad in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437177 [ 18936 ] {} <Fatal> BaseDaemon: 5. ? @ 0x14b9fd36 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437186 [ 18936 ] {} <Fatal> BaseDaemon: 6. unw_step @ 0x14b98e78 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437193 [ 18936 ] {} <Fatal> BaseDaemon: 7. ? @ 0x14ba17ef in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437203 [ 18936 ] {} <Fatal> BaseDaemon: 8. _Unwind_Resume @ 0x14ba1b33 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437212 [ 18936 ] {} <Fatal> BaseDaemon: 9. ? @ 0x9e7e7ea in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437224 [ 18936 ] {} <Fatal> BaseDaemon: 10. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437236 [ 18936 ] {} <Fatal> BaseDaemon: 11. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437259 [ 18936 ] {} <Fatal> BaseDaemon: 12. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437267 [ 18936 ] {} <Fatal> BaseDaemon: 13. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437274 [ 18936 ] {} <Fatal> BaseDaemon: 14. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437281 [ 18936 ] {} <Fatal> BaseDaemon: 15. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437288 [ 18936 ] {} <Fatal> BaseDaemon: 16. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437295 [ 18936 ] {} <Fatal> BaseDaemon: 17. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437302 [ 18936 ] {} <Fatal> BaseDaemon: 18. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437309 [ 18936 ] {} <Fatal> BaseDaemon: 19. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437315 [ 18936 ] {} <Fatal> BaseDaemon: 20. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437322 [ 18936 ] {} <Fatal> BaseDaemon: 21. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437329 [ 18936 ] {} <Fatal> BaseDaemon: 22. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437341 [ 18936 ] {} <Fatal> BaseDaemon: 23. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437348 [ 18936 ] {} <Fatal> BaseDaemon: 24. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437355 [ 18936 ] {} <Fatal> BaseDaemon: 25. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437362 [ 18936 ] {} <Fatal> BaseDaemon: 26. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437369 [ 18936 ] {} <Fatal> BaseDaemon: 27. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437376 [ 18936 ] {} <Fatal> BaseDaemon: 28. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437382 [ 18936 ] {} <Fatal> BaseDaemon: 29. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437389 [ 18936 ] {} <Fatal> BaseDaemon: 30. DB::ExpressionBlockInputStream::readImpl() @ 0x114dc197 in /usr/bin/clickhouse\r\n2021.09.12 01:23:44.437396 [ 18936 ] {} <Fatal> BaseDaemon: 31. DB::IBlockInputStream::read() @ 0x10a7675d in /usr/bin/clickhouse\r\n\r\n```\r\nScenario\uff1aour clickhouse cluster runs stably for a time, yesterday a beginner developer runs a lot of \"alter table\" query try to \"update\" a field in a big table, the server crashed. then we add \r\n```\r\n<cleanup_delay_period>60</cleanup_delay_period>\r\n<task_max_lifetime>60</task_max_lifetime>\r\n<max_tasks_in_queue>200</max_tasks_in_queue>\r\n```\r\nthis to the clickhouse config to clean up the DDL queue. after that, some of clickhouse servers could not start up.\r\nAny ideas",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28903/comments",
    "author": "liu316484231",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-09-11T21:47:14Z",
        "body": "It's not official build.\n\n---\n\nAbout how to fix your issue: it has caused by large number of ALTER mutations. You should find and delete the offending mutation record. Its location depends on whether you are using MergeTree or ReplicatedMergeTree."
      },
      {
        "user": "liu316484231",
        "created_at": "2021-09-11T23:03:54Z",
        "body": "> About how to fix your issue: it has caused by large number of ALTER mutations. You should find and delete the offending mutation record. Its location depends on whether you are using MergeTree or ReplicatedMergeTree.\r\n\r\nhow to find and delete these mutations?we use MergeTree. I didnt see any log related to the alter mutation after modify the config before crashes\n\n---\n\n> It's not official build.\r\n\r\nyou mean the version we use (version 20.8.3.18, no build id) is not official recommanded?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-09-11T23:05:19Z",
        "body": "Mutations are located in `mutation_*.txt` files. You can delete them.\r\n\r\n> you mean the version we use (version 20.8.3.18, no build id) is not official recommanded?\r\n\r\nYes. And it is not our build. It is somehow modified or you have built it manually."
      },
      {
        "user": "liu316484231",
        "created_at": "2021-09-13T02:29:35Z",
        "body": "> Mutations are located in `mutation_*.txt` files. You can delete them.\r\n> \r\n> > you mean the version we use (version 20.8.3.18, no build id) is not official recommanded?\r\n> \r\n> Yes. And it is not our build. It is somehow modified or you have built it manually.\r\n\r\nThank you for your reply, we fixed the problem"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to identify and safely remove problematic ALTER mutations from MergeTree tables",
      "Addressing potential issues with non-official ClickHouse builds",
      "Method to prevent server crashes during startup due to mutation backlog"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 00:50:43"
    }
  },
  {
    "number": 23698,
    "title": "Kernel panic: \"Bad page state in process BgSchPool\" unsing KafkaEngine",
    "created_at": "2021-04-27T14:38:05Z",
    "closed_at": "2021-04-29T14:04:00Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/23698",
    "body": "**Describe the bug**\r\nSpontaneous kernel panic using tables with Kafka engine\r\n\r\n**How to reproduce**\r\n* ClickHouse server version 20.8.6 revision 54438\r\n* Clickhouse cluster of 3 replicas\r\n* Kafka cluster of 3 replicas\r\n* 60 tables with kafka engine + materialized views to ReplicatedMergeTree target tables\r\n* changed background_schedule_pool_size=100\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2021-04-26 @ 16:18:53.709513,935,843\r\nhost\r\n2021.04.26 16:18:53.709596 [ 28230 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128404_31024 covered by 202104_0_128407_31027 (state Committed)\r\n2021-04-26 @ 16:18:55.479513,936,026\r\nhost\r\n2021.04.26 16:18:55.479125 [ 28239 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128408_31028 covered by 202104_0_128411_31031 (state Committed)\r\n2021-04-26 @ 16:20:42.197513,936,209\r\nhost\r\n2021.04.26 16:20:42.197118 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Checking part 202104_0_128603_31217\r\n2021-04-26 @ 16:20:42.212513,936,362\r\nhost\r\n2021.04.26 16:20:42.212608 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 202104_0_128603_31217.\r\n2021-04-26 @ 16:20:42.217513,936,541\r\nhost\r\n2021.04.26 16:20:42.217504 [ 28336 ] {} <Warning> database.table (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 202104_0_128603_31217. Hoping that it will eventually appear as a result of a merge.\r\n2021-04-26 @ 16:20:42.256513,936,827\r\nhost\r\n2021.04.26 16:20:42.256184 [ 28228 ] {} <Warning> database.table: Tried to commit obsolete part 202104_0_128601_31215 covered by 202104_0_128603_31217 (state Committed)\r\n2021.04.26 16:26:49.174485 [ 17341 ] {} <Error> ServerErrorHandler: Poco::Exception. Code: 1000, e.code() = 104, e.displayText() = Connection reset by peer, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::IOException::IOException(int) @ 0x18bc80ff in /usr/bin/clickhouse\r\n1. Poco::Net::ConnectionResetException::ConnectionResetException(int) @ 0x18ab943d in /usr/bin/clickhouse\r\n2. ? @ 0x18ad61ea in /usr/bin/clickhouse\r\n3. Poco::Net::SocketImpl::receiveBytes(void*, int, int) @ 0x18ad48e3 in /usr/bin/clickhouse\r\n4. Poco::Net::HTTPSession::receive(char*, int) @ 0x18aa2c58 in /usr/bin/clickhouse\r\n5. Poco::Net::HTTPSession::get() @ 0x18aa2cc3 in /usr/bin/clickhouse\r\n6. Poco::Net::HTTPHeaderStreamBuf::readFromDevice(char*, long) @ 0x18a9787a in /usr/bin/clickhouse\r\n7. Poco::BasicBufferedStreamBuf<char, std::__1::char_traits<char>, Poco::Net::HTTPBufferAllocator>::underflow() @ 0x18a941d8 in /usr/bin/clickhouse\r\n8. std::__1::basic_streambuf<char, std::__1::char_traits<char> >::uflow() @ 0x19b3ef0e in ?\r\n9. std::__1::basic_istream<char, std::__1::char_traits<char> >::get() @ 0x19b46506 in ?\r\n10. Poco::Net::HTTPRequest::read(std::__1::basic_istream<char, std::__1::char_traits<char> >&) @ 0x18a9a957 in /usr/bin/clickhouse\r\n11. Poco::Net::HTTPServerRequestImpl::HTTPServerRequestImpl(Poco::Net::HTTPServerResponseImpl&, Poco::Net::HTTPServerSession&, Poco::Net::HTTPServerParams*) @ 0x18aa0d85 in /usr/bin/clickhouse\r\n12. Poco::Net::HTTPServerConnection::run() @ 0x18a9f74c in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerConnection::start() @ 0x18add87b in /usr/bin/clickhouse\r\n14. Poco::Net::TCPServerDispatcher::run() @ 0x18addd0b in /usr/bin/clickhouse\r\n15. Poco::PooledThread::run() @ 0x18c5c7e6 in /usr/bin/clickhouse\r\n16. Poco::ThreadImpl::runnableEntry(void*) @ 0x18c57be0 in /usr/bin/clickhouse\r\n17. start_thread @ 0x7fa3 in /usr/lib/x86_64-linux-gnu/libpthread-2.28.so\r\n18. __clone @ 0xf94cf in /usr/lib/x86_64-linux-gnu/libc-2.28.so\r\n (version 20.8.6.6 (official build))\r\n\r\n2021-04-26 @ 16:26:49.1305200172.356694] BUG: Bad page state in process BgSchPool  pfn:1fcdb35\r\n2021-04-26 @ 16:26:49.1305200172.394974] page:ffffe7c0ff36cd40 count:0 mapcount:0 mapping:ffff9324edb25b00 index:0x0\r\n2021-04-26 @ 16:26:49.1305200172.444695] flags: 0x17fffc000000000()\r\n2021-04-26 @ 16:26:49.1305200172.468918] raw: 017fffc000000000 dead000000000100 dead000000000200 ffff9324edb25b00\r\n2021-04-26 @ 16:26:49.1305200172.517059] raw: 0000000000000000 0000000000270027 00000000ffffffff ffff93406e2dc000\r\n2021-04-26 @ 16:26:49.1305200172.565211] page dumped because: page still charged to cgroup\r\n2021-04-26 @ 16:26:49.1305200172.601395] page->mem_cgroup:ffff93406e2dc000\r\n2021-04-26 @ 16:26:49.1305200172.629259] Modules linked in: drbg ansi_cprng authenc echainiv xfrm6_mode_tunnel xfrm4_mode_tunnel binfmt_misc xfrm_user xfrm4_tunnel tunnel4 ipcomp xfrm_ipcomp esp4 ah4 af_key xfrm_algo ifb sch_fq_codel bonding ip6table_mangle nf_log_ipv6 ip6t_REJECT nf_reject_ipv6 iptable_nat nf_nat_ipv4 nf_nat xt_connmark xt_DSCP xt_length xt_dscp iptable_mangle nf_log_ipv4 nf_log_common xt_set ipt_REJECT nf_reject_ipv4 xt_owner xt_multiport xt_LOG xt_limit xt_policy xt_conntrack xt_tcpudp ip_set_hash_ip ip_set nfnetlink ip6table_filter ip6_tables iptable_filter nls_ascii nls_cp437 vfat fat intel_rapl sb_edac x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel kvm irqbypass mgag200 crct10dif_pclmul crc32_pclmul ttm ghash_clmulni_intel intel_cstate drm_kms_helper mxm_wmi intel_uncore efi_pstore mei_me intel_rapl_perf\r\n2021-04-26 @ 16:26:49.1305200173.070265]  iTCO_wdt pcc_cpufreq drm efivars pcspkr joydev evdev mei iTCO_vendor_support sg ioatdma ipmi_ssif wmi acpi_power_meter acpi_pad button nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 libcrc32c ipmi_si ipmi_devintf ipmi_msghandler efivarfs ip_tables x_tables autofs4 ext4 crc16 mbcache jbd2 crc32c_generic fscrypto ecb dm_mod hid_generic usbhid hid sd_mod ahci libahci xhci_pci ehci_pci xhci_hcd ehci_hcd crc32c_intel libata megaraid_sas aesni_intel aes_x86_64 crypto_simd i40e(OE) usbcore igb cryptd i2c_algo_bit scsi_mod usb_common lpc_ich i2c_i801 glue_helper mfd_core dca\r\n2021-04-26 @ 16:26:49.1305200173.070307] CPU: 20 PID: 28333 Comm: BgSchPool Tainted: G    B   W  OEL    4.19.0-13-amd64 #1 Debian 4.19.160-2\r\n2021-04-26 @ 16:26:49.1305200173.070312] Hardware name: Intel Corporation S2600WT2R/S2600WT2R, BIOS SE5C610.86B.01.01.0020.122820161512 12/28/2016\r\n2021-04-26 @ 16:26:49.1305200173.516116] Call Trace:\r\n2021-04-26 @ 16:26:49.1305200173.533853]  dump_stack+0x66/0x90\r\n2021-04-26 @ 16:26:49.1305200173.556753]  bad_page.cold.116+0x7f/0xb2\r\n2021-04-26 @ 16:26:49.1305200173.583290]  free_pcppages_bulk+0x4b4/0x660\r\n2021-04-26 @ 16:26:49.1305200173.611387]  free_unref_page_list+0x111/0x190\r\n2021-04-26 @ 16:26:49.1305200173.640536]  release_pages+0x215/0x450\r\n2021-04-26 @ 16:26:49.1305200173.666167]  tlb_flush_mmu_free+0x3d/0x60\r\n2021-04-26 @ 16:26:49.1305200173.693343]  arch_tlb_finish_mmu+0x89/0x100\r\n2021-04-26 @ 16:26:49.1305200173.721564]  tlb_finish_mmu+0x1f/0x30\r\n2021-04-26 @ 16:26:49.1305200173.746536]  zap_page_range+0xde/0x140\r\n2021-04-26 @ 16:26:49.1305200173.772146]  ? do_futex+0xc8/0xbe0\r\n2021-04-26 @ 16:26:49.1305200173.795671]  __x64_sys_madvise+0x663/0x7c0\r\n2021-04-26 @ 16:26:49.1305200173.823237]  ? do_syscall_64+0x53/0x110\r\n2021-04-26 @ 16:26:49.1305200173.849265]  ? __ia32_sys_madvise+0x7c0/0x7c0\r\n2021-04-26 @ 16:26:49.1305200173.878400]  do_syscall_64+0x53/0x110\r\n2021-04-26 @ 16:26:49.1305200173.903382]  entry_SYSCALL_64_after_hwframe+0x44/0xa9\r\n2021-04-26 @ 16:26:49.1305200173.936667] RIP: 0033:0x7f126714c2d7\r\n2021-04-26 @ 16:26:49.1305200173.961237] Code: ff ff ff ff c3 48 8b 15 b7 6b 0c 00 f7 d8 64 89 02 b8 ff ff ff ff eb bc 66 2e 0f 1f 84 00 00 00 00 00 90 b8 1c 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 89 6b 0c 00 f7 d8 64 89 01 48\r\n2021-04-26 @ 16:26:49.1305200174.078867] RSP: 002b:00007f12082ac648 EFLAGS: 00000206 ORIG_RAX: 000000000000001c\r\n2021-04-26 @ 16:26:49.1305200174.127252] RAX: ffffffffffffffda RBX: 0000000000008000 RCX: 00007f126714c2d7\r\n2021-04-26 @ 16:26:49.1305200174.173033] RDX: 0000000000000004 RSI: 000000000003c000 RDI: 00007f0aa007e000\r\n2021-04-26 @ 16:26:49.1305200174.218905] RBP: 00007f12082ada60 R08: 000000000003c000 R09: 0000000000000014\r\n2021-04-26 @ 16:26:49.1305200174.264703] R10: 0000000000000000 R11: 0000000000000206 R12: 00007f0763210540\r\n2021-04-26 @ 16:26:49.1305200174.310591] R13: 00007f12082ac6d0 R14: 00007f123c0008c0 R15: 00007f123c004510\r\n```\r\n\r\n**Additional context**\r\nWe faced the problem twice, but unfortunately we cannot reproduce it. The problem occurs spontaneously and rarely, but because of this, our server crashes.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/23698/comments",
    "author": "suvorovis",
    "comments": [
      {
        "user": "azat",
        "created_at": "2021-04-27T19:49:51Z",
        "body": "I doubt that the clickhouse is the culprit, I would say that it is either hardware or the kernel.\r\n\r\nCan you please answer the following questions:\r\n- seems that you are running clickhouse in containers (docker/lxc/...) ?\r\n- which version of the kernel do you have? can you upgrade it to latest?\r\n- do you have memory with ECC?"
      },
      {
        "user": "suvorovis",
        "created_at": "2021-04-28T09:05:36Z",
        "body": "Thank you for the reply. Here are the answers you asked for:\r\n- we don't use any type of containers with clickhouse\r\n- kernel version 4.19.0-13-amd64\r\n- memory with ECC"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-04-28T09:24:50Z",
        "body": "Does the issue reproduce on all three replicas consistently or only on a subset of replicas?"
      },
      {
        "user": "azat",
        "created_at": "2021-04-28T18:16:29Z",
        "body": ">we don't use any type of containers with clickhouse\r\n\r\nInteresting\r\nCan you share kernel config, `/proc/cmdline` and `systemctl show clickhouse-server`?\r\n\r\n> kernel version 4.19.0-13-amd64\r\n\r\nI would recommend you to upgrade the kernel (I can't say which stable release do you have since I can't find any information about your distro specific versioning, but looks like some old debian kernel), is it possible?"
      },
      {
        "user": "suvorovis",
        "created_at": "2021-04-29T11:23:37Z",
        "body": "Issue was reproduced sequentially with the two days gap on two (#1 and #2) of three replicas once, and on one replica (#1) a second time. The kernel on replica #2 was upgraded and after that the server worked without this problem (about a month). So, maybe you are right about the reason.\r\nWe are going to upgrade the kernel on the first replica and watch the situation.\r\nThank you for your help!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies root cause relationship between ClickHouse operations and kernel panic",
      "Provides stability assurance for high-load KafkaEngine setups",
      "Explains interaction between background_schedule_pool_size and kernel memory management",
      "Offers diagnostic guidance for non-reproducible kernel issues",
      "Addresses kernel version compatibility concerns"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 00:53:49"
    }
  },
  {
    "number": 19658,
    "title": "Execute Clickhouse compressor -- decompress to return xshell",
    "created_at": "2021-01-26T13:14:08Z",
    "closed_at": "2021-01-27T03:05:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19658",
    "body": "SELECT *\r\nFROM mt2\r\n\r\n\u250c\u2500a\u2500\u252c\u2500\u2500b\u2500\u252c\u2500\u2500c\u2500\u2510\r\n\u2502 3 \u2502  4 \u2502 10 \u2502\r\n\u2502 3 \u2502  5 \u2502  9 \u2502\r\n\u2502 3 \u2502  6 \u2502  8 \u2502\r\n\u2502 3 \u2502  7 \u2502  7 \u2502\r\n\u2502 3 \u2502  8 \u2502  6 \u2502\r\n\u2502 3 \u2502  9 \u2502  5 \u2502\r\n\u2502 3 \u2502 10 \u2502  4 \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\r\n\r\nThe data directory is as follows\r\n\r\n[root@ck mt2]# tree\r\n.\r\n\u251c\u2500\u2500 3_1_1_0\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 a.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 a.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 b.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 b.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 c.bin\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 checksums.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 c.mrk\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 columns.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 count.txt\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 minmax_a.idx\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 partition.dat\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 primary.idx\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 skp_idx_idx_c.idx\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 skp_idx_idx_c.mrk\r\n\u251c\u2500\u2500 detached\r\n\u2514\u2500\u2500 format_version.txt\r\n\r\nExecute clickhouse-compressor like this\r\n\r\n[root@ck mt2]# clickhouse-compressor --decompress < 3_1_1_0/b.bin2                                                                                                                \t\r\n[root@ck mt2]# Xshell\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19658/comments",
    "author": "xiedeyantu",
    "comments": [
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-26T13:46:08Z",
        "body": "[root@ck mt2]# clickhouse-compressor --decompress < 3_1_1_0/b.bin\r\n[root@ck mt2]# Xshell\r\n\r\nnot b.bin2"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-26T18:55:35Z",
        "body": "That is correct and you just read some binary data from your table into your terminal."
      },
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-27T02:19:31Z",
        "body": "Why show Xshell instead of data\uff1fI want to see the structure of the bin file"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-27T02:38:18Z",
        "body": "Binary data with ANSI escape sequences can be interpreted by terminal."
      },
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-27T02:44:34Z",
        "body": "Can you give me a shell command? How to operate?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-01-27T02:53:49Z",
        "body": "Could you please tell me how to reproduce this result?\r\n\r\nE.g. CREATE TABLE statement, INSERT...\n\n---\n\nTo display binary data I also recommend `xxd` tool:\r\n\r\n`clickhouse-compressor --decompress < 3_1_1_0/b.bin | xxd`"
      },
      {
        "user": "xiedeyantu",
        "created_at": "2021-01-27T03:05:24Z",
        "body": "This shell command is easy to use. Thank you very much"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to view decompressed binary data in human-readable format",
      "Explains how to inspect ClickHouse's compressed column data format",
      "Uses standard shell utilities compatible with terminal workflows",
      "Avoids terminal interpretation artifacts from binary data"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 00:57:38"
    }
  },
  {
    "number": 15314,
    "title": "Fetch Single Row From Select Query Performance",
    "created_at": "2020-09-25T16:00:13Z",
    "closed_at": "2020-10-17T15:59:11Z",
    "labels": [
      "question",
      "performance",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15314",
    "body": "In a SELECT query, I want to return a single row record out of billions of rows in ClickHouse. I know that Clickhouse is not meant for single queries but here I have no other choice. I would like to enhance the performance of the query as much as possible. The query is the following:\r\n\r\n```select * from products where un_id='us-f032f8df-65c9-4f0b-8df2-ddb3a436ae7e' and organization_id='test' and request_time >= '2020-09-25 00:00:00' limit 1```\r\n\r\norganization_id and request_time are both partitioning keys. \r\n\r\nMy default settings for max_threads are 4 while for max_block_size is 65505. I have also tried setting max_threads=1, max_block_size=1024 (answer from a previous post here) but this did not really help with the speed of the query. \r\n\r\n I would like to achieve a response in less than 4 seconds. Is sth like this possible with this amount of data (billions of records)? \r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15314/comments",
    "author": "stcharitak",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-25T16:31:29Z",
        "body": "You have no choice except to move un_id to the beginning of PRIMARYINDEX.\r\nAnd try to clean un_id from 'us-' and save UUID as UUID (or as hex/FIXED String) not a String.\r\n\r\n```\r\n\r\ncreate table product (\r\n         country String, \r\n         un_id UUID, \r\n         organization_id LowCardinality(String),\r\n         request_time DateTime) \r\nEngine=MergeTree \r\npartition by (organization_id, toYYYYMM(request_time))\r\norder by (country, un_id, request_time)\r\nsettings index_granularity=1024;\r\n\r\ninsert into product select 'us', generateUUIDv4(), 'test', toDateTime('2020-01-01 00:00:00') + intDiv(number, 100)\r\nfrom numbers(1000000000);\r\n\r\n0 rows in set. Elapsed: 551.801 sec.\r\n\r\n\r\nSELECT *\r\nFROM product\r\nLIMIT 100000, 1\r\n\r\n\u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n\u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nTEST:\r\n\r\n\r\nSET max_threads = 1;\r\n\r\n\r\nSELECT *\r\nFROM product\r\nWHERE (organization_id = 'test') AND (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')\r\n\r\n\u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n\u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.004 sec. Processed 15.36 thousand rows, 491.85 KB (4.33 million rows/s., 138.74 MB/s.)\r\n\r\n\r\n\r\n\r\n```\r\n\r\nElapsed: 0.004 sec. is less than 4 sec.\r\nbecause PRIMARYindex is  `(country, un_id, request_time)` and it addresses ` (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')`\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-25T18:26:42Z",
        "body": "It's better to make ORDER BY `organization_id`, `request_time`, not PARTITION BY."
      },
      {
        "user": "stcharitak",
        "created_at": "2020-10-01T16:04:23Z",
        "body": "> You have no choice except to move un_id to the beginning of PRIMARYINDEX.\r\n> And try to clean un_id from 'us-' and save UUID as UUID (or as hex/FIXED String) not a String.\r\n> \r\n> ```\r\n> \r\n> create table product (\r\n>          country String, \r\n>          un_id UUID, \r\n>          organization_id LowCardinality(String),\r\n>          request_time DateTime) \r\n> Engine=MergeTree \r\n> partition by (organization_id, toYYYYMM(request_time))\r\n> order by (country, un_id, request_time)\r\n> settings index_granularity=1024;\r\n> \r\n> insert into product select 'us', generateUUIDv4(), 'test', toDateTime('2020-01-01 00:00:00') + intDiv(number, 100)\r\n> from numbers(1000000000);\r\n> \r\n> 0 rows in set. Elapsed: 551.801 sec.\r\n> \r\n> \r\n> SELECT *\r\n> FROM product\r\n> LIMIT 100000, 1\r\n> \r\n> \u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n> \u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> \r\n> ------------------------------------------------------------------------------------------------------------------------------------------------\r\n> \r\n> TEST:\r\n> \r\n> \r\n> SET max_threads = 1;\r\n> \r\n> \r\n> SELECT *\r\n> FROM product\r\n> WHERE (organization_id = 'test') AND (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')\r\n> \r\n> \u250c\u2500country\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500un_id\u2500\u252c\u2500organization_id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500request_time\u2500\u2510\r\n> \u2502 us      \u2502 755c7b41-89e2-4bc7-8619-74063068dd67 \u2502 test            \u2502 2020-02-29 20:50:32 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> \r\n> 1 rows in set. Elapsed: 0.004 sec. Processed 15.36 thousand rows, 491.85 KB (4.33 million rows/s., 138.74 MB/s.)\r\n> ```\r\n> \r\n> Elapsed: 0.004 sec. is less than 4 sec.\r\n> because PRIMARYindex is `(country, un_id, request_time)` and it addresses ` (country = 'us') AND (un_id = '755c7b41-89e2-4bc7-8619-74063068dd67') AND (request_time >= '2020-02-29 20:50:00')`\r\n\r\nthanks for the detailed answer. I already test it and I get really fast results.\n\n---\n\nUnfortunately, I am still unable to make fast SELECT queries after 150.000.000 inserts. I have the following table:\r\n\r\n```\r\nCREATE TABLE default.products_sharded\r\n(\r\n    `request_time` DateTime DEFAULT now(),\r\n    `un_id` UUID,\r\n    `organization_id` LowCardinality(String),\r\n    `investor` String,\r\n    `provider` String,\r\n    `publisher` String,\r\n    `creator` String,\r\n    `code` String,\r\n    `description` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/products_sharded', '{replica}')\r\nPARTITION BY (organization_id, toMonday(request_time))\r\nORDER BY (un_id)\r\nTTL request_time + INTERVAL + 30 DAY\r\nSETTINGS index_granularity = 1024;\r\n```\r\n\r\nThen I insert data to the table like this:\r\n```\r\nINSERT INTO products_sharded (request_time, un_id, organization_id, investor, provider, publisher, creator, code, description) SELECT\r\n    toDateTime(now()) + toInt64(number) AS request_time,\r\n    generateUUIDv4() AS un_id,\r\n    'test_1' AS organization_id,\r\n    concat('investor_', toString((number % 50) + 1)) AS investor,\r\n    concat('provider_', toString((number % 50) + 1)) AS provider,\r\n    concat('publisher_', toString((number % 50) + 1)) AS publisher,\r\n    concat('creator_', toString((number % 50) + 1)) AS creator,\r\n    'C98A1D7F-30EC-1016-9C72-43350A39E86C' AS code,\r\n    'some description for the product' AS description\r\nFROM system.numbers\r\nLIMIT 10000000\r\n```\r\n\r\nThen I would like to do the following query:\r\n```\r\nSELECT * FROM products_distributed WHERE un_id='0b0ed88e-e645-4d49-9b43-1a6b3f8fac4e ' limit 1\r\n```\r\n\r\nWhen I make the query I also know the organization_id, investor, provider, and publisher. So I could add those fields to the query if it is needed to make it faster. Unfortunately, I cannot know the request time. So I cannot include it in the query. This is why also I have set a TTL for my table to 30 days. The organization_id is part of the partition so I guess it should make the query much faster when there are 50 organizations for example. I would like the query to be also fast if the `un_id` does not exist in the table. \r\n\r\nWhen my table contains 100.000.000, the query is quite fast.\r\n\r\n```\r\nSET max_threads=1\r\n\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = '661ca451-3f9a-405e-8000-351a80a1cb7c'\r\nLIMIT 1\r\n\r\n0 rows in set. Elapsed: 0.155 sec. Processed 227.33 thousand rows, 42.35 MB (1.46 million rows/s., 272.81 MB/s.)\r\n\r\nand even better when `un_id` exists\r\n\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = 'a6c90405-ca2a-4a90-9a6a-77edc629055f'\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nrequest_time:    2020-10-01 15:37:07\r\nun_id:           a6c90405-ca2a-4a90-9a6a-77edc629055f\r\norganization_id: test_1\r\ninvestor:        investor_1\r\nprovider:        provider_1\r\npublisher:       publisher_1\r\ncreator:         creator_1\r\ncode:            C98A1D7F-30EC-1016-9C72-43350A39E86C\r\ndescription:     some description for the product\r\n\r\n1 rows in set. Elapsed: 0.108 sec. Processed 126.98 thousand rows, 23.66 MB (1.18 million rows/s., 219.00 MB/s.)\r\n```\r\n\r\nIf I repeat the process after adding more than 200.000.000 records for one organization_id (here partitioning cannot help, for now, it can be useful later with more organization_ids. But an organization_id might have 1 billion of products) then the query is really slow:\r\n\r\n```\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = '8aa40f91-119c-4e26-9afb-81b538caddbc'\r\nLIMIT 1\r\n\r\n0 rows in set. Elapsed: 13.764 sec. Processed 435.20 thousand rows, 81.08 MB (31.62 thousand rows/s., 5.89 MB/s.)\r\n\r\nSELECT *\r\nFROM products_distributed\r\nWHERE un_id = '6b8be299-bdb9-4632-871d-f5d3ca530bf7'\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nrequest_time:    2020-10-01 15:48:40\r\nun_id:           6b8be299-bdb9-4632-871d-f5d3ca530bf7\r\norganization_id: test_1\r\ninvestor:        investor_1\r\nprovider:        provider_1\r\npublisher:       publisher_1\r\ncreator:         creator_1\r\ncode:            C98A1D7F-30EC-1016-9C72-43350A39E86C\r\ndescription:     some description for the product\r\n\r\n1 rows in set. Elapsed: 5.748 sec. Processed 272.38 thousand rows, 50.75 MB (47.39 thousand rows/s., 8.83 MB/s.)\r\n\r\n```\r\n\r\nHere I would like to point out that if I do the following:\r\n\r\n```\r\nSELECT *\r\nFROM products_distributed\r\nLIMIT 100000, 1\r\n```\r\n\r\nand get the `un_id` from the result and query by that `un_id` of course this will be really fast since I guess there has sth to do with Clickhouse caching. \r\n\r\nI think the proper way to test this is to manually insert a new record with known `un_id` then insert a few millions of records and then make the query with the known `un_id`\r\n\r\nI have tried many combinations e.g. removing request time from partitioning adding organization_id as the primary key as well and different select queries including other fields as well. Nothing has worked so far when it comes to more than 200.000.000 records. \r\n\r\nI hope now it is clearer what I've been trying to achieve. I am wondering if I can achieve sth like that with Clickhouse. Are there any other recommendations?\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-03T23:00:25Z",
        "body": "It's because you partitioned your table by `organization_id`--  `PARTITION BY (organization_id, toMonday(request_time))`\r\nBut your queries does not have `organization_id` predicate.\r\n\r\nSimply change partition key to `PARTITION BY (toMonday(request_time))`\r\nor better to `PARTITION BY (toYYYYMM(request_time))`\r\n\r\nAnd you can change granularity to 256 `index_granularity=256`\r\n\r\nAlso if you use sharding you can shard by un_id and enable `optimize_skip_unused_shards`.\r\n\r\n\r\nAnd the last thing. Why do you use CH? CH is not designed for K/V. Try K/V databases. Redis, Cassandra. You will speedup your queries 1000 times with a cluster of  Cassandra."
      }
    ],
    "satisfaction_conditions": [
      "Optimized primary index structure that enables efficient filtering by un_id",
      "Effective partitioning strategy aligned with query patterns",
      "Appropriate data type optimization for UUIDs",
      "Realistic expectations for key-value workload performance at scale"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 01:02:37"
    }
  }
]