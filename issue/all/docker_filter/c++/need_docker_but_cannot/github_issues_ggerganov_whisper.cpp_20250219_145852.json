[
  {
    "number": 1301,
    "title": "still low performances with CoreML model (using ANE) compared to Vosk (Kaldi) using CPU",
    "created_at": "2023-09-17T09:19:15Z",
    "closed_at": "2023-09-18T12:22:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1301",
    "body": "Hi,\r\n\r\nusing the large CoreML encoder  provided by huggingface I still have performances very low compared to Vosk with Kaldi and I don't get why.\r\n\r\nwhen I run it:\r\n\r\nwhisper_init_state: Core ML model loaded\r\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | COREML = 1 | OPENVINO = 0 | \r\n\r\nso everything is finely set and in theory I should use ANE that should be really performant.\r\n\r\nfor converting 3h audio (16khz and 1channel) it took 1h, while with Vosk using Kaldi (I use only the CPU), same quality but it took 11min, how is it possible? Am I missing something?\r\n\r\nThank you\r\nLuca",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1301/comments",
    "author": "xcottos",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-09-17T10:08:53Z",
        "body": ">  I still have performances very low compared to Vosk with Kaldi and I don't get why.\r\n\r\nCompared to the Whisper large model, which is 2.9 GiB, the Vosk and Kaldi models are relatively small. In addition, the `KAIDI GigaSpeech ASR` models were trained on datasets that are an order of magnitude smaller than `Whisper` (10,000 hours vs 680,000 hours). If you're aiming for faster transcription speeds, you might want to consider using `Metal` instead of `CoreML` or other smaller models (eg. small, medium).\r\n\r\n> In this work we close that gap, scaling weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data. We call our approach Whisper."
      },
      {
        "user": "xcottos",
        "created_at": "2023-09-17T11:30:31Z",
        "body": "Thank you bobqianic,\r\n\r\nI can see, indeed, that the transcription with whisper is much accurate (writes numbers as numbers, the punctuation is much better etc) so I assume the better accuracy is also the result of a larger training set/model.\r\n\r\nI am studying the options I have and I would like to push whisper at its max performances on my M1 Max (64GB Ram and 32 cores GPU). Now I started studying the acceleration with my Mac and as far as I understood ANE should be even more performant compared to Metal but it's still only theory for me that's why I'm experimenting.\r\n\r\nFor using ANE the only way (as far as I could get) is using CoreML and apparently I am using it now (so in theory I am using the NPU)\r\n\r\nIs there already the support for Metal with whisper to test its performances on my GPU?\r\n\r\nThank you\r\nLuca"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-09-17T11:41:19Z",
        "body": "> Is there already the support for Metal with whisper to test its performances on my GPU?\r\n\r\nYes. See #1270"
      },
      {
        "user": "xcottos",
        "created_at": "2023-09-18T12:20:52Z",
        "body": "well... great improvement it took 20 min now with Metal  :)\r\n\r\nlarge model and 3h audio in Italian\r\n\r\nI still don't get why ANE is less performant...\r\n\r\nThanks for your suggestion bobqianic, really appreciate it\r\n\r\nLuca"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why CoreML/ANE underperforms compared to Metal/CPU alternatives",
      "Guidance on optimizing Whisper performance for Apple Silicon hardware",
      "Comparison of hardware acceleration tradeoffs (ANE vs Metal vs CPU)"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 01:20:19"
    }
  }
]