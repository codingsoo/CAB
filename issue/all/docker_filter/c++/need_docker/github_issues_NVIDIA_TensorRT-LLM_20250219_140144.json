[
  {
    "number": 1748,
    "title": "\u2018cudaStream_t\u2019 has not been declared when building tensorrt_llm v0.10.0",
    "created_at": "2024-06-06T06:09:39Z",
    "closed_at": "2024-06-06T06:29:02Z",
    "labels": [
      "question",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1748",
    "body": "### System Info\n\nCPU: INTEL\r\nGPU Name: A100-SXM4-80GB\r\nTensorRT-LLM: tag v0.10.0\r\nContainer Used: No\r\nDriver Version: 535.54.03\r\nCUDA Version: 12.1\r\nOS: Ubuntu 22.04\r\nOthers: tensorrt==10.0.1.16, pytorch==2.2.0+cu121, python==3.10.12\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\npython scripts/build_wheel.py --trt_root=/usr/local/tensorrt --clean --install --cuda_architectures=\"80-real\"\n\n### Expected behavior\n\nWorks fine\n\n### actual behavior\n\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:48:37: error: \u2018cudaStream_t\u2019 has not been declared\r\n   48 | void invokeRGLRU(lruParams& params, cudaStream_t stream);\r\n      |                                     ^~~~~~~~~~~~\r\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:51:43: error: \u2018cudaStream_t\u2019 has not been declared\r\n   51 | void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);\r\n      |                                           ^~~~~~~~~~~~\n\n### additional notes\n\nNo",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1748/comments",
    "author": "zhangts20",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:12:26Z",
        "body": "Env issue\r\nTry this container: nvcr.io/nvidia/tritonserver:24.05-trtllm-python-py3"
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:22:10Z",
        "body": "@hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks"
      },
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:29:35Z",
        "body": "> @hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks\r\n\r\nYou could build the container that come FROM this container\r\nOr use pip install tensorrt_llm==xxx in your container."
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:46:14Z",
        "body": "@hijkzzz thanks! I have installed it successfully using pip install tensorrt_llm==xxx"
      },
      {
        "user": "yorickvP",
        "created_at": "2024-06-12T13:19:32Z",
        "body": "In case anyone else wants to build from source, add `#include <cuda_runtime.h>` to `cpp/tensorrt_llm/kernels/lruKernel.h`"
      }
    ],
    "satisfaction_conditions": [
      "Resolves missing CUDA runtime declarations during compilation",
      "Provides a way to install tensorrt_llm without requiring source compilation",
      "Ensures compatibility with user's existing CUDA 12.1 and driver stack",
      "Works in user's custom Docker environment without mandatory use of NVIDIA's container",
      "Addresses header inclusion order/dependencies in TensorRT-LLM source code"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:19:39"
    }
  }
]