[
  {
    "number": 57382,
    "title": "Distributed queries is giving inconsistency output between replicas and shards",
    "created_at": "2023-11-30T10:02:08Z",
    "closed_at": "2023-12-05T12:23:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/57382",
    "body": "I have 3 shards and 2 replicas of clickhouse cluster and also zookeeper with 3 znodes for replication in my eks cluster. In that, I created a database called \"app\" with the replicated engine in all shards and replicas (i.e., in all 6 pods). After I created a mergeTree table called \"alerts_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"alerts\" for inserting and retrieving the data evenly in all pods.\r\n\r\nClickhouse config:\r\n`\r\n\r\n```\r\napiVersion: \"clickhouse.altinity.com/v1\"\r\nkind: \"ClickHouseInstallation\"\r\nmetadata:\r\n  name: \"clickhouse\"\r\nspec:\r\n  defaults:\r\n    replicasUseFQDN: \"no\"\r\n    distributedDDL:\r\n      profile: admin_profile\r\n    templates:\r\n      podTemplate: clickhouse-pod\r\n      dataVolumeClaimTemplate: clickhouse-storage\r\n      serviceTemplate: clickhouse-svc\r\n  configuration:\r\n    users:\r\n      default/networks/host_regexp: clickhouse.svc.cluster.local$\r\n      default/networks/ip:\r\n        - 127.0.0.1\r\n        - 0.0.0.0\r\n      admin/networks/host_regexp: clickhouse.svc.cluster.local$\r\n      admin/networks/ip:\r\n        - 127.0.0.1\r\n        - 0.0.0.0\r\n      admin/k8s_secret_password: clickhouse/clickhouse-operator-altinity-clickhouse-operator/password\r\n      admin/access_management: 1\r\n      admin/named_collection_control: 1\r\n      admin/show_named_collections: 1\r\n      admin/show_named_collections_secrets: 1\r\n      admin/profile: admin_profile\r\n      admin/quote: admin_quote\r\n    profiles:\r\n      admin_profile/max_memory_usage: 600000000000\r\n      admin_profile/readonly: 0\r\n      admin_profile/max_insert_threads: 32\r\n    quotas:\r\n      admin_quota/interval/duration: 3600\r\n      admin_quota/interval/queries: 0\r\n      admin_quota/interval/errors: 0\r\n      admin_quota/interval/result_rows: 0\r\n      admin_quota/interval/read_rows: 0\r\n      admin_quota/interval/execution_time: 0\r\n    settings:\r\n      disable_internal_dns_cache: 1\r\n    files:\r\n      conf.d/chop-generated-storage.xml: |\r\n        <yandex>\r\n            <storage_configuration>\r\n                <disks>\r\n                    <default>\r\n                        <metadata_path>/var/lib/clickhouse/</metadata_path>\r\n                    </default>\r\n                    <fast_ssd>\r\n                        <path>/mnt/fast_ssd/clickhouse/</path>\r\n                    </fast_ssd>\r\n                    <s3>\r\n                        <type>s3</type>\r\n                        <endpoint>S3 URL</endpoint>\r\n                        <use_environment_credentials>true</use_environment_credentials>\r\n                        <metadata_path>/var/lib/clickhouse/disks/s3/</metadata_path>\r\n                    </s3>\r\n                </disks>\r\n                <policies>\r\n                    <moving_from_hot_to_cold>\r\n                        <volumes>\r\n                            <default>\r\n                                <disk>default</disk>\r\n                            </default>\r\n                            <hot>\r\n                                <disk>fast_ssd</disk>\r\n                            </hot>\r\n                            <cold>\r\n                                <disk>s3</disk>\r\n                            </cold>\r\n                        </volumes>\r\n                    </moving_from_hot_to_cold>\r\n            \t</policies>\r\n            </storage_configuration>\r\n        </yandex>\r\n    zookeeper:\r\n      nodes:\r\n        - host: zookeeper-0.zookeepers.zoo-keeper\r\n          port: 2181\r\n        - host: zookeeper-1.zookeepers.zoo-keeper\r\n          port: 2181\r\n        - host: zookeeper-2.zookeepers.zoo-keeper\r\n          port: 2181\r\n    clusters:\r\n      - name: prod-cluster\r\n        templates:\r\n          podTemplate: clickhouse-pod\r\n          dataVolumeClaimTemplate: clickhouse-storage\r\n        layout:\r\n          shardsCount: 3\r\n          replicasCount: 2\r\n          shards:\r\n            - name: shard0\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n            - name: shard1\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n            - name: shard2\r\n              internalReplication: \"true\"\r\n              templates:\r\n                podTemplate: clickhouse-pod\r\n                dataVolumeClaimTemplate: clickhouse-storage\r\n  templates:\r\n    serviceTemplates:\r\n      - name: clickhouse-svc\r\n        spec:\r\n          ports:\r\n            - name: http\r\n              port: 8123\r\n            - name: tcp\r\n              port: 9000\r\n          externalTrafficPolicy: Cluster\r\n          type: LoadBalancer\r\n    podTemplates:\r\n      - name: clickhouse-pod\r\n        spec:\r\n          containers:\r\n            - name: clickhouse-pod\r\n              image: clickhouse/clickhouse-server:23.7\r\n              resources:\r\n                limits:\r\n                  cpu: \"3\"\r\n                  memory: 8Gi\r\n                requests:\r\n                  cpu: \"2\"\r\n                  memory: 6Gi\r\n              volumeMounts:\r\n                - name: clickhouse-storage\r\n                  mountPath: /var/lib/clickhouse\r\n    volumeClaimTemplates:\r\n      - name: clickhouse-storage\r\n        spec:\r\n          storageClassName: default\r\n          accessModes:\r\n            - ReadWriteOnce\r\n          resources:\r\n            requests:\r\n              storage: 100Gi\r\n```\r\n\r\n`\r\n              \r\n I have inserted million of data in \"alerts\" table in one month. But when I try select number of count() in alerts table where source='high', it giving inconsistency output in replicas and shards\r\n \r\n In 3 pods, chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0 it giving below output\r\n \r\n```\r\n SELECT count()\r\nFROM alerts\r\nWHERE source = 'high'\r\n\r\nQuery id: 81a6ed42-5583-4b9d-957b-9a69bccbac40\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    1389 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.232 sec. Processed 56.50 million rows, 1.02 GB (243.85 million rows/s., 4.39 GB/s.)\r\n```\r\n\r\nIn 3 pods, chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0, it giving below output\r\n\r\n```\r\nSELECT count()\r\nFROM alerts\r\nWHERE source = 'high'\r\n\r\nQuery id: 4c07a903-7ad6-4c92-bbc2-1ac86dcf8691\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    1489 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.216 sec. Processed 56.96 million rows, 1.03 GB (263.13 million rows/s., 4.74 GB/s.)\r\n```\r\n\r\n\r\nWhy there is inconsistency in output between shards and replicas? Can anyone from clickhouse help here.. what might be cause of this issue?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/57382/comments",
    "author": "Ragavendra-Vigneshwaran-R",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-11-30T17:05:50Z",
        "body": ">mergeTree table called \"alerts_storage\"\r\n\r\nMergeTree or ReplicatedMergeTree?\r\nProvide `create table alerts_storage` `create table alerts`"
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-01T05:03:39Z",
        "body": "@den-crane MergeTree Table. Identified the cause. I checked my alerts data by date wise and found out that this inconsistency in output occurs at 05/11/2023. Before 05/11/2023, the distributed data is giving correct output in both shard and replica. At 05/11/2023, I have restarted zookeeper pod to reduce its memory and cpu limit. That were problem occurs. After restarting the zookeeper, this inconsistency is starts to happen. Is there any way solve this one?"
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-03T09:43:48Z",
        "body": "Please reset your \"replicasCount: 2\" to \"replicasCount: 1\" if you use MergeTree table engine and query them from a distributed table. Multiple replicas only suitable for Replicated* tables querying from distributed tables consistently."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-03T12:53:05Z",
        "body": "@lampjian If I reset the replicasCount to 1, what will happen to the data in 2nd replicas? Will those data will be moved to shards or deleted? "
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-03T16:46:50Z",
        "body": "The data in 2nd replica still stores somewhere you can not query anymore. It seems to be deleted. You shall transfer the data to 1st replica before you remove the other replica."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-04T04:56:34Z",
        "body": "@lampjian  Okay. I have one more doubt. When during resetting the replicaCount to 1 for 2nd replicas, the shards(chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) and it's pvc storage won't be affected, right? They will remains as it is, right? Or will the pvc storage of above shards will be recreated?"
      },
      {
        "user": "lampjian",
        "created_at": "2023-12-04T14:08:13Z",
        "body": "Probably unchanged, and your local and remote S3 storage configurations are all alike. But you shall merge the data to one replica and backup data first of all, your pod instances may run on different machines after restarting the cluster. Try to test all of the operations as much as possible in a similar smaller environment."
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-04T18:03:34Z",
        "body": "@lampjian Tested in dev environment from the sctrach. Found out that this is not zookeeper problem. From my understanding, it's how the clickhouse actually works with my current settings(shardsCount - 3, replicasCount - 2). \r\n        \r\nIn dev enironment, I created a database called \"app\" with the replicated engine in all shards and replicas. After I created a mergeTree table called \"num_storage\"( which is created in other shards and replicas because of the replicated database engine) for storing the data. On top of it, I have created a distributed table called \"num\" for inserting and retrieving the data evenly.\r\n        \r\nWhen I inserted 1 to 1000 numbers into the pod chi-clickhouse-prod-cluster-shard0-0-0, the data are distributed evenly inserted across the three shards (chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) only and in its 2nd replicas which is came as independent pod, the data are not inserted.\r\n\r\n\r\n```\r\nchi-clickhouse-prod-cluster-shard0-0-0.chi-clickhouse-prod-cluster-shard0-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 997bd316-c646-49d3-8454-6fea1f7d6e5d\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     351 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard1-0-0.chi-clickhouse-prod-cluster-shard1-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: c00fc7d2-359d-462e-9251-4b953eb0698a\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     315 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard2-0-0.chi-clickhouse-prod-cluster-shard2-0.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 2fbdbe2a-3728-4d7b-a95c-34f2d3c3c8d7\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     334 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n```\r\n \r\n\r\nWhen I inserted 1 to 10 numbers into pod chi-clickhouse-prod-cluster-shard0-1-0(which is a 2nd replicas of shard0-0), it data is evenly inserted across the 2nd replicas of shards(chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0).\r\n        \r\n ```\r\nchi-clickhouse-prod-cluster-shard0-1-0.chi-clickhouse-prod-cluster-shard0-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 07eb979c-6348-4b6b-8870-c6faa60630e2\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       4 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard1-1-0.chi-clickhouse-prod-cluster-shard1-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 2022a66e-6188-4443-b36c-be685658250c\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\nchi-clickhouse-prod-cluster-shard2-1-0.chi-clickhouse-prod-cluster-shard2-1.clickhouse.svc.cluster.local :) select count() from app.num_storage;\r\n\r\nSELECT count()\r\nFROM app.num_storage\r\n\r\nQuery id: 00f53697-ae60-4e14-8c21-7f1809db002c\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.001 sec.\r\n\r\n```\r\n\r\n\r\nSo In conclusion, When we have clickhouse with 3 shards and 2 replicas count and it's use MergeTree table engine and query them from a distributed table on a replicated database, the clickhouse will consider the shards(chi-clickhouse-prod-cluster-shard0-0-0, chi-clickhouse-prod-cluster-shard1-0-0, chi-clickhouse-prod-cluster-shard2-0-0) as one cluster and its 2nd replicas which came as independent pod(chi-clickhouse-prod-cluster-shard0-1-0, chi-clickhouse-prod-cluster-shard1-1-0, chi-clickhouse-prod-cluster-shard2-1-0) as a another cluster. \r\n\r\n\r\nIs my conclusion is right? If yes, Could you please tell me what this behaviour of clickhouse is called?\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2023-12-04T18:24:56Z",
        "body": "@Ragavendra-Vigneshwaran-R \r\n\r\n>database called \"app\" with the replicated engine \r\n\r\nreplicated database does not replicate data. It synchronizes the structures of the tables.\r\nYou need to use replicated **tables**.\r\n"
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-12-05T12:23:15Z",
        "body": "Okay. I have reseted my replicasCount to 1 after migrating the data from 2nd replica to 1st replicas. Thanks @lampjian @den-crane for you clarification!!! It's really helped for doing the changes in my producation environment without any data loses. Thanks for your help."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how MergeTree tables interact with replica configurations in distributed queries",
      "Clarification of replica behavior when using non-Replicated* table engines with distributed tables",
      "Guidance on proper replication configuration for data consistency across shards and replicas",
      "Explanation of data distribution patterns between replicas in ClickHouse clusters",
      "Safe data migration strategy when changing replica counts"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:39:43"
    }
  },
  {
    "number": 56848,
    "title": "Can't connect zookeeper",
    "created_at": "2023-11-16T10:11:35Z",
    "closed_at": "2023-11-16T10:58:37Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/56848",
    "body": "Hi:\r\n\r\nI deployed clickhouse cluster of three nodes ,clickhouse  version is 23.10.3.5, my zookeeper version is 3.4.14,I used below SQL to check whether my clickhouse can connect zookeeper\r\n\r\n\r\nck1 :) SELECT * FROM system.zookeeper WHERE path = '/clickhouse';\r\n\r\nSELECT *\r\nFROM system.zookeeper\r\nWHERE path = '/clickhouse'\r\n\r\nQuery id: 2cc89062-023a-43a3-aa8a-bbf105d3dac9\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u252c\u2500path\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 task_queue \u2502       \u2502 /clickhouse \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec. \r\n\r\nDoes this result mean clickhouse cluster can't connect zookeeper\uff0cI have no idea,please help me to check\r\nthanks\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/56848/comments",
    "author": "calvin2020",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-11-16T10:58:37Z",
        "body": ">\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u252c\u2500path\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 task_queue \u2502 \u2502 /clickhouse \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nIt means your Clickhouse have  connected to Zookeeper successfully."
      },
      {
        "user": "20orange",
        "created_at": "2023-11-16T11:24:19Z",
        "body": "If your clickhouse cluster can't connect zookeeper.\r\nWhen you use the sql: \r\n\r\n> SELECT * FROM system.zookeeper WHERE path = '/clickhouse';\r\n\r\n> Received exception from server (version 23.4.2):\r\nCode: 999. DB::Exception: Received from localhost:9008. Coordination::Exception. Coordination::Exception: Connection loss, path: All connection tries failed while connecting to ZooKeeper. nodes:"
      },
      {
        "user": "calvin2020",
        "created_at": "2023-11-20T10:11:41Z",
        "body": "ok,thanks,but I can't get clickhouse cluster info after I deployed according to clickhouse official document,\r\nck1 :) select * from system.clusters;\r\n\r\nSELECT *\r\nFROM system.clusters\r\n\r\nQuery id: 401f755e-afa2-447b-8520-a258f91e045c\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.002 sec. \r\n\r\nWould you please check it again,thanks\n\n---\n\nresolve it,just ignore above question,thanks"
      }
    ],
    "satisfaction_conditions": [
      "Clarification of how to interpret the system.zookeeper query results to confirm a successful connection",
      "Identification of error patterns that indicate a failed Zookeeper connection"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:39:55"
    }
  },
  {
    "number": 56454,
    "title": "distributed engine inserts exceed memory, even if there is no limit set",
    "created_at": "2023-11-08T10:02:42Z",
    "closed_at": "2023-11-09T08:26:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/56454",
    "body": "Creating new ticket as #50744 is closed and issue is not resolved.\r\n\r\nBackground inserts into distributed tables started throwing exception:\r\n DB::Exception: Memory limit (for query) exceeded: would use 9.31 GiB (attempt to allocate chunk of 4360448 bytes), maximum: 9.31 GiB\r\n\r\nEven if i run SYSTEM FLUSH DISTRIBUTED ON CLUSTER cluster default.table, i get the same error.\r\n\r\nInserts on local node work ok. It also works ok with insert_distributed_sync=1. But as i would prefer to use async, i would like to go back to background inserts.\r\n\r\nMemory limits are the same on all nodes:\r\n```\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default\u2500\u252c\u2500value\u2500\u2510\r\n\u2502 max_memory_usage                 \u2502 0       \u2502 0     \u2502\r\n\u2502 max_memory_usage_for_user        \u2502 0       \u2502 0     \u2502\r\n\u2502 max_memory_usage_for_all_queries \u2502 0       \u2502 0     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nsystem.distribution_queue has 2 entries(1 for each node it is trying to insert to). \r\ndata_compressed_bytes: 9692170902\r\n\r\nEach shard has a queue of around 13k files, ~10G in size. Even if i leave just 1 file in the queue, it still throws memory exceeded.\r\nIf i remove the first file, i get file not found exception.\r\n\r\nHow do i tell clickhouse to not use 10G memory limit? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/56454/comments",
    "author": "Nikoslav",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-11-08T11:12:37Z",
        "body": "It's because `.bin` file stores settings which were applied during the initial insertion and the distributed table applies them during propagation of data into *MergeTree.\r\n\r\nHere is an example\r\n\r\n```\r\ncreate table T ( A Int64 ) Engine MergeTree partition by A order by A;\r\n\r\ncreate table TD as T Engine Distributed (default, currentDatabase(), T);\r\n\r\nset prefer_localhost_replica = 0;\r\nset max_partitions_per_insert_block = 1;\r\n\r\ninsert into TD select * from numbers(100);\r\n\r\nselect substr(last_exception,1, 150) from system.distribution_queue format Vertical;\r\nCode: 252. DB::Exception: Received from localhost:9000. DB::Exception: Too many partitions for single INSERT block (more than 1).\r\n```\r\n\r\nNow TD is unable to re-insert `1.bin` and it's impossible to change `1.bin` to redefine `max_partitions_per_insert_block`. \r\n\r\nyou can:\r\n\r\n* recreate table TD (drop/create and lost all not inserted data, all .bin files)\r\n* detach table, move bin files to user_files and try to read them\r\n\r\n```\r\n:) detach table TD;\r\n\r\n# cd /var/lib/clickhouse/data/default/TD/shard1_replica1/\r\n\r\n# mv *.bin /var/lib/clickhouse/user_files/\r\n\r\n:) attach table TD;\r\n\r\n-- data is accessible using `Distributed` format\r\n:) select * from file('*.bin', Distributed) limit 3\r\n\u250c\u2500A\u2500\u2510\r\n\u2502 0 \u2502\r\n\u2502 1 \u2502\r\n\u2502 2 \u2502\r\n\u2514\u2500\u2500\u2500\u2518\r\n\r\n:) select count() from file('*.bin', Distributed);\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n:) insert into T select * from file('*.bin', Distributed);\r\n\r\n:) select count() from T;\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502     100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n# rm /var/lib/clickhouse/user_files/*.bin\r\n```\n\n---\n\nAlso you can do this\r\n\r\n```\r\ncreate table TDNew as TD Engine Distributed (default, currentDatabase(), T);\r\nexchange tables TDNew and TD;\r\n```\r\nthen not inserted .bin files will be in TDNew (/var/lib/clickhouse/data/default/TDNew/shard1_replica1/)"
      },
      {
        "user": "Nikoslav",
        "created_at": "2023-11-09T08:26:52Z",
        "body": "Thanks a lot! Detach, move files, attach and insert worked perfectly."
      }
    ],
    "satisfaction_conditions": [
      "Solution must resolve memory limit errors during asynchronous distributed inserts without requiring sync mode",
      "Must address handling of stuck .bin files retaining original insertion settings",
      "Must provide a way to recover queued data without loss",
      "Should explain how to prevent settings persistence in distributed queue files",
      "Must work without requiring permanent memory limit increases"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:40:44"
    }
  },
  {
    "number": 52620,
    "title": "(branch 23.5) `Looking for mutations for part` appears in client with `send_logs_level=debug`",
    "created_at": "2023-07-26T12:50:22Z",
    "closed_at": "2023-07-26T12:56:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/52620",
    "body": "```sql\r\nALTER TABLE tableA MATERIALIZE INDEX ginidx;\r\n\r\nSET send_logs_level=debug;\r\n\r\nINSERT INTO tableB SELECT * FROM  tableA LIMIT 1000000\r\n\r\nQuery id: 865a8654-159d-40a3-ada1-90febde40416\r\n\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.162059 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> executeQuery: Query span trace_id for opentelemetry log: 00000000-0000-0000-0000-000000000000\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.163246 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> executeQuery: (from 127.0.0.1:57642, user: operator) INSERT INTO tableB SELECT * FROM  tableA LIMIT 1000000; (stage: Complete)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167003 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 00538dfb52fdf41c1a36ff5f8a9b3f86_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167077 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0298f752bf557334fc80759843abe567_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167104 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0413d695373e7a54f73ae0886afed664_0_0_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167272 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0413d695373e7a54f73ae0886afed664_1_1_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167298 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 04ccd2b5bf76dec05640587b037abdcf_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167367 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 0816cf91eb7efa18de40dcdb1c8ed388_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167410 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 093315f287eafd6f14001e3a1f1ae873_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167481 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 098efa9cec0b236ab538700d75525ef8_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 20:39:06.167514 [ 380 ] {865a8654-159d-40a3-ada1-90febde40416} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part 09d7a7a3a5c5c0cb9867f4c2d26b760b_0_0_0_5 (part data version 5, part metadata version 2)\r\n...(repeated logs)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/52620/comments",
    "author": "cangyin",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T12:56:44Z",
        "body": "As expected"
      },
      {
        "user": "cangyin",
        "created_at": "2023-07-26T12:59:13Z",
        "body": "@tavplubix  please explain a bit more"
      },
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T13:00:27Z",
        "body": "@cangyin, please explain what exactly was unexpected for you"
      },
      {
        "user": "cangyin",
        "created_at": "2023-07-26T13:10:52Z",
        "body": "I'm just surprised at first glance. This becomes reasonable for me now.\n\n---\n\nBut the same batch of  log `Looking for mutations for part ...` keeps showing if I repeat my query\r\n```sql\r\nSELECT * FROM  tableA \r\n```\r\n\r\nIs this also expected?\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T13:44:50Z",
        "body": "Yes, because some unfinished mutations have to be applied when reading (for example, if data type of some column was altered)"
      },
      {
        "user": "cangyin",
        "created_at": "2023-07-26T13:54:17Z",
        "body": "Nop, there is no unfinished merge/mutations\r\n```sql\r\nSELECT create_time,database,table,command,mutation_id,parts_to_do,parts_to_do_names,is_done,latest_fail_reason FROM system.mutations WHERE is_done=0\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.011 sec.\r\n\r\nSELECT * FROM  tableA\r\n\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.864773 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part e9b788f63533f3ad7f314f7a2d25d601_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.864891 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part eb5ee5089fab350478a03192fd3d224c_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.864959 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part ec919c9b451468ea6fab9ae43edc9f82_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865020 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part ee342e7e28f7515269f147012d5125b1_0_0_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865111 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part ee342e7e28f7515269f147012d5125b1_1_1_0_6 (part data version 6, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865215 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part eeb786a6f9edf84e9db8b6ee168f750d_0_0_0_5 (part data version 5, part metadata version 2)\r\n[chi-datalake-ck-cluster-0-0-0] 2023.07.26 21:44:13.865311 [ 380 ] {911a5153-3d75-46e2-b973-89c273d8e0ba} <Debug> business.network_security_log_local (ReplicatedMergeTreeQueue): Looking for mutations for part efd25dbd5fad5eaf16ba3e577063d29a_0_0_0_6 (part data version 6, part metadata version 2)\r\n...\r\n```\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2023-07-26T14:02:52Z",
        "body": "But SELECT queries need to check if there are some mutations to apply. Even if there are no unfinished mutations, a SELECT query doesn't know that without checking."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why SELECT queries trigger 'Looking for mutations' logs even when system.mutations shows no pending mutations",
      "Description of normal vs abnormal mutation check patterns in debug logs",
      "Explanation of mutation check triggers beyond explicit ALTER operations"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:41:14"
    }
  },
  {
    "number": 51474,
    "title": "Cannot allocate RAFT instance due to \"Address family not supported by protocol\" (IPv6 disabled)",
    "created_at": "2023-06-27T11:09:14Z",
    "closed_at": "2023-07-10T09:44:41Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51474",
    "body": "\r\nIt's look like that resolving of issue #33381 hasn't fixed the problem with ClickHouse-Keeper on machines with disabled IPv6:\r\n\r\nIn /var/log/clickhouse-server/clickhouse-server.log we have:\r\n\r\n2023.06.27 13:47:36.661896 [ 113898 ] {} <Error> RaftInstance: got exception: open: Address family not supported by protocol [system:97] on port 9234\r\n2023.06.27 13:47:36.661947 [ 113898 ] {} <Debug> KeeperDispatcher: Server initialized, waiting for quorum\r\n2023.06.27 13:50:36.662692 [ 113898 ] {} <Error> void DB::KeeperDispatcher::initialize(const Poco::Util::AbstractConfiguration &, bool, bool): Code: 568. DB::Exception: Failed to wait RAFT initialization. (RAFT_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa3ec73a in /usr/bin/clickhouse\r\n1. DB::KeeperServer::waitInit() @ 0x16062e35 in /usr/bin/clickhouse\r\n2. DB::KeeperDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool, bool) @ 0x1604df4a in /usr/bin/clickhouse\r\n3. DB::Context::initializeKeeperDispatcher(bool) const @ 0x14875d4d in /usr/bin/clickhouse\r\n4. DB::Server::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa48d880 in /usr/bin/clickhouse\r\n5. Poco::Util::Application::run() @ 0x189a11a6 in /usr/bin/clickhouse\r\n6. DB::Server::run() @ 0xa47e8fe in /usr/bin/clickhouse\r\n7. mainEntryClickHouseServer(int, char**) @ 0xa47be97 in /usr/bin/clickhouse\r\n8. main @ 0xa3da86b in /usr/bin/clickhouse\r\n9. __libc_start_main @ 0x3ad85 in /usr/lib64/libc-2.28.so\r\n10. _start @ 0xa19a46e in /usr/bin/clickhouse\r\n (version 22.8.6.71 (official build))\r\n\r\n...and no listening ports from ClickHouse...\r\nss -tunlp\r\nNetid                 State                  Recv-Q                 Send-Q                                 Local Address:Port                                  Peer Address:Port                Process                \r\nudp                   UNCONN                 0                      0                                            0.0.0.0:111                                        0.0.0.0:*                                          \r\nudp                   UNCONN                 0                      0                                          127.0.0.1:323                                        0.0.0.0:*                                          \r\nudp                   UNCONN                 0                      0                                            0.0.0.0:45755                                      0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:10050                                      0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:111                                        0.0.0.0:*                                          \r\ntcp                   LISTEN                 0                      128                                          0.0.0.0:22                                         0.0.0.0:*  \r\n\r\nThere are no uncommented IPv6 wildcards in config.xml, config.d configs and only one default user. To config.xml we added only:\r\n<listen_host>0.0.0.0</listen_host>\r\n<interserver_listen_host>0.0.0.0</interserver_listen_host>\r\nEverything else is default after ClickHouse installed from rpm.\r\n\r\nClickHouse release: 22.8.6.71-lts\r\nOS: RHEL 8\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51474/comments",
    "author": "SavelyevPavel",
    "comments": [
      {
        "user": "antonio2368",
        "created_at": "2023-06-28T12:27:06Z",
        "body": "Can you try setting `keeper_server.enable_ipv6` to `false` in your XML?\r\n`interserver_listen_host` support in Keeper is in 22.9+."
      },
      {
        "user": "SavelyevPavel",
        "created_at": "2023-07-10T09:02:47Z",
        "body": "Yes, `keeper_server.enable_ipv6` works! Thank you, @antonio2368! "
      }
    ],
    "satisfaction_conditions": [
      "Disable IPv6 usage specifically for ClickHouse-Keeper component",
      "Provide configuration that works without interserver_listen_host support in Keeper for versions <22.9",
      "Ensure RAFT initialization can complete in IPv4-only environments"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:41:46"
    }
  },
  {
    "number": 51401,
    "title": "DateTime Default value on missing/empty key/value pair.",
    "created_at": "2023-06-26T06:08:50Z",
    "closed_at": "2023-06-26T13:33:00Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51401",
    "body": "\r\n**Describe what's wrong**\r\n\r\nA DateTime column with Default expr as now() does not populate value from now if the value being inserted is empty or if the column is missing in the data source.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes. 23.2.2.20.\r\n\r\n**How to reproduce**\r\n\r\n* 23.2.2.20\r\n* clickhouse-client\r\n* Non-default settings: NA\r\n*\r\n\r\n```\r\nCREATE TABLE default.data_set\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Memory\r\n```\r\n\r\n\r\n```\r\nCREATE TABLE default.data_set_kafka\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Kafka\r\nSETTINGS kafka_broker_list = 'kafka-host:9092', kafka_topic_list = 'topic-name', kafka_group_name = 'kafka-group-name', kafka_format = 'JSONEachRow', kafka_skip_broken_messages = 10\r\n\r\n\r\nor you can create a regular table just for testing.\r\n\r\nCREATE TABLE default.data_set_kafka\r\n(\r\n    `createdAt` DateTime DEFAULT now()\r\n)\r\nENGINE = Memory\r\n```\r\n\r\n\r\n```\r\nCREATE MATERIALIZED VIEW default.data_set_mv TO default.data_set\r\n(\r\n    `createdAt` DateTime\r\n) AS\r\nSELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\nFROM default.data_set_kafka\r\n\r\n\r\nor even if you use just select without multiif, it does not work\r\n\r\n\r\nCREATE MATERIALIZED VIEW default.data_set_mv TO default.data_set\r\n(\r\n    `createdAt` DateTime\r\n) AS\r\nSELECT createdAt FROM default.data_set_kafka\r\n```\r\n\r\n\r\nFor regular kafka table (memory engine)\r\n```\r\nINSERT INTO default.data_set_kafka values ('2023-06-07')\r\nINSERT INTO default.data_set_kafka values (NULL)\r\nINSERT INTO default.data_set_kafka values ()\r\n```\r\n\r\nFor insertion using kcat command.\r\n```\r\n{}\r\n{\"createdAt\":\"\"}\r\n{\"createdAt\":\"2023-06-29\"}\r\n```\r\n\r\n\r\n* select * from default.data_set\r\n\r\n**Expected behavior**\r\n\r\nWe expect values to have the result of now() for missing column or empty value for the createdAt field. Explicitly specifying NULL works, however other options do not work.\r\n\r\n**Error message and/or stacktrace**\r\nNo error messages.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51401/comments",
    "author": "ilugid",
    "comments": [
      {
        "user": "UnamedRus",
        "created_at": "2023-06-26T11:06:08Z",
        "body": "> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\n\r\nBy default fields are non nullable\r\n\r\n> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt)\r\n\r\nIn MV you should match not positions of columns, but their names\r\n\r\nIE \r\n\r\n> SELECT multiIf(createdAt = NULL, now(), createdAt = 0, now(), createdAt) as createdAt"
      },
      {
        "user": "ilugid",
        "created_at": "2023-06-26T18:36:40Z",
        "body": "That worked. Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Handles non-nullable fields when applying default values",
      "Ensures materialized view column name matching",
      "Applies default values for missing/empty fields in data ingestion pipelines",
      "Works with Kafka engine tables and JSON data formats"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:41:58"
    }
  },
  {
    "number": 46757,
    "title": "Join in materialized view is 60x slower than in select statement.",
    "created_at": "2023-02-23T02:52:11Z",
    "closed_at": "2023-02-23T04:27:49Z",
    "labels": [
      "question",
      "comp-joins",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46757",
    "body": "**Describe the situation**\r\n\r\nSeems materialized view with join not working with **FillRightFirst** plan, please loot at the script blew.\r\n\r\nThank you for your time. \r\n\r\nSelect statement statistics: \r\n`0 rows in set. Elapsed: 0.021 sec. Processed 262.02 thousand rows, 2.10 MB (12.69 million rows/s., 101.50 MB/s.)`\r\n\r\nMaterialized view statistics(a insert statement to source table):\r\n\r\n`1 row in set. Elapsed: 1.422 sec.`\r\n\r\n**How to reproduce**\r\n1. Create null table t_null;\r\n2. Create mergetree table t_numbers with 100000000 rows;\r\n3. Execute Select Statement 1 as blow code;\r\n4. Create materialized view with the same query;\r\n5. Insert only 1 record into t_null to trigger materialized view;\r\n\r\n\r\n* Which ClickHouse server version to use\r\nClickHouse client version 22.8.13.20 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 22.8.13 revision 54460.\r\n\r\n* Non-default settings, if any\r\nNone\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n\r\n`CREATE TABLE t_numbers\r\n(\r\n    `A` UInt64,\r\n    `B` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY A;`\r\n\r\n `CREATE TABLE t_null\r\n(\r\n    `A` UInt64,\r\n    `B` String\r\n)\r\nENGINE = Null;`\r\n\r\n`CREATE MATERIALIZED VIEW mv_null\r\nENGINE = MergeTree\r\nORDER BY A AS\r\nSELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A;`\r\n\r\n* Sample data for all these tables\r\n\r\n`INSERT INTO t_numbers SELECT\r\n    number,\r\n    toString(cityHash64(number))\r\nFROM numbers(100000000)`\r\n\r\n* Queries to run that lead to slow performance\r\n\r\nQuery 1, which runs very fast!\r\n\r\n`SELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A;\r\n`\r\nQuery 2, which runs very slow!\r\n\r\n`INSERT INTO t_null VALUES(1, 'insert');`\r\n \r\n\r\n**Expected performance**\r\nStatement in materialized view runs as fast as select statement, instead of 67x slow.\r\n\r\n**Additional context**\r\n\r\nIf we check the plan, clickhouse choose FillRightFirst for both inner and outer join, may be materialized view is not using the same plan?\r\n\r\nEXPLAIN\r\nSELECT\r\n    null_outer.A,\r\n    concat(null_outer.B, subquery.B)\r\nFROM t_null AS null_outer\r\nLEFT JOIN\r\n(\r\n    SELECT\r\n        num.A,\r\n        'inner' AS B\r\n    FROM t_numbers AS num\r\n    INNER JOIN t_null AS null_inner ON num.A = null_inner.A\r\n) AS subquery ON null_outer.A = subquery.A\r\n\r\nQuery id: 0e4c95b7-204e-4922-8d01-414f6ad86758\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Expression ((Projection + Before ORDER BY))                                                      \u2502\r\n\u2502   Join (JOIN **FillRightFirst**)                                                                     \u2502\r\n\u2502     Expression (Before JOIN)                                                                     \u2502\r\n\u2502       ReadFromStorage (Null)                                                                     \u2502\r\n\u2502     Expression ((Joined actions + (Rename joined columns + (Projection + Before ORDER BY))))     \u2502\r\n\u2502       Join (JOIN **FillRightFirst**)                                                                 \u2502\r\n\u2502         Expression (Before JOIN)                                                                 \u2502\r\n\u2502           ReadFromMergeTree (rtd.t_numbers)                                                      \u2502\r\n\u2502         Expression ((Joined actions + (Rename joined columns + (Projection + Before ORDER BY)))) \u2502\r\n\u2502           ReadFromStorage (Null)                                                                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46757/comments",
    "author": "qiang5714",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-02-23T04:27:14Z",
        "body": "You compare incorrectly.\r\n\r\nt_null has ENGINE = Null, so it's empty in the SELECT and not empty in the MATVIEW!!!\r\n\r\n```sql\r\nCREATE TABLE t_Memory ( A UInt64,B String ) ENGINE = Memory;\r\nINSERT INTO t_Memory VALUES(1, 'insert');\r\n\r\nSELECT null_outer.A, concat(null_outer.B, subquery.B) \r\n  FROM t_Memory AS null_outer LEFT JOIN \r\n ( SELECT num.A, 'inner' AS B FROM t_numbers AS num \r\n   INNER JOIN t_Memory AS null_inner ON num.A = null_inner.A ) AS subquery ON null_outer.A = subquery.A; \r\n\r\n1 row in set. Elapsed: 0.406 sec. Processed 100.00 million rows, 800.00 MB (246.21 million rows/s., 1.97 GB/s.)\r\n```\n\n---\n\nYou can use this \r\n\r\n```sql\r\nCREATE MATERIALIZED VIEW mv_null ENGINE = MergeTree ORDER BY A \r\nAS SELECT null_outer.A, concat(null_outer.B, subquery.B) \r\n   FROM t_null AS null_outer \r\n   LEFT JOIN ( \r\n               SELECT A, 'inner' AS B FROM t_numbers where A in (select A from t_null)    \r\n             ) AS subquery ON null_outer.A = subquery.A;\r\n\r\nINSERT INTO t_null VALUES(1, 'insert');\r\n\r\n1 row in set. Elapsed: 0.021 sec.\r\n\r\nINSERT INTO t_null VALUES(1, 'insert');\r\n\r\n1 row in set. Elapsed: 0.014 sec.\r\n```"
      },
      {
        "user": "qiang5714",
        "created_at": "2023-02-23T06:37:49Z",
        "body": "@den-crane \r\nThank you for you time. I understand that t_null is not null in materialized view, I did this on purpose because I want to find a way to trigger a materialized view from another table except the left most one in the Join.\r\n\r\nSuppose we have two tables, t_left and t_right, a materialized view will not trigger by t_right if we use \r\n\r\n`t_left join t_right on t_left.id = t_right.id`\r\n\r\nbut, if my program can collect both t_left.id and t_right.id,  and we can insert all id into a t_null to do this, as we change the view to \r\n\r\n`t_null left join (t_left join t_right on t_left.id = t_right.id) as t_inner on t_null.id = t_inner.id `\r\n\r\nand do this:\r\n\r\n`insert into t_null values(t_left.id)(t_right.id);`\r\n\r\nthen all data changed in t_left and t_right will trigger the materialized view\u3002 And, If t_left and t_right is very large, we should use t_null as a filter, put it in the right side, that should make t_left join t_right very fast\uff0c that's what I want.  just like what your advice did:\r\n\r\n`t_null left join (t_left join t_right on t_left.id = t_right.id and **t_right.id in (select trigger.id from t_null trigger)**) as t_inner on t_null.id = t_inner.id `\r\n\r\nMy question is, Why my query is slow than yours, or **why IN operator is faster than join**?\r\n\r\nThank you."
      },
      {
        "user": "den-crane",
        "created_at": "2023-02-23T13:36:24Z",
        "body": "Clickhouse inner join has an optimization to join with empty tables:\r\n\r\n```sql\r\nCREATE TABLE left ( A UInt64,B String ) ENGINE = MergeTree ORDER BY A as \r\nSELECT number, toString(cityHash64(number)) FROM numbers(100000000);\r\n\r\nCREATE TABLE right ( A UInt64, B String ) ENGINE = MergeTree ORDER BY A;\r\n\r\nselect count() from left inner join right using A;\r\n\r\n1 row in set. Elapsed: 0.010 sec. Processed 261.64 thousand rows, 2.09 MB (27.24 million rows/s., 217.93 MB/s.)\r\n\r\n\r\ninsert  into right SELECT number, toString(cityHash64(number)) FROM numbers(1);\r\n\r\nselect count() from left inner join right using A;\r\n1 row in set. Elapsed: 0.367 sec. Processed 100.00 million rows, 800.00 MB (272.51 million rows/s., 2.18 GB/s.)\r\n```\r\nwhat's why SELECT with join with engine Null is fast.\r\n\r\n---------------------\r\nClickhouse `JOIN` does not use indexes, Clickhouse `IN` uses indexes.\r\n\r\n```sql\r\nselect count() from left inner join right using A;\r\n1 row in set. Elapsed: 0.367 sec. Processed 100.00 million rows, 800.00 MB (272.56 million rows/s., 2.18 GB/s.)\r\n\r\nselect count() from left inner join right using B;\r\n1 row in set. Elapsed: 1.030 sec. Processed 100.00 million rows, 2.84 GB (97.12 million rows/s., 2.76 GB/s.)\r\n\r\nselect count() from left where A in (select A from right);\r\n1 row in set. Elapsed: 0.010 sec. Processed 8.19 thousand rows, 65.54 KB (856.87 thousand rows/s., 6.85 MB/s.)\r\n\r\nselect count() from left where B in (select B from right);\r\n1 row in set. Elapsed: 0.932 sec. Processed 100.00 million rows, 2.84 GB (107.29 million rows/s., 3.05 GB/s.)\r\n```\r\n`where A in (` is fast because it uses primary index of the `left`\r\n"
      },
      {
        "user": "qiang5714",
        "created_at": "2023-02-24T02:23:46Z",
        "body": "Got it, no more questions. Thank you very much!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why JOIN performance differs between SELECT queries and materialized view processing",
      "Guidance on achieving efficient filtering in materialized view joins equivalent to IN clause performance",
      "Comparison of index usage between JOIN and IN operations in ClickHouse"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:42:43"
    }
  },
  {
    "number": 41714,
    "title": "When I bulk insert the replica table, all the data in the table becomes empty",
    "created_at": "2022-09-23T07:51:23Z",
    "closed_at": "2022-09-23T14:56:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41714",
    "body": "**Describe what's wrong**\r\n\r\n> When I bulk insert the replica table, all the data in the table becomes empty\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\nclickhouse-version: **v22.6.3.35-stable**\r\nzookeeper-version: **3.4.9**\r\n\r\n* Firstly, I create a replica table\r\n```sql\r\nCREATE TABLE IF NOT EXISTS test_db.test1 (\r\n    `token_id` String,\r\n    `event_id` String,\r\n    `login_id` String,\r\n    `distinct_id` String,\r\n    `ctime` DateTime64(3),\r\n    `utime` DateTime64(3),\r\n    `cdate` String,\r\n    `udate` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/test_db/test1', '{replica}')\r\nORDER BY ( distinct_id, login_id, utime, token_id)\r\nPARTITION BY (toYYYYMMDD(utime))\r\nTTL toDateTime(udate) + toIntervalMonth(4)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n* Then I batched 3 times, each batch of 40,000 pieces of data, a total of 120,000 pieces of data were inserted,\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 8b2071f7-7a50-49e2-a5fc-6dc18e46da07\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502  120000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.009 sec.\r\n```\r\n\r\n* Then I inserted 40,000 pieces of data in batches, each batch of 10,000 pieces of data, a total of 4 batches. During the insert process, I keep querying and find that the data has been decreasing\r\n\r\n> **The data inserted above, each batch has more than 100 partitions of data;**\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 174d0072-f9c9-44ea-b8c5-bff5d2527dc4\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502   22993 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.014 sec. \r\n\r\n```\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: 978eb77c-b517-4296-b84e-f6955efbec03\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502    8038 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.007 sec\r\n\r\n```\r\n\r\n```\r\nSELECT count()\r\nFROM test_db.test1\r\n\r\nQuery id: dd5287da-fb75-42f3-b455-225bbfa05803\r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502   10000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.012 sec\r\n\r\n```\r\n* In the end I only get 10000 pieces of data, but I want to get 160000. I don't know why the data is lost, I checked the logs, the local files are all cleaned up\r\n  \r\n```\r\n2022.09.23 15:36:55.302288 [ 16832 ] {} <Trace> test_db.test1 (ReplicatedMergeTreeCleanupThread): Cleared 196 old blocks from ZooKeeper\r\n2022.09.23 15:36:55.302539 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220725_1_6_1\r\n2022.09.23 15:36:55.302574 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220725_1_6_1\r\n2022.09.23 15:36:55.313579 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220726_1_6_1\r\n2022.09.23 15:36:55.313632 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220726_1_6_1\r\n2022.09.23 15:36:55.315411 [ 16720 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000891 - log-0000000891\r\n2022.09.23 15:36:55.319817 [ 16720 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2022.09.23 15:36:55.321111 [ 16663 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Executing DROP_RANGE 20220725_1_6_1\r\n2022.09.23 15:36:55.321264 [ 16663 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Removed 0 entries from queue. Waiting for 0 entries that are currently executing.\r\n2022.09.23 15:36:55.321386 [ 16663 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing parts.\r\n2022.09.23 15:36:55.325987 [ 16663 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 7 parts inside 20220725_1_6_1.\r\n2022.09.23 15:36:55.332289 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220727_1_6_1\r\n2022.09.23 15:36:55.332332 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220727_1_6_1\r\n2022.09.23 15:36:55.334647 [ 16734 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000892 - log-0000000892\r\n2022.09.23 15:36:55.336885 [ 16734 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2022.09.23 15:36:55.337758 [ 16691 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Executing DROP_RANGE 20220726_1_6_1\r\n2022.09.23 15:36:55.337782 [ 16691 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Removed 0 entries from queue. Waiting for 0 entries that are currently executing.\r\n2022.09.23 15:36:55.337837 [ 16691 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing parts.\r\n2022.09.23 15:36:55.342937 [ 16691 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 7 parts inside 20220726_1_6_1.\r\n2022.09.23 15:36:55.348930 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will drop empty part 20220728_1_6_1\r\n2022.09.23 15:36:55.348958 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Will try to insert a log entry to DROP_RANGE for part 20220728_1_6_1\r\n...\r\n...\r\n2022.09.23 15:36:57.503607 [ 16832 ] {} <Trace> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Found 560 old parts to remove.\r\n2022.09.23 15:36:57.503822 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing 560 old parts from ZooKeeper\r\n2022.09.23 15:36:57.511469 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_1_1_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512061 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_1_6_1 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512068 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_2_2_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512073 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_3_3_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512078 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_4_4_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512082 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_5_5_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512085 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220725_6_6_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512091 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_1_1_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512096 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_1_6_1 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512100 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_2_2_0 in ZooKeeper, it was only in filesystem\r\n2022.09.23 15:36:57.512104 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): There is no part 20220726_3_3_0 in ZooKeeper, it was only in filesystem\r\n...\r\n...\r\n2022.09.23 15:36:57.535842 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 560 old parts from ZooKeeper. Removing them from filesystem.\r\n2022.09.23 15:36:57.536166 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_1_6_1\r\n2022.09.23 15:36:57.536167 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_1_1_0\r\n2022.09.23 15:36:57.536836 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_2_2_0\r\n2022.09.23 15:36:57.537405 [ 16780 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000971 - log-0000000971\r\n2022.09.23 15:36:57.537988 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_3_3_0\r\n2022.09.23 15:36:57.538036 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_4_4_0\r\n2022.09.23 15:36:57.538724 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_5_5_0\r\n2022.09.23 15:36:57.538762 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220725_6_6_0\r\n2022.09.23 15:36:57.539189 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_1_1_0\r\n2022.09.23 15:36:57.539742 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_1_6_1\r\n2022.09.23 15:36:57.540188 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_2_2_0\r\n2022.09.23 15:36:57.540191 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20220726_3_3_0\r\n...\r\n2022.09.23 15:36:57.702064 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_3_3_0\r\n2022.09.23 15:36:57.702507 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_4_4_0\r\n2022.09.23 15:36:57.702957 [ 20682 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_5_5_0\r\n2022.09.23 15:36:57.702959 [ 16871 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removing part from filesystem 20221012_6_6_0\r\n2022.09.23 15:36:57.704515 [ 16832 ] {} <Debug> test_db.test1 (4352a7d2-72dd-45c6-992a-e55542fd1e98): Removed 560 old parts\r\n2022.09.23 15:36:57.721899 [ 16832 ] {} <Debug> test_db.test1 (ReplicatedMergeTreeCleanupThread): Removed 81 old log entries: log-0000000881 - log-0000000961\r\n2022.09.23 15:36:57.724084 [ 16832 ] {} <Trace> test_db.test1 (ReplicatedMergeTreeCleanupThread): Execution took 221 ms.\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41714/comments",
    "author": "JohnZp",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-09-23T10:55:48Z",
        "body": "> TTL toDateTime(udate) + toIntervalMonth(4)\r\n\r\nI suppose that's the answer "
      },
      {
        "user": "JohnZp",
        "created_at": "2022-09-23T13:44:08Z",
        "body": "> > TTL toDateTime(udate) + toIntervalMonth(4)\r\n> \r\n> I suppose that's the answer\r\n\r\nI can guarantee that the expiration time is not reached"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T14:12:14Z",
        "body": "@JohnZp check server time `select now()`,\r\nremove TTL and check data insertion without TTL."
      },
      {
        "user": "tavplubix",
        "created_at": "2022-09-23T14:12:58Z",
        "body": "> I can guarantee that the expiration time is not reached\r\n\r\nPlease share a reproducible example then, so we will check if there's a bug in TTL"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T14:15:44Z",
        "body": ">`udate` String\r\n>TTL toDateTime(udate)\r\n\r\nAlso probably your String contains something which is evaluated to `1970-01-01` with toDateTime.\r\n\r\n```\r\nselect toDateTime('haba-01-01 01:21:33');\r\n\u250c\u2500toDateTime('haba-01-01 01:21:33')\u2500\u2510\r\n\u2502               1970-01-01 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "JohnZp",
        "created_at": "2022-09-23T14:56:42Z",
        "body": "Oh! you are right! It is indeed caused by TTL. I used toDatetime('yyyyMMdd'), and then all were converted to 1970-xx-xx\r\n```\r\nSELECT toDateTime('20220701')\r\n\r\nQuery id: ae080147-81c5-474b-8cfa-50219519cd8b\r\n\r\n\u250c\u2500toDateTime('20220701')\u2500\u2510\r\n\u2502    1970-08-23 08:51:41 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.002 sec.\r\n```\r\n\r\n**Thank you very very very  much!!**  @den-crane @tavplubix \r\n\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-23T21:35:03Z",
        "body": "```sql\r\nSELECT parseDateTimeBestEffort('20220701')\r\n\r\n\r\n\u250c\u2500parseDateTimeBestEffort('20220701')\u2500\u2510\r\n\u2502                 2022-07-01 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how TTL configuration interacts with date/time column parsing",
      "Clarification of ClickHouse's date/time parsing behavior for string columns",
      "Guidance on proper TTL configuration for custom date formats",
      "Identification of data validation techniques for TTL policies"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:44:36"
    }
  },
  {
    "number": 39549,
    "title": "TCPHandlerFactory creating TCPhandler is slow. Takes 3 sec within intranet",
    "created_at": "2022-07-25T09:17:05Z",
    "closed_at": "2022-07-25T11:10:33Z",
    "labels": [
      "question",
      "obsolete-version"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39549",
    "body": "checked log file, /var/log/clickhouse-server/clickhouse-server.log, I got logs below,\r\n\r\n2022.07.25 17:02:23.535994 [ 12473 ] {} <Trace> TCPHandlerFactory: TCP Request. Address: [::ffff:172.22.254.xx]:57308\r\n2022.07.25 17:02:26.554058 [ 12473 ] {} <Debug> TCPHandler: Connected ClickHouse client version 1.1.0, revision: 54380, database: xxxxx, user: xxxx.\r\n\r\nIt takes 3 sec to create a TCP connection in my opion,\r\n\r\nAlready done below,\r\n1. Ping is OK, stable and fast. Intranet.\r\n2. It returns the correct results, just slow.\r\n3. ClickHouse Server version: 20.8.3 revision 54438\r\n4. Stop the firewall on both machines.\r\n5. No error logs in /var/log/clickhouse-server/clickhouse-server.err.log\r\n\r\nAny suggestion will be grateful. \r\n\r\nWarm Regards,\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39549/comments",
    "author": "daggerin3",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-07-25T11:12:02Z",
        "body": "> ClickHouse Server version: 20.8.3\r\n\r\nUpgrade to supported version\n\n---\n\n> ClickHouse client version 1.1.0\r\n\r\nAnd upgrade client as well (versions 1.x are almost 4 years old)"
      },
      {
        "user": "den-crane",
        "created_at": "2022-07-25T12:51:59Z",
        "body": "Looks like a DNS timeout / issue."
      },
      {
        "user": "daggerin3",
        "created_at": "2022-07-26T01:58:02Z",
        "body": "> Looks like a DNS timeout / issue.\r\n\r\nThanks a lot. Problem solved. 1st DNS server is unreachable."
      }
    ],
    "satisfaction_conditions": [
      "Identify network-related delays during TCP connection setup",
      "Address DNS resolution inefficiencies",
      "Consider environment-specific network configurations"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:45:51"
    }
  },
  {
    "number": 35342,
    "title": "AggregateFunction is not backward compatible",
    "created_at": "2022-03-16T18:32:44Z",
    "closed_at": "2022-03-16T20:46:45Z",
    "labels": [
      "question",
      "backward compatibility",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35342",
    "body": "**Describe the issue**\r\nWe were upgrading our Clickhouse cluster from **21.8.3.44** to **22.1.3.7** and found a lot of our queries are failing due to an error explained below \r\n\r\n**How to reproduce**\r\nCreate table 1\r\n```\r\nCREATE TABLE test.table_1 on cluster '{cluster}'\r\n(\r\n  `date` Date,\r\n  `uniques` AggregateFunction(uniqCombined64(17), Nullable(String))\r\n)\r\nENGINE = MergeTree()\r\nORDER BY date\r\n```\r\n\r\nCreate table 2\r\n```\r\nCREATE TABLE test.table_2 on cluster '{cluster}'\r\n(\r\n  `date` Date,\r\n  `id`  Nullable(String)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY date\r\n```\r\n\r\nInsert data in table 2\r\n```\r\nINSERT INTO test.table_2 (*) VALUES ('2022-04-01', null), ('2022-04-01', '1'), ('2022-04-01', '2'), ('2022-04-01', '3'), ('2022-04-01', '3'), ('2022-04-01', '4'), ('2022-04-01', '5');\r\n```\r\n\r\nInsert data into table 1 using table 1 data\r\n```\r\nINSERT INTO test.table_1 \r\nSELECT\r\n  date,\r\n  uniqCombined64State(17)(id)\r\nFROM test.table_2\r\nGROUP BY date\r\n```\r\n\r\nRun aggregate query on table 1\r\n```\r\nSELECT\r\n  date,\r\n  coalesce(uniqCombined64Merge(uniques), 0) AS uniques\r\nFROM test.table_1\r\nGROUP BY date \r\n```\r\n\r\nVersion **21.8.3.44** gives result\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500uniques\u2500\u2510\r\n\u2502 2022-04-01 \u2502       5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n**Error message and/or stacktrace**\r\nVersion **22.1.3.7** throws error\r\n```\r\nCode: 43. DB::Exception: Received from localhost:9000. DB::Exception: Illegal type AggregateFunction(uniqCombined64(17), Nullable(String)) of argument for aggregate function uniqCombined64Merge, expected AggregateFunction(uniqCombined64, Nullable(String)) or equivalent type. (ILLEGAL_TYPE_OF_ARGUMENT)\r\n```\r\n\r\n\r\n**Additional context**\r\nThis is blocking us to upgrade to newer version.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35342/comments",
    "author": "piyushsriv",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-03-16T18:56:30Z",
        "body": ">AggregateFunction(uniqCombined64(17),\r\n> uniqCombined64Merge(uniques)\r\n\r\nThis is a mis-usage which leads to incorrect results with any version.\r\nYou should use `uniqCombined64Merge(17)(uniques)` with any version of CH."
      },
      {
        "user": "piyushsriv",
        "created_at": "2022-03-16T20:46:45Z",
        "body": "Thanks for clarifying this."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the parameterized uniqCombined64(17) syntax causes incompatibility in newer ClickHouse versions",
      "Clear guidance on maintaining backward compatibility for aggregate functions during upgrades",
      "Demonstration of correct parameter usage pattern for uniqCombined64State/Merge functions",
      "Confirmation that the fix preserves original query semantics (accurate unique count calculation)"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:47:54"
    }
  },
  {
    "number": 34093,
    "title": "EXPLAIN SYNTAX doesn't report more than one column in GROUP BY",
    "created_at": "2022-01-28T12:21:32Z",
    "closed_at": "2022-01-28T13:07:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34093",
    "body": "For instance a query like this:\r\n\r\n```sql\r\nexplain syntax (SELECT sum(number) _number, count(), number, toDate(now()) date FROM numbers(10) GROUP BY number, date);\r\n\r\nEXPLAIN SYNTAX\r\nSELECT\r\n    sum(number) AS _number,\r\n    count(),\r\n    number,\r\n    toDate(now()) AS date\r\nFROM numbers(10)\r\nGROUP BY\r\n    number,\r\n    date\r\n\r\nQuery id: 985a47d6-644e-4821-8a6d-83de0925cfdd\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 SELECT                      \u2502\r\n\u2502     sum(number) AS _number, \u2502\r\n\u2502     count(),                \u2502\r\n\u2502     number,                 \u2502\r\n\u2502     toDate(now()) AS date   \u2502\r\n\u2502 FROM numbers(10)            \u2502\r\n\u2502 GROUP BY number             \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nshould report: `GROUP BY number, date` in the last line.\r\n\r\nTested on these versions with the same result: 20.7.2.30, 21.7.4.18, 21.9.5.16, 21.12.3.32, 22.1.2.2",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34093/comments",
    "author": "alrocar",
    "comments": [
      {
        "user": "CurtizJ",
        "created_at": "2022-01-28T12:34:33Z",
        "body": "`toDate(now())` is a constant and constants are eliminated from `GROUP BY` keys."
      },
      {
        "user": "alrocar",
        "created_at": "2022-01-28T13:07:36Z",
        "body": "Oh I see, a non constant date works:\r\n\r\n```sql\r\nexplain syntax (SELECT sum(number) _number, count(), number, toDate(rand()) date FROM numbers(10) GROUP BY number, date);\r\n\r\nEXPLAIN SYNTAX\r\nSELECT\r\n    sum(number) AS _number,\r\n    count(),\r\n    number,\r\n    toDate(rand()) AS date\r\nFROM numbers(10)\r\nGROUP BY\r\n    number,\r\n    date\r\n\r\nQuery id: 538725c3-0996-4018-a764-fac2c1e11933\r\n\r\n\u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 SELECT                      \u2502\r\n\u2502     sum(number) AS _number, \u2502\r\n\u2502     count(),                \u2502\r\n\u2502     number,                 \u2502\r\n\u2502     toDate(rand()) AS date  \u2502\r\n\u2502 FROM numbers(10)            \u2502\r\n\u2502 GROUP BY                    \u2502\r\n\u2502     number,                 \u2502\r\n\u2502     date                    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThanks!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why GROUP BY clauses with constant expressions are simplified in EXPLAIN SYNTAX output",
      "Identification of constant expression elimination as expected behavior rather than a bug",
      "Clarification about ClickHouse's GROUP BY optimization rules"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:48:49"
    }
  },
  {
    "number": 29620,
    "title": "Non standart aggregation with GROUP BY",
    "created_at": "2021-10-01T12:31:12Z",
    "closed_at": "2021-10-01T14:50:07Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/29620",
    "body": "** SQL **\r\n```\r\nSELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL;\r\n```\r\nCurrent result:\r\n\r\n```\r\n\u250c\u2500max(1)\u2500\u2510\r\n\u2502      0 \r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nExpected SQL Standart behavior (MySQL, sqlite, etc):\r\n\r\n```\r\nmysql> SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL;\r\nEmpty set (0,00 sec)\r\n```\r\n\r\nBut with a little changed SQL all ok\r\n\r\n```\r\nSELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0;\r\n```\r\n\r\nClickhouse\r\n```\r\n\u250c\u2500max(1)\u2500\u2510\r\n\u2502      0\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nMysql\r\n```\r\nmysql> SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0;\r\n+--------+\r\n| MAX(1) |\r\n+--------+\r\n|   NULL |\r\n+--------+\r\n1 row in set (0,00 sec)\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/29620/comments",
    "author": "Grian",
    "comments": [
      {
        "user": "nvartolomei",
        "created_at": "2021-10-01T12:49:37Z",
        "body": "You should try one of the latest stable/lts releases for first query.\r\n\r\nFor the second, there is `aggregate_functions_null_for_empty` setting that you can enable.\r\n\r\n```\r\nroot@a70daef2a898:/# clickhouse local -q 'SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 GROUP BY NULL'\r\nroot@a70daef2a898:/# clickhouse local -q 'SELECT MAX(1)  FROM (SELECT 1) one WHERE 1 = 0 SETTINGS aggregate_functions_null_for_empty = true'\r\n\\N\r\nroot@a70daef2a898:/# clickhouse local -q 'select version()'\r\n21.9.4.35\r\n```"
      },
      {
        "user": "Grian",
        "created_at": "2021-10-01T12:57:20Z",
        "body": "Thanks, with new release ok. We will upgrade.\r\n\r\n```\r\nSELECT MAX(1) FROM ( SELECT 1 ) AS one\r\nWHERE 1 = 0\r\nGROUP BY NULL\r\n\r\n0 rows in set. Elapsed: 0.038 sec.\r\n```\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Handling of aggregate functions on empty result sets with GROUP BY NULL must return an empty set rather than a default value",
      "Configuration option to return NULL instead of 0 for aggregate functions on empty result sets without GROUP BY",
      "Consistency with SQL standard behavior for empty aggregations across different query structures"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:50:26"
    }
  },
  {
    "number": 28506,
    "title": "i got a exception when create a table use RabbitMQ engine",
    "created_at": "2021-09-02T09:17:27Z",
    "closed_at": "2021-09-03T01:05:51Z",
    "labels": [
      "question",
      "question-answered",
      "comp-rabbitmq"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28506",
    "body": "the clickhouse run in docker\r\n```\r\nsudo mkdir /var/docker/clickhouse/\r\nsudo mkdir /var/docker/clickhouse/config\r\nsudo mkdir /var/docker/clickhouse/config/config.d\r\necho \"<yandex>\r\n     <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->\r\n    <listen_host>::</listen_host>\r\n    <listen_host>0.0.0.0</listen_host>\r\n    <listen_try>1</listen_try>\r\n\r\n    <!--\r\n    <logger>\r\n        <console>1</console>\r\n    </logger>\r\n    -->\r\n</yandex>\" | sudo tee /var/docker/clickhouse/config/config.d/docker_related_config.xml\r\necho \"<yandex>\r\n <rabbitmq>\r\n    <username>guest</username>\r\n    <password>guest</password>\r\n </rabbitmq>\r\n</yandex>\" | sudo tee /var/docker/clickhouse/config/config.d/rabbit.xml\r\n\r\ndocker container stop clickhouse && docker container rm clickhouse\r\ndocker run -d \\\r\n  --name clickhouse \\\r\n  --restart on-failure \\\r\n  --ulimit nofile=262144:262144 \\\r\n  -p 8123:8123 \\\r\n  -p 9000:9000 \\\r\n  --volume=/var/docker/clickhouse:/var/lib/clickhouse \\\r\n  --volume=/var/docker/clickhouse/config/users.d:/etc/clickhouse-server/users.d \\\r\n  --volume=/var/docker/clickhouse/config/config.d:/etc/clickhouse-server/config.d \\\r\n  yandex/clickhouse-server\r\n\r\n```\r\nrabbitMQ is docker also\r\n```\r\nsudo docker run \\\r\n  -d \\\r\n  --name rabbitmq \\\r\n  -p 5672:5672 \\\r\n  -p 15672:15672 \\\r\n  rabbitmq:management\r\n```\r\n\r\nthen i create a teble and query it\r\n```\r\nCREATE TABLE queue\r\n(\r\n\tkey   UInt64,\r\n\tvalue UInt64,\r\n\tdate  DateTime\r\n) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'localhost:5672',\r\n\trabbitmq_exchange_name = 'exchange1',\r\n\trabbitmq_format = 'JSONEachRow',\r\n\trabbitmq_num_consumers = 5,\r\n\tdate_time_input_format = 'best_effort';\r\n\r\nselect *\r\nfrom queue;\r\n```\r\n\r\ni got a exception\r\n`Code: 530, e.displayText() = DB::Exception: RabbitMQ setup not finished. Connection might be lost (version 21.8.4.51 (official build))`\r\n\r\nthe log of clickhouse\r\n```\r\n2021.09.02\u00a009:10:26.877248\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0RabbitMQConnectionTask:\u00a0Execution\u00a0took\u00a04002\u00a0ms.\r\n2021.09.02\u00a009:10:27.377358\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Trying\u00a0to\u00a0restore\u00a0connection\u00a0to\u00a0localhost:5672\r\n2021.09.02\u00a009:10:27.577754\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Error>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Library\u00a0error\u00a0report:\u00a0connection\u00a0lost\r\n2021.09.02\u00a009:10:31.379727\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0RabbitMQConnectionTask:\u00a0Execution\u00a0took\u00a04002\u00a0ms.\r\n2021.09.02\u00a009:10:31.879822\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Trace>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Trying\u00a0to\u00a0restore\u00a0connection\u00a0to\u00a0localhost:5672\r\n2021.09.02\u00a009:10:32.080192\u00a0[\u00a0216\u00a0]\u00a0{}\u00a0<Error>\u00a0StorageRabbitMQ\u00a0(queue):\u00a0Library\u00a0error\u00a0report:\u00a0connection\u00a0lost\r\n```\r\n\r\nclickhouse :\r\nselect version();=21.8.4.51\r\nrabbitmq is version 3.9.5\r\n\r\nsystem: windows 10 wsl debian\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28506/comments",
    "author": "AiSY-Yang",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2021-09-02T10:28:06Z",
        "body": "I assume it does not work becuase you have both clickhouse and rabbitmq in different docker containers and then pass `localhost` to RabbitMQ engine. It should not be localhost."
      },
      {
        "user": "AiSY-Yang",
        "created_at": "2021-09-03T01:05:51Z",
        "body": "> I assume it does not work becuase you have both clickhouse and rabbitmq in different docker containers and then pass `localhost` to RabbitMQ engine. It should not be localhost.\r\n\r\nthanks\r\nyou are right \r\ni add the argument  `  --link rabbitmq:rabbitmq`  when i run clickhouse\r\n\r\nset rabbitmq_host_port = 'rabbitmq:5672'\r\n\r\nI successfully created the table"
      }
    ],
    "satisfaction_conditions": [
      "Resolve Docker container networking configuration between ClickHouse and RabbitMQ",
      "Ensure proper inter-container communication using Docker networking features",
      "Correct RabbitMQ host configuration matching Docker service discovery",
      "Address containerized environment constraints in storage engine configuration"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:51:06"
    }
  },
  {
    "number": 28004,
    "title": "build failed error:FAILED: src/libdbms.so ",
    "created_at": "2021-08-23T02:03:36Z",
    "closed_at": "2021-08-25T10:58:00Z",
    "labels": [
      "question",
      "build",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28004",
    "body": "Hi\uff0cI compile cl on ubuntu 20.04, and always encounter this error. I don\u2019t know why. I also agree with the error on other debians.\r\n\r\n```\r\n12/672] Creating preprocessed file...c/lib/gssapi/krb5/gssapi_err_krb5.c\r\n+ /usr/bin/awk -f /home/hrp/Click/ClickHouse/contrib/krb5/src/util/et/et_h.awk outfile=gssapi_err_krb5.h /home/hrp/Click/ClickHouse/contrib/krb5/src/lib/gssapi/krb5/gssapi_err_krb5.et\r\n+ /usr/bin/awk -f /home/hrp/Click/ClickHouse/contrib/krb5/src/util/et/et_c.awk outfile=gssapi_err_krb5.c textdomain= localedir= /home/hrp/Click/ClickHouse/contrib/krb5/src/lib/gssapi/krb5/gssapi_err_krb5.et\r\n[421/672] Linking CXX shared library src/libdbms.so\r\nFAILED: src/libdbms.so\r\n: && /usr/bin/c++ -fPIC -fdiagnostics-color=always -fsized-deallocation  -\r\n\r\n```\r\n\r\nIs this file missing?  src/libdbms.so\uff1f\r\n\r\nI downloaded the source code  with submodules directly\uff0cis right\uff1f\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28004/comments",
    "author": "rouse2617",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-23T10:31:21Z",
        "body": "Check the amount of free space on your machine."
      },
      {
        "user": "rouse2617",
        "created_at": "2021-08-24T01:15:06Z",
        "body": "Thank you for your suggestion, I tried to expand the memory is compiled, but when packaging deb I encountered this situation, in addition, I would like to ask, packaging deb can be continued, each time have to wait a long time, to the end reported wrong, but also to come again, thank you\r\n\r\nthis is package deb error\r\n```\r\n\r\n/home/hrp/ClickHouse/programs/main.cpp:29:5: error: 'ENABLE_CLICKHOUSE_SERVER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_SERVER\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:32:5: error: 'ENABLE_CLICKHOUSE_CLIENT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_CLIENT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:35:5: error: 'ENABLE_CLICKHOUSE_LOCAL' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_LOCAL\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:38:5: error: 'ENABLE_CLICKHOUSE_BENCHMARK' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_BENCHMARK\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:41:5: error: 'ENABLE_CLICKHOUSE_EXTRACT_FROM_CONFIG' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_EXTRACT_FROM_CONFIG\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:44:5: error: 'ENABLE_CLICKHOUSE_COMPRESSOR' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_COMPRESSOR\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:47:5: error: 'ENABLE_CLICKHOUSE_FORMAT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_FORMAT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:50:5: error: 'ENABLE_CLICKHOUSE_COPIER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_COPIER\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:53:5: error: 'ENABLE_CLICKHOUSE_OBFUSCATOR' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_OBFUSCATOR\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:56:5: error: 'ENABLE_CLICKHOUSE_GIT_IMPORT' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_GIT_IMPORT\r\n    ^\r\n/home/hrp/ClickHouse/programs/main.cpp:59:5: error: 'ENABLE_CLICKHOUSE_KEEPER' is not defined, evaluates to 0 [-Werror,-Wundef]\r\n#if ENABLE_CLICKHOUSE_KEEPER\r\n```\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-24T12:38:52Z",
        "body": "This is wrong. These options should be enabled.\r\n\r\nI recommend to:\r\n- do static build (as default) instead of shared;\r\n- use all the defaults, don't specify any parameters;\r\n- if you need to build Debian packages, use `docker/packager`."
      }
    ],
    "satisfaction_conditions": [
      "Identify why ENABLE_CLICKHOUSE_* macros are undefined during build",
      "Resolve shared library linking failure for src/libdbms.so",
      "Provide reliable Debian packaging method that avoids repeated failures"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:51:30"
    }
  },
  {
    "number": 25698,
    "title": "Populating a materialized view results in unexpected values",
    "created_at": "2021-06-25T06:19:21Z",
    "closed_at": "2021-06-28T10:52:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25698",
    "body": "Please forgive me if i'm missing something fairly obvious here.\r\n\r\n**Describe the unexpected behaviour**\r\nPopulating a materialized view results in unexpected values.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: 21.3.13.9\r\n\r\n```bash\r\ndocker run -d --name some-clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:21.3.13.9\r\ndocker exec -it some-clickhouse-server clickhouse-client\r\n\r\n:) CREATE TABLE tmp_aggregated\r\n(\r\n  `window_start` DateTime64 Codec(DoubleDelta, LZ4),\r\n  `metrics_name` Array(LowCardinality(String)) Codec(LZ4),\r\n  `organization_id` LowCardinality(String) Codec(LZ4)\r\n)\r\nENGINE MergeTree()\r\nPARTITION BY (organization_id) ORDER BY (window_start)\r\n\r\n:) create materialized view tmp_names (\r\n  organization_id LowCardinality(String),\r\n  metric_names SimpleAggregateFunction(groupUniqArrayArray, Array(String)),\r\n  window_start_day DateTime64\r\n)\r\nEngine=MergeTree()\r\norder by (window_start_day)\r\npopulate as select\r\n  organization_id,\r\n  groupUniqArray(metrics_name),\r\n  toStartOfDay(window_start)\r\nfrom tmp_aggregated array join metrics_name\r\ngroup by toStartOfDay(window_start), organization_id\r\n\r\n:) insert into tmp_aggregated values ('2021-06-24 07:15:09.000', ['metric1'], 'org-id');\r\n\r\n:) select * from tmp_names \\G\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\norganization_id:  org-id\r\nmetric_names:     []\r\nwindow_start_day: 1970-01-01 00:00:00\r\n\r\n:) select * from tmp_aggregated \\G\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nwindow_start:    2021-06-24 07:15:09.000\r\nmetrics_name:    ['metric1']\r\norganization_id: org-id\r\n\r\n```\r\n\r\n**Expected behavior**\r\n\r\nWhen executing `select * from tmp_names \\G` I expected values stored in tmp_names to be:\r\n\r\n```\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\norganization_id:  org-id\r\nmetric_names:      ['metric1']\r\nwindow_start_day: 2021-06-24 07:15:09.000\r\n```\r\n\r\n**Error message and/or stacktrace**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25698/comments",
    "author": "shivamMg",
    "comments": [
      {
        "user": "SaltTan",
        "created_at": "2021-06-26T20:05:09Z",
        "body": "The names of the columns in the MV query and the destination table must match:\r\n\r\n as select\r\n  organization_id,\r\n  groupUniqArray(metrics_name) **as metric_names**,\r\n  toStartOfDay(window_start) **as window_start_day**\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of column name alignment between materialized view definition and source query"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:52:20"
    }
  },
  {
    "number": 25322,
    "title": "Kafka _timestamp / _timestamp_ms not working?",
    "created_at": "2021-06-16T10:57:19Z",
    "closed_at": "2021-06-16T11:21:17Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25322",
    "body": "**Describe the bug**\r\n\r\nAppears that _timestamp and _timestamp_ms virtual columns are returning 0 for kafka. Perhaps it is somehow due to JSONEachRow format?\r\n\r\nIssue is in docker 21.6.4.26 and 21.2.2.8 at least.\r\n\r\nI'm using amazon MSK 2.6.1\r\n\r\n**How to reproduce**\r\n\r\n```\r\ncreate table kafka ( name String ) ENGINE=Kafka() SETTINGS kafka_broker_list = '...', kafka_topic_list = 'test-events', kafka_group_name='test', kafka_format='JSONEachRow';\r\ncreate table t (time DateTime64(3), name String) ENGINE=MergeTree() ORDER BY tuple();\r\ncreate materialized view kafka_mv to t AS select _timestamp_ms, name FROM kafka;\r\n```\r\n\r\n```\r\necho '{\"name\":\"test\"}' | bin/kafka-console-producer.sh ...\r\n```\r\n\r\nClickhouse table just looks like:\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500time\u2500\u252c\u2500name\u2500\u2510\r\n\u2502 1970-01-01 00:00:00.000 \u2502 test \u2502\r\n...\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIf checking with kafka consumer:\r\n\r\n```\r\nbin/kafka-console-consumer.sh ... --from-beginning --property print.timestamp=true\r\nCreateTime:1623839664050        {\"name\":\"test\"}\r\nCreateTime:1623839989619        {\"name\":\"test\"}\r\nCreateTime:1623840562909        {\"name\":\"test\"}\r\nCreateTime:1623840285113        {\"name\":\"test\"}\r\n```\r\n\r\nI see the same with _timestamp col",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25322/comments",
    "author": "mzealey",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2021-06-16T11:18:18Z",
        "body": "Mapping of columns in mv happens by names, not by positions.\r\n\r\nSo that should help:\r\n```\r\ncreate materialized view kafka_mv to t AS select _timestamp_ms as time, name FROM kafka;\r\n```"
      },
      {
        "user": "mzealey",
        "created_at": "2021-06-16T11:21:17Z",
        "body": "Gah yes that fixed it. Normally I thought there were errors when creating a mv or inserting into it like this, but I did not see anything on console or in the logs :-/"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of column name mapping requirements in ClickHouse materialized views",
      "Clarification of how virtual columns are mapped between Kafka tables and materialized views",
      "Identification of required column aliasing for timestamp fields in Kafka ingestion",
      "Error visibility expectations for schema mapping mismatches"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:52:34"
    }
  },
  {
    "number": 24083,
    "title": "Can you make postgres interface?",
    "created_at": "2021-05-13T09:17:58Z",
    "closed_at": "2021-05-13T13:18:38Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24083",
    "body": "Can you make postgres interface?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24083/comments",
    "author": "javawin",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-05-13T13:18:38Z",
        "body": "We already have, see `config.xml`:\r\n\r\n```\r\n    <!-- Compatibility with PostgreSQL protocol.\r\n         ClickHouse will pretend to be PostgreSQL for applications connecting to this port.\r\n    -->\r\n    <postgresql_port>9005</postgresql_port>\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that a PostgreSQL-compatible interface exists"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:53:20"
    }
  },
  {
    "number": 22483,
    "title": "Modify ttl does not affects old parts.",
    "created_at": "2021-04-02T02:49:00Z",
    "closed_at": "2021-04-06T06:59:58Z",
    "labels": [
      "question",
      "comp-ttl",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22483",
    "body": "To my invesigation, ClickHouse uses old parts' ttl as a new part's ttl when merging, so it would not delete old parts desipte modifying table's ttl.\r\nMaybe clickhouse can use modified table's ttl as new part's ttl when merging to delete old parts\u2018 data.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22483/comments",
    "author": "vsop-479",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-04-02T03:44:29Z",
        "body": "What Clickhouse version do you use?\r\nModern Clickhouse recalculate TTL when alter table modify ttl applied.\r\n\r\nAlso use `ALTER TABLE ... MATERIALIZE TTL` to materialize ttl."
      },
      {
        "user": "vsop-479",
        "created_at": "2021-04-22T06:06:33Z",
        "body": "My Clickhouse version is 20.6.\r\nI will test it on a new version.\n\n---\n\n20.10.6.27 can delete old parts."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how TTL modifications propagate to existing data parts during merge operations",
      "Guidance on version compatibility or upgrade requirements for TTL behavior",
      "Method to trigger TTL recalculation for existing data"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:54:19"
    }
  },
  {
    "number": 21894,
    "title": "Distributed Tables missing records. \"Exception: Too large string size.\" in logs.",
    "created_at": "2021-03-18T21:48:33Z",
    "closed_at": "2021-03-19T02:38:48Z",
    "labels": [
      "question",
      "obsolete-version",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21894",
    "body": "Setup:\r\n```\r\n         <shard>\r\n              <weight>1</weight>\r\n              <internal_replication>false</internal_replication>\r\n              <replica>\r\n                  <host><server1domainname></host>\r\n                  <port>9000</port>\r\n              </replica>\r\n              <replica>\r\n                  <host><server2domainname></host>\r\n                  <port>9000</port>\r\n              </replica>\r\n          </shard>\r\n\r\n```\r\nOur code writes to a Distributed table on either server, and then clickhouse writes the record to the underlying real tables on both servers. \r\n\r\nNo zookeeper.\r\n\r\nThis has been working for a few years.\r\n\r\nIncident.\r\nTo get multivolume support, I upgraded from 18.14.19 to 20.8.12.2. I had a number of problems, and ran out of time to debug/fix, so I rolled back to 18.14.19. My problem probably started then.\r\n\r\nNow we have discovered that some records are not getting replicated to one of the servers. Or maybe replication is working from server1 -> server2, but not the other way. I'm not 100% sure. When reads randomly choose a server, they get different results.\r\n\r\nI have these error messages constantly on one server:\r\n\r\n`2021.03.19 02:35:13.294443 [ 39 ] {} <Error> <table>.Distributed.DirectoryMonitor: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = Exception: Too large string size., e.what() = Exception\r\n`\r\n\r\nI also noticed a small number of these errors on the other server, but they are not constant. Only a handful from a few hours ago.\r\n\r\n`2021.03.18 19:22:36.647630 [ 38 ] {} <Error> <table>.Distributed.DirectoryMonitor: Code: 252, e.displayText() = DB::Exception: Received from <serverdomainname>:9000, <ipaddress> DB::Exception: Too many parts (304). Merges are processing significantly slower than inserts..\r\n`\r\n\r\nI did some research into the first error, but I don't really understand which directory, or which string it is referring to.\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21894/comments",
    "author": "entropical",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-18T22:48:48Z",
        "body": "You don't have actual Replication. \r\nDistributed table propagates inserts into 2 CH nodes. Sometimes inserts can fail into one replica and succeed into another.\r\nThere is no surprise that Replicas has different data. It's documented behavior for this setup.\r\n\r\n >Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = Exception: Too large string size\r\n\r\nYou should stop insertion and check what is going in Distributed table catalog. \r\nProbably drop Distributed table. Remove all remains of Distributed folder (/var/lib/clickhouse/data/db/distributedtablename/) and create Distributed table back.\r\n\r\n> Too many parts (304). Merges are\r\n\r\nNeed to check what is going on with `<serverdomainname>:9000` in this server logs. And in `select * from system.merges`\r\n\r\n\n\n---\n\n>I did some research into the first error, but I don't really understand which directory, or which string it is referring to.\r\n\r\nIt's about `/var/lib/clickhouse/data/db/distributedtablename/.....`"
      },
      {
        "user": "entropical",
        "created_at": "2021-03-18T23:10:29Z",
        "body": "> You don't have actual Replication.\r\n> It's documented behavior for this setup.\r\n\r\nYeah, I did know this, but up until very recently it has been sufficient for our purpose.\r\n\r\n> It's about /var/lib/clickhouse/data/db/distributedtablename/.....\r\n\r\nRight! I noticed this directory when I had the failed upgrade.\r\n\r\nI have two directories here, but I think I should only have one. One has IP, one has domainname. I'm pretty sure this is related to the version change.\r\n\r\n/var/lib/clickhouse/data/noc/distributedtablename/\r\ndefault@111%2E222%2E333%2E444:9000/\r\ndefault@domainname%2Etld%2Etld%2Etld%2Ecom:9000/\r\n\r\n> Probably drop Distributed table. Remove all remains of Distributed folder (/var/lib/clickhouse/data/db/distributedtablename/) and create Distributed table back.\r\n\r\nYeah, this sounds like it might help. This is safe, right, because the Distributed table doesn't actually contain data? Trying not to lose any (more...) data."
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-18T23:44:29Z",
        "body": ">This is safe, right, because the Distributed table doesn't actually contain data? \r\n\r\nYes. It is safe. It contains only not propagated inserts. That's why I suggest to stop insertion.\r\nCheck that it stops to propagate. \r\nCheck the size of these subdirectories: `du -sh /var/lib/clickhouse/data/noc/distributedtablename/*`\r\nIf it's huge, check how many .bin files inside. Check the server logs."
      },
      {
        "user": "entropical",
        "created_at": "2021-03-19T00:20:21Z",
        "body": "On one server, both of these directories are very small.\r\nOn the other server, the one with the IP address in as the name is small. The one with the domain name appears to be so huge I can't even ls or du it.\r\n\r\n> Remove all remains of Distributed folder\r\n\r\nHypothetically, if there are orphaned bin files here, can I reintroduce them after I recreate the Distributed table, and it is working? I'm not _too_ concerned if not, just working out my options.\r\n\r\nThanks for your excellent help by the way."
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-19T00:53:01Z",
        "body": ">Hypothetically, if there are orphaned bin files here, \r\n>can I reintroduce them after I recreate the Distributed table, and it is working?\r\n> I'm not too concerned if not, just working out my options.\r\n\r\nIf you sure that both directories are the same destination you can move files from the one to anther.\r\nAnd yes, you can move files / rename (move) the whole  directory to somewhere to /var/lib/clickhouse\r\nrecreate the distr. table and move .bin files one by one or  1000 by 1000 ( batches ).\r\n\r\n"
      },
      {
        "user": "entropical",
        "created_at": "2021-03-19T02:38:48Z",
        "body": "I stopped my writers, and am now dropping the replicatedtable. It has taken about 10 minutes so far... \n\n---\n\nThe drop was taking forever. I think it was attempting to catch up on the pending propogations first.\r\n\r\nI moved the directory to a safe location, and the drop completed a few seconds later. I then recreated the distributed table, and started my writers. For the last hour it looks like the propagations are working fine.\r\n\r\nI'll now try to reparent my orphan bin files.\r\n\r\nThanks again for your help @den-crane."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how Distributed table directory structure impacts replication reliability",
      "Clear safety guarantees for Distributed table maintenance operations",
      "Method to handle orphaned .bin files from failed replication attempts",
      "Resolution path for 'Too many parts' merge backpressure"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:55:25"
    }
  },
  {
    "number": 21473,
    "title": "How to downgrade from version 20.12.7.3 to 20.4.4.18 with Atomic database created",
    "created_at": "2021-03-05T10:27:05Z",
    "closed_at": "2021-03-08T06:18:26Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21473",
    "body": "I created some Atomic databases in 20.12.7.3, when I need to downgrade the server to 20.4.4.18 there's always errors like below: \r\n`{} <Error> Application: DB::Exception: Syntax error (in file /var/lib/clickhouse/metadata/default.sql): failed at position 19 (line 1, col 19): UUID '6ef8d876-bd4e-44bc-bc44-2f2e950e3f20'\r\nENGINE = Atomic\r\n. Expected one of: storage definition, ENGINE, ON\r\n`\r\nHow can I smoothly do the downgrade? \r\nI tried to set allow_experimental_database_atomic=1 in users.xml, but it doesn't work. ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21473/comments",
    "author": "Zhile",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-05T13:03:47Z",
        "body": "in 20.12.7.3 You can move all tables to ordinary database by rename.\r\n\r\n```\r\ncreate database atomic_db Engine=Atomic;\r\ncreate database ordinary_db Engine=Ordinary;\r\ncreate table  atomic_db.x(A Int64) Engine=MergeTree order by A;\r\ninsert into atomic_db.x select number from numbers(100000);\r\nrename table atomic_db.x to ordinary_db.x;\r\nls -1 /var/lib/clickhouse/data/ordinary_db/x\r\nall_1_1_0\r\ndetached\r\nformat_version.txt\r\ndrop database atomic_db;\r\ndetach database ordinary_db;\r\nmv /var/lib/clickhouse/metadata/ordinary_db.sql /var/lib/clickhouse/metadata/atomic_db.sql\r\nvi /var/lib/clickhouse/metadata/atomic_db.sql\r\nmv /var/lib/clickhouse/metadata/ordinary_db /var/lib/clickhouse/metadata/atomic_db\r\nmv /var/lib/clickhouse/data/ordinary_db /var/lib/clickhouse/data/atomic_db\r\nattach database atomic_db;\r\nselect count() from atomic_db.x\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502  100000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nshow create database atomic_db\r\n\u250c\u2500statement\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 CREATE DATABASE atomic_db\r\nENGINE = Ordinary \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nYou can generate rename by \r\n```\r\nselect 'rename table ....atomdb.' ||name||' to ordin.'||name||';' from system.tables where db = atomdb and engine =\r\n```"
      },
      {
        "user": "Zhile",
        "created_at": "2021-03-08T06:18:23Z",
        "body": "Thanks @den-crane , that's really helpful!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must convert Atomic databases to a format compatible with ClickHouse 20.4.4.18",
      "Method must preserve table data during downgrade process",
      "Approach must handle database engine type conversion (Atomic \u2192 Ordinary)",
      "Process must include metadata/file structure adjustments for backward compatibility"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:55:59"
    }
  },
  {
    "number": 21177,
    "title": "How do I enable the compilation option -pie?",
    "created_at": "2021-02-25T08:22:21Z",
    "closed_at": "2021-06-13T21:33:28Z",
    "labels": [
      "question",
      "build",
      "st-fixed"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21177",
    "body": "In CMakeLists.txt\uff0cthere are following compilation options by default:\r\n_set (CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO} -fno-pie\")\r\nset (CMAKE_C_FLAGS_RELWITHDEBINFO \"${CMAKE_C_FLAGS_RELWITHDEBINFO} -fno-pie\")\r\nset (CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-no-pie\")_\r\n\r\nBecause I want to compile clickhouse in a more secure mode\uff0cI need to use \"-fpie\" or \"-pie\" compilation options in compiling\u3002But if I change \"-fno-pie\" to \"-fpie\" and \"-no-pie\" to \"-pie\",I can not complie clickhouse successfully,the following is my compilation command:\r\n_cmake .. -DUSE_INTERNAL_BOOST_LIBRARY=1  -DENABLE_READLINE=1 -DCMAKE_BUILD_TYPE=Release -DENABLE_MYSQL=0 -DENABLE_DATA_SQLITE=0 -DPOCO_ENABLE_SQL_SQLITE=0 -DENABLE_JEMALLOC=ON -DENABLE_EMBEDDED_COMPILER=1  -DENABLE_PARQUET=1  -DENABLE_ORC=1 -DENABLE_PROTOBUF=1 -DENABLE_ODBC=0 -DENABLE_SSL=1  -DNO_WERROR=1 -DCMAKE_CXX_COMPILER=g++ -DCMAKE_C_COMPILER=gcc -DUSE_INTERNAL_ODBC_LIBRARY=1 -DMAKE_STATIC_LIBRARIES=1_\r\n\r\nThe following is error info:\r\n\r\n-- Performing Test HAVE_PTRDIFF_T\r\n-- Performing Test HAVE_PTRDIFF_T - Failed\r\n-- Check size of void *\r\n-- Check size of void * - failed\r\n-- sizeof(void *) is  bytes\r\nCMake Error at contrib/zlib-ng/CMakeLists.txt:419 (message):\r\n  sizeof(void *) is neither 32 nor 64 bit\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n\r\nSo how can I config my compilation options to compile clickhouse with pie enabled successfully? Thank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21177/comments",
    "author": "wallace-clickhouse",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-28T22:55:19Z",
        "body": "Just remove `-fno-pie` and `-Wl,-no-pie`."
      },
      {
        "user": "wallace-clickhouse",
        "created_at": "2021-03-29T08:57:10Z",
        "body": "It works\uff0cthanks\uff01"
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable PIE compilation flags without causing configuration errors related to pointer size detection",
      "Answer must ensure successful compilation of ClickHouse with PIE security features enabled",
      "Approach must work with ClickHouse's existing CMake configuration structure"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:56:12"
    }
  },
  {
    "number": 18872,
    "title": "apply max_execution_time but without using an estimate?",
    "created_at": "2021-01-08T16:41:16Z",
    "closed_at": "2021-01-08T17:05:55Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18872",
    "body": "**Describe the bug**\r\n\r\nI have been using `max_execution_time` to limit query complexity. Normally this works great but there are some situations where very complex queries (1 page of sql so won't include below) are estimated at 1800s but actually complete in 4s. Because the estimate was 1800s I get the error message \"Estimated query execution time (1807.380046338318 seconds) is too long. Maximum: 60. Estimated rows to process: 2871497: While executing MergeTreeThread (version 20.11.3.3 (official build))\".\r\n\r\nAs there will always be issues around estimating query execution time, my request is to have an option alongside max_execution_time which will say whether to rely on estimates or whether to just run it and abort after the specified time.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18872/comments",
    "author": "mzealey",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-01-08T16:52:08Z",
        "body": "you need to set `timeout_before_checking_execution_speed=0` this disables estimation"
      },
      {
        "user": "mzealey",
        "created_at": "2021-01-08T17:05:55Z",
        "body": "ok thank you. i saw some comments around this but assumed it was only applicable for the `min_execution_speed` type options"
      }
    ],
    "satisfaction_conditions": [
      "Disables pre-execution time estimation checks",
      "Allows enforcing timeouts based on actual query execution duration",
      "Provides configuration to control estimation check behavior"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:58:11"
    }
  },
  {
    "number": 17054,
    "title": "why the first 1e8 rows inserted and the 2nd 1e8 rows failed",
    "created_at": "2020-11-16T05:12:07Z",
    "closed_at": "2020-11-17T22:05:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17054",
    "body": "```\r\ncreate table sfz15y engine=MergeTree()order by id as select a.number*10000+b.number id from numbers(50000)a,numbers(10000)b;\r\ninsert into  sfz15y select (a.number+50000)*10000+b.number id from numbers(50000)a,numbers(10000)b;\r\ninsert into  sfz15y select (a.number)*10000+b.number id from numbers(50000)a,numbers(10000)b where b.number%5=0;\r\ncreate table sfzcm engine=MergeTree()order by id as select id,count(*)c from sfz15y group by id having count(*)>1;\r\n```\r\n**failed, but the table was created** , then i try to reduce the rows.\r\n```\r\ninsert into sfzcm select id,count(*)c from sfz15y where id<100000000  group by id having count(*)>1;\r\nQuery id: da659738-3916-466c-a392-718224d9e178\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 10.615 sec. Processed 120.01 million rows, 960.10 MB (11.31 million rows/s., 90.45 MB/s.)\r\n\r\nDESKTOP-RS3EG9A.localdomain :) select count(*) from sfzcm;\r\n\r\nQuery id: bd043a67-f81a-475e-90da-ebc6a44aabef\r\n\r\n\u250c\u2500\u2500count()\u2500\u2510\r\n\u2502 20000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec.\r\n\r\ninsert into sfzcm select id,count(*)c from sfz15y where id>=100000000 and id<200000000  group by id having count(*)>1;\r\n```\r\nReceived exception from server (version 20.11.3):\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (total) exceeded: would use 11.08 GiB (attempt to allocate chunk of 6291456 bytes), maximum: 11.08 GiB: While executing AggregatingTransform.\r\n\r\n0 rows in set. Elapsed: 2.440 sec. Processed 92.93 million rows, 743.44 MB (38.08 million rows/s., 304.67 MB/s.)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17054/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-16T18:41:10Z",
        "body": "```\r\nset max_memory_usage='10G', max_bytes_before_external_group_by='3G';\r\n\r\nCREATE TABLE sfz15y\r\nENGINE = MergeTree()\r\nORDER BY id AS\r\nSELECT (a.number * 10000) + b.number AS id\r\nFROM numbers(50000) AS a\r\n, numbers(10000) AS b\r\n\r\n0 rows in set. Elapsed: 10.107 sec.\r\n\r\n\r\n\r\nINSERT INTO sfz15y SELECT (a.number * 10000) + b.number AS id\r\nFROM numbers(50000) AS a\r\n, numbers(10000) AS b\r\nWHERE (b.number % 5) = 0\r\n\r\n0 rows in set. Elapsed: 4.232 sec.\r\n\r\n\r\nCREATE TABLE sfzcm\r\nENGINE = MergeTree()\r\nORDER BY id AS\r\nSELECT\r\n    id,\r\n    count(*) AS c\r\nFROM sfz15y\r\nGROUP BY id\r\nHAVING count(*) > 1\r\n\r\n0 rows in set. Elapsed: 53.631 sec.\r\n\r\n\r\ninsert into sfzcm select id,count(*)c from sfz15y where id<100000000  group by id having count(*)>1;\r\n\r\n0 rows in set. Elapsed: 11.482 sec.\r\n\r\n\r\nselect count(*) from sfzcm;\r\n\r\n\u250c\u2500\u2500\u2500count()\u2500\u2510\r\n\u2502 120000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\ninsert into sfzcm select id,count(*)c from sfz15y where id>=100000000 and id<200000000  group by id having count(*)>1;\r\n\r\n0 rows in set. Elapsed: 11.149 sec.\r\n\r\n```\r\n"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-16T22:47:14Z",
        "body": "thanks, and I wonder why those two inserts need different memory size? "
      },
      {
        "user": "den-crane",
        "created_at": "2020-11-16T23:32:26Z",
        "body": "Which 2 ?"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-17T00:00:54Z",
        "body": "1.`insert into sfzcm select id,count(*)c from sfz15y where id<100000000  group by id having count(*)>1;`\r\n2.`insert into sfzcm select id,count(*)c from sfz15y where id>=100000000 and id<200000000  group by id having count(*)>1;`"
      },
      {
        "user": "den-crane",
        "created_at": "2020-11-17T00:29:44Z",
        "body": "```\r\nSET send_logs_level = 'debug'\r\n\r\n1 insert : MemoryTracker: Peak memory usage (for query): 4.93 GiB.\r\n2 insert : MemoryTracker: Peak memory usage (for query): 4.02 GiB.\r\n\r\nset max_memory_usage='40G', max_bytes_before_external_group_by=0\r\n\r\n1 insert : MemoryTracker: Peak memory usage (for query): 5.06 GiB.\r\n2 insert : MemoryTracker: Peak memory usage (for query): 5.06 GiB.\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-17T00:57:49Z",
        "body": "thanks, one more question\r\nhow to check the current value of  `max_memory_usage, max_bytes_before_external_group_by` etc"
      },
      {
        "user": "den-crane",
        "created_at": "2020-11-17T19:37:55Z",
        "body": "```sql\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.settings\r\nWHERE name IN ('max_memory_usage', 'max_bytes_before_external_group_by')\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_bytes_before_external_group_by \u2502 76027960320  \u2502\r\n\u2502 max_memory_usage                   \u2502 152055920640 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      },
      {
        "user": "l1t1",
        "created_at": "2020-11-17T22:05:18Z",
        "body": "got it. thanks"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why memory usage differs between similar GROUP BY operations on different data ranges",
      "Clarification of how ClickHouse memory settings affect GROUP BY operations with large datasets",
      "Guidance on monitoring and adjusting memory configuration parameters for aggregation queries",
      "Methodology for diagnosing memory usage patterns in ClickHouse queries"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:00:02"
    }
  },
  {
    "number": 16220,
    "title": "Query from distributed tables with sharded data return separate result for each shard",
    "created_at": "2020-10-21T10:58:43Z",
    "closed_at": "2020-11-08T12:56:09Z",
    "labels": [
      "question",
      "st-need-info",
      "st-need-repro"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16220",
    "body": "**Describe the bug**\r\nSelect aggregates from disributed table produce unexpected results.\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\nTested on \r\n1) \r\n```\r\n+----------------+----------------------------------------+\r\n|name            |value                                   |\r\n+----------------+----------------------------------------+\r\n|VERSION_FULL    |ClickHouse 20.3.11.97                   |\r\n|VERSION_DESCRIBE|v20.3.11.97-stable                      |\r\n|VERSION_INTEGER |20003011                                |\r\n|VERSION_GITHASH |952efc395509c10081e8c1b836ebb5c2e0f898fa|\r\n|VERSION_REVISION|54433                                   |\r\n+----------------+----------------------------------------+\r\n\r\n```\r\n\r\n2)\r\n```\r\n+----------------+----------------------------------------+\r\n|name            |value                                   |\r\n+----------------+----------------------------------------+\r\n|VERSION_FULL    |ClickHouse 20.7.2.30                    |\r\n|VERSION_DESCRIBE|v20.7.2.30-stable                       |\r\n|VERSION_INTEGER |20007002                                |\r\n|VERSION_GITHASH |e0529c753f9dd4fc27e38f2f25a14d50beedda65|\r\n|VERSION_REVISION|54437                                   |\r\n+----------------+----------------------------------------+\r\n```\r\n* Non-default settings, if any\r\nprefer_localhost_replica 0\r\n* `CREATE TABLE` statements for all tables involved\r\n```\r\ncreate table test\r\n(\r\n    date Date,\r\n    key  String\r\n) engine MergeTree() partition by date order by key;\r\ncreate table test_dist as test engine Distributed('cubes', default, test, date);\r\n```\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\nOn shard 1: \r\n```\r\ninsert into test values ('2020-01-01', '1');\r\ninsert into test values ('2020-01-01', '2');\r\n```\r\nOn shard 2\r\n```\r\ninsert into test values ('2020-01-01', '1', 1);\r\ninsert into test values ('2020-01-01', '2', 1);\r\ninsert into test values ('2020-01-03', '3', 1);\r\n```\r\n* Queries to run that lead to unexpected result\r\n```\r\nselect toMonday(date) mnt, uniq(key)\r\nfrom test_dist\r\nwhere mnt = '2019-12-30'\r\ngroup by mnt;\r\n```\r\n\r\nResult:\r\n```\r\n+----------+---------+\r\n|mnt       |uniq(key)|\r\n+----------+---------+\r\n|2019-12-30|2        |\r\n|2019-12-30|3        |\r\n+----------+---------+\r\n\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nExpected single row to return with 3 uniq(key)\r\n```\r\n+----------+---------+\r\n|mnt       |uniq(key)|\r\n+----------+---------+\r\n|2019-12-30|3        |\r\n+----------+---------+\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16220/comments",
    "author": "dmgburg",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-21T13:39:56Z",
        "body": "Can you  show cluster `cubes` description?\r\n\r\nAnd `select *, _shard_num from test_dist`"
      },
      {
        "user": "SaltTan",
        "created_at": "2020-10-21T22:03:42Z",
        "body": "Reminds me of #13331"
      },
      {
        "user": "dmgburg",
        "created_at": "2020-10-26T14:43:14Z",
        "body": "@den-crane \r\n Clusters layout\r\n```\r\n <remote_servers>\r\n        <cubes>\r\n            <shard>\r\n                <internal_replication>true</internal_replication>\r\n                <replica>\r\n                    <host>server</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n                <replica>\r\n                    <host>server2</host>\r\n                    <port>9000</port>\r\n                </replica>\r\n            </shard>\r\n        </cubes>\r\n    </remote_servers>\r\n```\r\n\r\nselect *, _shard_num from test_dist;\r\n\r\n```\r\n+----------+---+----------+\r\n|date      |key|_shard_num|\r\n+----------+---+----------+\r\n|2020-01-01|2  |1         |\r\n|2020-01-01|1  |1         |\r\n|2020-01-03|3  |1         |\r\n|2020-01-01|2  |1         |\r\n|2020-01-01|1  |1         |\r\n+----------+---+----------+\r\n\r\n```\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-26T15:00:28Z",
        "body": "@dmgburg you have confused SHARD vs REPLICA\r\n\r\nYou  config for 2 shards  must be \r\n```\r\nshard \r\n   replica\r\nshard\r\n   replica\r\n```\r\n\r\n```\r\n<remote_servers>\r\n       <cubes>\r\n           <shard>\r\n               <internal_replication>true</internal_replication>\r\n               <replica>\r\n                   <host>server</host>\r\n                   <port>9000</port>\r\n               </replica>\r\n           </shard>            \r\n           <shard>             \r\n               <replica>\r\n                   <host>server2</host>\r\n                   <port>9000</port>\r\n               </replica>\r\n           </shard>\r\n       </cubes>\r\n   </remote_servers>\r\n\r\n```\r\n\r\n---\r\n\r\nIf each of 2 shards have 3 replicas config have to be\r\n```\r\nshard \r\n   replica\r\n   replica\r\n   replica\r\nshard\r\n   replica\r\n   replica\r\n   replica\r\n```"
      },
      {
        "user": "dmgburg",
        "created_at": "2020-11-08T12:56:09Z",
        "body": "\u0414\u0430, \u0432\u0441\u0435 \u0442\u0430\u043a. \u0421 \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u043e. \u0421\u043f\u0430\u0441\u0438\u0431\u043e!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how ClickHouse Distributed tables handle shard vs replica configuration",
      "Demonstration of correct cluster configuration for sharded data aggregation",
      "Clarification on Distributed engine's aggregation behavior across shards"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:01:19"
    }
  },
  {
    "number": 15903,
    "title": "DB::Exception: Unknown data type family: DateTime64 while import from tsv, csv",
    "created_at": "2020-10-13T09:40:24Z",
    "closed_at": "2020-10-13T14:56:07Z",
    "labels": [
      "question",
      "comp-datetime",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15903",
    "body": "When I import data from external file (tsv, csv) I got error:\r\n\r\n**Code: 50. DB::Exception: Unknown data type family: DateTime64**\r\n\r\nServer version 20.9.2.\r\n\r\n```\r\n24b27b0d4af5 :) SELECT * FROM system.data_type_families WHERE name LIKE 'DateTime%';\r\n\r\nSELECT * FROM system.data_type_families WHERE name LIKE 'DateTime%'\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500case_insensitive\u2500\u252c\u2500alias_to\u2500\u2510\r\n\u2502 DateTime   \u2502                1 \u2502          \u2502\r\n\u2502 DateTime64 \u2502                1 \u2502          \u2502\r\n\u2502 DateTime32 \u2502                1 \u2502          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nDateTime64 is present.\r\n\r\nMy data:\r\n\r\n\r\n**database**\r\n```\r\nCREATE TABLE log_viewing  \r\n(\r\n  id UInt32,\r\n  ts DateTime64(6, 'Europe/Moscow'),\r\n  document_id UInt16,\r\n  user_id UInt16,\r\n  element_id_max UInt16,\r\n  element_id_max_child UInt16,\r\n  element_id_min UInt16,\r\n  element_id_min_child UInt16,\r\n  status UInt8,\r\n  tz_offset Int16,\r\n  ts_local DateTime64(6, 'Europe/Moscow'),\r\n  source UInt8,\r\n  duration UInt16\r\n) ENGINE Log;\r\n```\r\n\r\nFile **log_viewing-0.tsv**\r\n```\r\nid\tts\tdocument_id\tuser_id\telement_id_min\telement_id_min_child\telement_id_max\telement_id_max_child\tstatus\ttz_offset\tts_local\tsource\tduration\r\n1592845\t2019-07-23 12:31:31.997075\t4\t2\t1\t1\t10\t10\t2\t-180\t2019-07-23 12:31:31.997075\t1\t0\r\n1592846\t2019-07-23 12:31:33.997075\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:33.997075\t1\t2000\r\n1592847\t2019-07-23 12:31:35.497075\t4\t2\t2\t2\t6\t6\t2\t-180\t2019-07-23 12:31:35.497075\t1\t1000\r\n1592848\t2019-07-23 12:31:36.497075\t4\t2\t1\t1\t4\t4\t2\t-180\t2019-07-23 12:31:36.497075\t1\t1000\r\n1592849\t2019-07-23 12:31:37.997075\t4\t2\t2\t2\t5\t5\t2\t-180\t2019-07-23 12:31:37.997075\t1\t2000\r\n1592850\t2019-07-23 12:31:39.497075\t4\t2\t1\t1\t4\t4\t2\t-180\t2019-07-23 12:31:39.497075\t1\t1000\r\n1592851\t2019-07-23 12:31:40.497075\t4\t2\t2\t2\t4\t4\t2\t-180\t2019-07-23 12:31:40.497075\t1\t1000\r\n1592852\t2019-07-23 12:31:40.997075\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:40.997075\t1\t1000\r\n1592854\t2019-07-23 12:31:48.191737\t4\t2\t1\t1\t3\t3\t2\t-180\t2019-07-23 12:31:48.191737\t1\t7000\r\n```\r\n\r\nCommand for import:\r\n```\r\nclickhouse-client --query \"INSERT INTO spnav.log_viewing FORMAT TabSeparatedWithNames\" <log_viewing-0.tsv\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15903/comments",
    "author": "borisovcode",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-13T12:57:45Z",
        "body": "Your clickhouse-client is outdated and does not support DateTime64.\r\nInstall clickhouse-client with version 20.9.2 \r\n\r\nIn case of \r\n`clickhouse-client --query \"INSERT INTO spnav.log_viewing FORMAT TabSeparatedWithNames\" <log_viewing-0.tsv`\r\nclickhouse-client reads a stream and does a stream parsing and sends parsed data in the Native format to a server.\r\n\r\n\r\n\r\n```\r\n$ clickhouse-client --query \"INSERT INTO spnav.log_viewing FORMAT TabSeparatedWithNames\" <log_viewing-0.tsv\r\n\r\n\r\nClickHouse client version 20.11.1.4897 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 20.11.1 revision 54441.\r\n\r\nselect count() from log_viewing\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502       9 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why DateTime64 is recognized by server but rejected during import",
      "Identification of version compatibility requirements between client and server",
      "Clarification about client's role in data parsing/formatting during imports"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:01:38"
    }
  },
  {
    "number": 15464,
    "title": "Can't Import Parquet on macOS",
    "created_at": "2020-09-30T08:45:34Z",
    "closed_at": "2020-09-30T13:10:58Z",
    "labels": [
      "question",
      "build",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15464",
    "body": "MacOS Catalina\r\n\r\n```\r\n\u279c  ~ docker ps\r\nCONTAINER ID        IMAGE                                COMMAND             CREATED             STATUS              PORTS                                                      NAMES\r\nb9d8daab2501        yandex/clickhouse-server:20.9.2.20   \"/entrypoint.sh\"    2 hours ago         Up 2 hours          0.0.0.0:8123->8123/tcp, 0.0.0.0:9000->9000/tcp, 9009/tcp   adoring_nash\r\n\r\n\u279c  ~ cat ~/Downloads/cleand.parquet | clickhouse-client --query=\"INSERT INTO xm_rspd_data FORMAT Parquet\"\r\nCode: 73. DB::Exception: Unknown format Parquet: data for INSERT was parsed from stdin\r\n\r\n\u279c  ~ clickhouse-client\r\nClickHouse client version 20.10.1.4800 (official build).\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 20.9.2 revision 54439.\r\n\r\nClickHouse server version is older than ClickHouse client. It may indicate that the server is out of date and can be upgraded.\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15464/comments",
    "author": "pan3793",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-30T13:04:54Z",
        "body": "Parquet is excluded from MacOS build, and mysql. \r\nCheck make file. \r\n\r\nYou can use dockerized `clickhouse-client` as well.\r\ndocker run -it --rm --link some-clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server"
      },
      {
        "user": "pan3793",
        "created_at": "2020-09-30T13:10:58Z",
        "body": "> Parquet is excluded from MacOS build, and mysql.\r\n> Check make file.\r\n> \r\n> You can use dockerized `clickhouse-client` as well.\r\n> docker run -it --rm --link some-clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server\r\n\r\nThanks for explanation, it works in docker."
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable Parquet format support in ClickHouse on macOS",
      "Must provide a way to use ClickHouse client with Parquet functionality on macOS",
      "Should address environment limitations of macOS builds"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:02:15"
    }
  },
  {
    "number": 14872,
    "title": "Create a UDF in ClickHouse 20.8.1.1",
    "created_at": "2020-09-16T07:51:56Z",
    "closed_at": "2020-09-16T10:16:56Z",
    "labels": [
      "question",
      "st-community-taken",
      "comp-functions",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14872",
    "body": "I have create file under the Function file.And it  compiles in libclickhouse_functions.a.After I update the Clickhouse and run it.it's throw the error DB::Exception: Unknown function sayHello.Is there any else I need to do it,f or running my UDF?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14872/comments",
    "author": "roycyz",
    "comments": [
      {
        "user": "sundy-li",
        "created_at": "2020-09-16T08:24:37Z",
        "body": "@roycyz  did you register the function? see `registerFunctions.cpp`"
      },
      {
        "user": "roycyz",
        "created_at": "2020-09-16T08:43:42Z",
        "body": "> @roycyz did you register the function? see `registerFunctions.cpp`\r\nthanks for replaying,It worked."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of function registration requirements in ClickHouse",
      "Clarification of post-compilation deployment steps",
      "Identification of system integration requirements"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:03:23"
    }
  },
  {
    "number": 13327,
    "title": "joinGet result invalid.",
    "created_at": "2020-08-04T09:57:59Z",
    "closed_at": "2020-08-07T13:21:55Z",
    "labels": [
      "question",
      "comp-joins",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13327",
    "body": "**Describe the bug**\r\nclickhouse version: 20.6.1.4066\r\n\r\n\r\n* Queries to run that lead to unexpected result\r\n select joinGet('db.T2','id',tid) as nodeId,count(*) from db.T1 where tid='1000' group by nodeId\r\n\uff08db.T2 use storageJoin engine, join type parameter: left)\r\nresult:\r\nnodeId  count(*)\r\n0\t593\r\n43\t70\r\n\r\n**Expected behavior**\r\nexpected result:\r\nnodeId  count(*)\r\n43\t663\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13327/comments",
    "author": "templarzq",
    "comments": [
      {
        "user": "templarzq",
        "created_at": "2020-08-04T09:58:35Z",
        "body": "db.T2  column id type: int32"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-04T19:14:11Z",
        "body": "@templarzq  Do you have reproducible example? \r\nDoes it work before 20.6?\r\nDo you expect that joinGet have to return something instead of 0 in case of `left` ?"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-05T03:41:05Z",
        "body": "it works before version 20.5 (include 20.5.1.1)"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-05T21:18:21Z",
        "body": "OK. And how to reproduce it?\r\n\r\n@templarzq \r\n```\r\n\r\ncreate table T1 Engine=MergeTree order by tuple() as select intDiv(number,1000) tid from numbers(1000000);\r\ncreate table T2 Engine=Join(any, left,tid) as select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\n\r\nselect joinGet('db.T2','id',tid) as nodeId,count() from T1 where tid='333' group by nodeId\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      5 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-06T03:57:31Z",
        "body": "create table xxx on cluster bench_cluster(\r\nxxx\r\n)\r\nENGINE =Join(ANY, LEFT, Id)\n\n---\n\nmaybe the option \"on cluster xxx\"  lead to this result?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-06T04:04:22Z",
        "body": "> maybe the option \"on cluster xxx\" lead to this result?\r\n\r\nI don't see how. \r\nPlease provide reproducible example."
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-06T06:55:30Z",
        "body": "create table T1 on cluster bench_cluster Engine=MergeTree order by tuple() as select intDiv(number,1000) tid from numbers(1000000);\r\ncreate table T2 on cluster bench_cluster(\r\n  id UInt32,\r\n  tid UInt64\r\n) Engine=Join(any, left,tid);\r\ninsert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\ncreate table T3 on cluster bench_cluster as T1   ENGINE  = Distributed(bench_cluster, default,  T1, sipHash64(tid)); \r\n\r\n\r\nselect joinGet('default.T2','id',tid) as nodeId,count(*) from T3 where tid='333' group by nodeId\n\n---\n\nbench_cluster have more than 1 node."
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-06T13:42:50Z",
        "body": "It's because T2 (Join) is empty on shards (it's not a replicated engine). And joinGet works on shards against empty table.\r\nYou can fill T2 at all nodes with the same data or perform joinGet at the initiator using `from()`.\r\n\r\n```SQL\r\ncreate table T1 on cluster segmented (tid UInt64) Engine=MergeTree order by tuple();\r\ncreate table T3 on cluster segmented as T1 ENGINE = Distributed(segmented, currentDatabase(), T1, sipHash64(tid));\r\n\r\n-- data sharded on cluster\r\ninsert into T3  select intDiv(number,1000) tid from numbers(1000000);\r\n\r\ncreate table T2 on cluster segmented(id UInt32, tid UInt64) Engine=Join(any, left,tid);\r\n\r\n-- data only at current node in Engine=Join\r\ninsert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\n\r\n-- WRONG result\r\nSELECT\r\n    joinGet('default.T2', 'id', tid) AS nodeId,\r\n    count(*)\r\nFROM T3\r\nWHERE tid = 333\r\nGROUP BY nodeId\r\n\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      0 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n-- RIGTH result - joinGet is executed at the current node only\r\nSELECT\r\n    joinGet('default.T2', 'id', tid) AS nodeId,\r\n    count(*)\r\nFROM\r\n(\r\n    SELECT tid\r\n    FROM T3\r\n    WHERE tid = 333\r\n)\r\nGROUP BY nodeId\r\n\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      5 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n-- RIGTH result - joinGet is executed at all nodes but it has the same data at all nodes.\r\n-- execute at all nodes\r\nnode2: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\nnode3: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\nnode4: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\nnode5: insert into T2 select number%1000+5 id, intDiv(number,1000)+100 tid from numbers(1000000);\r\n\r\nSELECT\r\n    joinGet('default.T2', 'id', tid) AS nodeId,\r\n    count(*)\r\nFROM T3\r\nWHERE tid = 333\r\nGROUP BY nodeId\r\n\r\n\u250c\u2500nodeId\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502      5 \u2502    1000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\nClickHouse server version 19.13.7\r\n"
      },
      {
        "user": "templarzq",
        "created_at": "2020-08-07T02:00:57Z",
        "body": "ok,thanks."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how distributed cluster architecture affects Join table behavior",
      "Clarification of joinGet execution context in distributed queries",
      "Guidance on maintaining Join table consistency across cluster nodes"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:05:09"
    }
  },
  {
    "number": 9915,
    "title": "toStartHour doesn't work in MV?",
    "created_at": "2020-03-28T23:16:42Z",
    "closed_at": "2020-03-28T23:36:45Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9915",
    "body": "**Describe the bug**\r\n`toStartOfHour`  in a MV returns 0.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: `20.3.4.10 (official build)` in Ubuntu bionic\r\n* Which interface to use, if matters: `clickhouse-client -m -n`\r\n* Non-default settings, if any: none \r\n* `CREATE TABLE` statements for all tables involved\r\n```\r\nCREATE TABLE test.a (\r\n  t DateTime\r\n)\r\nENGINE=Memory();\r\nCREATE TABLE test.b\r\n(\r\n  t DateTime\r\n)\r\nENGINE=Memory();\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t)\r\nFROM test.a;\r\n```\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\n```\r\nINSERT INTO test.a (t) VALUES ('2020-03-28 11:22:33');\r\n```\r\n* Queries to run that lead to unexpected result\r\n```\r\nSELECT * FROM test.b;\r\n```\r\n**Expected behavior**\r\nI expected one row from table `b`\r\n```\r\n2020-03-28 11:00:00\r\n```\r\nThe actual result is table `b` is empty.\r\n\r\n**Error message and/or stacktrace**\r\nNo errors in the log.\r\n\r\n**Additional context**\r\nOne row is inserted if a column is added.  The value for column `t` is still wrong.\r\n\r\n```\r\nCREATE TABLE test.a (\r\n  t DateTime,\r\n  name String\r\n)\r\nENGINE=Memory();\r\nCREATE TABLE test.b\r\n(\r\n  t DateTime,\r\n  name String\r\n)\r\nENGINE=Memory();\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t),\r\n    name\r\nFROM test.a;\r\nINSERT INTO test.a (t, name) VALUES ('2020-03-28 11:22:33', 'myname');\r\nSELECT * FROM test.b;\r\n```\r\nThe output is\r\n```\r\n0000-00-00 00:00:00\tmyname\r\n```\r\n\r\nThe same SELECT returns correct values if I run it manually in the client.\r\n\r\n```\r\nSELECT\r\n    toStartOfHour(t),\r\n    name\r\nFROM test.a\r\n\r\n\u250c\u2500\u2500\u2500\u2500toStartOfHour(t)\u2500\u252c\u2500name\u2500\u2500\u2500\u2510\r\n\u2502 2020-03-28 11:00:00 \u2502 myname \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9915/comments",
    "author": "knoguchi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-28T23:34:40Z",
        "body": "Column names must be the same in a MV select and the target (TO) table \r\n\r\nCREATE MATERIALIZED VIEW test.a2b TO test.b AS\r\nSELECT\r\n    toStartOfHour(t) **as t**\r\nFROM test.a;\r\n\r\nMV uses column names when it does insert into the target table."
      },
      {
        "user": "knoguchi",
        "created_at": "2020-03-28T23:36:16Z",
        "body": "ouch.  I overlooked it.  Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of column name requirements for Materialized View target tables",
      "Clarification of data mapping mechanics in Materialized Views",
      "Identification of silent failure patterns in ClickHouse schema mismatches"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:06:57"
    }
  },
  {
    "number": 9870,
    "title": "Cannot replicate table from 19.3.3 to 20.3.3",
    "created_at": "2020-03-25T19:17:58Z",
    "closed_at": "2020-03-30T14:24:47Z",
    "labels": [
      "question",
      "backward compatibility",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9870",
    "body": "We created a new node using version 20.3.3 and tried to replicate a number of tables from a 19.3.3 node.  The initial replication worked, but upon restarting the 20.3.3 node got the following failure (this happened for several tables):\r\n\r\n```\r\nExisting table metadata in ZooKeeper differs in index granularity bytes. Stored in\r\nZooKeeper: 10485760, local: 0: Cannot attach table `<db>`.`<table>` from metadata\r\nfile /opt/data/clickhouse/metadata/<db>/<table> from query ATTACH TABLE <table>\r\n(`datetime` DateTime, `kafka_time` DateTime, `hostname` String, `message` String)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/<db.table>, '{replica}')\r\nPARTITION BY toDate(datetime) ORDER BY (datetime, hostname) SETTINGS\r\nindex_granularity = 8192\r\n```\r\n\r\nIndex granularity on both tables is 8192, metadata .sql file is identical.  metadata from zookeeper node:\r\n\r\n```\r\nmetadata format version: 1\r\ndate column: \r\nsampling expression: \r\nindex granularity: 8192\r\nmode: 0\r\nsign column: \r\nprimary key: datetime, hostname\r\ndata format version: 1\r\npartition key: toDate(datetime)\r\n```\r\n\r\nStack trace:\r\n\r\n```0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x102d352c in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x8f2d989 in /usr/bin/clickhouse\r\n2. ? @ 0xd94cdce in /usr/bin/clickhouse\r\n3. DB::StorageReplicatedMergeTree::checkTableStructure(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xd5b426b in /usr/bin/clickhouse\r\n4. DB::StorageReplicatedMergeTree::StorageReplicatedMergeTree(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bo\r\nol, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata const&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char\r\n> > const&, DB::MergeTreeData::MergingParams const&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >, bool) @ 0xd5d9b4b in /usr/bin/clickhouse\r\n5. ? @ 0xd957dba in /usr/bin/clickhouse\r\n6. std::__1::__function::__func<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&), std::__1::allocator<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&)>, std::__1::shared_ptr<DB::IStorage> (DB::Sto\r\nrageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) @ 0xd95b2d3 in /usr/bin/clickhouse\r\n7. DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, DB::Context&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) cons\r\nt @ 0xd4fbc4c in /usr/bin/clickhouse\r\n8. DB::createTableFromAST(DB::ASTCreateQuery, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool\r\n) @ 0xcedc09e in /usr/bin/clickhouse\r\n9. ? @ 0xced2bcf in /usr/bin/clickhouse\r\n10. ? @ 0xced3381 in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x8f515e7 in /usr/bin/clickhouse\r\n12. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleI\r\nmpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const @ 0x8f51c34 in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8f50b07 in /usr/bin/clickhouse\r\n14. ? @ 0x8f4f00f in /usr/bin/clickhouse\r\n15. start_thread @ 0x7e65 in /usr/lib64/libpthread-2.17.so\r\n16. clone @ 0xfe88d in /usr/lib64/libc-2.17.so\r\n (version 20.3.3.6 (official build))\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9870/comments",
    "author": "genzgd",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-25T19:25:37Z",
        "body": "20.3.3 by default uses adaptive index granularity. Such feature is not existed in 19.3.3.\r\n\r\nCreate tables without `adaptive index granularity`\r\n\r\n20.3.3\r\ncreate table ....\r\nsettings index_granularity =8192,  index_granularity_bytes = 0;\r\n\r\n19.3.3\r\ncreate table ....\r\nsettings index_granularity =8192;\r\n\r\nBut there is one problem. LZ4 compression format is incompatible < 19.7 and  >= 19.7 .\r\nYou can temporary use replication 20.3.3 <-> 19.3.3. But you need to upgrade 19.3.3 as soon as possible."
      },
      {
        "user": "genzgd",
        "created_at": "2020-03-30T15:07:32Z",
        "body": "Yes, we definitely had some weirdness because we upgrade to 19.17 at one point and then downgraded back to 19.3.3, so there was some unexpected inconsistency in zookeeper metadata definitions around index granularity.  Thanks for pointing us in the right direction!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of version compatibility issues between ClickHouse 19.3.3 and 20.3.3 regarding index granularity settings",
      "Method to ensure consistent metadata between ZooKeeper and local table definitions across versions",
      "Configuration approach for backward compatibility during version upgrades",
      "Guidance on preventing metadata inconsistencies during cluster upgrades/downgrades"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:07:19"
    }
  },
  {
    "number": 8592,
    "title": "Memory limit (for query) exceeded on SELECT",
    "created_at": "2020-01-09T13:29:06Z",
    "closed_at": "2020-01-11T17:52:42Z",
    "labels": [
      "question",
      "memory"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8592",
    "body": "Hi,\r\n\r\nI am running a quite complex SELECT query on a clickhouse 19.16.5.15, and I have the following error :\r\n`DB::Exception: Memory limit (for query) exceeded: would use 723.47 MiB (attempt to allocate chunk of 5201580 bytes), maximum: 720.00 MiB.`\r\n\r\nPreviously, playing with max_bytes_before_external_sort and max_bytes_before_external_group_by (setting them to half of the max_memory_usage), allow me to run such queries, but it is no more be the case. \r\nMy current configuration is :\r\n```\r\n<max_memory_usage>754974720</max_memory_usage>\r\n<max_bytes_before_external_sort>377487360</max_bytes_before_external_sort>\r\n<max_bytes_before_external_group_by>377487360</max_bytes_before_external_group_by>\r\n<max_memory_usage_for_all_queries>1509949440</max_memory_usage_for_all_queries>\r\n```\r\n\r\nIf I activated debug log, I could see that the query seems to go on disk (what I expect), since I get several:\r\n`2020.01.08 14:22:46.881201 [ 46 ] {767e6850-f1b4-49ae-af81-a62e8e24573c} <Debug> Aggregator: Writing part of aggregation data into temporary file /data/tmp/tmp30010qaaaaa.`\r\n\r\nNevertheless, I finally got this stacktrace:\r\n```\r\n0. 0x3582798 StackTrace::StackTrace() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n1. 0x358b1df DB::Exception::Exception(std::string const&, int) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n2. 0x5bf6a99 DB::IBlockInputStream::checkTimeLimit() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n3. 0x5bfb2d0 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n4. 0x6240336 DB::FilterBlockInputStream::readImpl() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n5. 0x5bfb2f5 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n6. 0x62373a8 DB::ExpressionBlockInputStream::readImpl() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n7. 0x5bfb2f5 DB::IBlockInputStream::read() /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n8. 0x626c7a9 DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n9. 0x626ce6b ThreadFromGlobalPool::ThreadFromGlobalPool<void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*, std::shared_ptr<DB::ThreadGroupStatus>, unsigned long&>(void (DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::*&&)(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long), DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>*&&, std::shared_ptr<DB::ThreadGroupStatus>&&, unsigned long&)::{lambda()#1}::operator()() const /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n10. 0x35bf902 ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>) /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n11. 0x722562f execute_native_thread_routine /opt/InfoVista/5ViewSDM/bin/clickhouse\r\n12. 0x7fe8c1fdfdd5 start_thread /usr/lib64/libpthread-2.17.so\r\n13. 0x7fe8c1b04ead clone /usr/lib64/libc-2.17.so\r\n```\r\n\r\nI also try to decrease max_insert_block_size and max_block_size, without any improvement.\r\n\r\nDo you know other settings I could play for allowing this query to be executed ? (even if it is slow).\r\n\r\nThe exact query is the following:\r\n```\r\nselect\r\n\t\t/* @TOPN_SELECT_PART@ */\r\n\t\t\r\n\t\tany(ifNull(if(c1 = 0, '(UNDEFINED)',dictGetString('agents', 'name', toUInt64(c1))),'(UNDEFINED)')) as `co1`, \r\n\t\tany(ifNull(if(c2 = 0, '(UNDEFINED)',dictGetString('applications', 'name', toUInt64(c2))),'(UNDEFINED)')) as `co2`, \r\n\t\tany(ifNull(if(c3 = 0, '(UNDEFINED)',dictGetString('netflow_sources', 'name', toUInt64(c3))),'(UNDEFINED)')) as `co3`, \r\n\t\tany(ifNull(if(c4 = 0, '(UNDEFINED)',dictGetString('wakb_ip_protocols', 'name', toUInt64(c4))),'(UNDEFINED)')) as `co4`, \r\n\t\tany(ifNull(if(c5 = 0, '(UNDEFINED)',dictGetString('offices', 'name', toUInt64(c5))),'(UNDEFINED)')) as `co5`, \r\n\t\tany(ifNull(if(c6 = 0, '(UNDEFINED)',dictGetString('offices', 'name', toUInt64(c6))),'(UNDEFINED)')) as `co6`, \r\n\t\tany(ifNull(if(c7 = 0, '(UNDEFINED)',dictGetString('netflow_interfaces', 'name', toUInt64(c7))),'(UNDEFINED)')) as `co7`, \r\n\t\tany(ifNull(if(c8 = 0, '(UNDEFINED)',dictGetString('netflow_interfaces', 'name', toUInt64(c8))),'(UNDEFINED)')) as `co8`, \r\n\t\tany(ifNull(t.`ClientIp`,'(UNDEFINED)')) as `co9`, \r\n\t\tany(ifNull(t.`ServerIp`,'(UNDEFINED)')) as `co10`, \r\n\t\tany(ifNull(if(c11 = 0, '(UNDEFINED)',dictGetString('classes_of_service', 'name', toUInt64(c11))),'(UNDEFINED)')) as `co11`, \r\n\t\tany(ifNull(if(c12 = 0, '(UNDEFINED)',dictGetString('classes_of_service', 'name', toUInt64(c12))),'(UNDEFINED)')) as `co12`, \r\n\t\tany(ifNull(t.`Port`, 0)) as `co13`, \r\n\t\tany(ifNull(t.`DomainName`,'(UNDEFINED)')) as `co14`, \r\n\t\tany(ifNull(t.`Flags`, 0)) as `co15`, \r\n\t\tmax(t.`ClientNetworkTimeMax`) as `co16`, \r\n\t\tmin(t.`ClientNetworkTimeMin`) as `co17`, \r\n\t\tsum(t.`ClientNetworkTimeSum`) as `co18`, \r\n\t\tsum(t.`ClientPackets`) as `co19`, \r\n\t\tsum(t.`ClientBytes`) as `co20`, \r\n\t\tsum(t.`ClientDataBytes`) as `co21`, \r\n\t\tsum(t.`ClientDataPackets`) as `co22`, \r\n\t\tsum(t.`ServerResponseTimeSum`) as `co23`, \r\n\t\tmin(t.`ServerResponseTimeMin`) as `co24`, \r\n\t\tmax(t.`ServerResponseTimeMax`) as `co25`, \r\n\t\tsum(t.`ServerPackets`) as `co26`, \r\n\t\tsum(t.`ServerBytes`) as `co27`, \r\n\t\tsum(t.`1000msResponsesNb`) as `co28`, \r\n\t\tsum(t.`100msResponsesNb`) as `co29`, \r\n\t\tsum(t.`10msResponsesNb`) as `co30`, \r\n\t\tsum(t.`2msResponsesNb`) as `co31`, \r\n\t\tsum(t.`500msResponsesNb`) as `co32`, \r\n\t\tsum(t.`50msResponsesNb`) as `co33`, \r\n\t\tsum(t.`5msResponsesNb`) as `co34`, \r\n\t\tsum(t.`LateResponsesNb`) as `co35`, \r\n\t\tsum(t.`NewConnectionsNb`) as `co36`, \r\n\t\tmax(t.`ResponseTimeMax`) as `co37`, \r\n\t\tmin(t.`ResponseTimeMin`) as `co38`, \r\n\t\tsum(t.`ResponseTimeSum`) as `co39`, \r\n\t\tsum(t.`ResponsesNb`) as `co40`, \r\n\t\tsum(t.`RetransmissionsNb`) as `co41`, \r\n\t\tsum(t.`ServerDataBytes`) as `co42`, \r\n\t\tsum(t.`ServerDataPackets`) as `co43`, \r\n\t\tmax(t.`ServerNetworkTimeMax`) as `co44`, \r\n\t\tmin(t.`ServerNetworkTimeMin`) as `co45`, \r\n\t\tsum(t.`ServerNetworkTimeSum`) as `co46`, \r\n\t\tmax(t.`TotalNetworkTimeMax`) as `co47`, \r\n\t\tmin(t.`TotalNetworkTimeMin`) as `co48`, \r\n\t\tsum(t.`TotalNetworkTimeSum`) as `co49`, \r\n\t\tmax(t.`TotalResponseTimeMax`) as `co50`, \r\n\t\tmin(t.`TotalResponseTimeMin`) as `co51`, \r\n\t\tsum(t.`TotalResponseTimeSum`) as `co52`, \r\n\t\tmax(t.`TotalTransactionTimeMax`) as `co53`, \r\n\t\tmin(t.`TotalTransactionTimeMin`) as `co54`, \r\n\t\tsum(t.`TotalTransactionTimeSum`) as `co55`, \r\n\t\tsum(t.`TransactionsNb`) as `co56`, \r\n\t\tsum(t.`WaasDreInput`) as `co57`, \r\n\t\tsum(t.`WaasDreOutput`) as `co58`, \r\n\t\tsum(t.`WaasInputBytes`) as `co59`, \r\n\t\tsum(t.`WaasLzInput`) as `co60`, \r\n\t\tsum(t.`WaasLzOutput`) as `co61`, \r\n\t\tsum(t.`WaasOutputBytes`) as `co62`, \r\n\t\tany(t.`c1`) as `co63`, \r\n\t\tany(t.`c2`) as `co64`, \r\n\t\tany(t.`c3`) as `co65`, \r\n\t\tany(t.`c4`) as `co66`, \r\n\t\tany(t.`c5`) as `co67`, \r\n\t\tany(t.`c6`) as `co68`, \r\n\t\tany(t.`c7`) as `co69`, \r\n\t\tany(t.`c8`) as `co70`, \r\n\t\tany(t.`c11`) as `co71`, \r\n\t\tany(t.`c12`) as `co72` ,\r\n\t\tany(ranking_row) as final_ranking\r\nfrom (\r\n\tselect\r\n\t\t\t/* @OUTER_SELECT_PART@ */\r\n\t\t\tc9 as `ClientIp`,\r\n\t\t\tc10 as `ServerIp`,\r\n\t\t\tc13 as `Port`,\r\n\t\t\tc14 as `DomainName`,\r\n\t\t\tc15 as `Flags`,\r\n\t\t\tc16 as `ClientNetworkTimeMax`,\r\n\t\t\tc17 as `ClientNetworkTimeMin`,\r\n\t\t\tc18 as `ClientNetworkTimeSum`,\r\n\t\t\tc19 as `ClientPackets`,\r\n\t\t\tc20 as `ClientBytes`,\r\n\t\t\tc21 as `ClientDataBytes`,\r\n\t\t\tc22 as `ClientDataPackets`,\r\n\t\t\tc23 as `ServerResponseTimeSum`,\r\n\t\t\tc24 as `ServerResponseTimeMin`,\r\n\t\t\tc25 as `ServerResponseTimeMax`,\r\n\t\t\tc26 as `ServerPackets`,\r\n\t\t\tc27 as `ServerBytes`,\r\n\t\t\tc28 as `1000msResponsesNb`,\r\n\t\t\tc29 as `100msResponsesNb`,\r\n\t\t\tc30 as `10msResponsesNb`,\r\n\t\t\tc31 as `2msResponsesNb`,\r\n\t\t\tc32 as `500msResponsesNb`,\r\n\t\t\tc33 as `50msResponsesNb`,\r\n\t\t\tc34 as `5msResponsesNb`,\r\n\t\t\tc35 as `LateResponsesNb`,\r\n\t\t\tc36 as `NewConnectionsNb`,\r\n\t\t\tc37 as `ResponseTimeMax`,\r\n\t\t\tc38 as `ResponseTimeMin`,\r\n\t\t\tc39 as `ResponseTimeSum`,\r\n\t\t\tc40 as `ResponsesNb`,\r\n\t\t\tc41 as `RetransmissionsNb`,\r\n\t\t\tc42 as `ServerDataBytes`,\r\n\t\t\tc43 as `ServerDataPackets`,\r\n\t\t\tc44 as `ServerNetworkTimeMax`,\r\n\t\t\tc45 as `ServerNetworkTimeMin`,\r\n\t\t\tc46 as `ServerNetworkTimeSum`,\r\n\t\t\tc47 as `TotalNetworkTimeMax`,\r\n\t\t\tc48 as `TotalNetworkTimeMin`,\r\n\t\t\tc49 as `TotalNetworkTimeSum`,\r\n\t\t\tc50 as `TotalResponseTimeMax`,\r\n\t\t\tc51 as `TotalResponseTimeMin`,\r\n\t\t\tc52 as `TotalResponseTimeSum`,\r\n\t\t\tc53 as `TotalTransactionTimeMax`,\r\n\t\t\tc54 as `TotalTransactionTimeMin`,\r\n\t\t\tc55 as `TotalTransactionTimeSum`,\r\n\t\t\tc56 as `TransactionsNb`,\r\n\t\t\tc57 as `WaasDreInput`,\r\n\t\t\tc58 as `WaasDreOutput`,\r\n\t\t\tc59 as `WaasInputBytes`,\r\n\t\t\tc60 as `WaasLzInput`,\r\n\t\t\tc61 as `WaasLzOutput`,\r\n\t\t\tc62 as `WaasOutputBytes`,\r\n\t\t\tc1 as `c1`,\r\n\t\t\tc2 as `c2`,\r\n\t\t\tc3 as `c3`,\r\n\t\t\tc4 as `c4`,\r\n\t\t\tc5 as `c5`,\r\n\t\t\tc6 as `c6`,\r\n\t\t\tc7 as `c7`,\r\n\t\t\tc8 as `c8`,\r\n\t\t\tc11 as `c11`,\r\n\t\t\tc12 as `c12`,\r\n\t\t\trowNumberInAllBlocks() as ranking_row\r\n\tfrom\r\n\t\t(select \r\n\t\t\t\t/* @AGGR_OUT_PART@ */\r\n\t\t\t\taggr_in.`Agent` as c1,\r\n\t\t\t\taggr_in.`Application` as c2,\r\n\t\t\t\taggr_in.`Source` as c3,\r\n\t\t\t\taggr_in.`Protocol` as c4,\r\n\t\t\t\taggr_in.`ClientOffice` as c5,\r\n\t\t\t\taggr_in.`ServerOffice` as c6,\r\n\t\t\t\taggr_in.`ClientInterface` as c7,\r\n\t\t\t\taggr_in.`ServerInterface` as c8,\r\n\t\t\t\taggr_in.`ClientIp` as c9,\r\n\t\t\t\taggr_in.`ServerIp` as c10,\r\n\t\t\t\taggr_in.`ClientCos` as c11,\r\n\t\t\t\taggr_in.`ServerCos` as c12,\r\n\t\t\t\taggr_in.`Port` as c13,\r\n\t\t\t\taggr_in.`DomainName` as c14,\r\n\t\t\t\taggr_in.`Flags` as c15,\r\n\t\t\t\tmax(aggr_in.`ClientNetworkTimeMax`) as c16,\r\n\t\t\t\tmin(aggr_in.`ClientNetworkTimeMin`) as c17,\r\n\t\t\t\tsum(aggr_in.`ClientNetworkTimeSum`) as c18,\r\n\t\t\t\tsum(aggr_in.`ClientPackets`) as c19,\r\n\t\t\t\tsum(aggr_in.`ClientBytes`) as c20,\r\n\t\t\t\tsum(aggr_in.`ClientDataBytes`) as c21,\r\n\t\t\t\tsum(aggr_in.`ClientDataPackets`) as c22,\r\n\t\t\t\tsum(aggr_in.`ServerResponseTimeSum`) as c23,\r\n\t\t\t\tmin(aggr_in.`ServerResponseTimeMin`) as c24,\r\n\t\t\t\tmax(aggr_in.`ServerResponseTimeMax`) as c25,\r\n\t\t\t\tsum(aggr_in.`ServerPackets`) as c26,\r\n\t\t\t\tsum(aggr_in.`ServerBytes`) as c27,\r\n\t\t\t\tsum(aggr_in.`1000msResponsesNb`) as c28,\r\n\t\t\t\tsum(aggr_in.`100msResponsesNb`) as c29,\r\n\t\t\t\tsum(aggr_in.`10msResponsesNb`) as c30,\r\n\t\t\t\tsum(aggr_in.`2msResponsesNb`) as c31,\r\n\t\t\t\tsum(aggr_in.`500msResponsesNb`) as c32,\r\n\t\t\t\tsum(aggr_in.`50msResponsesNb`) as c33,\r\n\t\t\t\tsum(aggr_in.`5msResponsesNb`) as c34,\r\n\t\t\t\tsum(aggr_in.`LateResponsesNb`) as c35,\r\n\t\t\t\tsum(aggr_in.`NewConnectionsNb`) as c36,\r\n\t\t\t\tmax(aggr_in.`ResponseTimeMax`) as c37,\r\n\t\t\t\tmin(aggr_in.`ResponseTimeMin`) as c38,\r\n\t\t\t\tsum(aggr_in.`ResponseTimeSum`) as c39,\r\n\t\t\t\tsum(aggr_in.`ResponsesNb`) as c40,\r\n\t\t\t\tsum(aggr_in.`RetransmissionsNb`) as c41,\r\n\t\t\t\tsum(aggr_in.`ServerDataBytes`) as c42,\r\n\t\t\t\tsum(aggr_in.`ServerDataPackets`) as c43,\r\n\t\t\t\tmax(aggr_in.`ServerNetworkTimeMax`) as c44,\r\n\t\t\t\tmin(aggr_in.`ServerNetworkTimeMin`) as c45,\r\n\t\t\t\tsum(aggr_in.`ServerNetworkTimeSum`) as c46,\r\n\t\t\t\tmax(aggr_in.`TotalNetworkTimeMax`) as c47,\r\n\t\t\t\tmin(aggr_in.`TotalNetworkTimeMin`) as c48,\r\n\t\t\t\tsum(aggr_in.`TotalNetworkTimeSum`) as c49,\r\n\t\t\t\tmax(aggr_in.`TotalResponseTimeMax`) as c50,\r\n\t\t\t\tmin(aggr_in.`TotalResponseTimeMin`) as c51,\r\n\t\t\t\tsum(aggr_in.`TotalResponseTimeSum`) as c52,\r\n\t\t\t\tmax(aggr_in.`TotalTransactionTimeMax`) as c53,\r\n\t\t\t\tmin(aggr_in.`TotalTransactionTimeMin`) as c54,\r\n\t\t\t\tsum(aggr_in.`TotalTransactionTimeSum`) as c55,\r\n\t\t\t\tsum(aggr_in.`TransactionsNb`) as c56,\r\n\t\t\t\tsum(aggr_in.`WaasDreInput`) as c57,\r\n\t\t\t\tsum(aggr_in.`WaasDreOutput`) as c58,\r\n\t\t\t\tsum(aggr_in.`WaasInputBytes`) as c59,\r\n\t\t\t\tsum(aggr_in.`WaasLzInput`) as c60,\r\n\t\t\t\tsum(aggr_in.`WaasLzOutput`) as c61,\r\n\t\t\t\tsum(aggr_in.`WaasOutputBytes`) as c62\r\n\t\tfrom (\r\n\t\t\tselect\r\n\t\t\t\t\t/* @AGGR_IN_PART@ */\r\n\t\t\t\t\ttoStartOfMinute(data_table.`Timestamp`, 'Europe/Paris') as Timestamp,\r\n\t\t\t\t\tmax(data_table.`ClientNetworkTimeMax`) as `ClientNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ClientNetworkTimeMin`) as `ClientNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ClientNetworkTimeSum`) as `ClientNetworkTimeSum`,\r\n\t\t\t\t\tsum(data_table.`ClientPackets`) as `ClientPackets`,\r\n\t\t\t\t\tsum(data_table.`ClientBytes`) as `ClientBytes`,\r\n\t\t\t\t\tsum(data_table.`ClientDataBytes`) as `ClientDataBytes`,\r\n\t\t\t\t\tsum(data_table.`ClientDataPackets`) as `ClientDataPackets`,\r\n\t\t\t\t\tsum(data_table.`ServerResponseTimeSum`) as `ServerResponseTimeSum`,\r\n\t\t\t\t\tmin(data_table.`ServerResponseTimeMin`) as `ServerResponseTimeMin`,\r\n\t\t\t\t\tmax(data_table.`ServerResponseTimeMax`) as `ServerResponseTimeMax`,\r\n\t\t\t\t\tsum(data_table.`ServerPackets`) as `ServerPackets`,\r\n\t\t\t\t\tsum(data_table.`ServerBytes`) as `ServerBytes`,\r\n\t\t\t\t\tsum(data_table.`1000msResponsesNb`) as `1000msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`100msResponsesNb`) as `100msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`10msResponsesNb`) as `10msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`2msResponsesNb`) as `2msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`500msResponsesNb`) as `500msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`50msResponsesNb`) as `50msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`5msResponsesNb`) as `5msResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`LateResponsesNb`) as `LateResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`NewConnectionsNb`) as `NewConnectionsNb`,\r\n\t\t\t\t\tmax(data_table.`ResponseTimeMax`) as `ResponseTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ResponseTimeMin`) as `ResponseTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ResponseTimeSum`) as `ResponseTimeSum`,\r\n\t\t\t\t\tsum(data_table.`ResponsesNb`) as `ResponsesNb`,\r\n\t\t\t\t\tsum(data_table.`RetransmissionsNb`) as `RetransmissionsNb`,\r\n\t\t\t\t\tsum(data_table.`ServerDataBytes`) as `ServerDataBytes`,\r\n\t\t\t\t\tsum(data_table.`ServerDataPackets`) as `ServerDataPackets`,\r\n\t\t\t\t\tmax(data_table.`ServerNetworkTimeMax`) as `ServerNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`ServerNetworkTimeMin`) as `ServerNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`ServerNetworkTimeSum`) as `ServerNetworkTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalNetworkTimeMax`) as `TotalNetworkTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalNetworkTimeMin`) as `TotalNetworkTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalNetworkTimeSum`) as `TotalNetworkTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalResponseTimeMax`) as `TotalResponseTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalResponseTimeMin`) as `TotalResponseTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalResponseTimeSum`) as `TotalResponseTimeSum`,\r\n\t\t\t\t\tmax(data_table.`TotalTransactionTimeMax`) as `TotalTransactionTimeMax`,\r\n\t\t\t\t\tmin(data_table.`TotalTransactionTimeMin`) as `TotalTransactionTimeMin`,\r\n\t\t\t\t\tsum(data_table.`TotalTransactionTimeSum`) as `TotalTransactionTimeSum`,\r\n\t\t\t\t\tsum(data_table.`TransactionsNb`) as `TransactionsNb`,\r\n\t\t\t\t\tsum(data_table.`WaasDreInput`) as `WaasDreInput`,\r\n\t\t\t\t\tsum(data_table.`WaasDreOutput`) as `WaasDreOutput`,\r\n\t\t\t\t\tsum(data_table.`WaasInputBytes`) as `WaasInputBytes`,\r\n\t\t\t\t\tsum(data_table.`WaasLzInput`) as `WaasLzInput`,\r\n\t\t\t\t\tsum(data_table.`WaasLzOutput`) as `WaasLzOutput`,\r\n\t\t\t\t\tsum(data_table.`WaasOutputBytes`) as `WaasOutputBytes`,\r\n\t\t\t\t\tdata_table.`Agent` as `Agent`,\r\n\t\t\t\t\tdata_table.`Application` as `Application`,\r\n\t\t\t\t\tdata_table.`Source` as `Source`,\r\n\t\t\t\t\tdata_table.`Protocol` as `Protocol`,\r\n\t\t\t\t\tdata_table.`ClientOffice` as `ClientOffice`,\r\n\t\t\t\t\tdata_table.`ServerOffice` as `ServerOffice`,\r\n\t\t\t\t\tdata_table.`ClientInterface` as `ClientInterface`,\r\n\t\t\t\t\tdata_table.`ServerInterface` as `ServerInterface`,\r\n\t\t\t\t\tdata_table.`ClientIp` as `ClientIp`,\r\n\t\t\t\t\tdata_table.`ServerIp` as `ServerIp`,\r\n\t\t\t\t\tdata_table.`ClientCos` as `ClientCos`,\r\n\t\t\t\t\tdata_table.`ServerCos` as `ServerCos`,\r\n\t\t\t\t\tdata_table.`Port` as `Port`,\r\n\t\t\t\t\tdata_table.`DomainName` as `DomainName`,\r\n\t\t\t\t\tdata_table.`Flags` as `Flags`\r\n\t\t\tfrom\r\n\t\t\t\t\t/* @DATA_TABLE@ */\r\n\t\t\t\t\tavc.topconversationdetails as data_table\r\n\t\t\t\t\t/* @INNER_JOIN_PART@ */\r\n\t\t\t\t\t\r\n\t\t\twhere\r\n\t\t\t\t\t/* @WHERE_PART@ */\r\n\t\t\t\t\tdata_table.`Timestamp` >= toStartOfMinute(toDateTime('2020-01-01 15:54:00','Europe/Paris'), 'Europe/Paris')\r\n\t\t\t\t\tand data_table.`Timestamp` < toStartOfMinute(toDateTime('2020-01-08 15:54:00','Europe/Paris'), 'Europe/Paris')\r\n\t\t\t\t\tand ((toNullable(data_table.`Customer`)= (select a from (select `id` as a from sdm.customers where sdm.customers.`vmId` = 150005) as b)))\r\n\t\t\t\t\tand ((toNullable(data_table.`Service`)= (select a from (select `id` as a from sdm.services where sdm.services.`vmId` = 150006) as b)))\r\n\t\t\tgroup by \r\n\t\t\t\t\t/* @GROUPBY_IN_PART@ */\r\n\t\t\t\t\tTimestamp,\r\n\t\t\t\t\tdata_table.`Agent`,\r\n\t\t\t\t\tdata_table.`Protocol`,\r\n\t\t\t\t\t`ServerIp`,\r\n\t\t\t\t\tdata_table.`ServerCos`,\r\n\t\t\t\t\tdata_table.`ClientOffice`,\r\n\t\t\t\t\tdata_table.`ClientCos`,\r\n\t\t\t\t\t`Flags`,\r\n\t\t\t\t\tdata_table.`ServerInterface`,\r\n\t\t\t\t\tdata_table.`ClientInterface`,\r\n\t\t\t\t\tdata_table.`Application`,\r\n\t\t\t\t\tdata_table.`Source`,\r\n\t\t\t\t\tdata_table.`ServerOffice`,\r\n\t\t\t\t\t`Port`,\r\n\t\t\t\t\t`DomainName`,\r\n\t\t\t\t\t`ClientIp`\r\n\t\t) as aggr_in\r\n\t\t\t/* @MIDDLE_JOIN_PART@ */\r\n\t\t\t\r\n\t\tgroup by\r\n\t\t\t\t/* @GROUPBY_OUT_PART@ */\r\n\t\t\t\taggr_in.`Agent`,\r\n\t\t\t\taggr_in.`Protocol`,\r\n\t\t\t\tc10,\r\n\t\t\t\taggr_in.`ServerCos`,\r\n\t\t\t\taggr_in.`ClientOffice`,\r\n\t\t\t\taggr_in.`ClientCos`,\r\n\t\t\t\tc15,\r\n\t\t\t\taggr_in.`ServerInterface`,\r\n\t\t\t\taggr_in.`ClientInterface`,\r\n\t\t\t\taggr_in.`Application`,\r\n\t\t\t\taggr_in.`Source`,\r\n\t\t\t\taggr_in.`ServerOffice`,\r\n\t\t\t\tc13,\r\n\t\t\t\tc14,\r\n\t\t\t\tc9\r\n\t\torder by\r\n\t\t\t\t/* @ORDERBY_PART@ */\r\n\t\t\t\t( empty(c10) OR c10 = '(UNDEFINED)' OR c10 IS NULL) ASC,\r\n\t\t\t\tc15 IS NULL ASC,\r\n\t\t\t\tc13 IS NULL ASC,\r\n\t\t\t\tc14 IS NULL ASC,\r\n\t\t\t\t( empty(c9) OR c9 = '(UNDEFINED)' OR c9 IS NULL) ASC,\r\n\t\t\t\tc16 desc\r\n\t\t) as aggr_out\r\n\t) as t\r\n\t/* @OUTER_JOIN_PART@ */\r\n\t\r\ngroup by if(ranking_row<100, ranking_row, 101)\r\norder by final_ranking asc\r\n```\r\n\r\nWhere avc.topconversationdetails is a merge tree table, and the other tables are dictionnaries tables.\r\nIt is mostly a top N with others query, with 2 level of aggregation.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8592/comments",
    "author": "edonin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-09T18:27:08Z",
        "body": "723.47 MiB ridiculously unrealistic target. CH was designed to use 10GB as a base point.\r\n\r\ntry `set max_threads=1, max_read_buffer_size=100000, max_compress_block_size=100000, min_compress_block_size=100000`"
      },
      {
        "user": "edonin",
        "created_at": "2020-01-15T09:51:58Z",
        "body": "Thanks for the answer. For me, changing the max_thread to 1 is working. I am able to run my big query on a very small machine."
      }
    ],
    "satisfaction_conditions": [
      "Solution must allow execution of memory-intensive queries on resource-constrained systems",
      "Configuration adjustments should target per-operation memory allocation control",
      "Approach must work with complex aggregation patterns",
      "Solution should prioritize query completion over performance",
      "Must address memory spikes during query processing stages"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:08:42"
    }
  },
  {
    "number": 8122,
    "title": "OPTIMIZE FINAL makes skip index no longer work",
    "created_at": "2019-12-10T13:33:11Z",
    "closed_at": "2019-12-11T04:09:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8122",
    "body": "**Describe the bug or unexpected behaviour**\r\noptimize final makes skip index no longer work\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n19.17.4\r\n* Steps to reproduce\r\n```sql\r\nset allow_experimental_data_skipping_indices=1;\r\ncreate table test(I Int64, S String, INDEX s_index (S) TYPE bloom_filter() GRANULARITY 8192) Engine=MergeTree order by I;\r\ninsert into test select number, toString(rand()) from numbers(10000000);\r\ninsert into test values(45645645, '666');\r\nSET send_logs_level = 'trace';\r\nselect * from test where S = '666';\r\n```\r\n\r\nThis is the correct behavior before `optimize final`: 1 marks to read from 1 ranges, Read 1 rows\r\n```\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393157 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> executeQuery: (from 127.0.0.1:36838) SELECT * FROM test WHERE S = '666'\r\n\u2192 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) [bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393593 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"S = '666'\" moved to PREWHERE\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.393803 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Key condition: unknown\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.401436 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.402200 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.402954 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.403693 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404496 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 1 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404563 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 0 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404598 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> default.test (SelectExecutor): Selected 6 parts by date, 1 parts by key, 1 marks to read from 1 ranges\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404671 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Trace> default.test (SelectExecutor): Reading approx. 8192 rows with 1 streams\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404745 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.404813 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> executeQuery: Query pipeline:\r\nExpression\r\n Expression\r\n  MergeTreeThread\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.405284 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Information> executeQuery: Read 1 rows, 20.00 B in 0.012 sec., 82 rows/sec., 1.62 KiB/sec.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:07:24.405305 {e0215c84-9a4a-4895-8e5e-19a14ee72391} [ 89 ] <Debug> MemoryTracker: Peak memory usage (for query): 10.04 MiB.\r\n\r\n1 rows in set. Elapsed: 0.013 sec.\r\n```\r\n\r\nAfter `optimize table test final`\r\n```sql\r\nselect * from test where S = '666';\r\n```\r\n\r\nthis behavior is unexpected: 1221 marks to read from 1 ranges, Read 10000001 rows\r\n```\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389243 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> executeQuery: (from 127.0.0.1:36838) SELECT * FROM test WHERE S = '666'\r\n\u2197 Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) [bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389696 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"S = '666'\" moved to PREWHERE\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.389902 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Key condition: unknown\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398603 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Index `s_index` has dropped 0 granules.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398652 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> default.test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1221 marks to read from 1 ranges\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398716 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> default.test (SelectExecutor): Reading approx. 10002432 rows with 24 streams\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.398974 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.402274 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> executeQuery: Query pipeline:\r\nUnion\r\n Expression \u00d7 24\r\n  Expression\r\n   MergeTreeThread\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418648 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> UnionBlockInputStream: Waiting for threads to finish\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418689 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Trace> UnionBlockInputStream: Waited for threads to finish\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418751 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Information> executeQuery: Read 10000001 rows, 177.89 MiB in 0.029 sec., 339693250 rows/sec., 5.90 GiB/sec.\r\n[bigdata-clickhouse01.gz01] 2019.12.10 21:14:12.418778 {afcf44f6-a99a-4f00-8b46-78479b84a9ef} [ 89 ] <Debug> MemoryTracker: Peak memory usage (for query): 12.83 MiB.\r\n\r\n1 rows in set. Elapsed: 0.030 sec. Processed 10.00 million rows, 186.54 MB (329.23 million rows/s., 6.14 GB/s.)\r\n```\r\n\r\n**Expected behavior**\r\nOnly 1 mark to read, but 1221 marks to read\r\nOnly 1 Row should be read, but 10000001 rows were read",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8122/comments",
    "author": "kaijianding",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-12-10T16:53:40Z",
        "body": "First of all -- GRANULARITY 8192 is a nonsense. Your index granula will contain 8192*8192 rows.\r\nTry GRANULARITY 2.\r\n\r\nSecond. Before optimize 666 is stored in a separate part [insert into test values(45645645, '666');]. Only this Skip index's granula contains this value 666. Other parts don't. After optimize this 666 will be in a huge granula which points to 8192*8192 = 67108864 rows.\r\n"
      },
      {
        "user": "amosbird",
        "created_at": "2019-12-11T02:52:20Z",
        "body": "I always find the term \"granularity\" to be overly used. We have `index_granularity` meaning the max row number of a granule, and we have index granularity meaning the granules one index unit covers. "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-11T03:36:10Z",
        "body": "@amosbird Possible solutions:\r\n- rename GRANULARITY to GRANULARITY FACTOR and support old syntax only for ATTACH queries;\r\n- remove GRANULARITY from documentation example (so it will be 1 by default) and only briefly mention it;"
      },
      {
        "user": "kaijianding",
        "created_at": "2019-12-11T04:09:58Z",
        "body": "@den-crane thanks, it works after change GRANULARITY 8192 to GRANULARITY 1. \r\nclose this issue"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how skip index granularity interacts with data merging operations",
      "Clarification on proper skip index configuration for sparse values",
      "Documentation of index behavior changes during data part consolidation",
      "Guidance on choosing appropriate granularity values for bloom filter indexes"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:09:25"
    }
  },
  {
    "number": 7647,
    "title": "Change bitmapBuild result type from default UInt8",
    "created_at": "2019-11-06T10:03:10Z",
    "closed_at": "2019-11-06T13:00:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7647",
    "body": "Hi there! Can i somehow change result type of bitmapBuild([1,2,3,4]) not to ```AggregateFunction(groupBitmap, UInt8)``` which i assume selects type by selecting max integer in set, but to ```AggregateFunction(groupBitmap, UInt32)``` without using hacks like bitmapBuild([1,2,3,4, 4294967295])",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7647/comments",
    "author": "mrAndersen",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-06T10:20:06Z",
        "body": "CC @yuzhichang "
      },
      {
        "user": "yuzhichang",
        "created_at": "2019-11-06T12:58:59Z",
        "body": "@mrAndersen You can cast array to UInt32 explicitly, for example `bitmapBuild(cast([1,2,3,4] as Array(UInt32)))`."
      }
    ],
    "satisfaction_conditions": [
      "Allows explicit specification of the bitmap's integer type without relying on input data values",
      "Maintains compatibility with the bitmapBuild function's interface",
      "Provides a type declaration mechanism that is explicit and intentional"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:10:22"
    }
  },
  {
    "number": 7151,
    "title": "Log file not recreated if deleted",
    "created_at": "2019-09-30T09:42:09Z",
    "closed_at": "2019-09-30T12:02:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7151",
    "body": "Hi,\r\n\r\nMy ClickHouse is integrated with a system that regularly take the log files, backup them, and delete them locally. It is a common system to lots of services.\r\n\r\nNevertheless, I got an issue with clickhouse. If you delete one ClickHouse log file when ClickHouse is up and running, the log file will not be recreated, and all new clickhouse logs will be lost.\r\n\r\nForcing the reload of the config file or flushing the log  (SYSTEM RELOAD CONFIG or SYSTEM FLUSH LOGS) does not seems to fix the issue.\r\n\r\nSo is there a way to force clickhouse to recreate the log file ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7151/comments",
    "author": "edonin",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-09-30T09:46:07Z",
        "body": "You can send SIGHUP to clickhouse-server to create next log file."
      },
      {
        "user": "edonin",
        "created_at": "2019-09-30T12:02:44Z",
        "body": "Super, it is working.\r\n\r\nThanks"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to programmatically trigger log file recreation without restarting ClickHouse",
      "Ensures continued logging functionality after log file deletion",
      "Works with existing log rotation/backup systems"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:11:37"
    }
  },
  {
    "number": 4762,
    "title": "ch complains \"Password required for user default., e.what() = DB::Exception.\" when drop a partition .But actually it has the user. And when try many times it will succeed",
    "created_at": "2019-03-22T08:58:55Z",
    "closed_at": "2019-03-22T10:12:13Z",
    "labels": [
      "question",
      "comp-dddl"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4762",
    "body": "**Version of ch:**\r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) select version()\r\n\r\nSELECT version()\r\n\r\n\u250c\u2500version()\u2500\u2510\r\n\u2502 18.14.19  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n**Cluster Config:**\r\n\r\n> SELECT * FROM system.clusters \r\n\r\n\u250c\u2500cluster\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500shard_num\u2500\u252c\u2500shard_weight\u2500\u252c\u2500replica_num\u2500\u252c\u2500host_name\u2500\u2500\u2500\u2500\u2500\u252c\u2500host_address\u2500\u2500\u252c\u2500port\u2500\u252c\u2500is_local\u2500\u252c\u2500user\u2500\u2500\u2500\u2500\u252c\u2500default_database\u2500\u2510\r\n\u2502 ads_model_ck_cluster \u2502         1 \u2502            1 \u2502           1 \u2502 xx.xx.30.65  \u2502 xx.xx.30.65  \u2502 9000 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         1 \u2502            1 \u2502           2 \u2502 xx.xx.40.123 \u2502 xx.xx.40.123 \u2502 9000 \u2502        1 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         2 \u2502            1 \u2502           1 \u2502 xx.xx.30.64  \u2502 xx.xx.30.64  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502         2 \u2502            1 \u2502           2 \u2502 xx.xx.30.69  \u2502 xx.xx.30.69  \u2502 9000 \u2502              0 \u2502 default \u2502                  \u2502\r\n....\r\n\u2502 ads_model_ck_cluster \u2502        15 \u2502            1 \u2502           1 \u2502 xx.xx.30.86  \u2502 xx.xx.30.86  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2502 ads_model_ck_cluster \u2502        15 \u2502            1 \u2502           2 \u2502 xx.xx.30.83  \u2502 xx.xx.30.83  \u2502 9000 \u2502        0 \u2502 default \u2502                  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n**Then Create test ReplicatedMergeTree loal table and Distribute table** \r\n\r\n> CREATE TABLE default.test1 on cluster ads_model_ck_cluster ( id UInt64,  name String, d Date) ENGINE =ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/default/test1', '{replica}') PARTITION BY toMonday(d) ORDER BY (id, d) SETTINGS index_granularity = 8192;\r\n\r\n> CREATE TABLE default.test1_all on cluster ads_model_ck_cluster (id UInt64,  name String, d Date) ENGINE = Distributed('ads_model_ck_cluster', 'test', 'test1', rand());\r\n\r\nINSERT some volume data into the table \r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :)  select count() from test1_all;\r\n\r\nSELECT count()\r\nFROM test1_all \r\n\r\n\u250c\u2500count()\u2500\u2510\r\n\u2502 4390912 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n**Get Detailed info of the partitions:**\r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) select database,table,partition_id,partition,name,active,rows,path,modification_time from system.parts where table='test1';\r\n\r\nSELECT \r\n    database, \r\n    table, \r\n    partition_id, \r\n    partition, \r\n    name, \r\n    active, \r\n    rows, \r\n    path, \r\n    modification_time\r\nFROM system.parts \r\nWHERE table = 'test1'\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u252c\u2500partition_id\u2500\u252c\u2500partition\u2500\u2500\u2500\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500active\u2500\u252c\u2500\u2500\u2500rows\u2500\u252c\u2500path\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500modification_time\u2500\u2510\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_25_0 \u2502      0 \u2502      1 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_25_0/ \u2502 2019-03-22 16:05:07 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_30_1 \u2502      0 \u2502    123 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_30_1/ \u2502 2019-03-22 16:05:14 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_35_2 \u2502      0 \u2502   4642 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_35_2/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_40_3 \u2502      0 \u2502 146165 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_40_3/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_25_43_4 \u2502      1 \u2502 290307 \u2502 /export/data/clickhouse/data/default/test1/20190318_25_43_4/ \u2502 2019-03-22 16:07:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_26_26_0 \u2502      0 \u2502      8 \u2502 /export/data/clickhouse/data/default/test1/20190318_26_26_0/ \u2502 2019-03-22 16:05:11 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_27_27_0 \u2502      0 \u2502     11 \u2502 /export/data/clickhouse/data/default/test1/20190318_27_27_0/ \u2502 2019-03-22 16:05:11 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_28_28_0 \u2502      0 \u2502     17 \u2502 /export/data/clickhouse/data/default/test1/20190318_28_28_0/ \u2502 2019-03-22 16:05:12 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_29_29_0 \u2502      0 \u2502     26 \u2502 /export/data/clickhouse/data/default/test1/20190318_29_29_0/ \u2502 2019-03-22 16:05:13 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_30_30_0 \u2502      0 \u2502     60 \u2502 /export/data/clickhouse/data/default/test1/20190318_30_30_0/ \u2502 2019-03-22 16:05:13 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_31_31_0 \u2502      0 \u2502    142 \u2502 /export/data/clickhouse/data/default/test1/20190318_31_31_0/ \u2502 2019-03-22 16:05:14 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_32_32_0 \u2502      0 \u2502    305 \u2502 /export/data/clickhouse/data/default/test1/20190318_32_32_0/ \u2502 2019-03-22 16:05:15 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_33_33_0 \u2502      0 \u2502    553 \u2502 /export/data/clickhouse/data/default/test1/20190318_33_33_0/ \u2502 2019-03-22 16:05:15 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_34_34_0 \u2502      0 \u2502   1182 \u2502 /export/data/clickhouse/data/default/test1/20190318_34_34_0/ \u2502 2019-03-22 16:05:16 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_35_35_0 \u2502      0 \u2502   2337 \u2502 /export/data/clickhouse/data/default/test1/20190318_35_35_0/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_36_36_0 \u2502      0 \u2502   4585 \u2502 /export/data/clickhouse/data/default/test1/20190318_36_36_0/ \u2502 2019-03-22 16:05:17 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_37_37_0 \u2502      0 \u2502   9213 \u2502 /export/data/clickhouse/data/default/test1/20190318_37_37_0/ \u2502 2019-03-22 16:05:34 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_38_38_0 \u2502      0 \u2502  18316 \u2502 /export/data/clickhouse/data/default/test1/20190318_38_38_0/ \u2502 2019-03-22 16:05:55 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_39_39_0 \u2502      0 \u2502  36600 \u2502 /export/data/clickhouse/data/default/test1/20190318_39_39_0/ \u2502 2019-03-22 16:05:58 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_40_40_0 \u2502      0 \u2502  72809 \u2502 /export/data/clickhouse/data/default/test1/20190318_40_40_0/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_41_41_0 \u2502      0 \u2502    501 \u2502 /export/data/clickhouse/data/default/test1/20190318_41_41_0/ \u2502 2019-03-22 16:06:00 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_42_42_0 \u2502      0 \u2502  72817 \u2502 /export/data/clickhouse/data/default/test1/20190318_42_42_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_43_43_0 \u2502      0 \u2502  70824 \u2502 /export/data/clickhouse/data/default/test1/20190318_43_43_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2502 default  \u2502 test1 \u2502 20190318     \u2502 '2019-03-18' \u2502 20190318_44_44_0 \u2502      1 \u2502   3065 \u2502 /export/data/clickhouse/data/default/test1/20190318_44_44_0/ \u2502 2019-03-22 16:06:02 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n**Try to delete one partition using alter table** \r\n\r\n> ckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) alter table default.test1 on cluster ads_model_ck_cluster drop partition '2019-03-18';\r\n\r\nALTER TABLE default.test1 ON CLUSTER ads_model_ck_cluster\r\n    DROP PARTITION '2019-03-18'\r\n\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.66  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  29 \u2502               18 \u2502\r\n\u2502 xx.xx.30.69  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  28 \u2502               18 \u2502\r\n\u2502 xx.xx.30.70  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  27 \u2502               18 \u2502\r\n\u2502 xx.xx.30.76  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  26 \u2502               18 \u2502\r\n\u2502 xx.xx.30.71  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  25 \u2502               18 \u2502\r\n\u2502 xx.xx.30.79  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.82:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  24 \u2502               18 \u2502\r\n\u2502 xx.xx.217.46 \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  23 \u2502               18 \u2502\r\n\u2502 xx.xx.30.86  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  22 \u2502               18 \u2502\r\n\u2502 xx.xx.30.65  \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  21 \u2502               18 \u2502\r\n\u2502 xx.xx.40.123 \u2502 9000 \u2502      0 \u2502                                                                                                                                                           \u2502                  20 \u2502               18 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.72  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  19 \u2502               12 \u2502\r\n\u2502 xx.xx.30.67  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  18 \u2502               12 \u2502\r\n\u2502 xx.xx.217.49 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  17 \u2502               12 \u2502\r\n\u2502 xx.xx.30.81  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.217.52:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  16 \u2502               12 \u2502\r\n\u2502 xx.xx.217.47 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  15 \u2502               12 \u2502\r\n\u2502 xx.xx.30.74  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.78:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  14 \u2502               12 \u2502\r\n\u2502 xx.xx.30.85  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  13 \u2502               12 \u2502\r\n\u2502 xx.xx.30.77  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  12 \u2502               12 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.78  \u2502 9000 \u2502      0 \u2502       \u2502                  11 \u2502                9 \u2502\r\n\u2502 xx.xx.30.73  \u2502 9000 \u2502      0 \u2502       \u2502                  10 \u2502                9 \u2502\r\n\u2502 xx.xx.217.45 \u2502 9000 \u2502      0 \u2502       \u2502                   9 \u2502                9 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.50 \u2502 9000 \u2502      0 \u2502       \u2502                   8 \u2502                8 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.52 \u2502 9000 \u2502      0 \u2502       \u2502                   7 \u2502                7 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.82 \u2502 9000 \u2502      0 \u2502       \u2502                   6 \u2502                6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.68  \u2502 9000 \u2502      0 \u2502       \u2502                   5 \u2502                3 \u2502\r\n\u2502 xx.xx.40.126 \u2502 9000 \u2502      0 \u2502       \u2502                   4 \u2502                3 \u2502\r\n\u2502 xx.xx.30.64  \u2502 9000 \u2502      0 \u2502       \u2502                   3 \u2502                3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.48 \u2502 9000 \u2502      0 \u2502       \u2502                   2 \u2502                0 \u2502\r\n\u2502 xx.xx.30.80  \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\r\n\u2502 xx.xx.30.83  \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nReceived exception from server (version 18.14.19):\r\nCode: 194. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: There was an error on [xx.xx.30.79:9000]: Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.82:9000. DB::Exception: Password required for user default., e.what() = DB::Exception. \r\n\r\n30 rows in set. Elapsed: 1.072 sec. \r\n\r\n**Still some node has data** \r\nxx.xx.30.68 \r\n291660\r\nxx.xx.30.67 \r\n291660\r\nxx.xx.30.78 \r\n291577\r\nxx.xx.30.74 \r\n291577\r\nxx.xx.30.79 \r\n293352\r\nxx.xx.30.82 \r\n293352\r\nxx.xx.30.81 \r\n293279\r\nxx.xx.217.52 \r\n293279\r\n\r\n**Then I try it for the third Time** \r\nckadsmodel-d724d9d7-9dc5-4694-b6ad-a7ad7883ca65 :) alter table default.test1 on cluster ads_model_ck_cluster drop partition '2019-03-18';\r\n\r\nALTER TABLE default.test1 ON CLUSTER ads_model_ck_cluster\r\n    DROP PARTITION '2019-03-18'\r\n\r\n\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.78  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  29 \u2502               15 \u2502\r\n\u2502 xx.xx.30.67  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception  \u2502                  28 \u2502               15 \u2502\r\n\u2502 xx.xx.30.72  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  27 \u2502               15 \u2502\r\n\u2502 xx.xx.30.81  \u2502 9000 \u2502    194 \u2502 Code: 194, e.displayText() = DB::Exception: Received from xx.xx.217.52:9000. DB::Exception: Password required for user default., e.what() = DB::Exception \u2502                  26 \u2502               15 \u2502\r\n\u2502 xx.xx.30.65  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  25 \u2502               15 \u2502\r\n\u2502 xx.xx.30.86  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  24 \u2502               15 \u2502\r\n\u2502 xx.xx.217.47 \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  23 \u2502               15 \u2502\r\n\u2502 xx.xx.30.66  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  22 \u2502               15 \u2502\r\n\u2502 xx.xx.30.69  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  21 \u2502               15 \u2502\r\n\u2502 xx.xx.30.70  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  20 \u2502               15 \u2502\r\n\u2502 xx.xx.30.76  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  19 \u2502               15 \u2502\r\n\u2502 xx.xx.30.71  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  18 \u2502               15 \u2502\r\n\u2502 xx.xx.30.85  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  17 \u2502               15 \u2502\r\n\u2502 xx.xx.30.82  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  16 \u2502               15 \u2502\r\n\u2502 xx.xx.30.77  \u2502 9000 \u2502      0 \u2502                                                                                                                                                            \u2502                  15 \u2502               15 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.45 \u2502 9000 \u2502      0 \u2502       \u2502                  14 \u2502               12 \u2502\r\n\u2502 xx.xx.217.46 \u2502 9000 \u2502      0 \u2502       \u2502                  13 \u2502               12 \u2502\r\n\u2502 xx.xx.40.123 \u2502 9000 \u2502      0 \u2502       \u2502                  12 \u2502               12 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.52 \u2502 9000 \u2502      0 \u2502       \u2502                  11 \u2502               11 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.217.50 \u2502 9000 \u2502      0 \u2502       \u2502                  10 \u2502                6 \u2502\r\n\u2502 xx.xx.30.73  \u2502 9000 \u2502      0 \u2502       \u2502                   9 \u2502                6 \u2502\r\n\u2502 xx.xx.40.126 \u2502 9000 \u2502      0 \u2502       \u2502                   8 \u2502                6 \u2502\r\n\u2502 xx.xx.217.48 \u2502 9000 \u2502      0 \u2502       \u2502                   7 \u2502                6 \u2502\r\n\u2502 xx.xx.30.79  \u2502 9000 \u2502      0 \u2502       \u2502                   6 \u2502                6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.68  \u2502 9000 \u2502      0 \u2502       \u2502                   5 \u2502                4 \u2502\r\n\u2502 xx.xx.217.49 \u2502 9000 \u2502      0 \u2502       \u2502                   4 \u2502                4 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.64 \u2502 9000 \u2502      0 \u2502       \u2502                   3 \u2502                2 \u2502\r\n\u2502 xx.xx.30.74 \u2502 9000 \u2502      0 \u2502       \u2502                   2 \u2502                2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500port\u2500\u252c\u2500status\u2500\u252c\u2500error\u2500\u252c\u2500num_hosts_remaining\u2500\u252c\u2500num_hosts_active\u2500\u2510\r\n\u2502 xx.xx.30.80 \u2502 9000 \u2502      0 \u2502       \u2502                   1 \u2502                0 \u2502\r\n\u2502 xx.xx.30.83 \u2502 9000 \u2502      0 \u2502       \u2502                   0 \u2502                0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nReceived exception from server (version 18.14.19):\r\nCode: 194. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: There was an error on [xx.xx.30.67:9000]: Code: 194, e.displayText() = DB::Exception: Received from xx.xx.30.68:9000. DB::Exception: Password required for user default., e.what() = DB::Exception. \r\n\r\n30 rows in set. Elapsed: 1.071 sec. \r\n\r\n**Although there are errors warning, the data is deleted successfully.**\r\nThis is very strange. Can anyone give a hint on this?  \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4762/comments",
    "author": "inolddays",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-03-22T09:49:05Z",
        "body": "Duplicate of #1861 \r\n\r\nPR #3598 fixes that (should work properly in versions > 19.1)\r\n\r\nGenerally, when you do some DDL on non-leader replica it forwards the request to a leader, and during that forwarding \"default\" passwordless user was used. "
      },
      {
        "user": "inolddays",
        "created_at": "2019-03-22T10:08:49Z",
        "body": "Many Thanks! Alexey "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why password authentication fails intermittently during cluster-wide DDL operations despite user existence",
      "Identification of the root cause for inconsistent authentication requirements between cluster nodes",
      "Solution ensuring consistent authentication across all cluster nodes during DDL propagation",
      "Clarification of ClickHouse's internal user impersonation during distributed operations",
      "Version-specific workaround or upgrade recommendation"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:14:39"
    }
  },
  {
    "number": 3434,
    "title": "Kafka EOF reached for partition ...",
    "created_at": "2018-10-21T09:09:39Z",
    "closed_at": "2018-10-22T20:14:22Z",
    "labels": [
      "question",
      "comp-kafka"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3434",
    "body": "```\r\nClickHouse client version 18.14.9.\r\nConnecting to database monitoring at localhost:9000 as user broker.\r\nConnected to ClickHouse server version 18.14.9 revision 54409.\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u252c\u2500changed\u2500\u252c\u2500description\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 max_insert_block_size    \u2502 1048576 \u2502       0 \u2502 The maximum block size for insertion, if we control the creation of blocks for insertion. \u2502\r\n\u2502 stream_flush_interval_ms \u2502 7500    \u2502       0 \u2502 Timeout for flushing data from streaming storages.                                        \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\nI just upgrade from 1.1.54385 to 18.14.9 and I'm receiving \"EOF reached for partition...\" when any of my materialized views are running. Queues with little traffic don't show this message so much but a queue that gets 100+ messages per second is constantly getting this message and some queue messages aren't written to the MergeTree table. Any advice would be greatly appreciated.\r\n\r\n```\r\n2018.10.21 08:57:38.093341 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146709\r\n2018.10.21 08:57:38.620841 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146723\r\n2018.10.21 08:57:38.767349 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146724\r\n2018.10.21 08:57:38.973400 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146727\r\n2018.10.21 08:57:39.254782 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146730\r\n2018.10.21 08:57:39.706890 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146739\r\n2018.10.21 08:57:40.088991 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146745\r\n2018.10.21 08:57:40.242327 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146746\r\n2018.10.21 08:57:40.766095 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146749\r\n2018.10.21 08:57:41.090802 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146751\r\n2018.10.21 08:57:41.216236 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146752\r\n2018.10.21 08:57:41.587863 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146756\r\n2018.10.21 08:57:41.705616 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146759\r\n2018.10.21 08:57:41.933883 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146763\r\n2018.10.21 08:57:42.111605 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146765\r\n2018.10.21 08:57:42.585086 [ 33 ] {} <Trace> StorageKafka (monitoring_middle_registration_kafka_queue): EOF reached for partition 0 offset 260146773\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3434/comments",
    "author": "daledude",
    "comments": [
      {
        "user": "vavrusa",
        "created_at": "2018-10-22T19:26:39Z",
        "body": "This just means that Kafka consumer reached EOF for given partition. Background consumer uses this as a signal for closing current batch, instead of waiting for timeout. It's useful for tracing/debugging (which is why it's emitted at trace level)."
      },
      {
        "user": "daledude",
        "created_at": "2018-10-22T20:14:22Z",
        "body": "Thank you @vavrusa! My msg loss is unrelated then."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why 'EOF reached for partition' messages occur in ClickHouse's Kafka integration"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:16:12"
    }
  },
  {
    "number": 53469,
    "title": "Allow editing query in clickhouse-client with external `$EDITOR`",
    "created_at": "2023-08-16T08:29:54Z",
    "closed_at": "2023-09-07T15:37:36Z",
    "labels": [
      "help wanted",
      "feature",
      "minor"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/53469",
    "body": "**Use case**\r\n\r\nAllow easier editing of (possibly long) queries in clickhouse-client,\r\nwith editors such as vim, emacs, nano, VS Code etc.\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nVarious shells like bash and fish allow editing the command you are writing\r\nin an external editor. In bash you can press `<C-x><C-e>` to trigger this.\r\nThis works by opening a temp file in the editor.\r\n\r\nIt would be great if clickhouse-client could also support this; allowing to press\r\nctrl+e to open the current command in an editor so that complex queries\r\ncan be changed more quickly.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/53469/comments",
    "author": "hkrutzer",
    "comments": [
      {
        "user": "mkmkme",
        "created_at": "2023-09-07T09:07:58Z",
        "body": "Could you assign it to me please?"
      },
      {
        "user": "amosbird",
        "created_at": "2023-09-07T14:57:29Z",
        "body": "Have you tried `Alt-Shift-e` ?"
      },
      {
        "user": "hkrutzer",
        "created_at": "2023-09-07T15:37:36Z",
        "body": "Very nice"
      },
      {
        "user": "den-crane",
        "created_at": "2023-09-07T19:56:38Z",
        "body": "hm, \r\nAlt-Shift-e (cmd+shit+e) does not work in iterm, it seems iterm has own binding `show timestamps` for this shortcut, and no way to change it."
      },
      {
        "user": "cangyin",
        "created_at": "2023-09-08T02:32:23Z",
        "body": "> Have you tried `Alt-Shift-e` ?\r\n\r\nIn MobaXterm:\r\n\r\n```\r\n:) <Alt-Shift-e>\r\n:) Bye.\r\n```\r\nIt quits."
      },
      {
        "user": "amosbird",
        "created_at": "2023-09-08T03:36:02Z",
        "body": "> it seems iterm has own binding show timestamps for this shortcut, and no way to change it.\r\n\r\nHeh. I guess both iterm and clickhouse-client have a secret pact to make you memorize their hotkeys...\r\n"
      },
      {
        "user": "hkrutzer",
        "created_at": "2023-09-08T09:19:27Z",
        "body": "@den-crane I don't usually use iTerm but when I tried it, it works with cmd+option+e"
      },
      {
        "user": "mkmkme",
        "created_at": "2023-09-08T12:56:33Z",
        "body": "@den-crane on macOS, you need to press and release Esc as the Meta modifier.\r\n\r\nSo on my macOS, on both iTerm and macOS terminal, I have to:\r\n1. Press Esc\r\n2. Release Esc\r\n3. Press Shift + e at the same time"
      },
      {
        "user": "den-crane",
        "created_at": "2023-09-08T13:06:48Z",
        "body": "> @den-crane on macOS, you need to press and release Esc as the Meta modifier.\r\n> \r\n> So on my macOS, on both iTerm and macOS terminal, I have to:\r\n> \r\n>     1. Press Esc\r\n> \r\n>     2. Release Esc\r\n> \r\n>     3. Press Shift + e at the same time\r\n\r\nyes, thanks, this works."
      }
    ],
    "satisfaction_conditions": [
      "Support for invoking an external editor to edit queries in clickhouse-client",
      "Customizable keybinding for editor invocation",
      "Cross-terminal compatibility",
      "Clear documentation of editor invocation method",
      "No disruptive behavior when using the feature"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:17:02"
    }
  }
]