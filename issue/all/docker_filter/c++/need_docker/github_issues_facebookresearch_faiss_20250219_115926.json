[
  {
    "number": 2953,
    "title": "IndexFlatL2 multithread is slower than single thread",
    "created_at": "2023-07-14T09:33:48Z",
    "closed_at": "2024-06-30T22:34:20Z",
    "labels": [
      "question",
      "Performance"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2953",
    "body": "python faiss-cpu 1.7.4 installed with pip3.x\r\nMultithread performance is pool on my 32-processor machine\r\n\r\nmodel name\t: Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz\r\n************ nthread= 1\r\n*********** nq= 100\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=1.393 ms (\u00b1 0.1564)\r\nsearch k= 10 t=2.679 ms (\u00b1 0.0422)\r\nsearch k=100 t=6.473 ms (\u00b1 0.4788)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=11.656 ms (\u00b1 23.1539)\r\nsearch k= 10 t=3.664 ms (\u00b1 0.4651)\r\nsearch k=100 t=6.653 ms (\u00b1 0.6943)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=4.447 ms (\u00b1 0.4957)\r\nsearch k= 10 t=4.460 ms (\u00b1 0.0903)\r\nsearch k=100 t=8.210 ms (\u00b1 0.8620)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=7.682 ms (\u00b1 1.1851)\r\nsearch k= 10 t=8.133 ms (\u00b1 1.1031)\r\nsearch k=100 t=10.987 ms (\u00b1 1.5985)\r\nrestab=\r\n 1.39302\t2.67902\t6.4728\r\n11.6563\t3.66396\t6.65313\r\n4.44698\t4.45956\t8.20962\r\n7.68209\t8.13305\t10.9866\r\n*********** nq= 10000\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.080 s (\u00b1 0.0044)\r\nsearch k= 10 t=0.257 s (\u00b1 0.0085)\r\nsearch k=100 t=0.564 s (\u00b1 0.0193)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.259 s (\u00b1 0.0097)\r\nsearch k= 10 t=0.321 s (\u00b1 0.0092)\r\nsearch k=100 t=0.635 s (\u00b1 0.0237)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.368 s (\u00b1 0.0306)\r\nsearch k= 10 t=0.410 s (\u00b1 0.0379)\r\nsearch k=100 t=0.681 s (\u00b1 0.0412)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.599 s (\u00b1 0.0144)\r\nsearch k= 10 t=0.645 s (\u00b1 0.0107)\r\nsearch k=100 t=0.921 s (\u00b1 0.0569)\r\nrestab=\r\n 0.0801447\t0.257458\t0.56392\r\n0.259316\t0.321337\t0.635152\r\n0.368472\t0.410237\t0.680965\r\n0.599093\t0.644711\t0.921228\r\n************ nthread= 32\r\n*********** nq= 100\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=12.850 ms (\u00b1 7.3587)\r\nsearch k= 10 t=326.201 ms (\u00b1 9.8362)\r\nsearch k=100 t=331.151 ms (\u00b1 16.7528)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=181.012 ms (\u00b1 20.5017)\r\nsearch k= 10 t=325.893 ms (\u00b1 12.7326)\r\nsearch k=100 t=325.874 ms (\u00b1 24.1845)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=181.696 ms (\u00b1 14.6625)\r\nsearch k= 10 t=329.945 ms (\u00b1 17.0235)\r\nsearch k=100 t=329.392 ms (\u00b1 14.8352)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=176.828 ms (\u00b1 9.2367)\r\nsearch k= 10 t=326.336 ms (\u00b1 16.2117)\r\nsearch k=100 t=325.248 ms (\u00b1 13.9408)\r\nrestab=\r\n 12.8498\t326.201\t331.151\r\n181.012\t325.893\t325.874\r\n181.696\t329.945\t329.392\r\n176.828\t326.336\t325.248\r\n*********** nq= 10000\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.027 s (\u00b1 0.0119)\r\nsearch k= 10 t=0.980 s (\u00b1 0.0149)\r\nsearch k=100 t=1.029 s (\u00b1 0.0168)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.524 s (\u00b1 0.0138)\r\nsearch k= 10 t=0.986 s (\u00b1 0.0122)\r\nsearch k=100 t=1.066 s (\u00b1 0.0379)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.572 s (\u00b1 0.0328)\r\nsearch k= 10 t=0.999 s (\u00b1 0.0171)\r\nsearch k=100 t=1.090 s (\u00b1 0.0780)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.721 s (\u00b1 0.0103)\r\nsearch k= 10 t=1.059 s (\u00b1 0.0262)\r\nsearch k=100 t=1.147 s (\u00b1 0.0235)\r\nrestab=\r\n 0.0267251\t0.979833\t1.02869\r\n0.523988\t0.985733\t1.0658\r\n0.571997\t0.999151\t1.09039\r\n0.721175\t1.05897\t1.14676\r\n\r\n# Reproduction instructions\r\n\r\nbench_index_flat.py \r\nI modified faiss.cvar.distance_compute_min_k_reservoir from 5 to 100",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2953/comments",
    "author": "RongchunYao",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-07-24T07:20:39Z",
        "body": "Please install Faiss with conda to make sure that the proper MKL version is installed. \r\nOn intel, we sometimes observe worse MKL perf with nthread = nb cores. Please try 16 threads"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-07-24T12:28:27Z",
        "body": "> \r\nIt tried out that nthread = nb cores/2 works good for me on another server which has 16 amd processors (both training and query). Thank you so much && I wonder why the performance is bad  with nthread = nb cores :-)"
      },
      {
        "user": "alexanderguzhva",
        "created_at": "2023-07-24T16:41:19Z",
        "body": "@RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores.\r\nHope it helps. \r\n"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-11-30T15:30:14Z",
        "body": "> @RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores. Hope it helps.\r\n\r\nThank you!\n\n---\n\n> @RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores. Hope it helps.\r\n\r\nHi, I recently run faiss with openblas that compiled with omp, and I set the omp thread to 32. I run the jobs in batch on some computing platform, most machines gain great acceleration, but some machine runs very slow (each machine has similar\r\n workload). What's stranger is that part of the slow machine has a high cpu utilization ( same as normal machine ).\r\n\r\nI wonder the potential reasons, could the tasks submited to the machine by other users be a great influence factor?\r\nLooking forward to your reply."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why hyper-threading negatively impacts FAISS performance with full core utilization",
      "Guidance on optimal thread configuration strategies for different CPU architectures",
      "Identification of environmental factors affecting multithreading consistency",
      "Clarification of BLAS implementation impacts on threading behavior"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:03:56"
    }
  },
  {
    "number": 2361,
    "title": "Clone not supported for this type of IndexIVF",
    "created_at": "2022-06-19T19:27:41Z",
    "closed_at": "2022-06-28T16:41:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2361",
    "body": "# Summary\r\n\r\nI'm trying to move a trained composite index to a GPU, so that adding embeddings (~5.8B) to the index is faster. However, my IndexIVF cannot be cloned onto the GPU. Here's a minimal reproducing snippet:\r\n\r\n```\r\nimport faiss\r\n\r\nindex = faiss.index_factory(128, \"OPQ4_64,IVF16384_HNSW32,PQ16x4fs\")\r\nxt = faiss.rand((20000, 128))\r\nindex.train(xt)\r\n\r\nfaiss.index_cpu_to_all_gpus(index)\r\n```\r\n\r\nwhich yields:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tmp.py\", line 7, in <module>\r\n    faiss.index_cpu_to_all_gpus(index)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 887, in index_cpu_to_all_gpus\r\n    index_gpu = index_cpu_to_gpus_list(index, co=co, gpus=None, ngpu=ngpu)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 899, in index_cpu_to_gpus_list\r\n    index_gpu = index_cpu_to_gpu_multiple_py(res, index, co, gpus)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 882, in index_cpu_to_gpu_multiple_py\r\n    index = index_cpu_to_gpu_multiple(vres, vdev, index, co)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py\", line 10278, in index_cpu_to_gpu_multiple\r\n    return _swigfaiss_avx2.index_cpu_to_gpu_multiple(provider, devices, index, options)\r\nRuntimeError: Error in virtual faiss::IndexIVF* faiss::Cloner::clone_IndexIVF(const faiss::IndexIVF*) at /root/miniconda3/conda-bld/faiss-pkg_1641228905850/work/faiss/clone_index.cpp:71: clone not supported for this type of IndexIVF\r\n```\r\n\r\nIs this expected behavior? The IndexIVF I'm using doesn't seem to be special. I've also tried:\r\n\r\n```\r\nindex_ivf = faiss.extract_index_ivf(index)\r\nindex_ivf = faiss.index_cpu_to_all_gpus(index_ivf)\r\n```\r\n\r\nwith similar results.\r\n\r\n# Platform\r\n\r\nOS: `Linux 53143a0863f8 5.4.0-94-generic #106~18.04.1-Ubuntu SMP Fri Jan 7 07:23:53 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux`\r\n(Docker image `nvidia/cuda:11.3.0-devel-ubuntu20.04`)\r\n\r\nFaiss version: \r\n\r\n```\r\nroot@fddb9798ebfc:/src# conda list\r\n# packages in environment at /opt/conda:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main\r\n_openmp_mutex             4.5                       1_gnu\r\nattrs                     21.4.0                   pypi_0    pypi\r\nblas                      1.0                         mkl\r\nbrotlipy                  0.7.0           py38h27cfd23_1003\r\nca-certificates           2022.4.26            h06a4308_0\r\ncertifi                   2022.5.18.1      py38h06a4308_0\r\ncffi                      1.15.0           py38hd667e15_1\r\ncharset-normalizer        2.0.4              pyhd3eb1b0_0\r\ncolorama                  0.4.4              pyhd3eb1b0_0\r\nconda                     4.13.0           py38h06a4308_0\r\nconda-content-trust       0.1.1              pyhd3eb1b0_0\r\nconda-package-handling    1.8.1            py38h7f8727e_0\r\ncryptography              37.0.1           py38h9ce1e76_0\r\ncudatoolkit               11.3.1               h2bc3f7f_2\r\neinops                    0.4.1                    pypi_0    pypi\r\nfaiss-gpu                 1.7.2           py3.8_h28a55e0_0_cuda11.3    pytorch\r\nfilelock                  3.7.1                    pypi_0    pypi\r\nfire                      0.4.0                    pypi_0    pypi\r\nhuggingface-hub           0.7.0                    pypi_0    pypi\r\nidna                      3.3                pyhd3eb1b0_0\r\nimportlib-metadata        4.11.1                   pypi_0    pypi\r\nintel-openmp              2021.4.0          h06a4308_3561\r\njsonlines                 3.0.0                    pypi_0    pypi\r\nld_impl_linux-64          2.35.1               h7274673_9\r\nlibfaiss                  1.7.2           hfc2d529_0_cuda11.3    pytorch\r\nlibffi                    3.3                  he6710b0_2\r\nlibgcc-ng                 9.3.0               h5101ec6_17\r\nlibgomp                   9.3.0               h5101ec6_17\r\nlibstdcxx-ng              9.3.0               hd4cf53a_17\r\nlibuv                     1.40.0               h7b6447c_0\r\nmkl                       2021.4.0           h06a4308_640\r\nmkl-service               2.4.0            py38h7f8727e_0\r\nmkl_fft                   1.3.1            py38hd3c417c_0\r\nmkl_random                1.2.2            py38h51133e4_0\r\nncurses                   6.3                  h7f8727e_2\r\nnumpy                     1.22.3           py38he7a7128_0\r\nnumpy-base                1.22.3           py38hf524024_0\r\nopenssl                   1.1.1o               h7f8727e_0\r\npackaging                 21.3                     pypi_0    pypi\r\npip                       21.2.4           py38h06a4308_0\r\npycosat                   0.6.3            py38h7b6447c_1\r\npycparser                 2.21               pyhd3eb1b0_0\r\npyopenssl                 22.0.0             pyhd3eb1b0_0\r\npyparsing                 3.0.9                    pypi_0    pypi\r\npysocks                   1.7.1            py38h06a4308_0\r\npython                    3.8.13               h12debd9_0\r\npytorch                   1.10.2          py3.8_cuda11.3_cudnn8.2.0_0    pytorch\r\npytorch-mutex             1.0                        cuda    pytorch\r\npyyaml                    6.0                      pypi_0    pypi\r\nreadline                  8.1.2                h7f8727e_1\r\nregex                     2022.6.2                 pypi_0    pypi\r\nrequests                  2.27.1             pyhd3eb1b0_0\r\nretro-pytorch             0.3.7                    pypi_0    pypi\r\nruamel_yaml               0.15.100         py38h27cfd23_0\r\nsentencepiece             0.1.96                   pypi_0    pypi\r\nsetuptools                61.2.0           py38h06a4308_0\r\nsix                       1.16.0             pyhd3eb1b0_1\r\nsqlite                    3.38.2               hc218d9a_0\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\ntk                        8.6.11               h1ccaba5_0\r\ntokenizers                0.12.1                   pypi_0    pypi\r\ntqdm                      4.63.0             pyhd3eb1b0_0\r\ntransformers              4.20.0                   pypi_0    pypi\r\ntyping_extensions         4.1.1              pyh06a4308_0\r\ntzdata                    2022a                hda174b7_0\r\nurllib3                   1.26.8             pyhd3eb1b0_0\r\nwheel                     0.37.1             pyhd3eb1b0_0\r\nxz                        5.2.5                h7b6447c_0\r\nyaml                      0.2.5                h7b6447c_0\r\nzipp                      3.8.0                    pypi_0    pypi\r\nzlib                      1.2.12               h7f8727e_1\r\n```\r\n\r\nInstalled from: Anaconda\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [X] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2361/comments",
    "author": "mitchellgordon95",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-27T23:50:11Z",
        "body": "The index type that you build here is tuned for CPU indexing. \r\n\r\n\"OPQ4_64,IVF16384_HNSW32,PQ16x4fs\"\r\n\r\n- IVFx_HNSW is not supported (and not necessary) on GPU: use IVF16386\r\n\r\n- the \"fs\" variant of PQ is not supported on GPU. Only 8-bit PQ is supported (and more accurate anyways). \r\n\r\nSo this boils down to \"OPQ8_64,IVF16386,PQ8\"\r\n"
      },
      {
        "user": "mitchellgordon95",
        "created_at": "2022-06-28T16:41:30Z",
        "body": "Ah, thank you for replying! This answers my question so I will close the issue. \r\n\r\nI am not sure I will be able to get away with \"IVF...,PQ8\" since I have >5B vectors, but I will do some benchmarking to see what works for me.\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why specific IndexIVF types cannot be cloned to GPU",
      "Identification of GPU-compatible alternatives for unsupported components (IVF_HNSW and PQfs variants)",
      "Clarification of GPU-specific limitations in Faiss index composition",
      "Recommendations for equivalent GPU-optimized index structures"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:04:46"
    }
  },
  {
    "number": 2346,
    "title": "How to use single thread when do batch search",
    "created_at": "2022-06-07T02:20:36Z",
    "closed_at": "2022-08-31T09:24:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2346",
    "body": "# Summary\r\n\r\nwe want to do ivfpq search by single thread, so we use pthread function to  bind ivfpq search on a cpu core, how to do it.\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: ubuntu 18.04\r\n\r\nFaiss version: last\r\n\r\nInstalled from: compiled \r\n\r\n\r\n\r\nRunning on:\r\n- [ \u00d7 ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ \u00d7] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2346/comments",
    "author": "jackhouchina",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-07T09:05:31Z",
        "body": "you can call omp_set_num_threads(1) to avoid the openmp overhead. "
      }
    ],
    "satisfaction_conditions": [
      "Ensures IVFPQ search runs in a single-threaded environment",
      "Avoids OpenMP multi-threading overhead",
      "Works with Faiss' C++ interface",
      "Does not require GPU-specific implementations"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:04:53"
    }
  },
  {
    "number": 2285,
    "title": "ProductQuantizer  compute_codes get wrong codes when nbits not 8",
    "created_at": "2022-04-04T00:18:25Z",
    "closed_at": "2022-04-04T12:31:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2285",
    "body": "    \r\n\r\n\r\n    d = 10\r\n    n = 400000\r\n    cs = 5\r\n    np.random.seed(123)\r\n    x = np.random.random(size=(n, d)).astype('float32')\r\n    testInputs=np.random.random(size=(1, d)).astype('float32')\r\n    print(testInputs)\r\n    pq = faiss.ProductQuantizer(d, cs,6)\r\n    pq.verbose=True\r\n    pq.train(x)\r\n    codes=pq.compute_codes(testInputs)\r\n    #here expect 5 code range from 0-64, but get 4 and also code number not range 0-64\r\n    print(codes.shape)\r\n  \r\n   ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2285/comments",
    "author": "jasstionzyf",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-04-04T07:51:04Z",
        "body": "This is because the codes are packed into ceil(5 * 6 / 8) = 4 bytes.  \r\nTo access the individual codes, use `BitstringReader`: \r\n\r\n```python\r\nbs = faiss.BitstringReader(faiss.swig_ptr(codes[0]), codes.shape[1])\r\nfor i in range(cs): \r\n    print(bs.read(6))  # read 6 bits at a time\r\n````\r\n\r\nAdmittedly, the `BitstringReader` API could be made more python friendly."
      },
      {
        "user": "jasstionzyf",
        "created_at": "2022-04-04T12:28:55Z",
        "body": "@mdouze   thanks very much!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how code packing works when nbits is not 8",
      "Method to correctly access individual codes from packed bytes",
      "Clarification of code storage format in ProductQuantizer"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:05:08"
    }
  },
  {
    "number": 2259,
    "title": "Chain an existing OPQMatrix with a new IVFPQ index",
    "created_at": "2022-03-15T08:01:49Z",
    "closed_at": "2022-03-16T03:47:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2259",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nI have trained an IVFOPQ index and I want to migrate the OPQMatrix to the top of a new(untrained) IVFPQ index. Here is my code:\r\n```\r\nimport faiss\r\n\r\nold = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # suppose it is already trained, the opqmatrix is not empty\r\nnew = faiss.index_factory(128, \"IVF4,PQ16\")   # a new index that I want to prepend an OPQMatrix to\r\n\r\nvector_transform = faiss.downcast_VectorTransform(old.chain.at(0))\r\nold_opq_matrix = vector_transform.A\r\nold_opq_array = faiss.vector_to_array(old_opq_matrix)\r\n\r\nnew_opq_matrix = faiss.OPQMatrix(vector_transform.d_in, 1, vector_transform.d_out)\r\nfaiss.copy_array_to_vector(old_opq_array, new_opq_matrix.A)\r\nnew_index = faiss.IndexPreTransform(new_opq_matrix, new)\r\n```\r\nI don't think it's a good idea that we should copy the vector to a new array then copy them back. Is there a easier way to do this? I just need to chain the **old** VectorTransform and a **new** IVFPQ. \r\n\r\nI tried the following but none of them worked (throwing segmentation error when adding embeddings to the new index):\r\n```\r\nnew_index = faiss.IndexPreTransform(old.chain.at(0), new)\r\nnew_index = faiss.IndexPreTransform(faiss.downcast_VectorTransform(old.chain.at(0)), new)\r\n```\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.7.1 <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: pip <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2259/comments",
    "author": "namespace-Pt",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-03-15T17:04:49Z",
        "body": "Maybe the easiest is to do \r\n```\r\nold = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # suppose it is already trained, the opqmatrix is not empty\r\nnew = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # a new index that I want to prepend an OPQMatrix to\r\n\r\n... train old opq\r\n\r\nopq_old = faiss.downcast_VectorTransform(old.chain.at(0))\r\nopq_new = faiss.downcast_VectorTransform(new.chain.at(0))\r\nopq_new.A = opq_old.A\r\nopq_new.b = opq_old.b\r\nopq_new.is_trained = opq_old.is_trained\r\n```\r\n"
      },
      {
        "user": "namespace-Pt",
        "created_at": "2022-03-16T03:48:39Z",
        "body": "Got it. Thank you.\n\n---\n\n@mdouze BTW, I wonder is there an Inner Product version of HNSWPQ?"
      },
      {
        "user": "mdouze",
        "created_at": "2022-03-31T11:25:51Z",
        "body": "no"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates how to transfer a trained OPQMatrix between indexes without manual array conversion",
      "Ensures the new composite index remains functional after transformation transfer",
      "Uses Faiss's native components for parameter transfer between indexes",
      "Preserves the original OPQ transformation characteristics in the new index"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:05:20"
    }
  },
  {
    "number": 2057,
    "title": "QUESTION: Can I create an IndexIVFPQ object with custom centroids?",
    "created_at": "2021-09-21T15:23:20Z",
    "closed_at": "2021-09-21T16:32:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2057",
    "body": "Hello. I'm trying to run an experiment that involves using some custom centroids (that I generate) with the IVFPQ indexing structure. Since faiss provides highly optimised infrastructure and support for IVFPQ indexing, I would like to use it to perform my experiments.\r\n\r\nIs it possible to to create an `IndexIVFPQ` object whose coarse and fine quantizer centroids are initialised to vectors I provide?\r\n\r\nHere's what I tried doing to achieve this:\r\n\r\n```python\r\nquantizer = faiss.IndexFlatL2(d)  \r\nindex = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\r\ncustom_coarse_centroids = <a numpy array>\r\ncustom_pq_centroids = <a numpy array>\r\nquantizer.add(custom_coarse_centroids)\r\nindex.train(custom_coarse_centroids)\r\nfaiss.copy_array_to_vector(custom_pq_centroids.ravel(), index.pq.centroids)\r\n```\r\n\r\nAfter doing this, I verified by reading the corresponding centroids using `index.quantizer.reconstruct_n(0, index.nlist)` and `faiss.vector_to_array(index.pq.centroids).reshape(index.pq.M, index.pq.ksub, index.pq.dsub)` that the centroids are correctly set to what I want them to be. However, when I try to perform a query, I get nonsensical results such as negative distance estimates.\r\n\r\n```python\r\nindex.add(xb)\r\nD, I = index.search(xb[:5], k) # sanity check\r\nprint(I)\r\nprint(D)\r\n```\r\nI understand that certain distances and inner products are precomputed and stored inside an `IndexIVFPQ` object when the index is trained. Am I correct in thinking that what remains to be done to make my custom `IndexIVFPQ` object work correctly is to perform those precomputations? How can I make the `IndexIVFPQ` object carry out the relevant precomputations with the centroids I've just inserted?\r\n\r\nAlternatively, is there a better way to achieve this? My end goal is to create a queryable `IndexIVFPQ` object with my own custom centroids instead of relying on `.train()` to learn them.\r\n\r\nThanks in advance for any help you can offer!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2057/comments",
    "author": "anirudhajith",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-09-21T15:43:29Z",
        "body": "A few things to keep in mind: \r\n\r\n- by default the IVFPQ encodes the residual of the vectors wrt. the centroids they are assigned to, not the vectors themselves\r\n\r\n- the precomputed tables (used only for L2 search with residuals) are initialized after training so if you update the coarse or fine centroids after training you should call \r\n\r\n```\r\nindex.verbose = True # to see what happens\r\nindex.precompute_table()\r\n```"
      },
      {
        "user": "anirudhajith",
        "created_at": "2021-09-21T16:32:32Z",
        "body": "`index.precompute_table()` is exactly what I was looking for! It's working exactly as expected now. Thanks a lot!\r\n\r\nI'm aware of the bit about the residuals being encoded, thanks."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of required precomputation steps after setting custom centroids",
      "Method to initialize both coarse and fine quantizer centroids without using .train()",
      "Validation that index returns valid distance metrics after customization"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:05:34"
    }
  },
  {
    "number": 884,
    "title": "AttributeError: 'IndexPreTransform' object has no attribute 'invlists'",
    "created_at": "2019-07-05T10:19:52Z",
    "closed_at": "2019-07-05T11:48:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/884",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform centOS\r\n\r\nFaiss version:  1.5.2\r\n\r\n\r\nRunning on: CPU\r\n\r\nInterface:   Python\r\n\r\n\r\n I changed the \"IVF4096,Flat\" to  'OPQ20_80,IMI2x12,PQ20'  and run demo_ondisk_ivf.py failed.\r\n\r\nerror information : \r\nAttributeError: 'IndexPreTransform' object has no attribute 'invlists'\r\n\r\nIf I use 'OPQ20_80,IMI2x12,PQ20'  how to merge the index file.\r\n\r\nThanks\r\n \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/884/comments",
    "author": "winself",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-05T11:48:36Z",
        "body": "You're adding an index pre-transform, so you'll need to call `index.index.invlists`."
      },
      {
        "user": "winself",
        "created_at": "2019-07-05T12:18:11Z",
        "body": "but I meet \"AttributeError: 'Index' object has no attribute 'invlists'\" \n\n---\n\n@beauby \r\nwhen i call index.index.invlists\r\n I meet \"AttributeError: 'Index' object has no attribute 'invlists'\"\r\nthanks"
      },
      {
        "user": "beauby",
        "created_at": "2019-07-05T12:55:58Z",
        "body": "`faiss.downcast_Index(index.index).invlists`"
      },
      {
        "user": "winself",
        "created_at": "2019-07-08T07:20:25Z",
        "body": "@beauby \r\nas you say , I use the faiss.downcast_Index(index.index).invlists . it works \r\nbut i meet another problem .\r\n**RuntimeError: Error in size_t faiss::OnDiskInvertedLists::merge_from(const faiss::InvertedLists\\**, int, bool) at OnDiskInvertedLists.cpp:602: Error: 'il->nlist == nlist && il->code_size == code_size' failed**\r\n\r\ni try print nlist  of  the train.index and block_x.index .they are same. \r\n\r\ncan you give some sugestion ? \r\n\r\n # my code  ( just changed the \"IVF4096,Flat\" to 'OPQ20_80,IMI2x12,PQ20') \r\n\r\n`ivfs = []\r\nfor bno in range(2):\r\n    print(\"read \" + indexdir + \"block_%d.index\" % bno)\r\n    index = faiss.read_index(indexdir + \"block_%d.index\" % bno,\r\n                             faiss.IO_FLAG_MMAP)\r\n    ivfs.append(faiss.downcast_index(index.index).invlists)\r\n    index.own_invlists = False\r\n\r\nindex = faiss.read_index(\"./data/trained.index\")\r\n\r\ninvlists = faiss.OnDiskInvertedLists(\r\n    faiss.downcast_index(index.index).nlist, faiss.downcast_index(index.index).code_size,\r\n    indexdir + \"merged_index.ivfdata\")\r\n\r\nivf_vector = faiss.InvertedListsPtrVector()\r\n\r\nfor ivf in ivfs:\r\n    ivf_vector.push_back(ivf)\r\n\r\n\r\nprint(\"merge %d inverted lists \" % ivf_vector.size())\r\nntotal = invlists.merge_from(ivf_vector.data(), ivf_vector.size())\r\n\r\nindex.ntotal = ntotal\r\nfaiss.downcast_index(index.index).replace_invlists(invlists)\r\n\r\nprint(\"write \" + indexdir + \"all.index\")\r\nfaiss.write_index(index, indexdir + \"all.index\")`\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to properly handle inverted lists merging with pre-transformed indexes",
      "Clarification on maintaining consistent code_size across merged indexes",
      "Guidance on handling IndexPreTransform structure during merging",
      "Verification methodology for inverted list compatibility"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:08:17"
    }
  },
  {
    "number": 458,
    "title": "Libgomp: Thread creation failed: Resource temporarily unavailable",
    "created_at": "2018-05-23T09:48:52Z",
    "closed_at": "2018-06-12T10:15:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/458",
    "body": "Released a faiss service with thrift, my thrift service opened 100 threads, requests more than one, it will give an error:\r\nLibgomp: Thread creation failed: Resource temporarily unavailable\r\n\r\nulimit -u 65535",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/458/comments",
    "author": "fuchao01",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-05-23T22:31:03Z",
        "body": "Hi \r\nYou may want to compile Faiss without threading if you are using thrift to do the multi-threading. OpenMP has a non-trivial overhead when a new non-openmp thread is started.\r\n"
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T02:53:27Z",
        "body": "@mdouze Thank you for your reply.How to compile faiss without threads"
      },
      {
        "user": "mdouze",
        "created_at": "2018-05-25T08:11:55Z",
        "body": "In `makefile.inc` in the `CFLAGS` variable replace `-fopenmp` with `-fno-openmp`. Adding `-Wno-error=unknown-pragmas` will quiet all the warnings. "
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T15:30:26Z",
        "body": "This really does. But there is a problem, performance is not as good as before. Can you specify the maximum number of openmp threads?\n\n---\n\nfaiss.omp_set_num_threads() This parameter is not set openmp open thread number?\n\n---\n\nI have 200w indexed data, qps 100/s, thrift server 100 threads. The faiss flat index is used. The server is basically running at full capacity. 32-core cpu, 128g memory, load reaches 40+. Is the amount of data too large for the index?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:15:20Z",
        "body": "No clear question. Closing."
      }
    ],
    "satisfaction_conditions": [
      "Resolve thread resource conflicts between OpenMP and thrift's multi-threading"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:09:57"
    }
  },
  {
    "number": 2377,
    "title": "Getting Cosine similarity different for \"Flat\" & \"HNSW32Flat\" Indexes",
    "created_at": "2022-07-07T05:45:32Z",
    "closed_at": "2024-07-24T18:25:52Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2377",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: linux <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\nHello,\r\n\r\nI am trying to find the cosine similarity with HNSW.\r\nBut the cosine similarity found to be incorrect below is the code and comparison of \"Flat\", \"HNSW\" & \"scipy\"\r\n```\r\nimport faiss\r\nemb1 = np.fromfile(\"emb1.raw\", dtype=np.float32)\r\nemb2 = np.fromfile(\"emb2.raw\", dtype=np.float32)\r\n```\r\nScipy code & result\r\n\r\n```\r\nfrom scipy import spatial\r\nresult = 1 - spatial.distance.cosine(emb1, emb2)\r\nprint('Cosine Similarity by scipy:{}'.format(result))\r\n```\r\nResult:\r\n`Cosine Similarity by scipy::0.991761326789856`\r\n\r\nIndexFlatL2/Flat code & result\r\n```\r\nxb = np.expand_dims(emb1,axis=0)\r\nxq = np.expand_dims(emb2,axis=0)\r\n\r\nindex = faiss.index_factory(128, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex.ntotal\r\nfaiss.normalize_L2(xb)\r\nindex.add(xb)\r\nfaiss.normalize_L2(xq)\r\ndistance, index = index.search(xq, 1)\r\nprint('[FAISS] Cosine Similarity by Flat:{}'.format(distance))\r\n```\r\nResult:\r\n`[FAISS] Cosine Similarity by Flat:[[0.9917611]]`\r\n\r\nIndexHNSWFlat/HNSW32Flat code & result\r\n\r\n```\r\nxb = np.expand_dims(emb1,axis=0)\r\nxq = np.expand_dims(emb2,axis=0)\r\n\r\nindex = faiss.index_factory(128, \"HNSW32Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex.ntotal\r\nfaiss.normalize_L2(xb)\r\nindex.add(xb)\r\nfaiss.normalize_L2(xq)\r\ndistance, index = index.search(xq, 1)\r\nprint('[FAISS] Cosine Similarity by HNSW32Flat:{}'.format(distance))\r\n```\r\nResult:\r\n`[FAISS] Cosine Similarity by HNSW32Flat:[[0.01647742]]`\r\n\r\n**The results of Scipy & Flat are matching.\r\nWhereas the result is incorrect for HNSW.\r\nVerified the results using C++ & Python API's**",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2377/comments",
    "author": "Kapil-23",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-07-08T08:39:07Z",
        "body": "This is with an old version of Faiss, HNSW32Flat is not a valid index_factory string, it should be HNSW32,Flat. \r\nIn addition, the faiss.METRIC_INNER_PRODUCT is not taken into account, so it computes L2 distances. \r\nThis is fine, it just requires to do the translation to cosine similarity: \r\n\r\n2 - 2 * 0.9917611 = 0.0164778"
      },
      {
        "user": "Kapil-23",
        "created_at": "2022-07-08T10:33:53Z",
        "body": "@mdouze Thanks for your reply !!!\r\n\r\nYes the faiss python version that was installed was (1.5.3) after upgrading to 1.7.2 the issue resolved. \r\nUpdated the api \r\n`faiss.index_factory(128, \"HNSW32,Flat\", faiss.METRIC_INNER_PRODUCT)`\r\nCorrect Result : `0.9917613`\r\n\r\n**Note : Results are direct from API (Not used: 2 - 2 * 0.9917611 = 0.0164778)**\r\n\r\nWith respect to C++ I am facing the same issue of incorrect results (i.e getting Euclidean distance) instead of cosine similarity.\r\nI am using the following code.\r\nFaiss compiled from repo : latest version\r\n```\r\nfaiss::IndexHNSWFlat index(128,64);\r\nindex.metric_type = faiss::METRIC_INNER_PRODUCT;\r\n\r\nnormalize(xb)\r\nindex.add(xb)\r\nnormalize(xq)\r\n\r\nindex.search(...)\r\n```\r\nResult: `-0.0164774` \r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why HNSW index returns different metric results than Flat index",
      "Clarification on proper index configuration for inner product metric in HNSW",
      "Consistent behavior between Python and C++ implementations",
      "Understanding of metric type handling in different Faiss versions",
      "Explanation of normalization requirements for cosine similarity"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:10:46"
    }
  },
  {
    "number": 1001,
    "title": "IndexIVFFlat on 2M embeddings from FaceNet is giving poor results",
    "created_at": "2019-10-23T16:53:21Z",
    "closed_at": "2019-10-23T21:31:22Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1001",
    "body": "# Summary\r\nI am using embeddings computed from the popular FaceNet model. I have calculate about 2.5M embeddings in d=512 and am looking at performance of the `IndexIVFFlat` compared to the simple `Flat` index. Even with large `k` I see flat results in the recall\r\n\r\nRunning on:\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n```\r\nxb = np.ascontiguousarray(X[::2][:2*1000*1000])\r\nxq = np.ascontiguousarray(X[1::2][:10*1000])\r\nd = xq.shape[1]\r\n\r\n# compute gt\r\nflat_index = faiss.index_factory(d, \"Flat\")\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.index_cpu_to_gpu(res, 0, flat_index, None)\r\nflat_index.train(xb)\r\nflat_index.add(xb)\r\nD, gt = flat_index.search(xq, k)\r\n\r\n# try an approximate method\r\nindex = faiss.index_factory(d, \"IVF<n_centroids>,Flat\")\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.index_cpu_to_gpu(res, 0, index, None)\r\nindex.train(xb)\r\nindex.add(xb)\r\n\r\ndef evaluate(index, xq, gt, k):\r\n    nq = xq.shape[0]\r\n    t0 = time.time()\r\n    D, I = index.search(xq, k)  # noqa: E741\r\n    t1 = time.time()\r\n    recalls = {}\r\n    i = 1\r\n    while i <= k:\r\n        recalls[i] = (I[:, :i] == gt[:, :1]).sum() / float(nq)\r\n        i *= 10\r\n\r\n    return (t1 - t0) * 1000.0 / nq, recalls\r\n\r\nevaluate(flat_index, xq, gt, 1000)\r\n>>\r\n(2.1849388122558593, \r\n {1: 0.99850000000000005, \r\n  10: 1.0, \r\n  100: 1.0, \r\n  1000: 1.0})\r\n\r\nevaluate(index, xq, gt, 1000)\r\n\r\n>>\r\n(0.038869810104370114,\r\n {1: 0.35210000000000002,\r\n  10: 0.35289999999999999,\r\n  100: 0.35289999999999999,\r\n  1000: 0.35299999999999998})\r\n```\r\nNotice how the recall is not increasing as k increases.\r\n\r\nI have tried many ,<n_centroids>, between  4096 to 20000 and I do not see any improvement. \r\n\r\n### Questions:\r\n1. Is it possible that the data distribution is not conducive to this method? \r\n\r\n2. Am I possibly splitting my query and training set incorrectly?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1001/comments",
    "author": "ljstrnadiii",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-10-23T20:12:05Z",
        "body": "You are only looking in a single IVF list, as `nprobe` is by default 1.\r\n\r\nIncrease `nprobe` rather than `k`.\r\n"
      },
      {
        "user": "ljstrnadiii",
        "created_at": "2019-10-23T21:31:22Z",
        "body": "of course, merci beaucoup!\r\n\r\nI did want to ask about the typical strategy to split your datasets. In some examples I have noticed that you build an xb, xt, xq dataset: one for training, one for adding and the last for query (equivalent to a test set). I am not sure what is the typical split for this field. Do you usually train on xt, add [xt, xb] (or does xb already contain xt?) to the index, and search with xt? It is hard to tell how you have constructed your memmap files. What proportion of the whole dataset is xq, xt and xb typically?\r\n\r\nthanks for such a killer project!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how nprobe parameter affects recall performance in IVF indices",
      "Guidance on proper dataset splitting strategies for training/indexing/querying",
      "Clarification of relationship between k (search depth) and nprobe (search scope)",
      "Best practices for configuring IVF parameters with high-dimensional embeddings",
      "Explanation of data distribution requirements for IVF effectiveness"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 00:11:14"
    }
  }
]