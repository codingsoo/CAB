[
  {
    "number": 76040,
    "title": "How to configure to stop insert data into local table in specific machine with cluster table scenario?",
    "created_at": "2025-02-13T09:03:51Z",
    "closed_at": "2025-02-14T02:32:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/76040",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nIn such situation, a clickhouse cluster with 4 machines: ck0, ck1, ck2, ck3,\nand local table test_tb in all 4 machines, and corresponding cluster table test_tb_cluster in ck0.\nSelect test_tb_cluster table data in ck0 will query every test_tb table data from all 4 machines.\nWhat I want is store table test_tb data in just ck1, ck2, ck3 these 3 machines, not store in ck0(reduct insert stress and select stress in this machine, just use for insert&select task distribution), \nMust create table test_tb in ck0, right? Otherwise operate test_tb_cluster will raise error: \nThere is no table `dbxxx`.`test_tb ` on server: ck0:9000\n\nSo how can I config table test_tb in ck0 to make it just as a empty local table? \nInsert data into test_tb_cluster will never choice test_tb in ck0 to store.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/76040/comments",
    "author": "flyly0755",
    "comments": [
      {
        "user": "cangyin",
        "created_at": "2025-02-14T01:52:38Z",
        "body": "I come up with 3 ways, hope one of them can be helpful:\n\n1. Configure the ingestion behavior on client side (for example only let the client know about the addresses of ck1 ~ ck3).\n\n2. Another cluster and another Distributed table\n\n    Create another logical cluster in `<remote_servers>` without ck0, say `cluster_no_ck0`\n\n    Create another Distributed table on cluster `cluster_no_ck0`\n\n\n3. With `insert_distributed_one_random_shard`\n \n    Recreate the distributed table with NO sharding key.\n\n    Configure the weight of shard 0 to zero in `<remote_servers>`.\n\n    And insert with settings `insert_distributed_one_random_shard=1`\n\n"
      },
      {
        "user": "flyly0755",
        "created_at": "2025-02-14T02:32:00Z",
        "body": "I have tried the second ways, which is perfect for solving this problem, thx!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must allow cluster table operations without requiring local table data storage on ck0",
      "Configuration must prevent ck0 from being selected as a storage node during inserts",
      "Approach must maintain cluster table accessibility without 'table missing' errors",
      "Solution should work at cluster configuration level rather than requiring client-side changes",
      "Must support existing cluster architecture with ck0 as coordination node"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:37:51"
    }
  },
  {
    "number": 72695,
    "title": "Does attach part support rollback if it fails?",
    "created_at": "2024-12-02T12:07:39Z",
    "closed_at": "2024-12-02T12:40:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/72695",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nIf ALTER TABLE %s ATTACH PART '%s' fails, is detach rollback supported?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/72695/comments",
    "author": "caicancai",
    "comments": [
      {
        "user": "rschu1ze",
        "created_at": "2024-12-02T12:10:04Z",
        "body": "Maybe I don't understand your question right but if the ATTACH operation fails, the part is not attached. Why should DETACH be rolled back?"
      }
    ],
    "satisfaction_conditions": [
      "Clarification of ClickHouse's behavior when ATTACH PART operations fail",
      "Explanation of state management for parts during failed operations",
      "Transactional integrity guarantees for DETACH/ATTACH operations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:37:59"
    }
  },
  {
    "number": 72530,
    "title": "Can i use a nested path in JSONExtractArrayRaw?",
    "created_at": "2024-11-27T05:09:54Z",
    "closed_at": "2024-11-27T14:46:44Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/72530",
    "body": "### Company or project name\r\n\r\n_No response_\r\n\r\n### Question\r\n\r\nClickHouse 24.10.3.21 running on x64 Debian\r\n\r\nI'm trying to extract, ideally as a ClickHouse array, values of a nested array from JSON. I can do it in quite a convoluted way:\r\n\r\n```\r\nSELECT JSONExtractArrayRaw(JSONExtract(JSONExtract('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'String'), 'details', 'String'), 'hobbies')\r\n\r\nQuery id: 2628ff82-8ad1-4669-98df-183700e03779\r\n\r\n   \u250c\u2500JSONExtractArrayRaw(JSONExtract(JSONExtract('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'String'), 'details', 'String'), 'hobbies')\u2500\u2510\r\n1. \u2502 ['{\"key\":1}','{\"key\":2}','{\"key\":3}']                                                                                                                                  \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThat's verbose. \r\n\r\nIs it possible to pass nested path to JSONExtractArrayRaw - something like _user.details.hobbies_?\r\n\r\nI know that the new JSON data type / functions are on their way, but I'd like to stick to stable functionality.\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/72530/comments",
    "author": "pakud",
    "comments": [
      {
        "user": "tiagoskaneta",
        "created_at": "2024-11-27T08:37:33Z",
        "body": "`JSONExtract*` all support multiple keys:\r\n\r\n```\r\nSELECT JSONExtractArrayRaw('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', 'user', 'details', 'hobbies')\r\n```\r\n\r\nYou can also use `JSON_VALUE` if you prefer:\r\n\r\n```\r\nSELECT JSON_VALUE('{\"user\": {\"details\": {\"hobbies\": [{\"key\":1},{\"key\":2},{\"key\":3}]}}}', '$.user.details.hobbies') settings function_json_value_return_type_allow_complex=true\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Supports nested JSON path traversal in a single function call",
      "Works with stable JSON functions in ClickHouse 24.x",
      "Returns array elements in raw format",
      "Maintains compatibility with current JSONExtract* function family"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:38:06"
    }
  },
  {
    "number": 71340,
    "title": "clickhouse-keeper path gets deleted after ReplicatedMergeTree table recreation",
    "created_at": "2024-11-01T08:28:27Z",
    "closed_at": "2024-11-01T09:50:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/71340",
    "body": "### Company or project name\r\n\r\n_No response_\r\n\r\n### Question\r\nClickhouse Version: 24.9.1\r\nEnvironment:\r\n- ClickHouse cluster with 7 shards, 2 replicas each\r\n- One machine in shard 6 failed and was replaced\r\n\r\nSteps to reproduce:\r\n1. Replaced failed machine in shard 6\r\n2. Reconfigured the node and rejoined it to the cluster  (replica = 02)\r\n3. Recreated the table using ZooKeeper path: ``` (' /clickhouse/tables/7e5645d5-6728-4c27-ba8b-b96ba2dcb9bd/06','{replica}')```\r\n4. Table creation succeeded, Exec query ``` SELECT\r\n    database,\r\n    `table`,\r\n    is_leader,\r\n    is_readonly,\r\n    total_replicas,\r\n    active_replicas,\r\n    zookeeper_path,\r\n    queue_size,\r\n    inserts_in_queue,\r\n    merges_in_queue,\r\n    log_max_index,\r\n    log_pointer\r\nFROM system.replicas\r\nWHERE database = 'pro2_signoz_traces'  AND `table` = ''signoz_index_v2'') ```\r\n> total_replicas=2 active_replicas=2\r\n5. After a few minutes, the keeper path (/clickhouse/tables/7e5645d5-6728-4c27-ba8b-b96ba2dcb9bd/06/replicas/02) gets automatically deleted\r\n\r\n6. The table still exists in ClickHouse  \r\n> total_replicas=0 active_replicas=0\r\n\r\nQuestions:\r\n1. What could cause the ZooKeeper path to be automatically deleted while the table remains?\r\n2. How to properly recreate a replicated table after node failure?\r\nAny debug logs or configuration details that would be helpful in diagnosing this issue?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/71340/comments",
    "author": "youfu-fun",
    "comments": [
      {
        "user": "panzhilin007",
        "created_at": "2024-11-01T09:32:56Z",
        "body": "By default, ClickHouse will remove the zookeeeper path 480 seconds after you drop the table.\r\n\r\nTry to use `drop table tableName sync` instead.\r\n\r\n"
      },
      {
        "user": "youfu-fun",
        "created_at": "2024-11-01T09:41:18Z",
        "body": "> By default, ClickHouse will remove the zookeeeper path 480 seconds after you drop the table.\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cClickHouse \u4f1a\u5728\u4f60\u653e\u8868\u540e 480 \u79d2\u5220\u9664 zookeeeper \u8def\u5f84\u3002\r\n> \r\n> Try to use `drop table tableName sync` instead.\u5c1d\u8bd5\u6539\u7528 `drop table tableName sync`\u3002\r\n\r\nProblem solved, thanks \uff5e"
      }
    ],
    "satisfaction_conditions": [
      "Explains how to prevent automatic ZooKeeper path deletion during table recreation",
      "Identifies proper sequence for recreating replicated tables after node replacement",
      "Clarifies ClickHouse's automatic cleanup behavior for ZooKeeper paths",
      "Addresses synchronization requirements when dropping tables"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:38:14"
    }
  },
  {
    "number": 70599,
    "title": "ClickHouse distributed JOIN vs common JOIN",
    "created_at": "2024-10-12T19:24:28Z",
    "closed_at": "2024-10-14T10:19:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/70599",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\n1. Set up\r\n\r\nClickHouse cluster with 2 shards, 1 replica on each shard\r\nLocal table testjoin on each replica\r\n```\r\nCREATE TABLE testjoin\r\n(\r\n    `user` String,\r\n    `type` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/testjoin', '{replica}')\r\nPARTITION BY type\r\nORDER BY type\r\n```\r\nDistributed table testall\r\n```\r\nCREATE TABLE testall\r\n(\r\n    `user` String,\r\n    `type` String\r\n)\r\nENGINE = Distributed('cluster', 'default', 'testjoin', rand())\r\n```\r\nShard 1 Replica 1 has the following data\r\nSELECT *\r\nFROM testjoin\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u2510\r\n\u2502 user1     \u2502 type1     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nShard 2 Replica 1 has the following data\r\nSELECT *\r\nFROM testjoin\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u2510\r\n\u2502 user1     \u2502 type2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n2. Query with INNER JOIN and GLOBAL INNER JOIN yields the same result. distributed_product_mode = 'allow' or  'local' or 'deny' makes no difference either. Is this expected behavior?\r\n\r\nWITH\r\n    t1 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type1'\r\n    ),\r\n    t2 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type2'\r\n    )\r\nSELECT *\r\nFROM t1\r\nINNER JOIN t2 ON t1.user = t2.user\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nWITH\r\n    t1 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type1'\r\n    ),\r\n    t2 AS\r\n    (\r\n        SELECT *\r\n        FROM testall\r\n        WHERE type = 'type2'\r\n    )\r\nSELECT *\r\nFROM t1\r\nGLOBAL INNER JOIN t2 ON t1.user = t2.user\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/70599/comments",
    "author": "wlzywang",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2024-10-14T10:19:36Z",
        "body": "When JOINing subqueries, including those from CTEs (rather than regular tables), JOIN is fully performed on the initiating node, and does not depend on the mode (local or global)."
      },
      {
        "user": "wlzywang",
        "created_at": "2024-10-14T17:01:27Z",
        "body": "Hi Alexey-Milovidov\r\n\r\nEven if I do not use WITH, the result is the same\r\n\r\n```\r\nSELECT *\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM testall\r\n    WHERE type = 'type1'\r\n) AS t1\r\nINNER JOIN\r\n(\r\n    SELECT *\r\n    FROM testall\r\n    WHERE type = 'type2'\r\n) AS t2 ON t1.user = t2.user\r\n```\r\n\r\n\u250c\u2500user\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500t2.user\u2500\u252c\u2500t2.type\u2500\u2510\r\n\u2502 user1 \u2502 type1 \u2502 user1   \u2502 type2   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nCould you please explain a bit more?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2024-10-14T18:28:14Z",
        "body": "When JOINing subqueries, JOIN is fully performed on the initiating node, and does not depend on the mode (local or global)."
      },
      {
        "user": "wlzywang",
        "created_at": "2024-10-14T19:19:50Z",
        "body": "I see. Now I feel safe in my usage. Thank you for confirming, Alexey-Milovidov."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why JOIN operations on distributed tables behave identically regardless of GLOBAL modifier or distributed_product_mode settings in this specific scenario",
      "Clarification of when distributed JOIN processing occurs vs centralized processing on the initiator node",
      "Identification of the specific query structure characteristics that force centralized JOIN execution",
      "General rule for when distributed_product_mode settings take effect vs when they're bypassed"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:38:21"
    }
  },
  {
    "number": 70548,
    "title": "To upgrade Clickhouse from 22.x to 24.x",
    "created_at": "2024-10-10T05:11:39Z",
    "closed_at": "2024-10-10T21:06:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/70548",
    "body": "### Company or project name\n\nTimePlay \n\n### Question\n\nI set up a test Clickhouse server running version 24.8.4.13 and noticed the data layout in the S3 bucket is different from that of  Clickhouse server running version 22.8.6.71.\r\n\r\nEssentially, with Clickhouse version 22.8.6.71, the S3 bucket contains object keys without  a prefix, for example,  \"aaapkcyerlxwvoeuyqfeqasxbsxpkdtq\", while with version 24.8.4.13, the object keys have a prefix, for example, \"abq/prlqajkzrohcpycwzbabdnnoehhls\"\r\n\r\nI wonder if this discrepancy in object key naming would cause any issues if I am to upgrade a  Clickhouse cluster from version 22.8.6.71 to 24.8.4.13.\r\n\r\nThank you.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/70548/comments",
    "author": "tlegit",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2024-10-10T11:14:00Z",
        "body": "No worries. It's a performance improvement to overcome some AWS S3 limitations. \r\nIt's backward and forward compatible.\r\n\r\n\r\nThe link to s3 object is stored in a metadata file (..../data/db/table/some.bin). Clickhouse-server reads this metadata file and finds that the object name is `aaapkcyerlxwvoeuyqfeqasxbsxpkdtq` or `abq/prlqajkzrohcpycwzbabdnnoehhls` or `hello.world`, then it reads the object. \r\nDuring writing Clickhouse-server generates some random name for an s3 object and saves this name to the metadata files."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of backward/forward compatibility between v22.x and v24.x S3 object key formats",
      "Explanation of how metadata files handle object key resolution across versions",
      "Clarification that data written by new versions remains accessible to old versions during transition",
      "Identification of the performance rationale behind the format change"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:38:29"
    }
  },
  {
    "number": 69232,
    "title": "s3 table function fetch the whole parquet instead of metadata for count when it's a small file",
    "created_at": "2024-09-03T19:37:10Z",
    "closed_at": "2024-09-04T15:53:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/69232",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\ni observed an issue with select count(*) from s3() reading parquet files that are small (900-1000KB)\r\nwhen i check how much byte it read\r\nit shows ReadBufferFromS3Bytes = sum of size of all files\r\nbut if i merge 2 files together and size is about 1.8MB\r\n```\r\nINSERT INTO FUNCTION s3(gcs_login, filename = 'clickhouse/test_merged.parquet') SELECT *\r\nFROM s3(gcs_login, filename = 'clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=..../time=..../{0,1}.parquet', format = 'Parquet')\r\n```\r\nand try again, it only fech 64KB which i think it's the metadata\r\n\r\n\r\n```\r\nSELECT\r\n    query,\r\n    formatReadableSize(ProfileEvents['ReadBufferFromS3Bytes']) AS bytes\r\nFROM system.query_log\r\nWHERE (event_date = today()) AND (query_id IN ('85c68382-d77f-4f23-929b-27752199a873', '209b6fd3-7dd8-432b-9c91-d7adbb9efd6d')) AND (type = 'QueryFinish')\r\nORDER BY event_time DESC;\r\n\r\nQuery id: 6f912e2d-ded3-4a29-ba2f-004bff20ff47\r\n\r\n   \u250c\u2500query\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500bytes\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502 select count(*) from s3(gcs_login,filename='clickhouse/test_merged.parquet', format='Parquet');                                                                                        \u2502 64.00 KiB \u2502\r\n2. \u2502 select count(*) from s3(gcs_login,filename='clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=..../time=..../*', format='Parquet'); \u2502 66.17 MiB \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n2 rows in set. Elapsed: 0.005 sec.\r\n```\r\n\r\nis there any threshold that decide to fetch the whole file or just the metadata? can i force clickhouse to only read the metadata? we need to run qa validation script on few TB data to monitor the count and most of them are small files (<1.5MB) so it only make sense if we can just fetch the metadata.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/69232/comments",
    "author": "jiayeZhu",
    "comments": [
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-03T21:56:53Z",
        "body": "additional info:\r\nI'm running clickhouse 24.8.3.59\r\nand i also tested `s3Cluster` table function. seems `s3Cluster` works fine and only read 64KB instead of the whole file"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-04T13:31:34Z",
        "body": "can you also  use `SETTINGS send_logs_level='trace'` for both queries to see what is going on exactly?"
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-04T15:07:40Z",
        "body": "it's a little bit hard to reproduce same result looks like once query on one s3 file it somehow no longer download the whole file again for the count.  but after few try i found this pattern\r\nfor small file:\r\n```\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.609867 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Debug> executeQuery: (from 127.0.0.1:44268) select count(*) from s3(gcs_login,filename='clickhouse/account_id=..../dataset_name=..../group_id=..../device_id=..../time_cadence=weekly/time=202431/{0,1}.parquet') SETTINGS send_logs_level = 'trace'; (stage: Complete)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.612962 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> S3Client: Provider type: GCS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.612983 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> S3Client: API mode of the S3 client: AWS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.657866 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Planner: Query to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.658639 [ 1450 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Planner: Query from stage FetchColumns to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.693984 [ 1422 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.693999 [ 1455 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n**[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694043 [ 1422 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 0 to 1 rows (from 0.00 B) in 0.000983196 sec. (0.000 rows/sec., 0.00 B/sec.)**\r\n**[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694052 [ 1455 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 0 to 1 rows (from 0.00 B) in 0.000983714 sec. (0.000 rows/sec., 0.00 B/sec.)**\r\n**[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694262 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> StorageObjectStorageSource: Downloading object of size 1105725 with initial prefetch**\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694273 [ 1467 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694294 [ 1467 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.694502 [ 1467 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 99999 to 1 rows (from 0.00 B) in 0.00143994 sec. (69446643.610 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.814955 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.814990 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.815278 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> AggregatingTransform: Aggregated. 100000 to 1 rows (from 0.00 B) in 0.122190712 sec. (818392.809 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.815292 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> Aggregator: Merging aggregated data\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:55:31.815319 [ 1434 ] {4c49d192-2d73-43d2-a27d-caaaa3f73fa5} <Trace> HashTablesStatistics: Statistics updated for key=4678785569880944277: new sum_of_sizes=4, median_size=1\r\n```\r\nfor large file:\r\n```\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:56:59.912725 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Debug> executeQuery: (from 127.0.0.1:44268) select count(*) from s3(gcs_login, filename='clickhouse/test2.parquet') SETTINGS send_logs_level = 'trace'; (stage: Complete)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:56:59.915309 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> S3Client: Provider type: GCS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:56:59.915333 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> S3Client: API mode of the S3 client: AWS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.055491 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Planner: Query to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.056022 [ 1450 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Planner: Query from stage FetchColumns to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111326 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111358 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111492 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> AggregatingTransform: Aggregated. 199999 to 1 rows (from 0.00 B) in 0.055219536 sec. (3621888.456 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111508 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> Aggregator: Merging aggregated data\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 14:57:00.111531 [ 1463 ] {f71ea783-cd0e-4666-8f16-1b0faf1a27b3} <Trace> HashTablesStatistics: Statistics updated for key=4218342127720216905: new sum_of_sizes=1, median_size=1\r\n```\r\nso looks like for small file the problem is it somehow did a first round aggregationg without any data (those  Aggregated. 0 to 1 rows) then try to download the whole file and aggregate"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-04T15:14:41Z",
        "body": "I see the smaller file is prefetched which makes sense based on the code and the size itself.\r\nWhat I don't understand is why 1.8MiB is determined to be a large file. The cutoff is `2 * max_download_buffer_size`.\r\n`max_download_buffer_size` by default is 10MiB.\r\n\r\nCan you try setting `max_download_buffer_size` to a really small value (maybe 1) just to confirm the theory?\r\nIn any case, the prefetch of 1MiB is done to optimize throughput. For larger files, parallel download should be used so no need to prefetch anything. \r\n"
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-04T15:44:16Z",
        "body": "thanks @antonio2368 \r\ni just created a new file with same content and with settings max_download_buffer_size=1\r\nhere is the trace\r\n```\r\nSELECT count(*)\r\nFROM s3(gcs_login, filename = 'clickhouse/test3.parquet')\r\nSETTINGS max_download_buffer_size = 1, send_logs_level = 'trace'\r\n\r\nQuery id: 4cb45d52-b28c-4dbc-be31-d849ef5f0623\r\n\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.025083 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> executeQuery: (from 127.0.0.1:39620) select count(*) from s3(gcs_login, filename='clickhouse/test3.parquet') SETTINGS max_download_buffer_size=1,send_logs_level = 'trace'; (stage: Complete)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.028246 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> S3Client: Provider type: GCS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.028266 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> S3Client: API mode of the S3 client: AWS\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.188557 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Planner: Query to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.189271 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Planner: Query from stage FetchColumns to stage Complete\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.220829 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> AggregatingTransform: Aggregating\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.220908 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Aggregator: Aggregation method: without_key\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.221074 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> AggregatingTransform: Aggregated. 199999 to 1 rows (from 0.00 B) in 0.031441883 sec. (6360910.382 rows/sec., 0.00 B/sec.)\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.221088 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> Aggregator: Merging aggregated data\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.221114 [ 1464 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Trace> HashTablesStatistics: Statistics updated for key=11572164019212161807: new sum_of_sizes=1, median_size=1\r\n   \u250c\u2500count()\u2500\u2510\r\n1. \u2502  199999 \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.222198 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> executeQuery: Read 199999 rows, 40.00 B in 0.197258 sec., 1013895.5074065438 rows/sec., 202.78 B/sec.\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.222431 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> MemoryTracker: Peak memory usage (for query): 225.56 KiB.\r\n[chi-ch-analytical-ch-analytical-2-0-0] 2024.09.04 15:38:08.222447 [ 1515 ] {4cb45d52-b28c-4dbc-be31-d849ef5f0623} <Debug> TCPHandler: Processed in 0.197843186 sec.\r\n\r\n1 row in set. Elapsed: 0.197 sec. Processed 200.00 thousand rows, 40.00 B (1.01 million rows/s., 202.57 B/s.)\r\nPeak memory usage: 225.56 KiB.\r\n\r\nchi-ch-analytical-ch-analytical-2-0-0.chi-ch-analytical-ch-analytical-2-0.clickhouse.svc.cluster.local :) SELECT\r\n    query,\r\n    formatReadableSize(ProfileEvents['ReadBufferFromS3Bytes']) AS bytes\r\nFROM system.query_log\r\nWHERE (event_date = today()) AND (query_id IN ('4cb45d52-b28c-4dbc-be31-d849ef5f0623')) AND (type = 'QueryFinish')\r\nORDER BY event_time DESC;\r\n\r\nSELECT\r\n    query,\r\n    formatReadableSize(ProfileEvents['ReadBufferFromS3Bytes']) AS bytes\r\nFROM system.query_log\r\nWHERE (event_date = today()) AND (query_id IN ('4cb45d52-b28c-4dbc-be31-d849ef5f0623')) AND (type = 'QueryFinish')\r\nORDER BY event_time DESC\r\n\r\nQuery id: ab87e7a3-b0bf-43a9-8e4a-a1af83d33d05\r\n\r\n   \u250c\u2500query\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500bytes\u2500\u2500\u2500\u2500\u2500\u2510\r\n1. \u2502 select count(*) from s3(gcs_login, filename='clickhouse/test3.parquet') SETTINGS max_download_buffer_size=1,send_logs_level = 'trace'; \u2502 64.00 KiB \u2502\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.008 sec.\r\n\r\nchi-ch-analytical-ch-analytical-2-0-0.chi-ch-analytical-ch-analytical-2-0.clickhouse.svc.cluster.local :) select _size from s3(gcs_login, filename='clickhouse/test3.parquet') limit 1;\r\n\r\nSELECT _size\r\nFROM s3(gcs_login, filename = 'clickhouse/test3.parquet')\r\nLIMIT 1\r\n\r\nQuery id: f85cf378-8dd1-44c8-a0e9-4da394382aa0\r\n\r\n   \u250c\u2500\u2500\u2500_size\u2500\u2510\r\n1. \u2502 2044228 \u2502 -- 2.04 million\r\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n---\n\nalso tried a new file with not same content but similar size (200k rows) still same result only downloaded 64KB"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-04T15:47:13Z",
        "body": "okay, so now the expected amount is downloaded?\r\n`max_download_buffer_size` should not be left to `1` as it is used by the parallel reader for larger file sizes. If you set it to such a low value, performance will suffer.\r\nI don't see a different way to forcefully disable it for smaller files only, but maybe we can add a new setting."
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-04T15:53:33Z",
        "body": "so seems like max_download_buffer_size=1 solve my issue with small files for now, we just don't want to download the whole file even if it has degraded performance. the inter region data transfer fee in gcs will be crazy without this settings.\r\nwe have plan to merge small files in the future so we no longer need to set max_download_buffer_size=1 and have better performance.\r\n\r\ni'm closing this one. thanks"
      },
      {
        "user": "antonio2368",
        "created_at": "2024-09-05T07:27:18Z",
        "body": "Please set `max_download_threads` to 1 so you explicitly disable parallel reading for such files and don't end up generating a lot of requests.\n\n---\n\n@jiayeZhu I found a better solution for those small files, try setting `remote_filesystem_read_prefetch` to `false`\r\nSeems like there is no need to add a new setting"
      },
      {
        "user": "jiayeZhu",
        "created_at": "2024-09-05T13:49:57Z",
        "body": "thanks @antonio2368. i tried `settings remote_filesystem_read_prefetch=0` it works. will use that when needed. thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how ClickHouse determines whether to read full Parquet files or only metadata for count(*) queries",
      "Configuration mechanism to force metadata-only reads for small Parquet files in s3 table function",
      "Guidance on optimizing settings for cost-sensitive scenarios with small files"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:38:44"
    }
  },
  {
    "number": 67878,
    "title": "Compression and uncompression of data flowing over a network",
    "created_at": "2024-08-06T04:43:10Z",
    "closed_at": "2024-08-06T13:29:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/67878",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nI want to copy data between clickhouses using the REMOTE function.\r\nWill the data flow over the network in compressed or uncompressed form?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/67878/comments",
    "author": "v1tam1nb2",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2024-08-06T12:14:52Z",
        "body": "By default, compression is enabled and lz4 is used.\r\n\r\n--network_compression_method arg                                                                   Allows you to select the method of data compression when writing.\r\n--network_zstd_compression_level arg                                                               Allows you to select the level of ZSTD compression.\r\n\r\n```\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 0.772 sec. Processed 10.07 million rows, 80.58 MB (13.04 million rows/s., 104.36 MB/s.)\r\n\r\n\r\nset network_compression_method='none';\r\n\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 1.430 sec. Processed 10.01 million rows, 80.06 MB (7.00 million rows/s., 55.98 MB/s.)\r\n\r\n\r\nset network_compression_method='zstd', network_zstd_compression_level=22;\r\n\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 0.673 sec. Processed 10.01 million rows, 80.06 MB (14.86 million rows/s., 118.87 MB/s.)\r\n\r\n\r\nset network_compression_method='lz4';\r\n\r\nselect 0 from remote('xxxx', system,numbers_mt, 'xxx', 'xxxx') limit 1e7 format Null;\r\n0 rows in set. Elapsed: 0.763 sec. Processed 10.07 million rows, 80.58 MB (13.20 million rows/s., 105.61 MB/s.)\r\n```"
      },
      {
        "user": "v1tam1nb2",
        "created_at": "2024-08-06T13:29:52Z",
        "body": "Thank you for your detailed response.\r\nThis question is closed."
      }
    ],
    "satisfaction_conditions": [
      "Clarification of default compression behavior for ClickHouse REMOTE function network transfers",
      "Explanation of performance implications related to compression settings",
      "Identification of configurable compression parameters",
      "Verification method for compression effectiveness"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:38:51"
    }
  },
  {
    "number": 66925,
    "title": "mysql engine on_duplicate_clause usage",
    "created_at": "2024-07-23T10:29:10Z",
    "closed_at": "2024-07-24T11:34:08Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/66925",
    "body": "### Company or project name\n\n_No response_\n\n### Question\n\nHi, I would like to ask why there are errors when using the MySQL engine to update data. I hope to receive your help\r\n\r\n```\r\nselect version();\r\n23.7.4.5\r\n```\r\n```\r\nCREATE NAMED COLLECTION mysql_test on cluster ck_cluster AS\r\n        host = 'xxx',\r\n        port = xxxx,\r\n        database = 'xxx',\r\n        user = 'xxx',\r\n        password = 'xxx' ,\r\nreplace_query = 0,\r\non_duplicate_clause = 1;\r\n```\r\n\r\n```\r\nCREATE TABLE test.test_01\r\n(\r\n    id UInt64 ,\r\n  package_name   Nullable(String) ,\r\n  app_name   Nullable(String) ,\r\n  source_app_name   Nullable(String) ,\r\n  update_time   Nullable(DateTime) ,\r\n  modify_time  Nullable(DateTime) \r\n) ENGINE = MySQL(mysql_test ,  table='cz_game_package_mapping')';\r\n```\r\n\r\n```\r\ninsert into test.test_01(id, package_name, app_name, source_app_name, update_time, modify_time)\r\nvalues( 775285052,'test_00',null, 'test_01', null,null) \r\nON DUPLICATE KEY UPDATE source_app_name = source_app_name;\r\n```\r\nerror message:\r\n`Code: 27. DB::ParsingException: Cannot parse input: expected '(' before: 'ON DUPLICATE KEY UPDATE source_app_name = source_app_name;':  at row 1: While executing ValuesBlockInputFormat: data for INSERT was parsed from query. (CANNOT_PARSE_INPUT_ASSERTION_FAILED)`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/66925/comments",
    "author": "12frame",
    "comments": [
      {
        "user": "pmusa",
        "created_at": "2024-07-23T20:49:03Z",
        "body": "Hey @12frame, it seems there is a bug when creating a table using named collections. When I try to create the table with `on_duplicate_clause=1`, clickhouse raises an error.\r\n\r\n```\r\nCREATE TABLE test_mysql3 (\r\n    id Int32,\r\n    name String\r\n) ENGINE = MySQL('localhost:3306', 'test', 'test', 'root', '', 0, 1);\r\n\r\nReceived exception:\r\nCode: 36. DB::Exception: Argument 'on_duplicate_clause' must be a literal with type String, got UInt64. (BAD_ARGUMENTS)\r\n ```\r\n\r\nBut indeed, if I do the same thing with named collections, no errors are raised.\r\n \r\nThe docs are not clear IMHO, but you need to define it as `on_duplicate_clause=UPDATE source_app_name = source_app_name`. And then, not pass anything when doing the insert: `insert into ... values ...;`\r\n\r\nBTW, your duplicate syntax didn't work as expected for me. You might need to look into MySQL correct syntax to update with the new value. It worked with a fixed string though: `on_duplicate_clause = 'UPDATE name=\\'duplicate\\''`."
      },
      {
        "user": "12frame",
        "created_at": "2024-07-24T02:02:29Z",
        "body": "Thank you very much. You can now work normally\r\nThe description in the document is indeed very vague, and there are no relevant cases to describe it. I hope you can supplement and explain it to him. Thank you"
      }
    ],
    "satisfaction_conditions": [
      "Clarification of correct syntax for MySQL engine's on_duplicate_clause parameter",
      "Explanation of required parameter types for named collections",
      "Documentation of conflict resolution workflow with MySQL engine",
      "Guidance on parameter formatting for SQL clause injection"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:38:57"
    }
  },
  {
    "number": 63100,
    "title": "Not executing fetch of part xxx because 8 fetches already executing, max 8",
    "created_at": "2024-04-29T04:17:38Z",
    "closed_at": "2024-04-30T14:44:14Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/63100",
    "body": "My ch version\r\n\r\nClickHouse client version 23.8.8.20 (official build).\r\n\r\nI have a lot of queues \uff0cI want to set background_fetches_pool_size = 32 but not work/\r\n\r\n```python\r\nSELECT\r\n    database,\r\n    table,\r\n    type,\r\n    max(last_exception),\r\n    max(postpone_reason),\r\n    min(create_time),\r\n    max(last_attempt_time),\r\n    max(last_postpone_time),\r\n    max(num_postponed) AS max_postponed,\r\n    max(num_tries) AS max_tries,\r\n    min(num_tries) AS min_tries,\r\n    countIf(last_exception != '') AS count_err,\r\n    countIf(num_postponed > 0) AS count_postponed,\r\n    countIf(is_currently_executing) AS count_executing,\r\n    count() AS count_all\r\nFROM system.replication_queue\r\nGROUP BY\r\n    database,\r\n    table,\r\n    type\r\nORDER BY count_all DESC\r\n\r\nQuery id: 345b6e7c-e993-4227-bc60-939ac2ee23a7\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u252c\u2500max(last_exception)\u2500\u252c\u2500max(postpone_reason)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500min(create_time)\u2500\u252c\u2500max(last_attempt_time)\u2500\u252c\u2500max(last_postpone_time)\u2500\u252c\u2500max_postponed\u2500\u252c\u2500max_tries\u2500\u252c\u2500min_tries\u2500\u252c\u2500count_err\u2500\u252c\u2500count_postponed\u2500\u252c\u2500count_executing\u2500\u252c\u2500count_all\u2500\u2510\r\n\u2502 xxx    \u2502 xxx \u2502 GET_PART \u2502                     \u2502 Not executing fetch of part ff8d5acf92437a06b529a9152e275fbc_4379_4379_0 because 8 fetches already executing, max 8. \u2502 2024-04-27 22:51:46 \u2502    2024-04-29 12:15:05 \u2502     2024-04-29 12:15:08 \u2502          2221 \u2502         1 \u2502         0 \u2502         0 \u2502          673114 \u2502               1 \u2502    673114 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT\r\n    type,\r\n    count(*)\r\nFROM system.replication_queue\r\nGROUP BY type\r\n\r\nQuery id: 0b339b1e-323d-4069-b2a8-8fc8222c65b3\r\n\r\n\u250c\u2500type\u2500\u2500\u2500\u2500\u2500\u252c\u2500count()\u2500\u2510\r\n\u2502 GET_PART \u2502  672841 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.settings\r\nWHERE name IN ('background_fetches_pool_size', 'background_schedule_pool_size', 'background_pool_size')\r\n\r\nQuery id: 5136cca9-d3e9-4682-9125-3a9c6628a240\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2510\r\n\u2502 background_pool_size          \u2502 16    \u2502\r\n\u2502 **\r\n\r\n> **background_fetches_pool_size**\r\n\r\n**  \u2502 16    \u2502\r\n\u2502 background_schedule_pool_size \u2502 128   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n``` \r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/63100/comments",
    "author": "kernel288",
    "comments": [
      {
        "user": "cangyin",
        "created_at": "2024-04-29T06:35:46Z",
        "body": "> ```\r\n> SELECT\r\n>     name,\r\n>     value\r\n> FROM system.settings\r\n> WHERE name IN ('background_fetches_pool_size', 'background_schedule_pool_size', 'background_pool_size')\r\n> ```\r\n\r\nThe pool size settings are server settings. Values in `system.settings` with same names are deprecated.\r\n\r\nCheck `system.server_settings` or `/var/lib/clickhouse/preprocessed_configs/config.xml`"
      },
      {
        "user": "kernel288",
        "created_at": "2024-04-29T06:45:21Z",
        "body": "> system.server_settings\r\n\r\nThank U for reply\r\n\r\n```python\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.server_settings\r\nWHERE name LIKE '%background%'\r\n\r\nQuery id: 38dc8001-dd92-4adc-aba5-0026dd7115b5\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 background_pool_size                          \u2502 16          \u2502\r\n\u2502 background_merges_mutations_concurrency_ratio \u2502 2           \u2502\r\n\u2502 background_merges_mutations_scheduling_policy \u2502 round_robin \u2502\r\n\u2502 background_move_pool_size                     \u2502 8           \u2502\r\n\u2502 background_fetches_pool_size                  \u2502 8           \u2502\r\n\u2502 background_common_pool_size                   \u2502 8           \u2502\r\n\u2502 background_buffer_flush_schedule_pool_size    \u2502 16          \u2502\r\n\u2502 background_schedule_pool_size                 \u2502 128         \u2502\r\n\u2502 background_message_broker_schedule_pool_size  \u2502 16          \u2502\r\n\u2502 background_distributed_schedule_pool_size     \u2502 16          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n10 rows in set. Elapsed: 0.004 sec. \r\n\r\ncat /etc/clickhouse-server/users.xml \r\n\r\n<?xml version=\"1.0\"?>\r\n<clickhouse>\r\n    <!-- See also the files in users.d directory where the settings can be overridden. -->\r\n\r\n    <!-- Profiles of settings. -->\r\n    <profiles>\r\n        <!-- Default settings. -->\r\n        <default>\r\n            <background_fetches_pool_size>16</background_fetches_pool_size>\r\n\r\n``` \r\n\r\n**It's still not work** \r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "cangyin",
        "created_at": "2024-04-29T06:49:43Z",
        "body": "```xml\r\n<clickhouse>\r\n    ...\r\n    <background_fetches_pool_size>16</background_fetches_pool_size>\r\n    ...\r\n</clickhouse>\r\n```\r\n\r\nIt's a server setting, should be placed under `<clickhouse>`, not inside profile settings (or user settings).\r\n"
      },
      {
        "user": "kernel288",
        "created_at": "2024-04-29T07:38:54Z",
        "body": "> ```\r\n>     <background_fetches_pool_size>16</background_fetches_pool_size>\r\n> ```\r\n\r\nThank U  for reply\r\n\r\n```python\r\n[root@SHPL007176031 ~]# head -n 15  /etc/clickhouse-server/users.xml \r\n<?xml version=\"1.0\"?>\r\n<clickhouse>\r\n    <!-- See also the files in users.d directory where the settings can be overridden. -->\r\n    <background_fetches_pool_size>16</background_fetches_pool_size>\r\n    <background_pool_size>96</background_pool_size>\r\n    <!-- Profiles of settings. -->\r\n    <profiles>\r\n        <!-- Default settings. -->\r\n        <default>\r\n            <!-- Maximum memory usage for processing single query, in bytes. -->\r\n            <max_memory_usage>100000000000</max_memory_usage>\r\n            <max_partitions_per_insert_block>5000000</max_partitions_per_insert_block>\r\n            <max_insert_block_size>100000000</max_insert_block_size>\r\n            <min_insert_block_size_rows>100000000</min_insert_block_size_rows>\r\n            <min_insert_block_size_bytes>500000000</min_insert_block_size_bytes>\r\n\r\n\r\nClickHouse client version 23.8.8.20 (official build).\r\nConnecting to database ztmdb at 127.0.0.1:9000 as user default.\r\nConnected to ClickHouse server version 23.8.8 revision 54465.\r\n\r\nWarnings:\r\n * Table system.session_log is enabled. It's unreliable and may contain garbage. Do not use it for any kind of security monitoring.\r\n\r\nSHPL007176031 :) select name,value from system.server_settings where name like '%background%' ;\r\n\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.server_settings\r\nWHERE name LIKE '%background%'\r\n\r\nQuery id: afedd27a-f4ce-4e21-8e5d-f8c560b42581\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 background_pool_size                          \u2502 16          \u2502\r\n\u2502 background_merges_mutations_concurrency_ratio \u2502 2           \u2502\r\n\u2502 background_merges_mutations_scheduling_policy \u2502 round_robin \u2502\r\n\u2502 background_move_pool_size                     \u2502 8           \u2502\r\n\u2502 background_fetches_pool_size                  \u2502 8           \u2502\r\n\u2502 background_common_pool_size                   \u2502 8           \u2502\r\n\u2502 background_buffer_flush_schedule_pool_size    \u2502 16          \u2502\r\n\u2502 background_schedule_pool_size                 \u2502 128         \u2502\r\n\u2502 background_message_broker_schedule_pool_size  \u2502 16          \u2502\r\n\u2502 background_distributed_schedule_pool_size     \u2502 16          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n``` \r\n\r\n**I add the server settings . But still not work** \n\n---\n\n> ```\r\n> <clickhouse>\r\n>     ...\r\n>     <background_fetches_pool_size>16</background_fetches_pool_size>\r\n>     ...\r\n> </clickhouse>\r\n> ```\r\n> \r\n> It's a server setting, should be placed under `<clickhouse>`, not inside profile settings (or user settings).\r\n\r\nYes u are right .\r\n\r\nI use the old config.xml  . I upgrade ck version . i use the old config .\r\n\r\n```python\r\nSELECT\r\n    name,\r\n    value\r\nFROM system.server_settings\r\nWHERE name LIKE '%background%'\r\n\r\nQuery id: ad8ced44-4f07-4327-adc0-7f65f4aab3d9\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 background_pool_size                          \u2502 128         \u2502\r\n\u2502 background_merges_mutations_concurrency_ratio \u2502 2           \u2502\r\n\u2502 background_merges_mutations_scheduling_policy \u2502 round_robin \u2502\r\n\u2502 background_move_pool_size                     \u2502 16          \u2502\r\n\u2502 background_fetches_pool_size                  \u2502 16          \u2502\r\n\u2502 background_common_pool_size                   \u2502 16          \u2502\r\n\u2502 background_buffer_flush_schedule_pool_size    \u2502 32          \u2502\r\n\u2502 background_schedule_pool_size                 \u2502 256         \u2502\r\n\u2502 background_message_broker_schedule_pool_size  \u2502 32          \u2502\r\n\u2502 background_distributed_schedule_pool_size     \u2502 32          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n``` "
      },
      {
        "user": "den-crane",
        "created_at": "2024-04-29T12:06:30Z",
        "body": "> /etc/clickhouse-server/users.xml \r\n\r\npool settings are in config.xml now"
      },
      {
        "user": "kernel288",
        "created_at": "2024-04-29T12:23:14Z",
        "body": "> > /etc/clickhouse-server/users.xml\r\n> \r\n> pool settings are in config.xml now\r\n\r\nThis is fix it . thank u "
      }
    ],
    "satisfaction_conditions": [
      "Correct identification of the proper configuration file location for server-level pool settings",
      "Validation mechanism for verifying active pool size settings",
      "Clarification of server setting persistence requirements",
      "Differentiation between deprecated and current configuration mechanisms"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:39:05"
    }
  },
  {
    "number": 59893,
    "title": "Any way to record system.query_log into files?",
    "created_at": "2024-02-12T14:23:36Z",
    "closed_at": "2024-02-13T04:04:51Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/59893",
    "body": "In order to analysis executed SQLs by popular tools like fluentd, is there any way to record system.query_log into files rather than executing sqls? Thanks~\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/59893/comments",
    "author": "CzyerChen",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2024-02-13T04:04:46Z",
        "body": "It is possible by using a materialized view pushing to a table with the `File` engine."
      },
      {
        "user": "CzyerChen",
        "created_at": "2024-02-21T07:18:01Z",
        "body": "> It is possible by using a materialized view pushing to a table with the `File` engine.\r\n\r\nIs it possible to support table to files in the future, or `using a materialized view pushing to a table with the File engine.` may be the best practice.\r\n\r\n\n\n---\n\n> > It is possible by using a materialized view pushing to a table with the `File` engine.\r\n> \r\n> Is it possible to support table to files in the future, or `using a materialized view pushing to a table with the File engine.` may be the best practice.\r\n\r\n@alexey-milovidov  cloud you reply it?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2024-02-21T07:24:30Z",
        "body": "You can specify the table engine `File` in the configuration, and it will write directly to a file."
      },
      {
        "user": "CzyerChen",
        "created_at": "2024-02-21T09:05:51Z",
        "body": "> You can specify the table engine `File` in the configuration, and it will write directly to a file.\r\n\r\nYes, I have tried the way you said, that works.\r\nBut now actually bring in additional manual works because not support by configs."
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable writing system.query_log data to files without requiring manual SQL execution",
      "Implementation should work through configuration rather than manual processes",
      "Must support direct integration with file-based storage systems",
      "Solution should be maintainable as a long-term approach"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:39:13"
    }
  },
  {
    "number": 58932,
    "title": "Does the Kafka engine automatically decompress messages when LZ4 compression is used?",
    "created_at": "2024-01-18T02:03:41Z",
    "closed_at": "2024-01-18T08:42:01Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/58932",
    "body": "Does the Kafka engine automatically decompress messages when LZ4 compression is used? I am currently using Azure Event Hubs and hoping to reduce data processing fees.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/58932/comments",
    "author": "Bamboo-devops",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2024-01-18T08:09:48Z",
        "body": "It you mean the compression on the level of the topic - yes, it should work out of the box."
      },
      {
        "user": "Bamboo-devops",
        "created_at": "2024-01-19T03:40:03Z",
        "body": "Do you mean the kafka engine is already getting messages with compression enabled? "
      },
      {
        "user": "Algunenano",
        "created_at": "2024-01-19T12:35:04Z",
        "body": "The kafka engine receives messages. If the messages are compressed (by whoever generates them) then it decompress them."
      }
    ],
    "satisfaction_conditions": [
      "Clarifies whether Kafka engine handles message decompression automatically when LZ4 is used",
      "Confirms the relationship between message compression at the producer and decompression at the Kafka engine level"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:39:21"
    }
  },
  {
    "number": 55231,
    "title": "Clickhouse user configuration",
    "created_at": "2023-10-04T11:03:25Z",
    "closed_at": "2023-10-05T01:55:26Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/55231",
    "body": "I have installed Clickhouse with 3 shards and 2 replicas in eks cluster. The user setting are in /etc/clickhouse-server/users.d/ folder in the clickhouse pod. In that folder, I have added the admin users setting. \r\n\r\nProblem description : \r\n\r\nWhen connected to the clickhouse in local( inside the clickhouse pod) with admin user and credentials, it throws the below exception \r\n\r\nException : \r\ncommand used --> clickhouse-client -u admin --password '****'\r\nClickHouse client version 23.7.5.30 (official build).\r\nConnecting to localhost:9000 as user admin.\r\nCode: 516. DB::Exception: Received from localhost:9000. DB::Exception: admin: Authentication failed: password is incorrect, or there is no user with such name.. (AUTHENTICATION_FAILED)\r\n\r\nBut when I connect to the clickhouse through the service URL as host with admin user credentials, it getting connected to clickhouse.\r\n\r\ncommand used --> clickhouse-client -h clickhouse-clickhouse -u admin --password '****'\r\nClickHouse client version 23.7.5.30 (official build).\r\nConnecting to clickhouse-clickhouse:9000 as user admin.\r\nConnected to ClickHouse server version 23.7.5 revision 54465.\r\nchi-clickhouse-prod-cluster-shard0-0-0.chi-clickhouse-prod-cluster-shard0-0.clickhouse.svc.cluster.local :)\r\n\r\n\r\nDoes this mean the admin user setting have to be present in under /etc/clickhouse-server/users.d/ folder and also be present in /etc/clickhouse-server/users.xml file in clickhouse?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/55231/comments",
    "author": "Ragavendra-Vigneshwaran-R",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-10-04T12:31:46Z",
        "body": "it means that you misconfigured `<network>` section and disallowed to connect from a localhost.\r\n\r\nplease share your configs / hide passwords with ****\r\n\r\n"
      },
      {
        "user": "Ragavendra-Vigneshwaran-R",
        "created_at": "2023-10-05T01:55:26Z",
        "body": "Got the issue. I didn't add localhost in the admin user network/ip section. Thanks @den-crane for your answer."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how ClickHouse handles user authentication for local vs remote connections",
      "Clarification of network/IP restrictions in user configuration",
      "Guidance on proper localhost connection configuration"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:40:54"
    }
  },
  {
    "number": 53397,
    "title": "INSERTing into a table named 'table' fails",
    "created_at": "2023-08-14T08:39:56Z",
    "closed_at": "2023-08-14T11:23:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/53397",
    "body": "```sql\r\n-- this works\r\nCREATE TABLE tab (val1 UInt32, val2 UInt32) ENGINE=Memory;\r\nINSERT INTO tab VALUES (42, 24);\r\n\r\n-- this doesn't:\r\nCREATE TABLE table (val1 UInt32, val2 UInt32) ENGINE=Memory;\r\nINSERT INTO table VALUES (42, 24); -- Code: 62. DB::Exception: Syntax error: failed at position 27 ('42'): 42, 24);. Expected one of: list of elements, insert element, COLUMNS matcher, COLUMNS, qualified asterisk, compound identifier, identifier, asterisk. (SYNTAX_ERROR)\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/53397/comments",
    "author": "rschu1ze",
    "comments": [
      {
        "user": "evillique",
        "created_at": "2023-08-14T10:22:37Z",
        "body": "The docs should probably be adjusted because this works:\r\n``` SQL\r\nINSERT INTO TABLE table VALUES (42, 24);\r\n```"
      },
      {
        "user": "rschu1ze",
        "created_at": "2023-08-14T10:57:51Z",
        "body": "Didn't know there is a long form of `INSERT INTO` \ud83d\ude04  I agree, a doc update will be fine."
      },
      {
        "user": "lampjian",
        "created_at": "2023-08-16T14:09:59Z",
        "body": "This also works.\r\n```SQL\r\nINSERT INTO `table` VALUES (42, 24);\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explains how to reference tables with reserved keywords as names",
      "Documents alternative INSERT syntax formats",
      "Clarifies identifier escaping requirements"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:41:00"
    }
  },
  {
    "number": 51875,
    "title": "about summingmergetree Keep the earliest records",
    "created_at": "2023-07-06T04:00:16Z",
    "closed_at": "2023-07-06T04:07:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/51875",
    "body": "create table test (\r\nid int64,\r\ncreated_time DateTime,\r\ncnt Int64\r\n)ENGINE=ReplicatedSummingMergeTree() order by id \r\n\r\ninsert into test (1,'2023-07-05 01:00:00',1)\r\ninsert into test (1,'2023-07-05 02:00:00',2)\r\ninsert into test (1,'2023-07-05 03:00:00',3)\r\nAfter data merging\r\nselect * from test:\r\nI hope it's the following result\r\n1    '2023-07-05 01:00:00'   6\r\nrather than\r\n1    '2023-07-05 03:00:00'   6\r\n\r\nWhat do I need to do\uff1f\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/51875/comments",
    "author": "yangshike",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-07-06T13:21:59Z",
        "body": "```sql\r\nCREATE TABLE test\r\n(\r\n    `id` Int64,\r\n    `created_time` SimpleAggregateFunction(min, DateTime),\r\n    `cnt` Int64\r\n)\r\nENGINE = SummingMergeTree\r\nORDER BY id;\r\n\r\ninsert into test values(1,'2023-07-05 01:00:00',1);\r\ninsert into test values(1,'2023-07-05 02:00:00',2);\r\ninsert into test values(1,'2023-07-05 03:00:00',3);\r\n\r\noptimize table test final;\r\n\r\nselect * from test;\r\n\u250c\u2500id\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500created_time\u2500\u252c\u2500cnt\u2500\u2510\r\n\u2502  1 \u2502 2023-07-05 01:00:00 \u2502   6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "yangshike",
        "created_at": "2023-07-07T07:56:18Z",
        "body": "thank you\uff01\uff01\r\n\r\n\r\nThe default seems to be to keep the first inserted line"
      }
    ],
    "satisfaction_conditions": [
      "Retain the earliest timestamp value during aggregation",
      "Sum numeric columns while preserving non-aggregated columns' first occurrence",
      "Work with ClickHouse's SummingMergeTree engine mechanics",
      "Explain aggregation function behavior for timestamp columns"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:41:29"
    }
  },
  {
    "number": 50591,
    "title": "Why SYSTEM RESTORE REPLICA ... is not supported for Materialized views with Replica engines?",
    "created_at": "2023-06-05T15:01:50Z",
    "closed_at": "2023-06-06T11:20:33Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/50591",
    "body": "I am using the `SYSTEM RESTORE REPLICA` functionality to restore a MV with `ReplicatedAggregatingMergeTree`. In the docs it is just stated that the restore will only work for replicated tables, but nothing about not supporting it from MV. \r\nThis is needed in case the source table of MV has retention (TTL), then recreating the MV will not work\r\nA workaround would be to create another table localle and send all the data there\r\nI can also provide the data schema if this is unexpected behavior respectively a bug\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/50591/comments",
    "author": "yasha-dev1",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2023-06-06T10:15:07Z",
        "body": "You can do that for .inner table of the MV. \r\n\r\nAlso best practice is to use explicit storage table for the MV (i.e. syntax `CREATE MATERIALIZED VIEW ... TO target_table AS SELECT ...`)"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to restore materialized view replicas when source tables have TTL constraints",
      "Clarification on supported restore mechanisms for materialized views with Replicated* engines",
      "Guidance on maintaining data availability during retention policy enforcement",
      "Best practices for MV architecture that enables replica restoration"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:42:04"
    }
  },
  {
    "number": 49379,
    "title": "How to return an error from External UDFs?",
    "created_at": "2023-05-01T21:02:02Z",
    "closed_at": "2024-05-10T16:14:03Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/49379",
    "body": "How do you return an error from an external UDF?   An external UDF is a daemon-like process that constantly running, reading from STDIN, and writing response to STDOUT.   One way is to let the process die when an error happens but it's not ideal because starting up the process is costly, and also I cannot return a meaningful error message.\r\n\r\nSuppose there is an external UDF `f(key)`.  A key is one of `a` or `b`.  When something else is passed, I would like to raise \"invalid key\" error, or at least raise a generic error without interrupting the process.\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/49379/comments",
    "author": "knoguchi",
    "comments": [
      {
        "user": "davenger",
        "created_at": "2023-05-04T14:21:08Z",
        "body": "There is no special way to return error, but you can modify your UDF to return a tuple with 2 elements: f(key) and error_message.\r\n\r\nSet the return type as \"Tuple(UInt64,String)\"\r\n```\r\n<functions>                                     \r\n    <function>                                  \r\n        <type>executable</type>                 \r\n        <name>test_function_python</name>       \r\n        <return_type>Tuple(UInt64,String)</return_type>       \r\n        <argument><type>String</type></argument>\r\n        <argument><type>String</type></argument>\r\n        <format>TabSeparated</format>           \r\n        <command>test_function.py</command>     \r\n        <execute_direct>1</execute_direct>      \r\n    </function>                                 \r\n</functions>\r\n```\r\n\r\nIn the UDF write the return value as \"(result, message)\"\r\n```\r\n#!/usr/bin/python3\r\n\r\nimport sys\r\n\r\nif __name__ == '__main__':\r\n    i = 0\r\n    for line in sys.stdin:\r\n        arg1, arg2 = line.rstrip().split('\\t')\r\n        message = f'arguments are: arg1={arg1} arg2={arg2}'\r\n        print(f'({i},\\'{message}\\')', end='\\n')\r\n        sys.stdout.flush()\r\n        i += 1\r\n\r\n```\r\n\r\nThen you can access the result value and the message as elements of the tuple:\r\n```\r\nSELECT\r\n    test_function_python(number, number + 1) AS res,\r\n    res.1 AS result,\r\n    res.2 AS message\r\nFROM numbers(5)\r\n\r\nQuery id: fbe4d0ee-a614-4a1c-9b5d-0cdfb0ca4279\r\n\r\n\u250c\u2500res\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500result\u2500\u252c\u2500message\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 (0,'arguments are: arg1=0 arg2=1') \u2502      0 \u2502 arguments are: arg1=0 arg2=1 \u2502\r\n\u2502 (1,'arguments are: arg1=1 arg2=2') \u2502      1 \u2502 arguments are: arg1=1 arg2=2 \u2502\r\n\u2502 (2,'arguments are: arg1=2 arg2=3') \u2502      2 \u2502 arguments are: arg1=2 arg2=3 \u2502\r\n\u2502 (3,'arguments are: arg1=3 arg2=4') \u2502      3 \u2502 arguments are: arg1=3 arg2=4 \u2502\r\n\u2502 (4,'arguments are: arg1=4 arg2=5') \u2502      4 \u2502 arguments are: arg1=4 arg2=5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "knoguchi",
        "created_at": "2023-05-04T19:34:32Z",
        "body": "Thanks for the idea.  That's one way to achieve my goal.\r\n\r\nHere is my random thought.  If we can introduce header+body just like HTTP, the UDF can return error easily.   There is a config `send_chunk_header` in the XML that adds a header in the request.  Similarly it could add something like receive_status_header.\r\n"
      },
      {
        "user": "davenger",
        "created_at": "2023-05-09T11:48:38Z",
        "body": "Actually returning error from UDF might not be the best approach because typically a query processes not one row but a set of rows. If the query calls UDF for those rows and one of the rows makes the UDF return an error, then the whole query will fail as there is now way to return error for one row and valid results for other rows. So the approach with returning a tuple of result and status (or error message) columns from UDF addresses this scenario."
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to return per-row errors without terminating the UDF process",
      "Support for structured error messages in responses",
      "Error handling that preserves query execution for valid rows",
      "Backward-compatible with existing UDF communication patterns"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:42:13"
    }
  },
  {
    "number": 48836,
    "title": "How clickhouse respect alter delete and insert order ",
    "created_at": "2023-04-17T07:01:08Z",
    "closed_at": "2023-04-20T11:52:54Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/48836",
    "body": "I have some ETL task that collects data and insert them into clickhouse (22.3.* version). Typically data are inserted into distributed + replicated table (4x4) in bathes. If there is some problem with one or more batches we need to reprocess some of them. Data in reprocessed batch may be same as data inserted before or differ, entirely or partially. Reprocess may occur when we spot problem after few minutes or few days there is no rule here (Mention this to address possibile problem with deduplication mechanism). On reprocessing we do query like: \r\n\r\n```\r\nALTER TABLE table ON CLUSTER cluster DELETE \r\nWHERE event_date > {batch_start_date} AND event_date <= {batch_end_date}\r\n```\r\n\r\nThis will create mutation and my question is whenever execution time of this mutation influence insert that occurs right after ALTER TABLE query?  If data inserted after ALTER QUERY called and before actual execution of mutation may disappear if they meet WHERE clause conditions? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/48836/comments",
    "author": "pulina",
    "comments": [
      {
        "user": "Algunenano",
        "created_at": "2023-04-19T08:45:35Z",
        "body": "For MergeTree tables any part has a `data_version` number which related to when the part was created and what mutations are pending to be applied.\r\n\r\nLet's say that currently your parts are in `data_version=5` and you send a mutation (delete, replace, whatever). Any new part that is inserted will have `data_version=6`, that is they don't need to apply anything, while old parts will be \"mutated\" in the background from version 5 to version 6 by applying the query.\r\nSo data inserted before the mutation was sent receives the value of 5 and will be mutated. Data inserted after the mutation was sent will receive the value of 6 and won't be mutation."
      },
      {
        "user": "pulina",
        "created_at": "2023-04-20T11:52:54Z",
        "body": "Ok and after that there probably will be some part merging and we will end up with correct amount of data in single partition.  Than you. "
      },
      {
        "user": "Algunenano",
        "created_at": "2023-04-20T11:54:47Z",
        "body": "> Ok and after that there probably will be some part merging and we will end up with correct amount of data in single partition.\r\n\r\nYes. And merges only happen across parts with the same `data_version`, so before merging old data with new data the old one needs to be mutated."
      }
    ],
    "satisfaction_conditions": [
      "Clarification of mutation execution order relative to subsequent inserts",
      "Explanation of data versioning during mutations",
      "Confirmation of merge behavior with pending mutations",
      "Clarity on mutation scope boundaries"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:42:19"
    }
  },
  {
    "number": 47949,
    "title": "Default compression behavior",
    "created_at": "2023-03-23T16:54:21Z",
    "closed_at": "2023-03-23T17:14:55Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/47949",
    "body": "I have a CH DB version 22.03 with some data running with compression method zstd.\r\nI have changed in the file config.xml to compression method lz4\r\nAfter restart CH, I have noted that only new data inserted in a table has parts with file default_compression_codec.txt CODEC(LZ4) and the old data previous to the change has parts with: CODEC(ZSTD(1))\r\nMy question is:\r\nA change in the default compression method only works for new data inserted or I need to run OPTIMIZE TABLE ... FINAL in all the tables to apply the changes?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/47949/comments",
    "author": "arodmond",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-03-23T17:14:55Z",
        "body": ">A change in the default compression method only works for new data inserted \r\n\r\nyes. Only newly inserted and merged data.\r\n\r\n> I need to run OPTIMIZE TABLE ... FINAL in all the tables to apply the changes?\r\n\r\nYes, you need."
      }
    ],
    "satisfaction_conditions": [
      "Clarify whether default compression configuration changes only affect newly inserted data",
      "Specify if manual table optimization is required to apply compression changes to existing data",
      "Explain how background merge processes interact with compression settings"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:42:27"
    }
  },
  {
    "number": 46943,
    "title": "Naming convention about 'abstract' class",
    "created_at": "2023-02-27T09:35:59Z",
    "closed_at": "2023-02-27T13:26:17Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46943",
    "body": "I am learning from CH code and have a question about naming convention for abstract class. I see some classes called 'I-***', for example IColumn, IDataType, have defined both pure virtual functions and normal virtual functions. In Java/C# context, these classes are 'abstract' classes. I am wondering if the classes with 'I-***' names are representing both abstract classes and interfaces in CH source code. \r\n\r\nBy the way, is there no specific distinction between the abstract class and the interface in CH source code? Just want to follow the convention.\r\n\r\nThank you. ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46943/comments",
    "author": "Alex-Cheng",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2023-02-27T13:26:17Z",
        "body": "I would say that C++ does not have such a concept as an \"interface\". Java and C# have restrictions on multiple inheritance (to avoid the \"diamond problem\"), that's why interfaces are required (to allow classes implementing multiple interfaces). In C++ it's totally fine to have \"interfaces\" (well, maybe it would be better to call them \"the most base abstract classes\") with non-pure virtual methods, non-virtual methods, and even with fields. So the 'I-' prefix in a class name indicates that it's \"the most base abstract class\": it may or may not be a pure interface, it does not really matter. "
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2023-02-27T14:10:06Z",
        "body": "Thank you very much. So in CH repository, the I-* named classes are abstract base classes or interfaces (if there are all pure virtual functions) and we don't care about distingushing them. "
      }
    ],
    "satisfaction_conditions": [
      "Clarify the naming convention for abstract base classes in ClickHouse codebase",
      "Explain how C++ language features influence the interface/abstract class distinction in CH",
      "Confirm whether CH conventions differentiate between pure interfaces and abstract classes with implementations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:42:33"
    }
  },
  {
    "number": 46470,
    "title": "Is there any way to view keeper data without system.zookeeper table?",
    "created_at": "2023-02-16T09:47:03Z",
    "closed_at": "2023-02-16T10:59:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/46470",
    "body": "I deployed a keeper cluster as a separate service. Is there a client or api that allows me to operate the keeper service directly ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/46470/comments",
    "author": "hhalei",
    "comments": [
      {
        "user": "maybedino",
        "created_at": "2023-02-16T10:40:53Z",
        "body": "You should be able to use any zookeeper client for clickhouse-keeper."
      },
      {
        "user": "hhalei",
        "created_at": "2023-02-16T11:38:53Z",
        "body": "This way is OK\uff0cthanks\uff01"
      }
    ],
    "satisfaction_conditions": [
      "Compatibility with existing ZooKeeper clients",
      "Direct interaction with keeper service without system.zookeeper table dependency"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:42:50"
    }
  },
  {
    "number": 45951,
    "title": "Why size field of StringRef is 64bit (8 bytes)",
    "created_at": "2023-02-02T08:17:10Z",
    "closed_at": "2023-02-02T17:42:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45951",
    "body": "StringRef is usually used for representing a string and contains a pointer and size. A pointer has to be 64 bit in my x64 machine, however the size is not necessarily 64bit in my opinion, because usually string's length is less then 65535 and two bytes is enough. \r\n\r\nFor each string, 6 bytes are wasted. For big amount of strings, the wasted memory is considerable.\r\n\r\nWhy we choose 64bit (size_t) for string's size? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45951/comments",
    "author": "Alex-Cheng",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2023-02-02T17:46:21Z",
        "body": "4 bytes are sometimes not enough (there are memory ranges larger than 4 GiB).\n\n---\n\nAlignment often makes this saving useless. For example, if you have two StringRefs, one adjacent to another, the second one must be aligned by 8 bytes."
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2023-02-06T03:00:36Z",
        "body": "in my cases, all strings are less than 1000, and we could avoid alignment by 8 bytes via designing a specific container class (i.e. another implementation of vector<string>. If we did it then we could save a lot of memory, e.g. for 1billion of strings it would save 8GiB memory.\n\n---\n\n@alexey-milovidov please consider about the idea. I cannot re-open the ticket."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2023-02-07T22:24:53Z",
        "body": "It is possible to have strings larger than 4 GB in ClickHouse. Therefore, we should not use just 32 bits for string size.\n\n---\n\nHere is an example with 5 GB string:\r\n\r\n```\r\nmilovidov-desktop :) SELECT length(*) FROM file('/home/milovidov/Downloads/output.tsv', RawBLOB)\r\n\r\nSELECT length(*)\r\nFROM file('/home/milovidov/Downloads/output.tsv', RawBLOB)\r\n\r\nQuery id: 89bbcc01-06b1-4461-9574-2dd8acfd3826\r\n\r\n\u250c\u2500length(raw_blob)\u2500\u2510\r\n\u2502       5491800000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 4.174 sec. \r\n\r\nmilovidov-desktop :)\r\n```\n\n---\n\nLimiting something to 32 bit is a signature of old software, I don't want to have these limitations in ClickHouse."
      },
      {
        "user": "Alex-Cheng",
        "created_at": "2023-02-09T08:58:24Z",
        "body": "Got it! And StringRef has two fields: ptr, size. As ptr is 8-bytes, then the size of StringRef still 8-bytes even if the size is changed to 32bit."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why 32-bit size limitations are insufficient for the system's use cases",
      "Consideration of alignment/padding tradeoffs in data structures",
      "Avoidance of artificial limitations matching modern system capabilities"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:42:57"
    }
  },
  {
    "number": 45945,
    "title": "light weight `ALTER DROP INDEX`",
    "created_at": "2023-02-02T04:41:25Z",
    "closed_at": "2023-02-02T14:13:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45945",
    "body": "Can I just do `ALTER DROP INDEX`, followed by `KILL MUTATION`, to make it a lightweight operation, that only deletes the index metadata and leave the data parts as is ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45945/comments",
    "author": "cangyin",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-02-02T13:48:02Z",
        "body": "yes you can , but it will leave garbage files in parts. Why do you need it?"
      },
      {
        "user": "cangyin",
        "created_at": "2023-02-02T14:13:29Z",
        "body": "Because we are going to upgrade index name from `gin` to `inverted`, and table contains to many data parts. As for garbage files,  we can leave them to TTL cleaner script :).\n\n---\n\nThanks!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must allow dropping an index without modifying existing data parts",
      "Must tolerate residual index-related files that can be cleaned asynchronously",
      "Solution should work efficiently for tables with large numbers of data parts"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:43:15"
    }
  },
  {
    "number": 45616,
    "title": "What endpoint to check with HAProxy",
    "created_at": "2023-01-25T16:26:27Z",
    "closed_at": "2023-01-25T16:37:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45616",
    "body": "Hello, we're using HAProxy to route http reqs to our cluster.\r\n\r\nWhat is the best endpoint to check the replica is alive? We've tried `/replicas_status` or `/ping`.\r\n\r\nIs there any recommended one? E.g. `/replicas_status` often gives errs when ZK decides to put a table to RO mode.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45616/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-01-25T16:31:51Z",
        "body": "`/ping` is fine.\r\n\r\nWhy do you want to use `/replicas_status` ? Distributed tables query not stale replicas automatically (even if the local replica is stale)."
      },
      {
        "user": "simPod",
        "created_at": "2023-01-25T16:37:19Z",
        "body": "Yes, we've been using `/replicas_status` and worked kinda well with 21.1 but we've upgraded to 22.12 and it caused issues so we've switched to `/ping`.\r\n\r\nThanks for confirmation."
      }
    ],
    "satisfaction_conditions": [
      "The endpoint must reliably indicate replica availability without dependency on Zookeeper/table state",
      "The endpoint should provide a simple health check unaffected by replica staleness",
      "The solution must work across ClickHouse version upgrades (21.1 \u2192 22.12)"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:43:20"
    }
  },
  {
    "number": 45477,
    "title": "Question: PARTITION BY efficiency in MergeTree table.",
    "created_at": "2023-01-20T16:22:59Z",
    "closed_at": "2023-01-22T14:06:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45477",
    "body": "Hello!\r\nSorry for disturbing, but I want to clarify can I use PARTITION BY in my case.\r\n**The case:**\r\n-  We have table with metrics for multitenant setup. We have columns like tenant_id and event_time. \r\n-  By requirements this table should be optimized for per-tenant use. \r\n-  Almost always tenant will query data for the LAST MONTH or for the LAST day. So we choose ORDER BY (tenant_id, event_time) key. And this works great.\r\n-  Analytics want to use this database also for aggregated statistics (query will have range for event_time and no tenant_id specified). So we decided to add PARTITION BY toYYYYMM(event_time). \r\n- Table will contain data only for one year (will be controlled by database (TTL feature)). So we will get only 12 unique partition keys.\r\n- Table engine will be ReplicatedMergeTree\r\n\r\n**Our results**\r\nTable with PARTITION BY gives us 25% less rows scanned for **analytical** queries on synthetic data. Tenant's queries are good in both cases.\r\n**Question**\r\nIn many places it is pointed that PARTITION BY is not recommended to use for MergeTree-family engines. So I'm afraid that we can run into issues on production because of this decision.\r\nCan we use PARTITION BY in this case or should we refuse to do this despite of better performance in our tests?\r\n\r\nThank you!\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45477/comments",
    "author": "ilya-girman",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-01-20T17:34:55Z",
        "body": ">In many places it is pointed that PARTITION BY is not recommended to use for MergeTree-family engines.\r\n\r\nWhat places?????? You have misunderstood something. \r\n\r\nPARTITION BY is only for MergeTree-family engines.\r\n\r\nIn your case I would recommend exactly `PARTITION BY toYYYYMM(event_time)`.\r\n\r\n"
      },
      {
        "user": "ilya-girman",
        "created_at": "2023-01-22T14:06:41Z",
        "body": "Thank you a lot!\r\nI wanted to tell that usually it is not recommended to assign PARTITION BY manually."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that PARTITION BY usage is appropriate in scenarios with limited partition count and TTL-based data retention",
      "Clear rationale for when manual partitioning is acceptable in MergeTree engines",
      "Guidance on balancing analytical query performance with tenant-specific query requirements"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:43:27"
    }
  },
  {
    "number": 45232,
    "title": "CANNOT_PARSE_TEXT errors exceeded 600,000 times",
    "created_at": "2023-01-12T17:48:59Z",
    "closed_at": "2023-01-12T18:48:58Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/45232",
    "body": "ClickHouse Version: 22.10.1.1248\r\n\r\nThe following errors occur in /var/log/clickhouse-server/clickhouse-server.err.log almost every second.\r\n```\r\n<Error> TCPHandler: Code: 6. DB::Exception: Cannot parse string '2022-11-30 019:48:33.237' as DateTime64(6): syntax error at position 19 (parsed just '2022-11-30 019:48:3'): while executing 'FUNCTION toDateTime64(time : 0, 6 :: 1) -> toDateTime64(time, 6) DateTime64(6) : 2'. (CANNOT_PARSE_TEXT), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. ./build_docker/../src/Common/Exception.cpp:69: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xb29f568 in /usr/bin/clickhouse\r\n1. DB::throwExceptionForIncompletelyParsedValue(DB::ReadBuffer&, DB::IDataType const&) @ 0x6ed06fc in /usr/bin/clickhouse\r\n2. bool DB::callOnIndexAndDataType<DB::DataTypeDateTime64, DB::FunctionConvert<DB::DataTypeDateTime64, DB::NameToDateTime64, DB::ToDateTimeMonotonicity>::executeInternal(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const::'lambda'(auto const&, auto const&)&, DB::ConvertDefaultBehaviorTag>(DB::TypeIndex, auto&&, DB::ConvertDefaultBehaviorTag&&) @ 0x73cec64 in /usr/bin/clickhouse\r\n3. DB::FunctionConvert<DB::DataTypeDateTime64, DB::NameToDateTime64, DB::ToDateTimeMonotonicity>::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x73ca5bc in /usr/bin/clickhouse\r\n4. ./build_docker/../src/Functions/IFunction.cpp:0: DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7b158 in /usr/bin/clickhouse\r\n5. ./build_docker/../contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:115: DB::IExecutableFunction::executeWithoutSparseColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7ba94 in /usr/bin/clickhouse\r\n6. ./build_docker/../contrib/libcxx/include/vector:399: DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xee7cf64 in /usr/bin/clickhouse\r\n7. ./build_docker/../contrib/boost/boost/smart_ptr/intrusive_ptr.hpp:115: DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0xf7d7378 in /usr/bin/clickhouse\r\n8. ./build_docker/../contrib/libcxx/include/vector:505: DB::ExpressionActions::execute(DB::Block&, bool) const @ 0xf7d81d0 in /usr/bin/clickhouse\r\n9. ./build_docker/../contrib/libcxx/include/vector:1416: DB::MergeTreePartition::executePartitionByExpression(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::Block&, std::__1::shared_ptr<DB::Context const>) @ 0x106413f0 in /usr/bin/clickhouse\r\n10. ./build_docker/../contrib/libcxx/include/list:916: DB::MergeTreeDataWriter::splitBlockIntoParts(DB::Block const&, unsigned long, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::Context const>) @ 0x106960e0 in /usr/bin/clickhouse\r\n11. ./build_docker/../contrib/libcxx/include/vector:1408: DB::MergeTreeSink::consume(DB::Chunk) @ 0x107b13e4 in /usr/bin/clickhouse\r\n12. ./build_docker/../contrib/libcxx/include/__memory/shared_ptr.h:702: DB::SinkToStorage::onConsume(DB::Chunk) @ 0x10b84270 in /usr/bin/clickhouse\r\n13. ./build_docker/../contrib/libcxx/include/__memory/shared_ptr.h:702: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::ExceptionKeepingTransform::work()::$_1, void ()> >(std::__1::__function::__policy_storage const*) @ 0x10af2474 in /usr/bin/clickhouse\r\n14. ./build_docker/../src/Processors/Transforms/ExceptionKeepingTransform.cpp:122: DB::runStep(std::__1::function<void ()>, DB::ThreadStatus*, std::__1::atomic<unsigned long>*) @ 0x10af2198 in /usr/bin/clickhouse\r\n15. ./build_docker/../contrib/libcxx/include/__functional/function.h:813: DB::ExceptionKeepingTransform::work() @ 0x10af1abc in /usr/bin/clickhouse\r\n16. ./build_docker/../src/Processors/Executors/ExecutionThreadContext.cpp:52: DB::ExecutionThreadContext::executeTask() @ 0x109471a0 in /usr/bin/clickhouse\r\n17. ./build_docker/../src/Processors/Executors/PipelineExecutor.cpp:228: DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) @ 0x1093c1ac in /usr/bin/clickhouse\r\n18. ./build_docker/../src/Processors/Executors/PipelineExecutor.cpp:127: DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) @ 0x1093b654 in /usr/bin/clickhouse\r\n19. ./build_docker/../src/Server/TCPHandler.cpp:713: DB::TCPHandler::processInsertQuery() @ 0x108eba3c i\r\n```\r\n\r\nI didn't call the toDateTime64 function, the only thing that may have affected is this table:\r\n```\r\n-- simplify\r\nCREATE TABLE test.test_tb(\r\n    `time` String,\r\n    a String,\r\n    b String,\r\n    c String\r\n) \r\nENGINE = ReplacingMergeTree()\r\nPARTITION BY toDate(toDateTime64(time, 6))\r\nORDER BY (a, b, c);\r\n```\r\nBut I have also truncate the table data\r\n\r\nWhy does this error keep happening? Is there a good way to locate it?\r\n\r\nThanks\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/45232/comments",
    "author": "Onehr7",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2023-01-12T18:48:31Z",
        "body": "> didn't call the toDateTime64 function, the only thing that may have affected is this table:\r\n\r\n`PARTITION BY toDate(          toDateTime64(           time, 6))`\r\n\r\n\r\n```\r\nselect toDateTime64('2022-11-30 019:48:33.237', 6);\r\n\r\nDB::Exception: Cannot parse string '2022-11-30 019:48:33.237' as DateTime64(6):\r\n```\r\n\r\n\r\n```sql\r\nselect parseDateTime64BestEffortOrZero('2022-11-30 019:48:33.237', 6);\r\n\u250c\u2500parseDateTime64BestEffortOrZero('2022-11-30 019:48:33.237', 6)\u2500\u2510\r\n\u2502                                     1970-01-01 00:00:00.000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect toDateTime64OrZero('2022-11-30 019:48:33.237', 6);\r\n\u250c\u2500toDateTime64OrZero('2022-11-30 019:48:33.237', 6)\u2500\u2510\r\n\u2502                        1970-01-01 00:00:00.000000 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n---\n\nI suggest to use `parseDateTime64BestEffortOrZero`\r\n\r\n```sql\r\nPARTITION BY toDate(parseDateTime64BestEffortOrZero(time, 6))\r\n```"
      },
      {
        "user": "Onehr7",
        "created_at": "2023-01-13T00:27:45Z",
        "body": "thanks, it works"
      }
    ],
    "satisfaction_conditions": [
      "Identify why invalid datetime strings are being passed to toDateTime64 function",
      "Provide a way to handle malformed datetime strings without causing exceptions",
      "Explain how partition key expressions interact with data ingestion",
      "Ensure solution prevents error accumulation in server logs"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:43:41"
    }
  },
  {
    "number": 42578,
    "title": "Passing generated value to quantiles?",
    "created_at": "2022-10-22T13:48:11Z",
    "closed_at": "2022-10-27T19:59:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42578",
    "body": "Hello clickhouse community,\r\n\r\nI don't really know if it is possible to do this kind of thing, but let me explain:\r\n\r\nI have a query like this:\r\n\r\n```sql\r\nselect quantiles(0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95)(price)\r\nfrom price\r\n```\r\nIs it possible to pass the generated value to quantiles? And if so, what does it look like? I would like to get something like :\r\n\r\n```sql\r\nselect quantiles(/* list of generated quantiles, maybe with select something */)(price)\r\nfrom price\r\n```\r\n\r\nThanks for any kind of response approuving or desapprouving the possibility of doing it.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42578/comments",
    "author": "MockingMagician",
    "comments": [
      {
        "user": "CamiloSierraH",
        "created_at": "2022-10-25T13:19:40Z",
        "body": "Hi @MockingMagician ,\r\n\r\nThe quantiles `level` parameter has some kind of mixed type, is not a string neither an array, so this makes not possible to use a select to get the values as required in your example.\r\nA workaround is use the clickhouse-client and some external command or code that will generate the list, here an example that will work in a bash script:\r\n```\r\nclickhouse-client --query \"select quantiles($(seq -s \",\" 0 .05 .95))(price) FROM price;\"\r\n```"
      },
      {
        "user": "MockingMagician",
        "created_at": "2022-10-27T19:59:47Z",
        "body": "Hi @CamiloSierraH,\r\n\r\nThanks a lot for your feedback... I was almost sure about the fact that you can not pass any kind of sub request as parameter to quantiles functions...\r\n\r\nBut thanks a lot for your idea of generated quantiles values, I've implemented something that looks as your idea, but I hope one day will could do it without leaving the `request scope` :smiling_face_with_tear: \r\n\r\nSee you"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates programmatic generation of quantile levels within SQL",
      "Works within ClickHouse's SQL constraints without external tools",
      "Handles array/list parameters for quantile functions"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:43:48"
    }
  },
  {
    "number": 42557,
    "title": "Join generated data",
    "created_at": "2022-10-21T08:58:34Z",
    "closed_at": "2022-10-21T11:49:36Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42557",
    "body": "Hello clickhouse community,\r\n\r\nI'm stuck on a statement that I can't make.\r\n\r\nI want to get a table with two columns containing generated data, let me explain:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)(price)\r\n        from pricing\r\n        limit 1)\r\n    ) as quant_min\r\n;\r\n```\r\n\r\nIt gives me:\r\n\r\n```txt\r\nquant_min\r\n0.0000056\r\n0.00850023\r\n0.013097947\r\n0.020124\r\n0.032167118\r\n0.0437904\r\n0.051556416\r\n0.0644\r\n0.0896712\r\n0.1346728\r\n```\r\n\r\nNow this one:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)(price)\r\n        from pricing\r\n        limit 1)\r\n    ) as quant_max\r\n;\r\n\r\ngive me:\r\n\r\n```txt\r\nquant_max\r\n0.00850023\r\n0.0131056\r\n0.02019656\r\n0.032187376\r\n0.0437904\r\n0.0515056\r\n0.06435264\r\n0.0894664\r\n0.1344904\r\n34.22802\r\n```\r\nI want a request where the result is:\r\n\r\n```txt\r\nquant_min       |     quant_max\r\n0.0000056       |     0.00850023\r\n0.00850023      |     0.0131056\r\n0.013097947     |     0.02019656\r\n0.020124        |     0.032187376\r\n0.032167118     |     0.0437904\r\n0.0437904       |     0.0515056\r\n0.051556416     |     0.06435264\r\n0.0644          |     0.0894664\r\n0.0896712       |     0.1344904\r\n0.1346728       |     34.22802\r\n```\r\n\r\nI had try this request:\r\n\r\n```sql\r\nselect\r\n    arrayJoin(\r\n        (select\r\n            quantilesExact(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)(price)\r\n        from zone_hit\r\n        where\r\n        event_date > today() - 3\r\n        and rtb_bid_price > 0\r\n        and hit_type_id = 'rtb'\r\n        limit 1)\r\n    ) as quant_min,\r\n    arrayJoin(\r\n        (select quantilesExact(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)(price)\r\n         from zone_hit\r\n         where event_date > today() - 3\r\n           and rtb_bid_price > 0\r\n           and hit_type_id = 'rtb'\r\n         limit 1)\r\n    ) as quant_max\r\n;\r\n```\r\n\r\nBut it give me 100 results because every quant_min is join to every quant_max.\r\n\r\nIf a life saver is in the neighberhood ?\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42557/comments",
    "author": "MockingMagician",
    "comments": [
      {
        "user": "nickitat",
        "created_at": "2022-10-21T10:43:07Z",
        "body": "you can use `arrayZip` to merge arrays:\r\n\r\n``` sql\r\nSELECT\r\n    A.1 AS quant_min,\r\n    A.2 AS quant_max\r\nFROM\r\n(\r\n    SELECT arrayJoin(arrayZip((\r\n            SELECT quantilesExact(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)(number)\r\n            FROM numbers_mt(100)\r\n        ), (\r\n            SELECT quantilesExact(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)(number)\r\n            FROM numbers_mt(100)\r\n        ))) AS A\r\n)\r\n```"
      },
      {
        "user": "MockingMagician",
        "created_at": "2022-10-21T11:49:36Z",
        "body": "Hello @nickitat \r\n\r\nIt works fucking great! That's just what I was looking for.\r\n\r\nHuge thanks to you!\n\n---\n\nI close it!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must pair corresponding elements from the quant_min and quant_max arrays in the same row",
      "Must preserve the order of quantile values from both arrays",
      "Avoid Cartesian product between the two quantile arrays",
      "Maintain array element positions during join operation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:43:56"
    }
  },
  {
    "number": 42375,
    "title": "Is there a way to wait while a replicated table will have consistent state between masters?",
    "created_at": "2022-10-17T06:41:00Z",
    "closed_at": "2023-09-16T00:06:48Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42375",
    "body": "In my project I often rename table or alter it's partitions. When the next time process connects to other master, it might try to change table where partitions wasn't changed yet. And this leads to data loss and mysterious behaviour.\r\n\r\n If there a way to wait while all manipulations with a replicated table are done on all masters before I'll apply further changes?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42375/comments",
    "author": "svetlyak40wt",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-10-17T07:14:15Z",
        "body": "`alter table ..... settings mutations_sync=2,replication_alter_partitions_sync=2`\r\n\r\nmutations_sync - Wait for synchronous execution of ALTER TABLE UPDATE/DELETE queries (mutations). 0 - execute asynchronously. 1 - wait current server. 2 - wait all replicas if they exist.\r\n\r\n\r\nreplication_alter_partitions_sync - Wait for actions to manipulate the partitions. 0 - do not wait, 1 - wait for execution only of itself, 2 - wait for everyone."
      },
      {
        "user": "svetlyak40wt",
        "created_at": "2022-10-17T09:17:35Z",
        "body": "Cool! Thank you, Den!\r\n\r\nIs there something like this for RENAME TABLE and EXCHANGE TABLES?"
      },
      {
        "user": "azat",
        "created_at": "2023-09-15T16:22:10Z",
        "body": "For `RENAME`/`EXCHANGE` you can use `ON CLUSTER`"
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to ensure schema changes propagate to all replicas before proceeding",
      "Solution works for multiple DDL operation types (ALTER, RENAME, EXCHANGE)",
      "Cluster-wide synchronization guarantee",
      "Explicit confirmation of operation completion"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:44:03"
    }
  },
  {
    "number": 42318,
    "title": "Example of how to use `remove` override in yaml format?",
    "created_at": "2022-10-14T14:58:20Z",
    "closed_at": "2022-10-15T08:02:59Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/42318",
    "body": "I'd like to leave the main config file untouched and simply remove the entries for `postgresql_port` and `mysql_port` in a separate config file. I'm using yaml rather than XML. I'm not seeing an example of this in the docs. Thanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/42318/comments",
    "author": "tasdflkjweio",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-10-14T16:35:15Z",
        "body": "```\r\n# cat /etc/clickhouse-server/users.d/xxxxx.yaml\r\nusers:\r\n  default:\r\n    \"@remove\": remove\r\n\r\n# cat /etc/clickhouse-server/config.d/postgresql_port.yaml\r\npostgresql_port:\r\n    \"@remove\": remove\r\nmysql_port:\r\n    \"@remove\": remove\r\n```"
      },
      {
        "user": "tasdflkjweio",
        "created_at": "2022-10-14T17:23:05Z",
        "body": "many thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates YAML syntax for removing specific configuration entries",
      "Uses override mechanism without modifying main configuration file"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:44:09"
    }
  },
  {
    "number": 41758,
    "title": "ClickHouse version 21.12.4.1 use BACKUP table async",
    "created_at": "2022-09-26T03:17:53Z",
    "closed_at": "2022-09-26T14:25:27Z",
    "labels": [
      "question",
      "obsolete-version",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41758",
    "body": "ClickHouse version 21.12.4.1 \r\n\r\nBACKUP database default  TO Disk('backups', 'backup-20220926/') ASYNC\r\n\r\nCode: 62. DB::Exception: Syntax error: failed at position 115 ('ASYNC'): ASYNC. Expected one of: FILTER, OVER, SETTINGS, end of query. (SYNTAX_ERROR) (version 21.12.4.1 (official build))\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41758/comments",
    "author": "hbzhu",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-26T14:25:26Z",
        "body": "`BACKUP database` command is available in the later releases. You need to upgrade to 22.9."
      },
      {
        "user": "hbzhu",
        "created_at": "2022-09-27T02:19:03Z",
        "body": "Thanks\uff0casynchronous backup tables need to upgrade to 22.9 to\uff1f"
      },
      {
        "user": "den-crane",
        "created_at": "2022-09-27T02:56:42Z",
        "body": "> Thanks\uff0casynchronous backup tables need to upgrade to 22.9 to\uff1f\r\n\r\nYes."
      },
      {
        "user": "hbzhu",
        "created_at": "2022-09-27T06:54:58Z",
        "body": "Thank you very much,I upgrated to 22.9.2.7,BACKUP commond  very nice!!!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies version compatibility requirements for the BACKUP command",
      "Provides a path to enable asynchronous backups functionality"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:44:21"
    }
  },
  {
    "number": 41101,
    "title": "The size of processed data",
    "created_at": "2022-09-08T10:14:14Z",
    "closed_at": "2022-09-08T14:27:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41101",
    "body": "```sql\r\nlocalhouse :) SELECT * FROM hits WHERE URL LIKE '%google%' ORDER BY EventTime LIMIT 10;\r\n...\r\n10 rows in set. Elapsed: 6.029 sec. Processed 100.00 million rows, 25.45 GB (16.58 million rows/s., 4.22 GB/s.) \r\n\r\nlocalhost :) SELECT * FROM hits ORDER BY EventTime LIMIT 10 Format Null\r\n...\r\n0 rows in set. Elapsed: 9.844 sec. Processed 100.00 million rows, 82.77 GB (10.16 million rows/s., 8.41 GB/s.)\r\n```\r\n\r\nThe size of processed data of the first SQL is 25.45 GB which is much less than the second SQL. That looks so cool. Can the primary index used in this case? Or any other magic technology here like deferred materialize? Any docs about this?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41101/comments",
    "author": "Lloyd-Pottiger",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-09-08T12:08:16Z",
        "body": "There is `PREWHERE` and `optimize_move_to_prewhere`.\r\n\r\nBecause of optimize_move_to_prewhere =1, CH moves `URL LIKE '%google%'` to PREWHERE.\r\n\r\nBecause of `PREWHERE URL LIKE '%google%'` CH reads first only URL column, filters out granules, and after that reads only that granules to solve remaining parts of a query `SELECT *`."
      },
      {
        "user": "Lloyd-Pottiger",
        "created_at": "2022-09-08T14:27:43Z",
        "body": "> There is `PREWHERE` and `optimize_move_to_prewhere`.\r\n> \r\n> Because of optimize_move_to_prewhere =1, CH moves `URL LIKE '%google%'` to PREWHERE.\r\n> \r\n> Because of `PREWHERE URL LIKE '%google%'` CH reads first only URL column, filters out granules, and after that reads only that granules to solve remaining parts of a query `SELECT *`.\r\n\r\nI get it. Thanks for your clear reply and your excellent work, it is really a wonderful idea."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of optimization techniques that reduce processed data volume",
      "Clarification of column filtering/selection strategies",
      "Explanation of query execution phases",
      "Description of automatic query optimizations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:44:41"
    }
  },
  {
    "number": 41073,
    "title": "CREATE   EMPTY AS SELECT  ERROR on 22.8",
    "created_at": "2022-09-07T08:33:26Z",
    "closed_at": "2022-09-07T09:55:06Z",
    "labels": [
      "question",
      "need-docs"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/41073",
    "body": "hello !\r\nwhen i run \r\n create table xx1 empty as select * from xx;\r\n\r\nCREATE TABLE xx1 EMPTY AS\r\nSELECT *\r\nFROM xx\r\n\r\nQuery id: fd018c3a-79db-4fbd-8e34-c893307ce9b6\r\n\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\nReceived exception from server (version 22.8.4):\r\nCode: 119. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Table engine is not specified in CREATE query. (ENGINE_REQUIRED)\r\n\r\nI tried  the table engine of  MergeTree() and Replicated_MergeTree().\r\nall of these engines  i got this error!\r\n\r\nwhat can i do?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/41073/comments",
    "author": "yangshike",
    "comments": [
      {
        "user": "yangshike",
        "created_at": "2022-09-07T08:42:02Z",
        "body": "/var/log/clickhouse/clickhouse-server.log\uff1a\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa38beba in /usr/bin/clickhouse\r\n1. DB::InterpreterCreateQuery::setDefaultTableEngine(DB::ASTStorage&, std::__1::shared_ptr<DB::Context const>) @ 0x14c3dc2b in /usr/bin/clickhouse\r\n2. DB::InterpreterCreateQuery::setEngine(DB::ASTCreateQuery&) const @ 0x14c3bcce in /usr/bin/clickhouse\r\n3. DB::InterpreterCreateQuery::getTablePropertiesAndNormalizeCreateQuery(DB::ASTCreateQuery&) const @ 0x14c38c67 in /usr/bin/clickhouse\r\n4. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x14c401a9 in /usr/bin/clickhouse\r\n5. DB::InterpreterCreateQuery::execute() @ 0x14c4a38d in /usr/bin/clickhouse\r\n6. ? @ 0x14fecab4 in /usr/bin/clickhouse\r\n7. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x14fe9f0e in /usr/bin/clickhouse\r\n8. DB::TCPHandler::runImpl() @ 0x15cb97ad in /usr/bin/clickhouse\r\n9. DB::TCPHandler::run() @ 0x15ccdd59 in /usr/bin/clickhouse\r\n10. Poco::Net::TCPServerConnection::start() @ 0x18a617b3 in /usr/bin/clickhouse\r\n11. Poco::Net::TCPServerDispatcher::run() @ 0x18a62c2d in /usr/bin/clickhouse\r\n12. Poco::PooledThread::run() @ 0x18c2d9c9 in /usr/bin/clickhouse\r\n13. Poco::ThreadImpl::runnableEntry(void*) @ 0x18c2b242 in /usr/bin/clickhouse\r\n14. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n15. __clone @ 0xfe9fd in /usr/lib64/libc-2.17.so\r\n"
      },
      {
        "user": "melvynator",
        "created_at": "2022-09-07T08:55:02Z",
        "body": "Hello\r\n\r\nIn your example you don't define the engine and you don't specify the primary key. The examples below should be working:\r\n\r\n```\r\nCREATE TABLE xx1 \r\nENGINE=MergeTree() \r\nORDER BY number \r\nEMPTY \r\nAS\r\n\tSELECT *\r\n\tFROM numbers(100)\r\n```\r\n\t\r\n\r\n`CREATE TABLE xx2 Engine=MergeTree() ORDER by number EMPTY AS SELECT * FROM xx1\r\n`"
      },
      {
        "user": "yangshike",
        "created_at": "2022-09-07T09:23:03Z",
        "body": "thanks, This is indeed the problem\uff01\r\n\r\n\r\nHowever, I think that when creating a new table based on the original table, most users want the structure, sorting, primary key and partition of the original table to be the same.\r\n\r\nCan this be the case: if the engine and other parameters are not specified, it is like this: create table xx1 empty as select * from XX; By default, all attributes are the same as the original table. If you need to modify parameters such as the engine, the creation syntax can be as follows:\r\n\r\ncreate table xx1 engine=xx empty as select * from xx;\r\n\r\ncreate table xx1 primary key xxx empty as select * from xx;\r\n\r\ncreate table xx1 order by xxx empty as select * from xx;\r\n\r\nFor unspecified parameters, the value of the original table is taken\n\n---\n\nLike mysql, create table xx1 like XX; No engine is specified. InnoDB is created by default"
      },
      {
        "user": "melvynator",
        "created_at": "2022-09-07T09:26:05Z",
        "body": "It's a feature we added recently, there is room for improvement. I will make sure these feedbacks are added as feature request. "
      },
      {
        "user": "yangshike",
        "created_at": "2022-09-07T09:27:40Z",
        "body": "Especially after upgrading the database engine atomic, replicatedmergetree adopts the default_ replica_ path and default_ replica_ After the name parameter is set, there should be no problem with this creation syntax.\r\n\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2022-09-07T09:55:06Z",
        "body": "> However, I think that when creating a new table based on the original table, most users want the structure, sorting, primary key and partition of the original table to be the same.\r\n\r\nBut in your example a new table is created based on the SELECT query, not on other table. SELECT query has result structure, but does not have engine. And we cannot simply take the storage definition from the table specified in FROM section, because SELECT query may contain UNION or JOIN or subquery and therefore may read from multiple different tables with different storage definitions. That's why you have to specify storage definition explicitly when using `CREATE AS SELECT`. But you can use `CREATE TABLE AS <table>`:\r\n```\r\nCREATE TABLE xx1 AS xx;\r\n```\r\nIt will take storage definition from the original table. \r\nSee also `default_table_engine` setting (but it will not completely solve your problem, because *MergeTree requires ORDER BY key which cannot be chosen automatically)."
      },
      {
        "user": "yangshike",
        "created_at": "2022-09-07T10:26:14Z",
        "body": "oh thank you\uff0cCREATE TABLE AS <table>: Solved my problem\u3002 But I have to enter on cluster XXX every time\r\n\r\nWhen creating a database, atomic and replicated can be specified at the same time\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Solution must allow creating a new table with the same structure, engine, and table properties (sorting key, primary key, partitioning) as the source table without manual specification",
      "Method must handle cluster configurations automatically when replicating tables",
      "Approach should work for both Atomic databases and ReplicatedMergeTree engine configurations",
      "Solution must clarify the distinction between creating tables from SELECT queries vs direct table copies"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:44:51"
    }
  },
  {
    "number": 40427,
    "title": "Don't know how to debug MySQLHandler: DB::Exception: Cannot read all data. ",
    "created_at": "2022-08-19T22:42:09Z",
    "closed_at": "2022-08-20T17:38:32Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40427",
    "body": "Hi,\r\n\r\nI have ClickHouse 22.1.3.7 installed and it is working fine and my inserts are going as expected. However in log I have multiple lines like this:\r\n\r\n`<Error> MySQLHandler: DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 3.`\r\n\r\nI have tried to set logging level to test, but I am unable to get any useful information to understand what is going on.\r\n\r\nCan you possibly hint me to the right direction?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40427/comments",
    "author": "edo888",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-08-20T17:38:32Z",
        "body": "@edo888 Someone is connecting to the MySQL endpoint and does not send any data.\r\nIt might be a monitoring script.\r\n\r\nThis error does not necessarily require attention."
      },
      {
        "user": "edo888",
        "created_at": "2022-08-20T18:23:59Z",
        "body": "Yes, that was exactly that. I have connection reuse mechanism in place and connections can stay open for up to 30 seconds.\r\n\r\nIs it better to do that or close and reopen connection every time a query needs to be pushed?\r\n\r\nThanks!"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-08-21T17:16:29Z",
        "body": "For clickhouse-server it does not make any difference. For clients, reusing connections can be better (for latency)."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the MySQLHandler error occurs despite normal system operation",
      "Clarification on whether idle connections are problematic for ClickHouse",
      "Guidance on connection management best practices for ClickHouse clients",
      "Confirmation about whether the error indicates a critical issue"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:44:59"
    }
  },
  {
    "number": 40384,
    "title": "Configured background pool size does not match system.settings",
    "created_at": "2022-08-19T07:31:43Z",
    "closed_at": "2022-08-19T08:01:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40384",
    "body": "**Describe what's wrong**\r\n\r\nValue of background_fetches_pool_size configured in config.xml (as per #36425):\r\n\r\n```xml\r\n<background_fetches_pool_size>64</background_fetches_pool_size>\r\n```\r\n\r\ndoes not show system.settings which has the default value instead:\r\n\r\n```sql\r\nSELECT name, value FROM system.settings WHERE name LIKE 'background_fetches_pool_size'\r\n\u250c\u2500value\u2500\u2510\r\n\u2502 8     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nwhile the correct value shows in the log:\r\n\r\n```\r\nInitialized background executor for fetches with num_threads=64, num_tasks=64\r\n```\r\n\r\nand BackgroundFetchesPoolTask sometimes exceeds the default so it looks like it's actually using the configured value\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes (suppose 22.8 LTS will be added to version_date.tsv)\r\n\r\n**How to reproduce**\r\n\r\nClickHouse server version 22.8.1.2097",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40384/comments",
    "author": "larry-cdn77",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-08-19T08:01:30Z",
        "body": "This setting is obsolete:\r\n```\r\nClickHouse client version 22.8.1.1.\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 22.8.1 revision 54460.\r\n\r\ndell9510 :) select * from system.settings where name='background_fetches_pool_size'\r\n\r\nSELECT *\r\nFROM system.settings\r\nWHERE name = 'background_fetches_pool_size'\r\n\r\nQuery id: c4256263-ee40-4cf4-ad2d-9352fea6b5e7\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500value\u2500\u252c\u2500changed\u2500\u252c\u2500description\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500min\u2500\u2500\u252c\u2500max\u2500\u2500\u252c\u2500readonly\u2500\u252c\u2500type\u2500\u2500\u2500\u2510\r\n\u2502 background_fetches_pool_size \u2502 8     \u2502       0 \u2502 Obsolete setting, does nothing. \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502        0 \u2502 UInt64 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 row in set. Elapsed: 0.036 sec. \r\n```\r\n\r\n It was replaced with configuration parameter with the same name."
      },
      {
        "user": "larry-cdn77",
        "created_at": "2022-08-22T09:18:05Z",
        "body": "Thank you, indeed the confusion I had was in thinking that this configuration parameter (config.xml) can be viewed via system.settings"
      }
    ],
    "satisfaction_conditions": [
      "Clarify the relationship between config.xml parameters and system.settings entries",
      "Explain how obsolete settings are handled in newer versions",
      "Confirm that configuration parameters can take effect without appearing in system.settings",
      "Differentiate between runtime settings and startup configuration parameters"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:45:26"
    }
  },
  {
    "number": 40327,
    "title": "Extract specific symbols in the string",
    "created_at": "2022-08-18T08:06:20Z",
    "closed_at": "2022-08-22T18:53:25Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/40327",
    "body": "Hi!\r\n\r\nI would be grateful if you help me to find specific symbols in the string. \r\nI have a table **'comment'** with the column **'text'**\r\nIn that column i have some text, for example : \r\n\"_Address: Chicago, 15 and Number of people: **2**_\" \r\n\r\nHow can I extract \"**2**\" out of this text? I mean I need only \"**2**\" in this string\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/40327/comments",
    "author": "sabinajvdva",
    "comments": [
      {
        "user": "DerekChia",
        "created_at": "2022-08-18T14:52:42Z",
        "body": "Hello! Does your text always conform to the structure of `Address: XX, YY and Number of people: ZZ`? And do you only want to extract the last integer?"
      },
      {
        "user": "den-crane",
        "created_at": "2022-08-18T22:05:23Z",
        "body": "extractAll ?\r\n\r\n```sql\r\nSELECT\r\n    'Address: Chicago, 15 and Number of people: 2' AS text,\r\n    extractAll(text, '2') AS r\r\n\r\nQuery id: f728b623-2743-4056-87b3-efe7e041cd41\r\n\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 2 \u2502 ['2'] \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n---\n\nsubstring ?\r\n\r\n```sql\r\nselect 'Address: Chicago, 15 and Number of people: 2' text, substring(text, -1) r;\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 2 \u2502 2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\r\n```\n\n---\n\n```sql\r\nSELECT\r\n    'Address: Chicago, 15 and Number of people: 2' AS text,\r\n    extract(text, '(\\\\d)$') AS r\r\n\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 2 \u2502 2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\r\n```\r\n\r\n```sql\r\nSELECT\r\n    'Address: Chicago, 15 and Number of people: 22' AS text,\r\n    extractAll(text, '(\\\\d+)') AS r\r\n\r\n\u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Address: Chicago, 15 and Number of people: 22 \u2502 ['15','22'] \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "sabinajvdva",
        "created_at": "2022-08-19T05:40:19Z",
        "body": "> Hello! Does your text always conform to the structure of `Address: XX, YY and Number of people: ZZ`? And do you only want to extract the last integer?\r\n\r\nNo text is always different, but it has 'Number of people: 2', so I need only the integer (2) out of this text. \r\n\r\n> ```sql\r\n> ```sql\r\n> SELECT\r\n>     'Address: Chicago, 15 and Number of people: 2' AS text,\r\n>     extract(text, '(\\\\d)$') AS r\r\n> \r\n> \u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2510\r\n> \u2502 Address: Chicago, 15 and Number of people: 2 \u2502 2 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\r\n> ```\r\n> \r\n> \r\n>     \r\n>       \r\n>     \r\n> \r\n>       \r\n>     \r\n> \r\n>     \r\n>   \r\n> ```sql\r\n> SELECT\r\n>     'Address: Chicago, 15 and Number of people: 22' AS text,\r\n>     extractAll(text, '(\\\\d+)') AS r\r\n> \r\n> \u250c\u2500text\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500r\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n> \u2502 Address: Chicago, 15 and Number of people: 22 \u2502 ['15','22'] \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> ```\r\n> ```\r\n\r\nThank you! I will try that. \r\n\r\nAnd I also found this way and it worked: \r\n```\r\ntoInt32OrZero(cast(\r\n\t\t\t\tif \r\n\t\t\t\t(\r\n\t\t\t\ttrim(\r\n\t\t\t\t\treplace(\r\n\t\t\t\t\t\tmid\r\n\t\t\t\t\t\t(text,\r\n\t\t\t\t\t\tPOSITION (text,'Number of people:')+LENGTH('Number of people:'),\r\n\t\t\t\t\t\tPOSITION(mid(bc.text,POSITION(text,'Number of people:')+LENGTH('Number of people:')),'.')\r\n\t\t\t\t\t\t)\r\n\t\t\t\t\t,'.',' ')\r\n\t\t\t\t)='',\r\n\t\t\t\ttrim(mid(text,POSITION(text,'Number of people:')+LENGTH('Number of people:'))),\r\n\t\t\t\ttrim(\r\n\t\t\t\t\treplace(\r\n\t\t\t\t\t\tmid\r\n\t\t\t\t\t\t(text,\r\n\t\t\t\t\t\tPOSITION(text,'Number of people:')+LENGTH('Number of people:'),\r\n\t\t\t\t\t\tPOSITION(mid(text,POSITION(text,'Number of people:')+LENGTH('Number of people:')),'.')\r\n\t\t\t\t\t\t)\r\n\t\t\t\t\t,'.',' ')\r\n\t\t\t\t)\r\n\t\t\t\t) as text ) \r\n```\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Extracts the integer value specifically following the 'Number of people:' pattern in the text",
      "Handles variable text structures where other numbers may be present",
      "Returns only the numeric value without additional characters or text",
      "Works when the target number appears at different positions in the string"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:45:36"
    }
  },
  {
    "number": 39407,
    "title": "Clickhouse analogue function",
    "created_at": "2022-07-20T08:28:46Z",
    "closed_at": "2022-07-20T13:40:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/39407",
    "body": "What will be the analogue of function DAYOFWEEK( date_format(b.createdon_date, '%Y-01-06'))? \r\n\r\nAnd also I need help to know how to start the week not from monday or sunday but from Thursday on clickhouse? \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/39407/comments",
    "author": "bilelik",
    "comments": [
      {
        "user": "evillique",
        "created_at": "2022-07-20T13:02:13Z",
        "body": "1. Considering Monday is 1, and Sunday is 7:\r\n`DAYOFWEEK(toDateTime(formatDateTime(date, '%Y-01-06')))`\r\n\r\n2. If we only need to change the day of the week:\r\n`(DAYOFWEEK(date) + 3) % 7 + 1`\r\n```\r\nWITH today() + number AS date\r\nSELECT\r\n    date,\r\n    DAYOFWEEK(date) AS old,\r\n    ((DAYOFWEEK(date) + 3) % 7) + 1 AS new\r\nFROM numbers(7)\r\n\r\nQuery id: 840ca0cb-9176-4ea0-a4f6-e319a9c58a8b\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500old\u2500\u252c\u2500new\u2500\u2510\r\n\u2502 2022-07-20 \u2502   3 \u2502   7 \u2502\r\n\u2502 2022-07-21 \u2502   4 \u2502   1 \u2502\r\n\u2502 2022-07-22 \u2502   5 \u2502   2 \u2502\r\n\u2502 2022-07-23 \u2502   6 \u2502   3 \u2502\r\n\u2502 2022-07-24 \u2502   7 \u2502   4 \u2502\r\n\u2502 2022-07-25 \u2502   1 \u2502   5 \u2502\r\n\u2502 2022-07-26 \u2502   2 \u2502   6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Equivalent ClickHouse function that handles date formatting and day-of-week calculation in one operation",
      "Custom week start adjustment mechanism that shifts ordinal day numbers to make Thursday the first day",
      "Clear mapping between original day numbers and adjusted Thursday-based numbering",
      "Handling of date format patterns similar to MySQL's '%Y-01-06' specification"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:46:03"
    }
  },
  {
    "number": 38358,
    "title": "When final query processing is done at Initiator node?",
    "created_at": "2022-06-23T19:07:23Z",
    "closed_at": "2022-06-23T20:56:23Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/38358",
    "body": " In terms of memory usage , \r\nIs there any difference when final query processing is done on the initiator node and when it is done on the shard and initiator only proxy the data.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/38358/comments",
    "author": "HeenaBansal2009",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-06-23T20:43:32Z",
        "body": "Distributed table is unable to perform FINAL agains shards query results. It's impossible."
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-23T20:46:35Z",
        "body": "@den-crane , I am not sure if I understand your answer . \r\nMy question is around the below comments in doc:\r\n\r\n0 \u2014 Disabled (final query processing is done on the initiator node).\r\n1 - Do not merge aggregation states from different servers for distributed query processing (query completelly processed on the shard, initiator only proxy the data), can be used in case it is for certain that there are different keys on different shards.\r\n\r\nKeeping this value to default , does it make any difference in terms of memory usage ?\r\n\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-23T20:50:55Z",
        "body": "I see. I thought you were asking about FROM ... FINAL."
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-23T20:53:55Z",
        "body": "> I see. I thought you were asking about FROM ... FINAL.\r\n\r\nNo , My bad I was not clear on the very first stage.  My question is around distributed_group_by_no_merge. \r\nDoes toggling its value make difference w.r.t memory usage for query processing ?\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-23T20:55:46Z",
        "body": "`1 - Do not merge aggregation` is faster and uses less memory. In this case initator just proxies the results from shards to the client (there is more settings to do sort at the initiator without aggregation)\r\n\r\n```\r\n  --distributed_group_by_no_merge arg                                               \r\n    If 1, Do not merge aggregation states from different servers for distributed queries (shards will process query up to the Complete stage, initiator just proxies the data\r\n                                                                                     from the shards).\r\n    If 2 the initiator will apply ORDER BY and LIMIT stages (it is not in case when shard process query up to the Complete stage)\r\n```\r\n\r\nFinal Aggregation is quite expensive it better to avoid it if possible.\r\n\r\nThere is a new feature\r\n\r\n```\r\n --optimize_distributed_group_by_sharding_key arg                                  \r\n Optimize GROUP BY sharding_key queries (by avoiding costly aggregation on the initiator server).\r\n```\r\nIt disables Final Aggregation automatically if groupby is suitable for sharding key."
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-23T21:00:17Z",
        "body": "So In my environment , When distributed_group_by_no_merge=0, I am getting OOM exception \r\nand when I a m setting to distributed_group_by_no_merge=1 ,  my query is processed fully without error.\r\n\r\nDo you think any possible reasons behind ?"
      },
      {
        "user": "den-crane",
        "created_at": "2022-06-23T21:02:16Z",
        "body": ">Do you think any possible reasons behind ?\r\n\r\nYes, distributed_group_by_no_merge=0 requires more resources.\r\n\r\nI already answered.\r\n\r\n> 1 - Do not merge aggregation is faster and **uses less memory.** "
      },
      {
        "user": "HeenaBansal2009",
        "created_at": "2022-06-24T13:47:16Z",
        "body": "> > Do you think any possible reasons behind ?\r\n> \r\n> Yes, distributed_group_by_no_merge=0 requires more resources.\r\n> @den-crane , I agree that executor nodes only processed subquery of initial query and merging of partial aggregation happens at initiator node. Hence this scenario consumes more memory.\r\nHowever , executing the same query on same set of data on standalone node works fine. \r\n\r\n> I already answered.\r\n> \r\n> > 1 - Do not merge aggregation is faster and **uses less memory.**\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of memory usage differences between distributed_group_by_no_merge=0 and =1 modes",
      "Clarification of initiator node's role in aggregation merging",
      "Comparison between distributed and standalone node memory requirements",
      "Identification of factors contributing to OOM in distributed processing"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:46:08"
    }
  },
  {
    "number": 38193,
    "title": "Error after changing clickhouse data dirs",
    "created_at": "2022-06-18T11:56:03Z",
    "closed_at": "2024-02-27T10:42:09Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/38193",
    "body": "Hi,\r\n\r\nClickhouse by default storing metadata/data into /var/lib/clickhouse. '/var/lib' is typically used for s/w install. I moved contents in /var/lib/clickhouse/* to ~/clickhouse after stopping server\r\n\r\nAlso ownership of ~/clickhouse was given to clickhouse:clickhouse recursively and also updated paths in config.xml \r\n\r\nMove is done as below \r\n\r\ncp -al /var/lib/clickhouse/* ~/clickhouse/\r\nrm -rf /var/lib/clickhouse\r\n\r\n\r\nServer is up after above activity and also client is connected and also executing sql successfully but there are lot of errors in server log. \r\n\r\nPlease suggest\r\n\r\nThank you\r\n\r\n\r\n```\r\n2022.06.18 19:38:45.358360 [ 19006 ] {} <Error> void DB::BackgroundJobsAssignee::threadFunc(): Code: 214. DB::ErrnoException: Could not calculate available disk space (statvfs), errno: 2, strerror: No such file or directory. (CANNOT_STATVFS), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xb8a147a in /usr/bin/clickhouse\r\n1. DB::throwFromErrnoWithPath(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int) @ 0xb8a252a in /usr/bin/clickhouse\r\n2. DB::getStatVFS(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xb8e4e5a in /usr/bin/clickhouse\r\n3. DB::DiskLocal::getAvailableSpace() const @ 0x15970be1 in /usr/bin/clickhouse\r\n4. DB::DiskLocal::getUnreservedSpace() const @ 0x15970d7a in /usr/bin/clickhouse\r\n5. DB::StoragePolicy::getMaxUnreservedFreeSpace() const @ 0x15ca8819 in /usr/bin/clickhouse\r\n6. DB::MergeTreeDataMergerMutator::getMaxSourcePartsSizeForMerge(unsigned long, unsigned long) const @ 0x16bf9cfa in /usr/bin/clickhouse\r\n7. DB::StorageMergeTree::selectPartsToMerge(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, std::__1::unique_lock<std::__1::mutex>&, std::__1::shared_ptr<DB::MergeTreeTransaction> const&, bool, DB::SelectPartsDecision*) @ 0x169808cd in /usr/bin/clickhouse\r\n8. DB::StorageMergeTree::scheduleDataProcessingJob(DB::BackgroundJobsAssignee&) @ 0x16984c3e in /usr/bin/clickhouse\r\n9. DB::BackgroundJobsAssignee::threadFunc() @ 0x16acab47 in /usr/bin/clickhouse\r\n10. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0x1556c2f8 in /usr/bin/clickhouse\r\n11. DB::BackgroundSchedulePool::threadFunction() @ 0x1556f5b6 in /usr/bin/clickhouse\r\n12. ? @ 0x1557042e in /usr/bin/clickhouse\r\n13. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xb94d0b7 in /usr/bin/clickhouse\r\n14. ? @ 0xb9504dd in /usr/bin/clickhouse\r\n15. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n16. /build/glibc-CVJwZb/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: clone @ 0x12161f in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n (version 22.6.1.1985 (official build))\r\n2022.06.18 19:38:45.791476 [ 19010 ] {} <Error> void DB::BackgroundJobsAssignee::threadFunc(): Code: 214. DB::ErrnoException: Could not calculate available disk space (statvfs), errno: 2, strerror: No such file or directory. (CANNOT_STATVFS), Stack trace (when copying this message, always include the lines below):\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/38193/comments",
    "author": "sigirisetti",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-06-20T17:01:19Z",
        "body": "`~/clickhouse/` is relative path. \r\n`~` == home folder. Home folder is different for different users.\r\nAvoid relative paths. Use absolute path."
      },
      {
        "user": "sigirisetti",
        "created_at": "2022-07-04T11:47:34Z",
        "body": "Ok. Changing to data drive worked fine. Thanks"
      },
      {
        "user": "danieladoghe",
        "created_at": "2022-07-08T13:08:07Z",
        "body": "Solution:\r\n\tUse absolute paths instead of relative paths"
      }
    ],
    "satisfaction_conditions": [
      "Use absolute paths in configuration rather than user-relative paths",
      "Ensure ClickHouse service can properly resolve and access the full directory path",
      "Resolve disk space monitoring failures caused by invalid storage paths"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:46:18"
    }
  },
  {
    "number": 37692,
    "title": "Merge Map by GROUP BY",
    "created_at": "2022-05-31T12:40:58Z",
    "closed_at": "2022-06-16T01:22:29Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37692",
    "body": "There is a table with map data\r\n```sql\r\nCREATE TABLE test.table1\r\n(\r\n    `id` String,\r\n    `test_map` Map(String, String)\r\n)\r\nENGINE = MergeTree\r\nORDER BY id;\r\n\r\nINSERT INTO test.table1 VALUES \r\n(1, {'a': '1','b': 'b', 'c': '2'})\r\n(2, {'d': 'd', 'a': '2'})\r\n(1, {'d': 'd', 'a': '2'});\r\n```\r\nCan I get the following data by `GROUP BY` id?\r\n```\r\nid      test_map\r\n1\t{'a':'2','b':'b','c':'2','d':'d'}\r\n2\t{'d':'d','a':'2'}\r\n```\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37692/comments",
    "author": "Onehr7",
    "comments": [
      {
        "user": "UnamedRus",
        "created_at": "2022-05-31T12:53:03Z",
        "body": "```\r\n\r\nSELECT\r\n    id,\r\n    maxMap(test_map)\r\nFROM test.table1\r\nGROUP BY id\r\n\r\nQuery id: 931d181a-0807-4684-ac67-f07d17a65831\r\n\r\n\u250c\u2500id\u2500\u252c\u2500maxMap(test_map)\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 2  \u2502 {'a':'2','d':'d'}                 \u2502\r\n\u2502 1  \u2502 {'a':'2','b':'b','c':'2','d':'d'} \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "Onehr7",
        "created_at": "2022-05-31T13:04:04Z",
        "body": "@UnamedRus Thanks for your help. but the maxMap function needs two parameters(key and value).Furthermore, maxMap seems work only for integers. is there any other aggregation map function."
      },
      {
        "user": "UnamedRus",
        "created_at": "2022-05-31T13:58:41Z",
        "body": "> but the maxMap function needs two parameters(key and value).\r\n\r\nNo, it also work for map data type\r\n\r\n> Furthermore, maxMap seems work only for integers. \r\n\r\nNo, in my example it works for Strings as well\r\n\r\nWhich version you are using?\r\n\r\n```\r\nSELECT version();\r\n```\r\n\r\n"
      },
      {
        "user": "Onehr7",
        "created_at": "2022-06-01T02:15:53Z",
        "body": "It's  21.11.6.7 \n\n---\n\n@UnamedRus It works after I upgrade my clickhouse to 22.2.2.1. Thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Merges map entries across multiple rows when grouping by ID",
      "Handles duplicate keys by selecting the latest/maximum value",
      "Works with Map(String, String) data type values",
      "Uses aggregation functions compatible with the user's ClickHouse version",
      "Provides explanation about map aggregation function behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:46:29"
    }
  },
  {
    "number": 37180,
    "title": "how often does clickhouse do sum in summingmergetree?",
    "created_at": "2022-05-13T08:47:46Z",
    "closed_at": "2022-05-13T12:41:01Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/37180",
    "body": "From document\r\n\r\n> ClickHouse merges the inserted parts of data periodically...\r\n\r\n**My questions is how often do it merge? is therey any way to control it?**\r\n\r\nI'm considering using summingmergetree for my analysis flow, about 10 million events a day, each event is ~1kb, summingmergetree seems a perfect solution for saving disk space and speed performance, the only concern is when data is not mergeed, I have do `optimize table`, which is costly, especially in large table.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/37180/comments",
    "author": "jasonbigl",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-05-13T12:36:24Z",
        "body": "When it necessary. Sum is a byproduct of merges. Usually CH merges after 6 inserts. In average I see 22 parts in a partition.\r\n\r\nMerges are eventual and may never happen. It depends on the number of inserts that happened after, the number of parts in the partition, size of parts. If the total size of input parts are greater than the maximum part size then they will never be merged.\r\n\r\nYou should not use `optimize table`. \r\nAll queries to summingmergetree should finalize aggregation using `sum / groupby`"
      },
      {
        "user": "jasonbigl",
        "created_at": "2022-05-13T12:41:01Z",
        "body": "@den-crane clear, informational answers!! thank you very much"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of merge trigger conditions",
      "Guidance on avoiding manual optimization",
      "Clarification of eventual consistency",
      "Architectural best practices for SummingMergeTree"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:46:34"
    }
  },
  {
    "number": 36927,
    "title": "Clickhouse use multiple columns in group by clause",
    "created_at": "2022-05-05T06:14:56Z",
    "closed_at": "2022-05-05T16:09:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36927",
    "body": "\r\nHi All, I am using the below query to generate materialized view for clickhouse but i want the result to be seperate by both the names ( name_top_apps, name_remote_top_emdpoints) can anyone help on this one\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS analytics.uflow_topapps_bytes_flowdir_or_013_mv\r\n  ENGINE = SummingMergeTree\r\n  PARTITION BY toYYYYMMDD ( Timestamp )\r\n  ORDER BY (Timestamp)\r\n  POPULATE\r\n  AS SELECT\r\n  toString(AppId) as name_top_apps,\r\n  IPv6NumToString(DstIP) as name_remote_top_emdpoints,\r\n  sum(FlowStatsBytesFwd) as upload_bytes,\r\n  sum(FlowStatsBytesRev) as download_bytes, \r\n  sum(FlowStatsBytesFwd + FlowStatsBytesRev) as cumulative_bytes,\r\n  sum(FlowStatsPktsFwd) as upload_flows,\r\n  sum(FlowStatsPktsFwd) as download_flows,\r\n  sum(FlowStatsPktsFwd + FlowStatsPktsRev) as cumulative_flows,\r\n  toInt64((sum(FlowStatsBytesFwd)* 8)/least(sum(Duration),60)) as upload_rate,\r\n  toInt64((sum(FlowStatsBytesRev)* 8)/least(sum(Duration),60)) as download_rate,\r\n  toInt64(((sum(FlowStatsBytesRev) + sum(FlowStatsBytesFwd)) * 8)/least(sum(Duration),60)) as cumulative_rate,\r\n  toStartOfInterval(`Timestamp`, INTERVAL 300 second) AS Timestamp,\r\n  CpeCNID as  CpeCNID\r\n  from analytics.sampled_uflow where AccessDenied = 0 and ( FlowDir == 2 or FlowDir == 0 or FlowDir == 3 )\r\n  Group by ( CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp)\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36927/comments",
    "author": "dhruvanand96",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-05-05T16:09:41Z",
        "body": "should be  `ORDER BY (CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp)`\r\n\r\nsee:\r\n\r\n```sql\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS analytics.uflow_topapps_bytes_flowdir_or_013_mv\r\nENGINE = SummingMergeTree PARTITION BY toYYYYMMDD ( Timestamp )\r\nORDER BY (CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp)\r\nPOPULATE\r\nAS SELECT\r\n   toString(AppId) as name_top_apps,\r\n   IPv6NumToString(DstIP) as name_remote_top_emdpoints,\r\n   sum(FlowStatsBytesFwd) as upload_bytes,\r\n   sum(FlowStatsBytesRev) as download_bytes,\r\n   sum(FlowStatsBytesFwd + FlowStatsBytesRev) as cumulative_bytes,\r\n   sum(FlowStatsPktsFwd) as upload_flows,\r\n   sum(FlowStatsPktsFwd) as download_flows,\r\n   sum(FlowStatsPktsFwd + FlowStatsPktsRev) as cumulative_flows,\r\n   toInt64((sum(FlowStatsBytesFwd)* 8)/least(sum(Duration),60)) as upload_rate,\r\n   toInt64((sum(FlowStatsBytesRev)* 8)/least(sum(Duration),60)) as download_rate,\r\n   toInt64(((sum(FlowStatsBytesRev) + sum(FlowStatsBytesFwd)) * 8)/least(sum(Duration),60)) as cumulative_rate,\r\n   toStartOfInterval(Timestamp, INTERVAL 300 second) AS Timestamp,\r\n   CpeCNID as CpeCNID\r\nfrom analytics.sampled_uflow where AccessDenied = 0 and ( FlowDir == 2 or FlowDir == 0 or FlowDir == 3 )\r\nGroup by  CpeCNID, name_top_apps, name_remote_top_emdpoints, Timestamp\r\n```\r\n\r\ndon't use `( )` in groupBY, there is a bug."
      }
    ],
    "satisfaction_conditions": [
      "Ensure grouping columns are properly specified in both GROUP BY and ORDER BY clauses",
      "Avoid syntax pitfalls in ClickHouse GROUP BY clauses",
      "Maintain correct column alignment between SELECT and aggregation clauses"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:46:39"
    }
  },
  {
    "number": 36903,
    "title": "Truncate replicated cluster",
    "created_at": "2022-05-04T12:06:34Z",
    "closed_at": "2022-05-04T14:00:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36903",
    "body": "What is the point of this error? \r\n```\r\ntruncate table numbers on cluster stage_cluster\r\nCode: 371, e.displayText() = DB::Exception: For a distributed DDL on circular replicated cluster its table name must be qualified by database name. (version 21.8.14.5 (official build))\r\n```\r\nstage cluster has 2 replicas per shard. Wanted to truncate tables in db1.numbers and db2.numbers\r\n\r\nApplication during its run is writing data to table for other applications. After some time it is to be cleared and feeded again. For long time it was solved by clearing one replica and it was cleared on other replicas in shard. This solution is not good because one node can be not present and then data will not be cleared or will be delayed causing weird situations. So it is not reliable at all.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36903/comments",
    "author": "rkozlo",
    "comments": [
      {
        "user": "tavplubix",
        "created_at": "2022-05-04T12:34:25Z",
        "body": "It literally means that table name must be qualified by database name if circular replication is used:\r\n```\r\ntruncate table db1.numbers on cluster stage_cluster;\r\ntruncate table db2.numbers on cluster stage_cluster;\r\n```"
      },
      {
        "user": "rkozlo",
        "created_at": "2022-05-04T13:00:31Z",
        "body": "That's what i see ;). I was asking more likely a reason for this fuse. Any dangerous?\r\n\r\n\r\n> truncate table db1.numbers on cluster stage_cluster;\r\ntruncate table db2.numbers on cluster stage_cluster;\r\n\r\nThat's how i replaced it now. Just wondered if it can be done with single query.\r\n"
      },
      {
        "user": "tavplubix",
        "created_at": "2022-05-04T13:57:09Z",
        "body": "Sorry, I misunderstood your question :)\r\nWhen circular replication is used each host is responsible for two replicas (db1.numbers and db2.numbers), so each host should execute query twice with different databases. It's not implemented, currently each host can execute only one query per one distributed DDL task. And when database name is not specified, then it's not clear which one to choose, so it just fails with the error like this."
      },
      {
        "user": "rkozlo",
        "created_at": "2022-05-04T14:00:39Z",
        "body": "Thanks for explanation!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why database qualification is required for circular replication truncation",
      "Clarification about single-query execution limitations in circular replication setups",
      "Explanation of circular replication constraints affecting DDL execution"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:46:51"
    }
  },
  {
    "number": 36709,
    "title": "Use named connection in  remote(...) ",
    "created_at": "2022-04-27T14:02:08Z",
    "closed_at": "2022-04-27T14:40:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36709",
    "body": "Can we use named connection in  remote(...) ?\r\n\r\nI try:\r\n`SELECT count() FROM remote(mxch, db='mx_master', table='health_watch');\r\n`\r\nbut get:\r\n\r\n`Code: 36. DB::Exception: Unexpected key-value argument.Got: db, but expected: sharding_key. (BAD_ARGUMENTS) (version 22.3.3.44 (official build))`",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36709/comments",
    "author": "oleg-savko",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2022-04-27T14:22:31Z",
        "body": "Please try `database` instead of `db`"
      },
      {
        "user": "oleg-savko",
        "created_at": "2022-04-27T14:35:04Z",
        "body": "\r\n\r\n\r\n> Please try `database` instead of `db`\r\n\r\nthanks thats work! It whould be grate if that will be in docs)"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct parameter name for database specification in ClickHouse's remote() function"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:47:03"
    }
  },
  {
    "number": 36105,
    "title": "What's the process in Memory Engine for update/delete mutation operation?",
    "created_at": "2022-04-10T13:54:58Z",
    "closed_at": "2022-04-10T16:44:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/36105",
    "body": "I would use Memory engine for some special requirement. After the 'ALTER UPDATE/DELET' operation, can I query this table immediately?  Is the mutation operation in Memory engine is also an asyc action? How can I check whether the \"UPDATE/DELETE\" action is done for Memory Engine/Table?  Thanks!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/36105/comments",
    "author": "taotaizhu-pw",
    "comments": [
      {
        "user": "ucasfl",
        "created_at": "2022-04-10T15:33:59Z",
        "body": "The execution of mutations for `Memory` engine is synchronous, you can query the table immediately after `alter` operation done.\n\n---\n\nBut `Memory` engine is not for production usage, and the mutation process is single-threaded."
      },
      {
        "user": "taotaizhu-pw",
        "created_at": "2022-04-10T16:25:01Z",
        "body": "> But `Memory` engine is not for production usage, and the mutation process is single-threaded.\r\n\r\nNeed I  manually to set the \"max_threads = 1\"?  Or it would be set by system when execute mutation operation? Thanks"
      },
      {
        "user": "ucasfl",
        "created_at": "2022-04-10T16:26:39Z",
        "body": "Default by system."
      },
      {
        "user": "taotaizhu-pw",
        "created_at": "2022-04-10T16:44:08Z",
        "body": "> Default by system.\r\n\r\nThanks! :)"
      }
    ],
    "satisfaction_conditions": [
      "Clarify whether mutations (UPDATE/DELETE) in Memory Engine are synchronous or asynchronous",
      "Explain how to verify mutation operation completion status",
      "Confirm if thread management is automatic for Memory Engine mutations",
      "Address production-readiness considerations for Memory Engine"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:47:10"
    }
  },
  {
    "number": 35605,
    "title": "Details about how distribute_group_by_no_merge works",
    "created_at": "2022-03-25T03:10:24Z",
    "closed_at": "2022-03-26T14:59:53Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35605",
    "body": "I'm wondering how distribute_group_by_no_merge = 1 works. \r\n\r\nI'm using distribute_group_by_no_merge=1 to optimize the following query\r\n\r\noriginal query: \r\n\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\nFROM (\r\n  SELECT client_id\r\n  , Date(ext_ingest_time) AS ext_ingest_date\r\n  FROM test\r\n  WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nAnd I wrote two queries using distributed_group_by_no_merge = 1 as optimized versions\r\n\r\nA:\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\nFROM (\r\n    SELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\n    FROM (\r\n      SELECT client_id\r\n      , Date(ext_ingest_time) AS ext_ingest_date\r\n      FROM test\r\n      WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\n    )\r\n    GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nB: \r\n```\r\nWITH Date(now()) AS in_endDate\r\n    SELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\n    FROM (\r\n        SELECT\r\n            COUNT(DISTINCT client_id) AS tmp\r\n            , Date(ext_ingest_time) AS ext_ingest_date\r\n            FROM test_table\r\n        WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n        AND ext_ingest_time <= in_endDate\r\n        GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nAnd I found the distributed_group_by_no_merge = 1 only works for B.  It seems because I imposed distributed_group_by_no_merge = 1 to a subquery in query A so it doesn't work. \r\n\r\nBut I need professional interpretation about the difference between A and B, as well as how distributed_group_by_no_merge = 1  influences these two queries.\r\n\r\nBesides, I have a few more questions:\r\n1. I know distributed_group_by_no_merge = 1  can be used to optimize `count distinct`. Does it also work for other aggregations like Min, Max, Sum, and Count?\r\n2. Can we use distributed_group_by_no_merge = 1  along with window functions?\r\n\r\nWait for your response! Thanks a lot!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35605/comments",
    "author": "catwang01",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-03-25T18:27:01Z",
        "body": "This is expected. Because of `( )` `select_at_initator ( select distributed ) `\r\n\r\nOnly part of the query inside `( .... )` is executed at shards.\r\n\r\n\r\n\r\n```\r\nWITH Date(now()) AS in_endDate\r\nSELECT SUM(tmp) AS count_distinct, ext_ingest_date\r\nFROM (\r\n    SELECT COUNT(DISTINCT client_id) AS tmp, ext_ingest_date\r\n    FROM (                                                          --- executed on initator\r\n      SELECT client_id                                              --- executed on shards\r\n      , Date(ext_ingest_time) AS ext_ingest_date\r\n      FROM test\r\n      WHERE ext_ingest_time >= date_add(day, -54, in_endDate)\r\n      AND ext_ingest_time <= in_endDate\r\n    )\r\n    GROUP BY ext_ingest_date SETTINGS distributed_group_by_no_merge = 1\r\n)\r\nGROUP BY ext_ingest_date\r\n```\r\n\r\nby ` SELECT client_id`  you are fetching all data to initator, after that `distributed_group_by_no_merge` loses sense because all data at the iniator.\n\n---\n\n>I know distributed_group_by_no_merge = 1 can be used to optimize count distinct.\r\n> Does it also work for other aggregations like Min, Max, Sum, and Count?\r\n\r\nyes it works with all agg.functions, also check \r\n\r\n```\r\n--optimize_distributed_group_by_sharding_key arg                     Optimize GROUP BY sharding_key queries (by avoiding costly aggregation on the initiator server).\r\n```\r\nit's the automatic mode for `distributed_group_by_no_merge`\r\n\r\n------\r\n\r\n>Can we use distributed_group_by_no_merge = 1 along with window functions?\r\n\r\nNo, window functions work at the query initator now. Use `optimize_distributed_group_by_sharding_key`."
      },
      {
        "user": "catwang01",
        "created_at": "2022-03-27T13:03:40Z",
        "body": "Thanks @den-crane ! That helps a lot!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why distributed_group_by_no_merge=1 works in Query B but not Query A based on query execution flow",
      "Clarification of how distributed_group_by_no_merge interacts with different query structures",
      "Confirmation of distributed_group_by_no_merge applicability to various aggregation functions",
      "Explanation of window function compatibility limitations",
      "Differentiation between shard-level vs initiator-level processing in distributed queries"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:47:20"
    }
  },
  {
    "number": 35444,
    "title": "data_compressed_bytes and data_uncompressed_bytes in system.columns table are zero",
    "created_at": "2022-03-20T14:23:37Z",
    "closed_at": "2022-03-20T14:45:30Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35444",
    "body": "Clickhouse version: 22.3.2.1\r\n\r\nI create a MergeTree table and add some data to it, then I want to see how much space is used by its columns querying `system.columns` table, but it returns 0 for both `data_compressed_bytes` and `data_uncompressed_bytes`. I believe it worked a few months ago.\r\n\r\n```sql\r\ncreate table t (number Int32) engine = MergeTree() order by number as\r\nselect * from numbers(100000);\r\n\r\nselect name, type, data_compressed_bytes, data_uncompressed_bytes, compression_codec\r\nfrom system.columns where table = 't';\r\n```\r\nI get the following results:\r\n| name | type | data\\_compressed\\_bytes | data\\_uncompressed\\_bytes | compression\\_codec |\r\n| :--- | :--- | :--- | :--- | :--- |\r\n| number | Int32 | 0 | 0 |  |",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35444/comments",
    "author": "stas-sl",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-03-20T14:45:30Z",
        "body": "It's expected/intended behaviour for compact parts.\r\n\r\n```\r\nselect table, name, part_type from system.parts where table = 't';\r\n\u250c\u2500table\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500part_type\u2500\u2510\r\n\u2502 t     \u2502 all_1_1_0 \u2502 Compact   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\ncreate table t (number Int32) engine = MergeTree() order by number settings min_bytes_for_wide_part=0 as\r\nselect * from numbers(100000);\r\n\r\nselect table, name, part_type from system.parts where table = 't';\r\n\u250c\u2500table\u2500\u252c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500part_type\u2500\u2510\r\n\u2502 t     \u2502 all_1_1_0 \u2502 Wide      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nselect name, type, data_compressed_bytes, data_uncompressed_bytes, compression_codec\r\nfrom system.columns where table = 't';\r\n\u250c\u2500name\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u252c\u2500data_compressed_bytes\u2500\u252c\u2500data_uncompressed_bytes\u2500\u252c\u2500compression_codec\u2500\u2510\r\n\u2502 number \u2502 Int32 \u2502                401725 \u2502                  400000 \u2502                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      },
      {
        "user": "stas-sl",
        "created_at": "2022-03-20T14:56:32Z",
        "body": "Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why data_compressed_bytes and data_uncompressed_bytes show zero values",
      "Clarification of relationship between part types (Compact/Wide) and column statistics visibility",
      "Guidance on configuring table settings to enable desired statistics collection",
      "Verification method for current part type configuration"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:47:30"
    }
  },
  {
    "number": 35402,
    "title": "Error create database with ENGINE = PostgreSQL",
    "created_at": "2022-03-18T10:01:37Z",
    "closed_at": "2022-03-18T10:12:29Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/35402",
    "body": "hi!\r\non clickhouse-server version  21.4.6.55 i am successfully create database with engine postgresql\r\nCREATE DATABASE b2b ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)\r\nbut on clickhouse-server version 22.2.2.1 i get an error:\r\n```\r\n<Error> executeQuery: Code: 170. DB::Exception: Bad get: has UInt64, requested String. (BAD_GET) (version 22.2.2.1) (from 12\r\n7.0.0.1:35954) (in query: CREATE DATABASE b2b2 ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xaebed1a in /usr/bin/clickhouse\r\n1. DB::Exception::Exception<std::__1::basic_string_view<char, std::__1::char_traits<char> >, DB::Field::Types::Which const&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allo\r\ncator<char> > const&, std::__1::basic_string_view<char, std::__1::char_traits<char> >&&, DB::Field::Types::Which const&) @ 0xafdf5a0 in /usr/bin/clickhouse\r\n2. auto& DB::Field::safeGet<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >() @ 0xba85663 in /usr/bin/clickhouse\r\n3. ? @ 0x14bcd930 in /usr/bin/clickhouse\r\n4. DB::DatabaseFactory::getImpl(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context const>) @ 0x14bc\r\nb7eb in /usr/bin/clickhouse\r\n5. DB::DatabaseFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context const>) @ 0x14bc8f47\r\n in /usr/bin/clickhouse\r\n6. DB::InterpreterCreateQuery::createDatabase(DB::ASTCreateQuery&) @ 0x14bb0237 in /usr/bin/clickhouse\r\n7. DB::InterpreterCreateQuery::execute() @ 0x14bc5e1b in /usr/bin/clickhouse\r\n8. ? @ 0x14ee8a79 in /usr/bin/clickhouse\r\n9. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x14ee65f5 in\r\n /usr/bin/clickhouse\r\n10. DB::TCPHandler::runImpl() @ 0x159ef43a in /usr/bin/clickhouse\r\n11. DB::TCPHandler::run() @ 0x15a03419 in /usr/bin/clickhouse\r\n12. Poco::Net::TCPServerConnection::start() @ 0x18667a0f in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerDispatcher::run() @ 0x18669e61 in /usr/bin/clickhouse\r\n14. Poco::PooledThread::run() @ 0x1881a549 in /usr/bin/clickhouse\r\n15. Poco::ThreadImpl::runnableEntry(void*) @ 0x18817c40 in /usr/bin/clickhouse\r\n16. start_thread @ 0x817a in /usr/lib64/libpthread-2.28.so\r\n17. __clone @ 0xfcdc3 in /usr/lib64/libc-2.28.so\r\n```\r\nbut if i am manually create database and create tables with engine postgres, everything is fine and I don't get any errors\r\nfor example create table like this work fine:\r\n```\r\nCREATE TABLE b2b.intouch_district(    id Int32,    name String,    city_id Int32)ENGINE = PostgreSQL('10.10.17.111:6432', 'b2b', 'intouch_district', 'login', 'password');\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/35402/comments",
    "author": "TipaOpa",
    "comments": [
      {
        "user": "kssenii",
        "created_at": "2022-03-18T10:12:03Z",
        "body": "> PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)\r\n\r\n1 is the 6th parameter.\n\n---\n\n> PostgreSQL('10.10.17.111:6432', 'b2b', 'login', 'password', 1)\r\n1 is the 6th parameter.\r\n\r\nexample:\r\n```\r\nCREATE DATABASE test_database ENGINE = PostgreSQL('postgres1:5432', 'test_database', 'postgres', 'mysecretpassword', '', 1)\")\r\n```"
      },
      {
        "user": "TipaOpa",
        "created_at": "2022-03-18T13:32:36Z",
        "body": "yes it works, thanks\r\nthe syntax has changed in the new version and when I updated the clickhouse-server it gave an error"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of parameter order changes in PostgreSQL database engine syntax between ClickHouse versions",
      "Clarification of required parameters for PostgreSQL database engine in newer ClickHouse versions",
      "Documentation of version compatibility considerations for database engine syntax"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:47:47"
    }
  },
  {
    "number": 34987,
    "title": "Confuse about the difference between background_schedule_pool_size  and background_fetches_pool_size ",
    "created_at": "2022-03-02T10:10:48Z",
    "closed_at": "2022-03-03T14:25:12Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34987",
    "body": "I think ReplicatedMerge table has two type background tasks , one is merge and another is fetch parts from another replica. \r\nIn my opinion, background_fetches_pool_size is for fetch and background_pool_size  is for merge, so i confuse why the document for clickhouse say background_schedule_pool_size is about background task for replicated task. Is there another backgraound task for replicated table ? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34987/comments",
    "author": "Yanbuc",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2022-03-02T14:00:44Z",
        "body": "There are fetches and merges which are background tasks. Recently there were a single pool for both type of tasks, but now we use different pools for better tuning."
      },
      {
        "user": "Yanbuc",
        "created_at": "2022-03-03T12:44:54Z",
        "body": "> There are fetches and merges which are background tasks. Recently there were a single pool for both type of tasks, but now we use different pools for better tuning.\r\n\r\nI see. Thank you ."
      }
    ],
    "satisfaction_conditions": [
      "Clarify the distinct purposes of background_schedule_pool_size and background_fetches_pool_size in relation to replicated table tasks",
      "Identify all background task types associated with ReplicatedMerge tables",
      "Explain why separate pools were introduced for different task types",
      "Map configuration parameters to specific replicated table operations (merge vs fetch)"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:48:01"
    }
  },
  {
    "number": 34978,
    "title": "Can clickhouse-copier rezone partitions?",
    "created_at": "2022-03-02T07:05:55Z",
    "closed_at": "2022-03-03T20:16:46Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34978",
    "body": "source cluster partition : '20220101'\r\ntarget cluster partition : ('20220101',22)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34978/comments",
    "author": "wbxsGithub",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2022-03-02T14:12:25Z",
        "body": "With copier you can copy data into a different table, with a different partition key. But you can't change partition \"in place\" for table. \r\nWith copier, full copy of data will happen."
      },
      {
        "user": "wbxsGithub",
        "created_at": "2022-03-03T14:09:56Z",
        "body": "It's correct!\n\n---\n\nIs it possible to change the 'sorting_key'?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-03-03T20:16:46Z",
        "body": "Yes, both partition and sorting key can change."
      },
      {
        "user": "wbxsGithub",
        "created_at": "2022-03-04T01:40:49Z",
        "body": "Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Clarifies whether clickhouse-copier supports modifying both partition keys and sorting keys during data copying",
      "Explains the relationship between data copying and schema changes (partition/sorting keys)",
      "Confirms that schema changes require full data copy rather than in-place modifications"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:48:14"
    }
  },
  {
    "number": 34474,
    "title": "Breaking a source table row into multiple rows using materialized views",
    "created_at": "2022-02-09T19:36:38Z",
    "closed_at": "2022-02-10T04:09:51Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34474",
    "body": "Is it possible to create two rows in a MV from a single row in a source table?\r\n\r\nExample:\r\nInsert in source table:\r\n|when | col_1 | col_2|\r\n|-----|-------|------|\r\n|datetime | val_1 | val_2|\r\n\r\nI would like to end up with something like this in the Materialized view:\r\n| when | col_x |\r\n|------|-------|\r\n|datetime | val_1|\r\n|datetime | val_2|\r\n\r\nIs it possible to do that? Thanks!!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34474/comments",
    "author": "a-dot",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2022-02-09T21:05:19Z",
        "body": "Yes it's possible `arrayJoin( [ col_1 , col_2 ] ) `\r\n\r\n```sql\r\nSELECT arrayJoin([col1, col2]) AS colx\r\nFROM\r\n(\r\n    SELECT\r\n        1 AS col1,\r\n        2 AS col2\r\n)\r\n\u250c\u2500colx\u2500\u2510\r\n\u2502    1 \u2502\r\n\u2502    2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\n\r\ncreate materialized view ...\r\n...\r\nselect arrayJoin( [ col_1 , col_2 ] ) as col_x, \r\nfrom ...\r\n```"
      },
      {
        "user": "a-dot",
        "created_at": "2022-02-10T01:37:35Z",
        "body": "Thanks den-crane, as always!\r\nOne follow up question if I may... the array approach is stumping me and I simply can't figure out how to do this. In my first post I gave you a simple example but what I'm trying to do is slightly more complicated.. I want to unfold col1 and col2 but I want to assign val_3 as such:\r\n\r\nBefore:\r\n|when|col_1|col_2|col_3|\r\n|------|-----|-----|------|\r\n| datetime | val_1 | val_2 | val_3|\r\n\r\nAfter:\r\n|when|col_x|col_y|col_z|\r\n|-----|-----|------|-----|\r\n|datetime| val_1|val_3|0|\r\n|datetime|val_2|0|val_3|\r\n\r\nHopefully you can still help! Thanks again!!"
      },
      {
        "user": "den-crane",
        "created_at": "2022-02-10T01:50:19Z",
        "body": "```sql\r\nSELECT col_x, col_y, col_z\r\nFROM\r\n(\r\n    SELECT\r\n        'val_1' AS col_1,\r\n        'val_2' AS col_2,\r\n        'val_3' col_3\r\n)\r\narray join [col_1, col_2] AS col_x, [col_3, '0'] as col_y, ['0', col_3] as col_z\r\n\u250c\u2500col_x\u2500\u252c\u2500col_y\u2500\u252c\u2500col_z\u2500\u2510\r\n\u2502 val_1 \u2502 val_3 \u2502 0     \u2502\r\n\u2502 val_2 \u2502 0     \u2502 val_3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\n```sql\r\nSELECT\r\n    (arrayJoin(arrayZip([col_1, col_2], [col_3, '0'], ['0', col_3])) AS xx).1 AS col_x,\r\n    xx.2 AS col_y,\r\n    xx.3 AS col_z\r\nFROM\r\n(\r\n    SELECT\r\n        'val_1' AS col_1,\r\n        'val_2' AS col_2,\r\n        'val_3' AS col_3\r\n)\r\n\u250c\u2500col_x\u2500\u252c\u2500col_y\u2500\u252c\u2500col_z\u2500\u2510\r\n\u2502 val_1 \u2502 val_3 \u2502 0     \u2502\r\n\u2502 val_2 \u2502 0     \u2502 val_3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "a-dot",
        "created_at": "2022-02-10T01:58:13Z",
        "body": "thank you so much!"
      }
    ],
    "satisfaction_conditions": [
      "Support splitting a single source row into multiple materialized view rows with different column mappings",
      "Handle dynamic value assignment across split rows (e.g., val_3 appearing in different columns)",
      "Maintain temporal consistency across split rows",
      "Support default value insertion (e.g., 0) in unused columns"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:48:22"
    }
  },
  {
    "number": 34233,
    "title": "DESCRIBE return wrong result for sumMap/minMap/maxMap",
    "created_at": "2022-02-01T18:47:03Z",
    "closed_at": "2022-02-03T00:15:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34233",
    "body": "I'd expect a DESCRIBE on a sumMap agg function to return \r\n\r\n```\r\nAggregateFunction(sumMap, Tuple(Array(UInt32), Array(UInt32)))\r\n```\r\n\r\nbut in recent versions is returning:\r\n\r\n```sql\r\nAggregateFunction(1, sumMap, Tuple(Array(UInt32), Array(UInt32)))\r\n```\r\n\r\nSee example:\r\n\r\n```\r\nSELECT version()\r\n\r\nQuery id: 72a9cc69-7bd0-4b64-b7ce-e3634e238291\r\n\r\n\u250c\u2500version()\u2500\u2510\r\n\u2502 21.11.1.1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec.\r\n\r\nlocalhost :)  describe (select sumMapState(([rand()], [rand()])));\r\n\r\nDESCRIBE TABLE\r\n(\r\n    SELECT sumMapState(([rand()], [rand()]))\r\n)\r\n\r\nQuery id: 2b9fc06f-640a-42b0-a6a9-4f133230109a\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 sumMapState(tuple(array(rand()), array(rand()))) \u2502 AggregateFunction(sumMap, Tuple(Array(UInt32), Array(UInt32))) \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.002 sec.\r\n```\r\n\r\n```\r\nSELECT version()\r\n\r\nQuery id: 66449025-c544-47ae-9c17-655813820200\r\n\r\n\u250c\u2500version()\u2500\u2500\u2510\r\n\u2502 21.12.3.32 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec.\r\n\r\nrunner--project-0-concurrent-0 :) describe (select sumMapState(([rand()], [rand()])))\r\n\r\nDESCRIBE TABLE\r\n(\r\n    SELECT sumMapState(([rand()], [rand()]))\r\n)\r\n\r\nQuery id: bc1bf0fe-8412-463d-93fd-1804f0413d5d\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 sumMapState(tuple(array(rand()), array(rand()))) \u2502 AggregateFunction(1, sumMap, Tuple(Array(UInt32), Array(UInt32))) \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n1 rows in set. Elapsed: 0.003 sec.\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34233/comments",
    "author": "alrocar",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-02-03T00:15:55Z",
        "body": "This is absolutely correct and indicates that the first version of the sumMap aggregate function is using.\r\nSee #12552."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the version number appears in AggregateFunction type definitions",
      "Clarification that this behavior is intentional and version-dependent"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:48:33"
    }
  },
  {
    "number": 34009,
    "title": "Do we consider add alignment for `ThreadStatus`",
    "created_at": "2022-01-26T11:01:34Z",
    "closed_at": "2022-01-26T12:42:18Z",
    "labels": [
      "question",
      "development",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/34009",
    "body": "Do we consider add alignment for class `ThreadStatus` like this to reduce false sharing of cpu cache between multiple threads ? \r\n``` cpp\r\nclass __attribute__((__aligned__(64))) ThreadStatus : public boost::noncopyable\r\n``` \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/34009/comments",
    "author": "taiyang-li",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-01-26T11:46:55Z",
        "body": "It is already allocated on thread stack, so no issue with false sharing should exist, as thread stacks are already far away of each other."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of whether false sharing is a risk for ThreadStatus instances given their memory allocation context",
      "Rationale about thread stack memory layout and cache behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:49:00"
    }
  },
  {
    "number": 33420,
    "title": "Does clickhouse allow to use openmp? ",
    "created_at": "2022-01-06T02:00:35Z",
    "closed_at": "2022-01-06T04:25:03Z",
    "labels": [
      "question",
      "development",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/33420",
    "body": "Background is I want to do a multi threading memset to set zero of a big memory about 100MB or more, openmp can do this very easily. But I don't see any openmp code in all this project even in the contrib code, So does clickhouse allow to use openmpt to do some parallel things?\r\nThanks in advance.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/33420/comments",
    "author": "zhanglistar",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2022-01-06T02:31:40Z",
        "body": "Parallelization in ClickHouse is usually done on the level of data pipeline, not for individual loops. So, no use for OpenMP.\r\nIt is better to avoid adding OpenMP. What is the place where memset has to be parallelized?\r\n"
      },
      {
        "user": "zhanglistar",
        "created_at": "2022-01-06T02:48:53Z",
        "body": "> \r\n\r\nIn clickhouse-keeper, there is a hashtable to store all the node data which maybe 10million or billon, resizing process will have to calloc a big memory maybe 100MB 200MB even more, in one thread, this is about 100ms even more to complete, while using openmp will decrease to 30ms which I have tested in local. This code is in critical path of raft state machine running in a single thread, resulting performance decreasing when doing pressing test. After using openmp, the writing speed is much more smooth."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-01-06T03:14:24Z",
        "body": "If we use our HashMap, it will do `mmap` for memory chunks 64MB or larger and avoid using memset from userspace, because OS kernel is already zeroing memory. And we cannot avoid this work that is done by OS kernel.\r\n\r\nAlso if we want to lower the latency (and we do) it makes sense to amortize resizes (by doing incremental data movement from old buffer to the new buffer). It will lower the latency to microseconds range."
      },
      {
        "user": "zhanglistar",
        "created_at": "2022-01-06T04:24:39Z",
        "body": "> If we use our HashMap, it will do `mmap` for memory chunks 64MB or larger and avoid using memset from userspace, because OS kernel is already zeroing memory. And we cannot avoid this work that is done by OS kernel.\r\n> \r\n> Also if we want to lower the latency (and we do) it makes sense to amortize resizes (by doing incremental data movement from old buffer to the new buffer). It will lower the latency to microseconds range.\r\n\r\nThanks for your reply. mmap is 30% faster than calloc, but still 2x slower than openmp version memset. OK, I will not use openmp, and using malloc and memset part of memory to reduce latency. \r\nBTW this is the first buffer that needed to malloc for new hashmap, and need to initialize the managing metadata, the following process is incremental."
      }
    ],
    "satisfaction_conditions": [
      "Avoids introducing OpenMP dependencies into ClickHouse",
      "Provides low-latency memory initialization for large allocations",
      "Works within ClickHouse's existing parallelism model",
      "Addresses latency spikes in state machine operations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:49:12"
    }
  },
  {
    "number": 33029,
    "title": "Why can not the \u201cmodify setting\u201d operation be synchronized to the other replica",
    "created_at": "2021-12-22T02:25:38Z",
    "closed_at": "2021-12-22T04:20:35Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/33029",
    "body": "If I modify setting in one replica, the other replica will not be modified synchronously.\r\n\r\n```\r\nvoid StorageReplicatedMergeTree::alter(\r\n    const AlterCommands & commands, const Context & query_context, TableLockHolder & table_lock_holder)\r\n{\r\n    assertNotReadonly();\r\n\r\n    auto table_id = getStorageID();\r\n\r\n    if (commands.isSettingsAlter()) \r\n    {\r\n        /// We don't replicate storage_settings_ptr ALTER. It's local operation.\r\n        /// Also we don't upgrade alter lock to table structure lock.\r\n        StorageInMemoryMetadata future_metadata = getInMemoryMetadata();\r\n        commands.apply(future_metadata, query_context);\r\n\r\n        merge_strategy_picker.refreshState();\r\n\r\n        changeSettings(future_metadata.settings_changes, table_lock_holder);\r\n\r\n        DatabaseCatalog::instance().getDatabase(table_id.database_name)->alterTable(query_context, table_id, future_metadata);\r\n        return;\r\n    }\r\n ...\r\n}\r\n```\r\n\r\nCould you tell me why not the \u201cmodify setting\u201d operation be synchronized to the other replica.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/33029/comments",
    "author": "zhanghuajieHIT",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-12-22T04:20:35Z",
        "body": "Not any setting, but only storage settings. \r\nOn the purpose. Storage can be different at replicas. \r\nOne replica can have all data locally at NVME, other replica can have tiered storage NVME -> HDD -> S3\r\n\r\n"
      },
      {
        "user": "zhanghuajieHIT",
        "created_at": "2021-12-22T04:47:37Z",
        "body": "supporting synchronization between replicas may be more convenient"
      },
      {
        "user": "den-crane",
        "created_at": "2021-12-22T04:51:47Z",
        "body": "> supporting synchronization between replicas may be more convenient\r\n\r\nAll settings are synchronized except  storage settings. \r\nDeliberately. Other users need different storage configurations for replicas."
      },
      {
        "user": "zhanghuajieHIT",
        "created_at": "2021-12-22T06:05:53Z",
        "body": "ok, thanks"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why storage settings are intentionally excluded from replication",
      "Clarification that non-storage settings are synchronized between replicas",
      "Identification of valid use cases for replica-specific storage configurations",
      "Differentiation between local operations and cluster-wide configuration changes"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:49:19"
    }
  },
  {
    "number": 32813,
    "title": "Clickhouse Materialized Views select column with different date",
    "created_at": "2021-12-15T18:08:29Z",
    "closed_at": "2021-12-15T19:19:54Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/32813",
    "body": "Hi guys,\r\n\r\ni have an issue when select MV column with different date period, example:\r\n\r\nsource table\r\n```\r\ncreate table if not exists source(\r\n      `original_timestamp` DateTime64(3) DEFAULT parseDateTimeBestEffort('0001-1-1 23:00:00') CODEC(DoubleDelta, LZ4),\r\n      `event` LowCardinality(String) DEFAULT '',\r\n      `identity` String DEFAULT ''\r\n    ) \r\n    ENGINE = ReplacingMergeTree()\r\n    partition by toWeek(identity,original_timestamp)\r\n        order by (id)\r\n        SETTINGS index_granularity = 8192\r\n```\r\ntarget table\r\n```\r\ncreate table if not exists target(\r\n     day DateTime,\r\n     identity String,\r\n     login AggregateFunction(countIf, String, UInt8),\r\n     register AggregateFunction(countIf, String, UInt8)\r\n    ) \r\n    ENGINE = AggregatingMergeTree()\r\n    partition by toWeek(day)\r\n        order by (day,identity)\r\n        SETTINGS index_granularity = 8192\r\n```\r\nmv table\r\n```\r\nCREATE MATERIALIZED VIEW if not exists target_mv \r\n    to target as\r\n    select\r\n    toStartOfDay(original_timestamp) as day,\r\n    identity,\r\n    countIf(event, event='login') as login,\r\n    countIf(event, event='register') as register\r\n    from source\r\n    group by identity, day\r\n```\r\n\r\nquery:\r\nselect\r\ncountIfMerge(login) as totalLoginEvent, //login event in the last 3 days\r\ncountIfMerge(register) as totalRegisterEvent //register event in the last 7 days\r\nfrom target\r\ngroup by identity,day\r\n\r\nexample expected result: totalLoginEvent is 10 (in the last 3 days) and totalRegisterEvent is 40 event in the last 7 days\r\n\r\nplease help, thanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/32813/comments",
    "author": "Kev1ntan",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-12-15T19:19:54Z",
        "body": "you don't need AggregateFunction(countIf\r\n\r\n```sql\r\nCREATE TABLE IF NOT EXISTS source\r\n(\r\n    `original_timestamp` DateTime64(3) DEFAULT parseDateTimeBestEffort('0001-1-1 23:00:00') CODEC(DoubleDelta, LZ4),\r\n    `event` LowCardinality(String) DEFAULT '',\r\n    `identity` String DEFAULT ''\r\n)\r\nENGINE = ReplacingMergeTree\r\nPARTITION BY toWeek(original_timestamp)\r\nORDER BY identity\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS target\r\n(\r\n    `day` DateTime,\r\n    `identity` String,\r\n    `login` SimpleAggregateFunction(sum, UInt64),\r\n    `register` SimpleAggregateFunction(sum, UInt64)\r\n)\r\nENGINE = AggregatingMergeTree\r\nPARTITION BY toWeek(day)\r\nORDER BY (day, identity)\r\nSETTINGS index_granularity = 8192\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS target_mv TO target AS\r\nSELECT\r\n    toStartOfDay(original_timestamp) AS day,\r\n    identity,\r\n    countIf(event = 'login') AS login,\r\n    countIf(event = 'register') AS register\r\nFROM source\r\nGROUP BY\r\n    identity,\r\n    day\r\n\r\n\r\ninsert into source select now() - interval 2 day , 'login', 1 from  numbers(10);\r\ninsert into source select now() - interval 20 day , 'login', 1 from  numbers(100);\r\n\r\ninsert into source select now() - interval 6 day , 'register', 1 from  numbers(40);\r\ninsert into source select now() - interval 20 day , 'register', 1 from  numbers(100);\r\n\r\n\r\nSELECT\r\n    sumIf(login, day >= (today() - 3)) AS totalLoginEvent,\r\n    sumIf(register, day >= (today() - 7)) AS totalRegisterEvent\r\nFROM target\r\nWHERE day >= (today() - 7)\r\n\r\n\u250c\u2500totalLoginEvent\u2500\u252c\u2500totalRegisterEvent\u2500\u2510\r\n\u2502              10 \u2502                 40 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\n"
      },
      {
        "user": "Kev1ntan",
        "created_at": "2021-12-16T01:42:47Z",
        "body": "thank you"
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable querying aggregated metrics with different time windows in a single query",
      "Approach must maintain daily aggregation while allowing flexible date range filtering",
      "Implementation must support conditional aggregation based on time ranges",
      "Solution should avoid using complex aggregation states when simple sums suffice"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:49:31"
    }
  },
  {
    "number": 31722,
    "title": "How to give priority to INSERT queries over SELECT queries on high load",
    "created_at": "2021-11-24T17:42:46Z",
    "closed_at": "2021-11-24T20:39:34Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/31722",
    "body": "We are using Clickhouse as our main data warehouse and want to make sure that incoming data being inserted in large batches does not fail. Sometimes when Clickhouse is under high load touching max memory usage, insert fails and we get timeout or too many queries errors. Is there any way to either:\r\n1. Give priority to write queries over select queries?\r\n2. Allocate max memory usage limit to certain user?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/31722/comments",
    "author": "faisalhasnain",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-11-24T20:39:34Z",
        "body": "There is `max_memory_usage_for_user` setting.\r\nYou can put all SELECT queries under a separate user and limit memory for it."
      },
      {
        "user": "faisalhasnain",
        "created_at": "2021-11-24T20:44:39Z",
        "body": "thank you, that's what i was looking for :)"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to control resource allocation (e.g., memory) for different query types",
      "Enables prioritization of write operations over read operations",
      "Works within ClickHouse's configuration capabilities",
      "Allows separation of query types/users for resource management"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:49:49"
    }
  },
  {
    "number": 31612,
    "title": "Are select queries aware of detach? ",
    "created_at": "2021-11-22T05:57:23Z",
    "closed_at": "2021-11-22T21:34:58Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/31612",
    "body": "If my select query is running and I issue a detach command on a partition(which is being processed by the select query in question), will the select query still read the partition data? \r\n\r\nI am not able to test this because my select queries are super fast so I would like to confirm it. \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/31612/comments",
    "author": "gauravphoenix",
    "comments": [
      {
        "user": "gauravphoenix",
        "created_at": "2021-11-22T17:12:15Z",
        "body": "I was able to experiment with this by putting a lot of data in my table and then performing a select. \r\nNot only I did `detach`, I went to the store directory and did `rm -rf *` to remove the detached partitions. \r\n\r\nso my question is no longer about \"if\" but rather \"how\". \r\n\r\nwhen select queries are being run, and I have already performed detach and removed the files, where is the data being read from? \n\n---\n\nThe mystery is solved now, I did some more investigations and it seems that when `detach` command is executed, the data is **not** removed immediately from the store directory when `select` queries are running. The copy to detach directory still occurs but not the removal and hence select queries are able to see the data. \r\n\r\n\r\nit will be nice to have documentation updated to reflect this "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-11-22T21:34:58Z",
        "body": "Yes, the data parts are refcounted, and removed only after the latest query finished."
      },
      {
        "user": "gauravphoenix",
        "created_at": "2021-11-23T15:14:27Z",
        "body": "@alexey-milovidov does refcount apply to attach commands too? Can I safely run attach commands when select queries are being run?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-11-23T16:28:14Z",
        "body": "Yes, you can.\r\n\r\nThe same mechanics allows to run multiple INSERT and SELECT queries concurrently and multiple background merges without locks. This is fundamental for ClickHouse.\n\n---\n\nAnd even more... if you use Atomic database (by default), you can DROP table during SELECT without any locks and waiting :)\r\nIt's magic."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how ClickHouse handles data availability during concurrent SELECT and DETACH operations",
      "Description of the refcounting mechanism for data part lifecycle management",
      "Clarification of safety guarantees for concurrent DDL and query operations",
      "Explanation of how ClickHouse achieves lock-free operations for common workflows"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:49:53"
    }
  },
  {
    "number": 29975,
    "title": "Is it possible to calculate number of distinct keys in map?",
    "created_at": "2021-10-11T01:49:23Z",
    "closed_at": "2021-10-11T09:36:19Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/29975",
    "body": "I have a map type field. Is there a way to calculate how many times each of distinct keys encounter in the whole table?\r\n\r\nSay I have a table from the example in the docs:\r\n\r\n`CREATE TABLE table_map (a Map(String, UInt64)) ENGINE=Memory;`\r\n`INSERT INTO table_map VALUES ({'key1':1, 'key2':10}), ({'key1':2,'key2':20}), ({'key1':3,'key2':30});`\r\n\r\nCan I write a query like this?\r\n\r\n`SELECT\r\n    a.keys,\r\n    count(a.keys) AS cnt\r\nFROM table_map\r\nGROUP BY a.keys\r\nORDER BY cnt DESC`\r\n\r\nThis query counts distinct set of keys, but can it be rewritten to count keys?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/29975/comments",
    "author": "sogawa-sps",
    "comments": [
      {
        "user": "vdimir",
        "created_at": "2021-10-11T08:09:19Z",
        "body": "You can use `arrayJoin` for that\r\n\r\n```sql\r\nSELECT arrayJoin(a.keys) as keys, count() as cnt FROM table_map GROUP BY keys ORDER BY cnt DESC;\r\n```\r\n\r\n```\r\n\u250c\u2500keys\u2500\u252c\u2500cnt\u2500\u2510\r\n\u2502 key2 \u2502   3 \u2502\r\n\u2502 key1 \u2502   3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nReopen this issue if this solution is not suitable for your case"
      },
      {
        "user": "sogawa-sps",
        "created_at": "2021-10-11T21:24:15Z",
        "body": "Thanks a lot!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must count occurrences of individual map keys across all rows",
      "Result must show distinct keys with their total occurrence count"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:50:00"
    }
  },
  {
    "number": 28849,
    "title": "What if version column in ReplacingMergeTree overflows?",
    "created_at": "2021-09-10T07:29:11Z",
    "closed_at": "2021-09-10T18:32:52Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28849",
    "body": "I know there is a possibility to use type `UInt256` for version column in `ReplacingMergeTree`, in which the overflow is probably unrealistic, but still possible. What happens then, when I reach the maximum \"version\" and I need to create a new one? How does ClickHouse handle it? Or how should I handle it? Thank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28849/comments",
    "author": "grongor",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-10T18:32:52Z",
        "body": "Even UInt16 or UInt32 should be enough. \r\n4294967295 is insane number of versions \"updates\"\r\n\r\nIf UInt16 is not enough for you, you are doing something weird. "
      },
      {
        "user": "grongor",
        "created_at": "2021-09-10T19:00:13Z",
        "body": "Well, that doesn't really answer the question. And it depends on the use cases... I'm for example trying to track all network prefixes and related data from BGP available in our network, in real time, which means  thousands of updates per second. So, when the counter overflows, there is no sane way to continue I guess. "
      },
      {
        "user": "den-crane",
        "created_at": "2021-09-10T19:16:19Z",
        "body": ">Well, that doesn't really answer the question.\r\n\r\nWhen numeric type is overflow then a number starts again with 0.\r\nReplacingMergeTree treats overflowed 0 as a version which is less than MAX_NUMBER.\r\nIt's your responsibility to deal with it.\r\n\r\n>which means thousands of updates per second. \r\n\r\nThis is insane. \r\n\r\nDo you understand that you can omit version in ReplacingMergeTree? \r\nIn this case ReplacingMergeTree will use internal/natural order of blocks (order of inserts)."
      },
      {
        "user": "grongor",
        "created_at": "2021-09-11T06:50:06Z",
        "body": "> It's your responsibility to deal with it.\r\n\r\nThanks. That's what I needed to know, if ClickHouses accounts for the overflow, or if I have to work around it somehow.\r\n\r\n> This is insane.\r\n\r\nYeah, but it must be done :D \r\n\r\n> Do you understand that you can omit version in ReplacingMergeTree?\r\n\r\nYes, but thank you for mentioning it. There will be parallel writes, that's why I went with the version column.\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-09-11T06:52:36Z",
        "body": "UInt64 is good enough for 1000 updates per second (with this rate it will require ~ 100 million years to overflow)."
      },
      {
        "user": "grongor",
        "created_at": "2021-09-11T06:56:17Z",
        "body": "Sure, I knew I would probably be okay with one of the UInt types :) The question was more or less academic - I just wanted to know how ClickHouse would handle this, if it ever were to happen :)"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of ClickHouse's behavior when version column overflows",
      "Clarification of version ordering semantics after overflow",
      "Guidance on responsibility for overflow handling",
      "Theoretical implications of versioning mechanism limitations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:50:49"
    }
  },
  {
    "number": 28681,
    "title": "How to prevent ATTACH in old versions previous to DETACH ... PERMANENTLY",
    "created_at": "2021-09-07T10:19:56Z",
    "closed_at": "2021-09-07T12:36:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28681",
    "body": "Hello,\r\nI have a problem with an old CH version, where old materializations are reattached after each startup.\r\nHow to remove a materialized view permanently (so it doesn\u2019t reattach on startup) in older ch versions where detach \u2026 permanently is not present? Should I just delete it from the `metadata` folder after DETACH?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28681/comments",
    "author": "inakisoriamrf",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-09-07T12:36:20Z",
        "body": "> Should I just delete it from the metadata folder after DETACH?\r\n\r\nYes, you can delete .sql file from metadata folder."
      },
      {
        "user": "inakisoriamrf",
        "created_at": "2021-09-08T07:46:54Z",
        "body": "Thank you @den-crane =)"
      }
    ],
    "satisfaction_conditions": [
      "Method to prevent materialized view from reattaching on ClickHouse server startup",
      "Compatibility with older ClickHouse versions lacking DETACH ... PERMANENTLY syntax",
      "Clear confirmation of metadata persistence mechanism",
      "Non-destructive data preservation approach"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:50:58"
    }
  },
  {
    "number": 28005,
    "title": "MergeTreeThread has feature spill data to disk?",
    "created_at": "2021-08-23T03:29:16Z",
    "closed_at": "2021-08-23T04:48:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/28005",
    "body": "when use sql to query distributed tables \"select * from dbname.tablename_all\", no limitations, it is easy to appear such exception:\r\n\"_DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 93.38 GiB (attempt to allocate chunk of 362340215 bytes), maximum: 93.13 GiB: **While executing MergeTreeThread.**_\"\r\n\r\nwhen the memory is not enough, should be spill on disk, does it has this feature \"While executing MergeTreeThread\" ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/28005/comments",
    "author": "aaawuanjun",
    "comments": [
      {
        "user": "aaawuanjun",
        "created_at": "2021-08-23T03:32:02Z",
        "body": "Although this it not a good sql, but my goal is to export data to other clusters"
      },
      {
        "user": "den-crane",
        "created_at": "2021-08-23T03:40:20Z",
        "body": "How many columns in your table?\r\nClickhouse version? `select version()`?\r\nCan you share `set send_logs_level='trace'; select * from dbname.tablename_all format Null;`\r\nTry `set max_threads=1, max_block_size=8192`"
      },
      {
        "user": "aaawuanjun",
        "created_at": "2021-08-23T04:47:35Z",
        "body": "clickhouse version: 21.5.5.12\r\ncolumns:34\r\nrows:644320978\r\n\r\nok, I am trying \r\n> set max_threads=1, max_block_size=8192\r\n\r\nat present , no error occur, I successfully to export data to local.\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable handling large data exports without triggering memory limits",
      "Approach must work with MergeTree engine operations",
      "Method should require minimal query structure changes",
      "Must prevent 'Memory limit exceeded' errors during distributed table queries"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:51:16"
    }
  },
  {
    "number": 27470,
    "title": "hdfs engine with hive default delimiter '0x01'",
    "created_at": "2021-08-09T12:49:30Z",
    "closed_at": "2021-09-01T08:24:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/27470",
    "body": "```sql\r\ncreate table hdfs_engine_table_1 on cluster datacenter\r\n(\r\n    name String,\r\n    address String\r\n)\r\n    engine = HDFS('hdfs://ns/user/hive/warehouse/a/b/*', 'CSV');\r\n```\r\n\r\nwhat format should i use?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/27470/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-08-16T00:33:04Z",
        "body": "Run this query before importing data: `SET format_csv_delimiter = '\\x01'`"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to configure ClickHouse's CSV parser to handle Hive's 0x01 delimiter",
      "Clarification on ClickHouse settings for custom delimiter specification",
      "Solution compatibility with HDFS engine table creation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:51:38"
    }
  },
  {
    "number": 26242,
    "title": "how to write the sql using clickhouse, if i want to get the aggregate results fo each items select from table?",
    "created_at": "2021-07-12T10:16:19Z",
    "closed_at": "2022-10-19T21:55:26Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/26242",
    "body": "search all the ways from google to sf, no same questions.\r\n\r\nsay i have a table\uff0cwhich is about 1T large:\r\n\r\nuser_id|  pay_time | pay_info\r\n-|-|-\r\n1 | 1232323 | {'num':10, \"total\":100}\r\n1  |1232324 | {'num':11, \"total\":110}\r\n1  |1232325  |{'num':12, \"total\":120}\r\n2  |1232326 | {'num':13, \"total\":130}\r\n2  |1232327 | {'num':14, \"total\":140}\r\n2  |1232328 | {'num':15, \"total\":150}\r\n2  |1232329 | {'num':16, \"total\":160}\r\n\r\nhow i get each user' sum total or sum num when he make payments,   he has already spent, which is pay_time less than this current payment's paytime. results as follows:\r\n\r\nuser_id | pay_time | sum_num| sum_total\r\n-|-|-|-\r\n1 | 1232323  |0 |0\r\n1|  12323234 |10| 100\r\n1 | 12323234 |21 |210\r\n2 | 1232326  |0 |0 \r\n2| 1232327   |13 |130\r\n2  |1232328  |27 |270\r\n2 | 1232329  |42 |420\r\n\r\ni have read the docs, but it seems no results.  \r\n\r\nand allow_experimental_window_functions seems not working, dont know why.\r\n\r\nthanks in advance\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/26242/comments",
    "author": "aohan237",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-07-12T12:49:27Z",
        "body": ">and allow_experimental_window_functions seems not working, dont know why.\r\n\r\nClickhouse version?  `select version()`"
      },
      {
        "user": "aohan237",
        "created_at": "2021-07-13T06:26:11Z",
        "body": "> > and allow_experimental_window_functions seems not working, dont know why.\r\n> \r\n> Clickhouse version? `select version()`\r\n\r\nversion is 21.7.2.7\r\n\r\ni cant use  \"set allow_experimental_window_functions=1\" in sql query to make it work,  but i config it in user.xml in profile, it works then.  \r\n\n\n---\n\n@den-crane\r\nnow that i can use window function to fulfill this.\r\nbut what should i do if i want exclude the current row?\r\n\r\nit seems that exclude is still unsupported,  is there any alternative ways?"
      },
      {
        "user": "akuzm",
        "created_at": "2021-07-15T14:21:21Z",
        "body": "`EXCLUDE CURRENT ROW` is not currently supported and we don't have a timeline for this, but for frames that don't go over current row, you can emulate it by switching from e.g. `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` to `ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING`."
      },
      {
        "user": "aohan237",
        "created_at": "2021-07-16T02:53:09Z",
        "body": "> `EXCLUDE CURRENT ROW` is not currently supported and we don't have a timeline for this, but for frames that don't go over current row, you can emulate it by switching from e.g. `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` to `ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING`.\r\n\r\nthanks\r\n\r\nwhat if i want condition sum, i only want to sum pay_time less than start time,  as follows: how do i fulfill this\r\n\r\nuser_id |start_time| pay_time | pay_info\r\n-- | --|-- | --\r\n1 | 2|5 | {'num':10, \"total\":100}\r\n1 | 2|2 | {'num':11, \"total\":110}\r\n1 | 3|3 | {'num':12, \"total\":120}\r\n1 | 3|3 | {'num':13, \"total\":130}\r\n\r\nresults like this\r\n\r\nuser_id |start_time| pay_time | sum_num | sum_total\r\n-- | --|-- | -- | --\r\n1 | 2|5 | 0|0\r\n1 | 2|2 |0|0\r\n1 | 3|3 |11|100\r\n1 | 4|4 |23|230\r\n\r\n\r\n\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-02-07T01:06:03Z",
        "body": "@akuzm Could you please help with this question?"
      },
      {
        "user": "akuzm",
        "created_at": "2022-02-07T09:44:17Z",
        "body": "> @akuzm Could you please help with this question?\r\n\r\nThis needs a `FILTER` clause which CH doesn't have, but it does have `-If` combinator that has a similar effect.  So `sumIf(payInfo['total'], pay_time < start_time)`."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2022-10-19T21:59:18Z",
        "body": "@akuzm We have added the FILTER clause:\r\n\r\n```\r\nmilovidov-desktop :) SELECT sum(number) FILTER(WHERE number % 2 = 0) FROM numbers(10)\r\n\r\nSELECT sumIf(number, (number % 2) = 0)\r\nFROM numbers(10)\r\n\r\nQuery id: 94a0a8ac-1a93-4ff1-b9b9-f043959893c0\r\n\r\n\u250c\u2500sumIf(number, equals(modulo(number, 2), 0))\u2500\u2510\r\n\u2502                                          20 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Solution must calculate cumulative sums of 'num' and 'total' for each user, excluding the current row's values in the calculation",
      "Approach must handle conditional aggregation based on time comparisons (e.g., pay_time < current payment's time)",
      "Method must work with ClickHouse's window function limitations (specifically exclusion of current row)",
      "Solution should account for ClickHouse version compatibility and required settings",
      "Approach must handle JSON field extraction from the 'pay_info' column"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:51:44"
    }
  },
  {
    "number": 25953,
    "title": "Is it possible to change zkpath for a ReplicatedMergeTree table?",
    "created_at": "2021-07-03T18:14:38Z",
    "closed_at": "2021-07-03T22:53:05Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25953",
    "body": "At first I have created a distributed table (let us name it **table_A**). And then I decided to move the data to a ReplicatedMergeTable. I did it as below:\r\n\r\n`create table table_B as table_A engine=ReplicatedMergeTree('/clickhouse/tables/01/{database}/{table}', '{replica}');`\r\n`insert into table_B select * from table_A;`\r\n\r\nSo far so good. After that, I drop the distributed table and rename the new one to the old name.\r\n\r\n`drop table table_A;`\r\n`rename table table_B to table_A;`\r\n\r\nThen I tried to insert some data into table_A, it threw an error:\r\n\r\n`Table is in readonly mode (zookeeper path: /clickhouse/tables/01/default/table_B)`\r\n\r\nI have tried to find some command to fix the zookeeper path for this table as alter table and etc. but nothing found.\r\n\r\nIs it a solution to make it out?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25953/comments",
    "author": "lucasguo",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-07-03T18:34:26Z",
        "body": "There is no good solution. ZK does not allow to rename znodes. \r\nYou can manually fix table_A.sql file in metadata folder. \r\n\r\nThis is the impossible situation in the latest CH versions. Because CH now expands {database}/{table} macros on a create command, so in the table's metadata (.sql) will be the real name `table_B`, instead of `{table}` macro.\r\n\r\nAlso the latest CH versions allow to use Atomic databases and uuid as ZK path. You  can simply omit ZK path parameter `ReplicatedMergeTree()`"
      },
      {
        "user": "lucasguo",
        "created_at": "2021-07-04T04:44:33Z",
        "body": "> There is no good solution. ZK does not allow to rename znodes.\r\n> You can manually fix table_A.sql file in metadata folder.\r\n> \r\n> This is the impossible situation in the latest CH versions. Because CH now expands {database}/{table} macros on a create command, so in the table's metadata (.sql) will be the real name `table_B`, instead of `{table}` macro.\r\n> \r\n> Also the latest CH versions allow to use Atomic databases and uuid as ZK path. You can simply omit ZK path parameter `ReplicatedMergeTree()`\r\n\r\nThanks for your fast response! After changing the table_A.sql under /clickhouse/metadata/default/ by replacing the {table} macro to the real table name,  and restarting the clickhouse service, everything goes fine now."
      }
    ],
    "satisfaction_conditions": [
      "Solution must address ZooKeeper path mismatch after table renaming",
      "Must provide a way to update table metadata without requiring Znode renaming",
      "Should prevent future path conflicts during schema changes",
      "Must restore write capability to the renamed table"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:51:55"
    }
  },
  {
    "number": 25897,
    "title": "clickhouse-local stops working on 21.4.6.55",
    "created_at": "2021-07-01T14:05:33Z",
    "closed_at": "2021-07-01T19:00:38Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/25897",
    "body": "Hello all,\r\n\r\nI cannot run clickhouse-local anymore on version 21.4.6.55 but I can run it without any issue on version 18.6.0.\r\n\r\nQuestion 1> What is the issue here and How I can fix it?\r\n\r\nQuestion 2> Why the clickhouse-local requires to write into config.xml?\r\n\r\nThank you\r\n\r\n```\r\n$ echo -e \"1,2\\n3,4\" | clickhouse-local --structure \"a Int64, b Int64\" --input-format \"CSV\" --query \"SELECT * FROM table\"\r\nProcessing configuration file 'config.xml'.\r\nInclude not found: clickhouse_remote_servers\r\nInclude not found: clickhouse_compression\r\nCouldn't save preprocessed config to /var/lib/clickhouse/preprocessed_configs/config.xml: Access to file denied: /var/lib/clickhouse/preprocessed_configs/config.xml\r\nLogging trace to /var/log/clickhouse-server/clickhouse-server.log\r\nPoco::Exception. Code: 1000, e.code() = 13, e.displayText() = Access to file denied: /var/log/clickhouse-server/clickhouse-server.log, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::FileImpl::handleLastErrorImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x12683f3c in /usr/bin/clickhouse\r\n1. Poco::FileStreamBuf::open(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned int) @ 0x126954e2 in /usr/bin/clickhouse\r\n2. Poco::FileOutputStream::FileOutputStream(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned int) @ 0x1269652d in /usr/bin/clickhouse\r\n3. Poco::LogFileImpl::LogFileImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1269f4c2 in /usr/bin/clickhouse\r\n4. Poco::FileChannel::unsafeOpen() @ 0x126892ee in /usr/bin/clickhouse\r\n5. Poco::FileChannel::open() @ 0x126891e1 in /usr/bin/clickhouse\r\n6. Loggers::buildLoggers(Poco::Util::AbstractConfiguration&, Poco::Logger&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x89c6483 in /usr/bin/clickhouse\r\n7. DB::LocalServer::initialize(Poco::Util::Application&) @ 0x89036a5 in /usr/bin/clickhouse\r\n8. Poco::Util::Application::run() @ 0x125cf046 in /usr/bin/clickhouse\r\n9. mainEntryClickHouseLocal(int, char**) @ 0x890e50b in /usr/bin/clickhouse\r\n10. main @ 0x87f1dce in /usr/bin/clickhouse\r\n11. __libc_start_main @ 0x22505 in /usr/lib64/libc-2.17.so\r\n12. _start @ 0x87bd06e in /usr/bin/clickhouse\r\n (version 21.4.6.55 (official build))\r\n\r\n$ clickhouse-local --version\r\nClickHouse client version 21.4.6.55 (official build).\r\n\r\n-rw-r--r-- 1 clickhouse clickhouse 16235 May 10 08:24 /var/lib/clickhouse/preprocessed_configs/config.xml\r\n```\r\n\r\n=============================\r\n```\r\n$ echo -e \"1,2\\n3,4\" | clickhouse-local --structure \"a Int64, b Int64\" --input-format \"CSV\" --query \"SELECT * FROM table\"\r\n1       2\r\n3       4\r\n$ clickhouse-local --version\r\nClickHouse client version 18.6.0.\r\n\r\n-rw-r--r-- 1 root root 16142 Apr 15  2019 /var/lib/clickhouse/preprocessed_configs/config.xml\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/25897/comments",
    "author": "Jack012a",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-01T18:03:09Z",
        "body": "TLDR: run `clickhouse-local` from another directory, without `config.xml`.\r\n\r\nYou are running `clickhouse-local`, it looks for a config in current directory. You are running it inside a directory where the server's config is located. But `clickhouse-server` config does not make sense for `clickhouse-local`.\r\n\r\nIn most cases `clickhouse-local` does not need any config at all."
      },
      {
        "user": "Jack012a",
        "created_at": "2021-07-01T18:11:54Z",
        "body": "This is not true. I didn't run clickhouse-local from a directory where it has config.xml."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-07-01T18:13:26Z",
        "body": "But it managed to find\r\n`Processing configuration file 'config.xml'.`\r\nanyhow.\r\n\r\nType `ls -l` in the current working directory.\r\n"
      },
      {
        "user": "Jack012a",
        "created_at": "2021-07-01T18:37:20Z",
        "body": "@alexey-milovidov You are right. By accident, my home directory does have a copy of the config.xml. Thank you very much!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why clickhouse-local requires config.xml access in version 21.4.6.55",
      "Resolution for file permission errors without requiring system-wide config changes",
      "Guidance on proper environment setup to avoid unintended config file detection"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:52:06"
    }
  },
  {
    "number": 24575,
    "title": "Question about shared_ptr_helper",
    "created_at": "2021-05-27T13:44:21Z",
    "closed_at": "2021-05-27T23:41:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24575",
    "body": "I have a question about this template class.\r\n```c++\r\n/** Allows to make std::shared_ptr from T with protected constructor.\r\n  *\r\n  * Derive your T class from shared_ptr_helper<T> and add shared_ptr_helper<T> as a friend\r\n  *  and you will have static 'create' method in your class.\r\n  */\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\n\r\nMany places use this pattern\r\nclass StorageReplicatedMergeTree final : public ext::shared_ptr_helper<StorageReplicatedMergeTree>, public MergeTreeData\r\n{\r\n    friend struct ext::shared_ptr_helper<StorageReplicatedMergeTree>;\r\n..\r\n};\r\n\r\nBut I think the friend class is redundant.\r\n\r\nFor example\r\n\r\n#include <iostream>\r\n#include <memory>\r\n\r\nusing namespace std;\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\nclass A:public shared_ptr_helper<A>{\r\n    public:\r\n    int a;\r\n};\r\n\r\nint main(){\r\n    std::shared_ptr<A> aObj = A::create();\r\n    cout << aObj->a << endl;\r\n}\r\n```\r\n\r\nThis code also can use the create method. Here is the question, what does the friend class do in this pattern. \r\nThanks.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24575/comments",
    "author": "wangzhen11aaa",
    "comments": [
      {
        "user": "kitaisreal",
        "created_at": "2021-05-27T14:37:36Z",
        "body": "@wangzhen11aaa we make ext::shared_ptr_helper friend because we want to create protected or private constructors, but force clients to use shared_ptr_helper create method for object construction."
      },
      {
        "user": "wangzhen11aaa",
        "created_at": "2021-05-27T23:41:41Z",
        "body": "#include <iostream>\r\n#include <memory>\r\n\r\nusing namespace std;\r\ntemplate <typename T>\r\nstruct shared_ptr_helper\r\n{\r\n    template <typename... TArgs>\r\n    static std::shared_ptr<T> create(TArgs &&... args)\r\n    {\r\n        return std::shared_ptr<T>(new T(std::forward<TArgs>(args)...));\r\n    }\r\n};\r\nclass A:public shared_ptr_helper<A>{\r\n    friend shared_ptr_helper<A>;\r\n    public:\r\n    int a;\r\n    protected:\r\n        A()=default;\r\n};\r\n\r\nint main(){\r\n    std::shared_ptr<A> aObj = A::create();\r\n    cout << aObj->a << endl;\r\n}\r\nOK"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why friend declaration is required when using protected/private constructors",
      "Clarification of the pattern's purpose to enforce factory method usage",
      "Differentiation between public vs protected constructor scenarios"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:52:50"
    }
  },
  {
    "number": 24251,
    "title": " DB::Exception: Aggregate function sum(postition) is found inside another aggregate function in query: While processing sum(postition) AS postition",
    "created_at": "2021-05-18T14:52:41Z",
    "closed_at": "2021-05-22T10:53:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24251",
    "body": "Hi\uff0c\r\n    When I execute this query sql :\r\nSELECT\r\n    avg(postition) AS avg,\r\n    sum(postition) AS postition\r\nFROM system.columns;\r\nand the exception happened,which was:\r\nReceived exception from server (version 21.4.4):\r\nCode: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function sum(postition) is found inside another aggregate function in query: While processing sum(postition) AS postition. \r\n\r\nBut this sql can run correctly in MySQL. This is Clickhouse's  special syntax ? \r\n        Thanks.\r\n                     Best Regards.\r\n                             Eward\r\n ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24251/comments",
    "author": "cwh2008",
    "comments": [
      {
        "user": "amosbird",
        "created_at": "2021-05-19T17:17:45Z",
        "body": "You can set  `prefer_column_name_to_alias = 1`."
      },
      {
        "user": "cwh2008",
        "created_at": "2021-05-22T10:55:07Z",
        "body": "Hi\uff0camosbird. Thanks a lot.\r\nYour solution is the key to this quetion."
      }
    ],
    "satisfaction_conditions": [
      "Resolve the column alias conflict between aggregate functions in ClickHouse",
      "Maintain compatibility with MySQL-style alias usage in aggregate functions",
      "Explain ClickHouse's specific requirements for aggregate function usage"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:53:00"
    }
  },
  {
    "number": 24034,
    "title": "how to pass client settings such like max_memory_useage for  http query",
    "created_at": "2021-05-12T01:26:43Z",
    "closed_at": "2021-05-12T05:12:15Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24034",
    "body": "in some cases,i can only run select  query via http interface ,like curl or wget,but some times it will out of memory limit\r\n\r\nhow to set these client settings just like in the clickhouse-client when using http interface? \r\n\r\n\r\nby setting  a small value in settings at the end of  select  query,it shows Memory limit (for query) exceeded\r\n\r\n\r\nSELECT\r\n    CAST('2021-05-10', 'date') AS log_date,\r\n    countDistinct(map_uid) AS uid_cnt\r\nFROM\r\n(\r\n    SELECT\r\n        map_uid,\r\n        countDistinct(type) AS type_cnt\r\n    FROM\r\n    (\r\n        SELECT\r\n            map_uid,\r\n            if(os_type = 'Mobile', 'mobile', 'desktop') AS type\r\n        FROM login\r\n        WHERE (toDate(log_time) >= subtractDays(CAST('2021-05-10', 'date'), 28)) AND (toDate(log_time) <= subtractDays(CAST('2021-05-10', 'date'), 1))\r\n        GROUP BY\r\n            map_uid,\r\n            type\r\n    )\r\n    GROUP BY map_uid\r\n)\r\nWHERE type_cnt = 2\r\nGROUP BY log_date\r\nSETTINGS max_memory_usage = 20\r\n\r\n\u2193 Progress: 12.01 million rows, 156.14 MB (37.41 million rows/s., 486.35 MB/s.)  0%\r\nReceived exception from server (version 20.9.3):\r\nCode: 241. DB::Exception: Received from  DB::Exception: Received from clickhouse_node4_4_1:9000. DB::Exception: Memory limit (for query) exceeded: would use 4.22 MiB (attempt to allocate chunk of 4421564 bytes), maximum: 20.00 B.\r\n\r\n\r\n\r\nbut i change the  max_memory_usage value to a very big value such as 10000000000000000, it doesn't work.\r\n\r\nfinaly it show the errors:\r\nmemory limit Received exception from server (version 20.9.3):\r\nCode: 241. DB::Exception: Received from 1 DB::Exception: Memory limit (for query) exceeded: would use 9.38 GiB (attempt to allocate chunk of 133939184 bytes), maximum: 9.31 GiB: While executing AggregatingTransform.\r\n\r\n\r\nand i use set max_memory_usage=10000000000000;  then run the query it works.\r\n\r\nso ,how to set the client settings via http interface ,pls help thx.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24034/comments",
    "author": "windylcx",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2021-05-12T02:10:07Z",
        "body": "In URL parameter:\r\n\r\n`...&max_memory_usage=10000000000000`"
      },
      {
        "user": "windylcx",
        "created_at": "2021-05-12T03:57:54Z",
        "body": "> In URL parameter:\r\n> \r\n> `...&max_memory_usage=10000000000000`\r\n\r\nit works, thank you !"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to pass client settings through HTTP interface parameters rather than query-level SETTINGS clause",
      "Clarification that settings must be applied at session/request level rather than query execution level",
      "Demonstration of parameter syntax that works with various HTTP clients (curl/wget)",
      "Confirmation that settings can override server-side memory limits when properly configured"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:53:26"
    }
  },
  {
    "number": 24014,
    "title": "How to append an element after each array in a 2-D Array",
    "created_at": "2021-05-11T09:36:23Z",
    "closed_at": "2021-05-14T06:50:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/24014",
    "body": "Suppose we have two arrays with the same length.  \r\nA = [[1,2],[3,4]].  \r\nB = [5,6].  \r\nIs there a easy way to get a new array C = [[1,2,5],[3,4,6]]?    \r\n     \r\nGenerally,   \r\nif A is with a shape of (n,a) and B is with a shape of (n,b),    \r\ncan we get a new array C with a shape of (n,a+b)?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/24014/comments",
    "author": "JIANCHUJUN",
    "comments": [
      {
        "user": "l1t1",
        "created_at": "2021-05-11T11:20:03Z",
        "body": "arrayMap(x,y->arrayConcat(x,[y]), [[1,2],[3,4]],[5,6])\n\n---\n\nselect arrayMap(x,y->arrayPushBack(x,y), [[1,2],[3,4]],[5,6])"
      }
    ],
    "satisfaction_conditions": [
      "Element-wise combination of corresponding sub-arrays from A and elements from B",
      "Preservation of array structure with correct dimension expansion",
      "Handling of arrays with matching first-dimension length (n)",
      "General applicability beyond the specific example"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:53:32"
    }
  },
  {
    "number": 22985,
    "title": "Conflicts and unexpected result of CAST in where condition",
    "created_at": "2021-04-12T04:22:14Z",
    "closed_at": "2021-04-12T08:04:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22985",
    "body": "Dear authors,\r\n\r\nIn my case, values of colomn _note_ is always **NULL**, but got unexpected query result:\r\n\r\nquery: `select record_id, note from demo where CAST(note AS DECIMAL(18,0)) is not null limit 3;`\r\nresult is:\r\n```\r\n\u250c\u2500record_id\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500note\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 2_120000_120000 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2502 2_120000_120001 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2502 2_120000_120002 \u2502 \u1d3a\u1d41\u1d38\u1d38          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nI think the result should be 0 rows, but got 3? Obviously, it is different from Mysql.\r\n\r\nAnd then I try to find out the value of cast by:\r\nquery: `select CAST(note AS DECIMAL(18,0)), record_id from demo where CAST(note AS DECIMAL(18,0)) is not null limit 3;`\r\nbut got: `DB::Exception: Cannot convert NULL value to non-Nullable type`\r\n\r\nSo if the convertion would fail, why it is not blocked by cast condition in advance?\r\nand vise versa, If where-cast condition tells it is not null, why the convertion failed, it should assign certain value to results.\r\n\r\nIf it's not a bug, I think there must be some reasons for the design of this feature.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22985/comments",
    "author": "SE2AI",
    "comments": [
      {
        "user": "SE2AI",
        "created_at": "2021-04-12T07:15:17Z",
        "body": "I try with Nullable type, it solves the problem\r\n\r\n`select record_id, note from demo where CAST(note AS Nullable(DECIMAL(18,0))) is not null limit 3;`"
      },
      {
        "user": "UnamedRus",
        "created_at": "2021-04-12T07:48:49Z",
        "body": "There is setting for that:\r\n\r\n```\r\nset cast_keep_nullable=1;\r\n```"
      },
      {
        "user": "SE2AI",
        "created_at": "2021-04-12T08:04:23Z",
        "body": "@UnamedRus Thanks, it's more convenient, and really a flexible way."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of NULL handling during CAST operations in ClickHouse",
      "Description of type conversion rules for NULL values",
      "Documentation of CAST function's nullability preservation",
      "Explanation of ClickHouse settings affecting type conversion behavior",
      "Comparison with MySQL's NULL handling behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:54:05"
    }
  },
  {
    "number": 22269,
    "title": "insert into select from hdfs engine table can be parallel ?",
    "created_at": "2021-03-29T08:11:17Z",
    "closed_at": "2021-03-30T03:33:43Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22269",
    "body": "",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22269/comments",
    "author": "gj-zhang",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2021-03-29T09:46:43Z",
        "body": "Hi!\r\nRead from `hdfs` will be parallel if you read from several files. Reading from single hdfs file is in single thread so far.\r\n`insert select` is parallel when `max_insert_threads` is more then 1.\r\nYou need both for your case."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how HDFS file count impacts read parallelism",
      "Identification of configuration parameters controlling insert parallelism",
      "Clarification that both read and write parallelism must be addressed"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:54:29"
    }
  },
  {
    "number": 22267,
    "title": " clickhouse-client --format_csv_delimiter='@@@' or --format_csv_delimiter=$'\\@\\@\\@' got Exception",
    "created_at": "2021-03-29T06:46:29Z",
    "closed_at": "2021-03-29T06:52:30Z",
    "labels": [
      "question",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22267",
    "body": "(you don't have to strictly follow this form)\r\n\r\nimport csv data into database ,like this\r\nclickhouse-client --format_csv_delimiter='@@@' --query=\"insert into default.tb_name select col1,col2,col3 from file('csv_file_name.csv','CSVWithNames','col1 String,col2 String,col3 String')\"\r\n\r\ngot \r\nCode: 19. DB::Exception: A setting's value string has to be an exactly one character long\r\n\r\nhow to translate Symbol @ ,I try \\@ not working too.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22267/comments",
    "author": "DreamUFO",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2021-03-29T06:49:45Z",
        "body": "Just like exception says: CSV supports only single character separators (i.e. single `@` could be ok, but `@@@` - not). \r\n\r\nYou can try to use `format Template`, `format  Regexp` or just preprocess your input with smth like `sed`\r\n\r\n"
      },
      {
        "user": "DreamUFO",
        "created_at": "2021-03-29T06:52:26Z",
        "body": "ok,thanks.I got.\r\nI'll deal with multi Symbol delimiter befor import."
      }
    ],
    "satisfaction_conditions": [
      "Support for multi-character CSV delimiters in ClickHouse data import",
      "Alternative data processing methods compatible with ClickHouse",
      "Clear explanation of ClickHouse's CSV format constraints"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:54:37"
    }
  },
  {
    "number": 22141,
    "title": "Top N of unique string",
    "created_at": "2021-03-25T16:33:28Z",
    "closed_at": "2021-04-06T09:28:10Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/22141",
    "body": "Hi,\r\n\r\nIn the main table, there is a string column for `ip` which is unique (mostly) per document. I want to return the top 10 IPs for this table (few billions of documents). Is there any performant way to do so?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/22141/comments",
    "author": "hatrena",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-25T16:39:46Z",
        "body": "What do you mean top10 if they are uniq? "
      },
      {
        "user": "hatrena",
        "created_at": "2021-03-25T16:44:48Z",
        "body": "to return something like:\r\n\r\n```\r\n\u250c\u2500\u2500\u2500count()\u2500\u252c\u2500click_ip\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 10024205 \u2502 47.253.44.xxx \u2502\r\n\u2502  9929538 \u2502 47.253.32.xxx \u2502\r\n\u2502  9927342 \u2502 47.253.32.xxx \u2502\r\n\u2502  9886397 \u2502 47.90.248.xxx \u2502\r\n\u2502  9876835 \u2502 47.253.33.xxx \u2502\r\n\u2502  9866026 \u2502 47.253.40.xxx \u2502\r\n\u2502  9850891 \u2502 47.253.44.xxx \u2502\r\n\u2502  9832420 \u2502 47.89.183.xxx \u2502\r\n\u2502  9830460 \u2502 47.253.47.xxx \u2502\r\n\u2502  9763984 \u2502 47.252.11.xxx \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n---\n\nsomething similar to `topK(N)` with count as well but performant. "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-04-05T14:32:54Z",
        "body": "Naive solution is:\r\n\r\n```\r\nSELECT click_ip, count() AS c FROM table GROUP BY click_ip ORDER BY c DESC LIMIT 10\r\n```\r\n\r\nIt may not fit in memory, but you can raise `max_memory_usage` and enable `max_bytes_before_external_group_by`.\r\nThen the query will be able to proceed even with multiple billion records. But it can be slow.\r\n\r\nOptimized variant is:\r\n\r\n```\r\nSELECT click_ip, count() AS c FROM table\r\nWHERE click_ip IN\r\n(\r\n    SELECT click_ip FROM table ORDER BY rand() LIMIT 1000000\r\n)\r\nGROUP BY click_ip ORDER BY c DESC LIMIT 10\r\n```\r\n\r\nIt relies on assumption that top 10 IP addresses most likely will be present in random sample of a million records from a table. It's not guaranteed to be true but it is almost always true.\r\n\r\nThis query can be slightly more optimal:\r\n\r\n```\r\nSELECT click_ip, count() AS c FROM table\r\nWHERE click_ip IN\r\n(\r\n    SELECT click_ip FROM table WHERE rand() % 10000 = 123\r\n)\r\nGROUP BY click_ip ORDER BY c DESC LIMIT 10\r\n```\r\n\r\nAnd if data in your table is uniformly distributed, you can also use this trick:\r\n```\r\nSET max_rows_to_group_by = 1000000, group_by_overflow_mode = 'any';\r\nSELECT click_ip, count() AS c FROM table GROUP BY click_ip ORDER BY c DESC LIMIT 10;\r\n```\r\n"
      },
      {
        "user": "hatrena",
        "created_at": "2021-04-06T07:19:47Z",
        "body": "The average response from the naive solution is `10 rows in set. Elapsed: 15.929 sec. Processed 204.23 million rows, 20.28 GB (12.82 million rows/s., 1.27 GB/s.)` which is much better than the other solutions somehow. those take minutes."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-04-06T09:28:10Z",
        "body": "Great!\r\n\r\nBy tuning the constants of the optimized variants (with subqueries) you can get better response time."
      }
    ],
    "satisfaction_conditions": [
      "Solution must efficiently process billions of records without prohibitive memory usage",
      "Approach must provide exact count statistics for IP frequency ranking",
      "Method must maintain acceptable performance on very large datasets",
      "Solution should work with standard SQL patterns unless optimization is clearly justified",
      "Approach must handle potential memory constraints through configuration or optimization"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:54:52"
    }
  },
  {
    "number": 21575,
    "title": "How to import csv file with  delimiter character ascll ",
    "created_at": "2021-03-10T03:20:15Z",
    "closed_at": "2021-03-10T06:51:30Z",
    "labels": [
      "question",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21575",
    "body": "How to import csv file with ascll delimiter character, example:\r\n\r\nLO\u0003OG_NFO\u0003110\u0003OU07\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nLN\u0003OG_NFO\u0003110\u0003OU0705\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nAN\u0003OG_NFO\u0003110\u0003OU075\u00030\u0003\u00032014-03-21-01.57.30.000000\r\nLN\u0003OG_FO\u0003110\u0003OU005\u00030\u0003\u00032014-03-21-01.57.30.000000\r\n\r\ndelimiter character \"\u0003\" is \"0X03\",\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21575/comments",
    "author": "joakapp",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-10T04:28:58Z",
        "body": "--format_csv_delimiter=$'\\x03'"
      },
      {
        "user": "joakapp",
        "created_at": "2021-03-10T06:51:30Z",
        "body": "Thanks a lot"
      }
    ],
    "satisfaction_conditions": [
      "Support for specifying non-printable ASCII characters as delimiters",
      "Compatibility with hexadecimal character representations",
      "Full-field parsing with specified delimiter"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:55:36"
    }
  },
  {
    "number": 21503,
    "title": "Date value is inconsistency between format",
    "created_at": "2021-03-07T09:14:42Z",
    "closed_at": "2021-03-08T06:01:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21503",
    "body": "I have a schema:\r\n```sql\r\nCREATE TABLE test(\r\n `field1` Int64,\r\n `field2` Int32,\r\n `field3` String,\r\n `field4` String,\r\n `field5` String,\r\n `field6` Int32,\r\n `field7` Int64,\r\n `field8` String,\r\n `time` Int64,\r\n `field9` Int64,\r\n `field10` Int64,\r\n `field11` Int32,\r\n `field12` String,\r\n `field13` Int32,\r\n `field14` Int32,\r\n `field15` Int64,\r\n `field16` Int64,\r\n `field17` Int64,\r\n `field18` Int64,\r\n `field19` String,\r\n `field20` String,\r\n `field21` Int64,\r\n `field22` Int32,\r\n `field23` String,\r\n `field24` Int32,\r\n `field25` Int32,\r\n `field26` Int64,\r\n `field27` Int64,\r\n `field28` Int64,\r\n `field29` Int64,\r\n `field30` String,\r\n `field31` String,\r\n `date_time` DateTime64\r\n) ENGINE =  ReplicatedMergeTree('/data/clickhouse/replicated/test_repl', 'replica_1')\r\nPARTITION BY toYYYYMM(date_time)\r\nORDER BY (time, field1, field2) SETTINGS index_granularity = **8192**\r\n```\r\nI perform SQL:\r\n\r\n```sql\r\nSELECT toDate(date_time), toYYYYMM(date_time), date_time FROM test\r\nWHERE toYYYYMM(date_time) BETWEEN 202101 AND 202102\r\nAND voucher_id=123456789;\r\n```\r\nThe result:\r\n| toDate      | toYYYYMM | date_time\r\n| ----------- | ----------- | ----------- |\r\n| 2021-02-01      | 202102       | 2021-01-31T17:00:00+00:00\r\n\r\nI confused about this result. Why toDate and toYYYYMM different  date_time? \r\nI expected toDate: 2021-31-01 and toYYYYMM: 202001.\r\n \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21503/comments",
    "author": "phamtai97",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-03-07T13:35:56Z",
        "body": "This result is from JDBC application? Right?\r\n\r\nCan you show the result of\r\n```\r\nselect timezone(), now(), toString(now());\r\n```"
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T14:11:57Z",
        "body": "The result of your SQL:\r\n\r\n|timezone()| now() |toString(now())|\r\n| ----------- | ----------- | ----------- |\r\n|Asia/Ho_Chi_Minh   | 2021-03-07T14:08:03+00:00       | 2021-03-07 21:08:03\r\n\r\n\r\n\n\n---\n\nI use lib:\r\n\r\n```pom\r\n<dependency>\r\n    <groupId>ru.yandex.clickhouse</groupId>\r\n    <artifactId>clickhouse-jdbc</artifactId>\r\n</dependency>\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-07T15:19:00Z",
        "body": "What tool do use to query data using JDBC ? DataGrip?\r\nWhat timezone at you local computer ? Windows/MACos -- Asia/Ho_Chi_Minh ?"
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T17:13:13Z",
        "body": "I develop aplication by Java anh use lib Clickhouse JDBC. Then, I deploy app on Linux server.\n\n---\n\nI try query this SQL on superset tool on Mac PC, the result is not change.\n\n---\n\nSo how the data is actually stored? Is it because lib JDBC has converted the time to the correct local time?\r\nWhat SQL do I have to use to compare the date_time properly?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-03-07T18:04:00Z",
        "body": ">So how the data is actually stored? \r\n\r\nit depends on how you insert data.\r\n\r\n>Is it because lib JDBC has converted the time to the correct local time?\r\n\r\nyes. JDBC converts datetime to the local TZ. It should work OK if you run JAVA app at the server with Asia/Ho_Chi_Minh.\r\n\r\n>What SQL do I have to use to compare the date_time properly?\r\n\r\nWHERE date_time >= toDateTime('2021-01-01 00:00:00', 'Asia/Ho_Chi_Minh') \r\n       AND date_time < toDateTime('2021-01-03 00:00:00', 'Asia/Ho_Chi_Minh')\r\n\r\n\r\n"
      },
      {
        "user": "phamtai97",
        "created_at": "2021-03-07T23:10:36Z",
        "body": "Thank you, I understand this issue."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of timezone handling in ClickHouse date/time functions",
      "Guidance on proper timezone-aware datetime comparisons",
      "Clarification of storage vs display formats for DateTime64",
      "Explanation of JDBC client timezone behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:55:50"
    }
  },
  {
    "number": 21099,
    "title": "ALTER TABLE UPDATE referencing field from table being updated",
    "created_at": "2021-02-23T09:31:42Z",
    "closed_at": "2022-04-11T15:12:16Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/21099",
    "body": "Hello. Thanks for the amazing software, I \u2764\ufe0f Clickhouse.\r\n\r\nWe had a bug in our app code that generated some bad data in the sessions table. I need to regenerate the `exit_page` field for all the historical data in our sessions table (CollapsingMergeTree).\r\n\r\nOur table layout is simpler but similar to the Yandex.Metrica example. Some names are different but you should get the idea `hits -> events, visits -> sessions`. Not that much data yet, less than a billion rows in the sessions table.\r\n\r\nComing from regular SQL, this was my first instinct:\r\n```sql\r\nALTER TABLE sessions UPDATE exit_page=(SELECT anyLast(pathname) FROM events WHERE events.session_id=sessions.session_id ORDER BY timestamp);\r\n```\r\nbut I get the following error:\r\n```\r\nCode: 47. DB::Exception: Received from clickhouse-server:9000. DB::Exception: Missing columns: 'sessions.session_id' while processing query: 'SELECT anyLast(pathname) FROM plausible_dev.events WHERE (domain = 'localtest.me') AND (session_id = sessions.session_id)', required columns: 'domain' 'pathname' 'session_id' 'sessions.session_id', source columns: [...]\r\n```\r\n\r\nIn regular SQL I am used to being able to reference columns from the row that I'm updating. I realize Clickhouse has very different semantics for updating and this might not be supported. Any other ideas how one might go about updating a field like this?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099/comments",
    "author": "ukutaht",
    "comments": [
      {
        "user": "ILYenBui",
        "created_at": "2022-04-10T03:26:33Z",
        "body": "Hi,\r\n\r\nClickHouse does not support updating the table's column like that. However, there is a workaround solution that you can have a look at:\r\n\r\n```\r\nCREATE TABLE exit_pages (session_id UInt64, exit_page String) Engine = Join(ANY, LEFT, session_id);\r\n\r\nINSERT INTO exit_pages SELECT session_id, anyLast(pathname) as exit_page from events group by session_id;\r\n\r\nALTER TABLE sessions \r\nUPDATE exit_page = joinGet('exit_pages', 'exit_page', session_id);\r\n\r\nDROP TABLE exit_pages; \r\n```\r\n"
      },
      {
        "user": "ukutaht",
        "created_at": "2022-04-11T15:12:16Z",
        "body": "Thanks! That's exactly what I ended up doing, wasn't too painful"
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable cross-table correlation using session_id",
      "Must provide a way to compute exit_page from events data",
      "Must work within ClickHouse's ALTER TABLE UPDATE limitations",
      "Solution should handle ~1B rows efficiently"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:56:19"
    }
  },
  {
    "number": 20612,
    "title": "Exception: Nested type Array(String) cannot be inside Nullable type (version 20.9.2.20 (official build))",
    "created_at": "2021-02-17T09:36:32Z",
    "closed_at": "2021-02-23T11:44:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/20612",
    "body": "i was created table:\r\nCREATE TABLE compare.test1\r\n(\r\n\tevent Nullable(String)\r\n) \r\nENGINE = MergeTree ORDER BY tuple();\r\n\r\nwhen i have construction as \r\nSELECT\r\n\tJSONExtractString(event, 'event_time') as s\r\n\t,event\r\n\t,splitByChar('T', JSONExtractString(event, 'event_time'))[1]\r\nfrom compare.test1\r\n\r\ni have error:\r\nException: Nested type Array(String) cannot be inside Nullable type (version 20.9.2.20 (official build))\r\n\r\nWhen in table i implement event String insted of Nullable(String) it's ok, i don't have any error",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/20612/comments",
    "author": "zav379",
    "comments": [
      {
        "user": "vdimir",
        "created_at": "2021-02-17T10:42:09Z",
        "body": "Now `Nullabe(String)` not supported by function `splitByChar`. Workaround is to covert `Nullable(String)` into `String` e.g. with `COALESCE` and (maybe you also want to add `WHERE event is not NULL`) :\r\n```\r\nSELECT\r\n    JSONExtractString(event, 'event_time') AS s,\r\n    event,\r\n    splitByChar('T', JSONExtractString(COALESCE(event, ''), 'event_time'))[1]\r\nFROM test1\r\nWHERE event is not NULL\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-17T14:36:43Z",
        "body": "splitByChar('T', assumeNotNull( ... ) ) "
      },
      {
        "user": "zav379",
        "created_at": "2021-02-18T09:19:11Z",
        "body": "thanks "
      },
      {
        "user": "BlackSinny",
        "created_at": "2022-01-26T03:52:56Z",
        "body": "when i run \r\n\r\n```sql\r\nALTER table ${databaseName} ON CLUSTER ${tmp } ADD COLUMN asset_get_id_list Nullable(Array(String)) DEFAULT NULL;\r\n```\r\n\r\nreceive\r\n```sql\r\nCode: 43. DB::Exception: Nested type Array(String) cannot be inside Nullable type. (ILLEGAL_TYPE_OF_ARGUMENT\r\n```\r\n\r\ncan you  help me...."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Nullable(String) causes incompatibility with array-returning functions",
      "Method to safely convert Nullable(String) to String before array operations",
      "Handling of potential null values before array processing",
      "Clarification of ClickHouse's type nesting restrictions"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:56:33"
    }
  },
  {
    "number": 20471,
    "title": "parseDateTimeBestEffortUS doesn't support OrNull modifier",
    "created_at": "2021-02-13T22:35:55Z",
    "closed_at": "2021-02-13T22:53:46Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/20471",
    "body": "Works:\r\n```sql\r\nselect parseDateTimeBestEffortUS('30/01/2021')\r\n```\r\nWorks:\r\n```sql\r\nselect parseDateTimeBestEffortOrNull('30/01/2021')\r\n```\r\nDoesn't work:\r\n```sql\r\nselect parseDateTimeBestEffortUSOrNull('30/01/2021')\r\n```\r\n> DB::Exception: Unknown function parseDateTimeBestEffortUSOrNull. Maybe you meant: ['parseDateTimeBestEffortOrNull', 'parseDateTime64BestEffortOrNull']: While processing parseDateTimeBestEffortUSOrNull('30/01/2021')",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/20471/comments",
    "author": "stas-sl",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-02-13T22:53:46Z",
        "body": "```\r\nSELECT parseDateTimeBestEffortUSOrNull('30/01/2021')\r\n\r\nQuery id: fcbb75e7-8b59-47c1-b6c8-3db067ad65cb\r\n\r\n\u250c\u2500parseDateTimeBestEffortUSOrNull('30/01/2021')\u2500\u2510\r\n\u2502                           2021-01-30 00:00:00 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\n\n---\n\nClickHouse release v21.2.2.8-stable, 2021-02-07\r\nAdded functions parseDateTimeBestEffortUSOrZero, parseDateTimeBestEffortUSOrNull. #19712 (Maksim Kita)."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that parseDateTimeBestEffortUSOrNull is supported in specific ClickHouse versions",
      "Clarification that the OrNull modifier is available for US date format parsing functions",
      "Guidance on accessing the function through version upgrades or configuration changes"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:56:41"
    }
  },
  {
    "number": 19878,
    "title": "Altering Enum in ordering key doesn't work on ReplicatedMergeTree",
    "created_at": "2021-01-31T11:07:29Z",
    "closed_at": "2021-02-02T08:54:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19878",
    "body": "**Describe the bug**\r\nReproduces at 20.8.12.2 Clickhouse version.\r\n\r\n```\r\nCREATE TABLE t ON CLUSTER replicated\r\n(\r\n    `a` UInt64,\r\n    `b` Enum8('a' = 1, 'b' = 2)\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/default/{shard}/t', '{replica}')\r\nORDER BY (b, a)\r\n\r\nINSERT INTO t VALUES (1,1),(2,2)\r\n\r\nALTER TABLE t ON CLUSTER replicated MODIFY COLUMN b Enum8('a' = 1)\r\n```\r\nThe last statement falls into error: `Cannot execute replicated DDL query, maximum retires exceeded`\r\n\r\nIn case of local alter: \r\n`ALTER TABLE t ON CLUSTER replicated MODIFY COLUMN b Enum8('a' = 1)`\r\n\r\nthe error is next:\r\n```\r\nReceived exception from server (version 20.8.12):\r\nCode: 524. DB::Exception: Received from localhost:9100. DB::Exception: ALTER of key column b from type Enum8('a' = 1, 'b' = 2) to type Enum8('a' = 1) must be metadata-only.\r\n```\r\n\r\n\r\n**Expected behavior**\r\nThe same as on non-Replicated MergeTree",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19878/comments",
    "author": "Blackmorse",
    "comments": [
      {
        "user": "alesapin",
        "created_at": "2021-02-01T13:23:52Z",
        "body": "Hi!\r\n\r\nAlter of the key columns in ClickHouse is quite limited. You can alter enums but only add new values. Removing old values can change the representation of the primary key. \r\n```\r\n:) ALTER TABLE t\r\n    MODIFY COLUMN `b` Enum8('a' = 1)\r\n\r\nReceived exception from server (version 21.2.1):\r\nCode: 524. DB::Exception: Received from localhost:9000. DB::Exception: ALTER of key column b from type Enum8('a' = 1, 'b' = 2) to type Enum8('a' = 1) is not safe because it can change the representation of primary key. \r\n```\r\nbut the extension is allowed:\r\n```\r\n\r\nALTER TABLE t\r\n    MODIFY COLUMN `b` Enum8('a' = 1, 'b' = 2, 'c' = 3)\r\n\r\nOk.\r\n\r\n```\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-01T14:03:43Z",
        "body": "> The same as on non-Replicated MergeTree\r\n\r\nBut how it's related to Replicated / non-Replicated MergeTree?"
      },
      {
        "user": "Blackmorse",
        "created_at": "2021-02-01T16:31:19Z",
        "body": "@alesapin , \r\n\r\n>  You can alter enums but only add new values\r\n\r\nBut documentation says:\r\n\r\n> The Enum type can be changed without cost using ALTER, if only the set of values is changed. It is possible to both add and remove members of the Enum using ALTER (removing is safe only if the removed value has never been used in the table)\r\n\r\n\n\n---\n\n@alexey-milovidov \r\n\r\n> But how it's related to Replicated / non-Replicated MergeTree?\r\n\r\nSorry, I was sure that the same operation works on simple MergeTree(). My bad"
      },
      {
        "user": "alesapin",
        "created_at": "2021-02-02T08:27:27Z",
        "body": "> @alesapin ,\r\n> \r\n> > You can alter enums but only add new values\r\n> \r\n> But documentation says:\r\n> \r\n> > The Enum type can be changed without cost using ALTER, if only the set of values is changed. It is possible to both add and remove members of the Enum using ALTER (removing is safe only if the removed value has never been used in the table)\r\n\r\nAs the documentation says:\r\n`> removing is safe only if the removed value has never been used in the table`\r\n\r\nWe restrict unsafe operations for the primary key columns because it can lead not only to the single-column corruption but to the whole table became broken."
      },
      {
        "user": "Blackmorse",
        "created_at": "2021-02-02T08:41:03Z",
        "body": "@alesapin , So , It is correct behaviour, right?"
      },
      {
        "user": "alesapin",
        "created_at": "2021-02-02T08:43:21Z",
        "body": "> @alesapin , So , It is correct behaviour, right?\r\n\r\nYes, it's expected, maybe we have to improve docs."
      },
      {
        "user": "Blackmorse",
        "created_at": "2021-02-02T08:54:35Z",
        "body": "Thanks"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why altering Enum values in ordering keys is restricted for ReplicatedMergeTree",
      "Clarification of safe vs unsafe Enum modifications in primary/ordering keys",
      "Differentiation between metadata-only alters and data-modifying alters for key columns"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:57:17"
    }
  },
  {
    "number": 19797,
    "title": "Is it possible to disable using timezone and set only UTC for DateTime64 format?",
    "created_at": "2021-01-29T07:18:26Z",
    "closed_at": "2021-01-29T13:01:41Z",
    "labels": [
      "question",
      "comp-datetime"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19797",
    "body": "\r\nIs it possible to disable using timezone and set only UTC for DateTime64 format?\r\n\r\nThanks.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19797/comments",
    "author": "lessenko",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2021-01-29T11:57:22Z",
        "body": "You can just set up default timezone to UTC system-wide, or in clickhouse configuration. "
      },
      {
        "user": "lessenko",
        "created_at": "2021-01-29T13:01:41Z",
        "body": "@filimonov,\r\nThank you. It can be closed. "
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates how to configure UTC as the default timezone for DateTime64 fields without per-column specifications",
      "Ensures DateTime64 operations use UTC implicitly without timezone conversions",
      "Applies to the entire ClickHouse instance or configuration level"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:57:22"
    }
  },
  {
    "number": 19315,
    "title": "ALTER DELETE not working",
    "created_at": "2021-01-20T13:15:53Z",
    "closed_at": "2021-08-17T07:27:13Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19315",
    "body": "I have a table that I want to mutate using the following query:\r\n\r\n```sql\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE (ProjectId, UserId, SessionId) NOT IN (\r\n        SELECT ProjectId, UserId, SessionId\r\n        FROM clarity.page_data_enrich\r\n        GROUP BY ProjectId, UserId, SessionId\r\n        HAVING argMax(IsFavorite, RowVersion)\r\n    );\r\n```\r\n\r\nWhen I run the query, I get the following exception in `system.mutations` table:\r\n\r\n```\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_10.txt\r\ncommand:                    DELETE WHERE (ProjectId, UserId, SessionId) NOT IN (SELECT ProjectId, UserId, SessionId FROM clarity.page_data_enrich GROUP BY ProjectId, UserId, SessionId HAVING argMax(IsFavorite, RowVersion))\r\ncreate_time:                2021-01-20 15:08:28\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [10]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-01-20 15:08:30\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 20.12.5.14 (official build))\r\n```\r\n\r\nEven though the following query runs with no problem:\r\n\r\n```sql\r\nSELECT *\r\nFROM clarity.page_data\r\nWHERE (ProjectId, UserId, SessionId) NOT IN\r\n(\r\n    SELECT ProjectId, UserId, SessionId\r\n    FROM clarity.page_data_enrich\r\n    GROUP BY ProjectId, UserId, SessionId\r\n    HAVING argMax(IsFavorite, RowVersion)\r\n)\r\n```\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500Timestamp\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500Date\u2500\u252c\u2500ProjectId\u2500\u252c\u2500UserId\u2500\u252c\u2500SessionId\u2500\u252c\u2500PageNum\u2500\u2510\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       1 \u2502\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       2 \u2502\r\n\u2502 2021-01-20 14:08:24 \u2502 2021-01-20 \u2502         2 \u2502      1 \u2502         1 \u2502       3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nI am not sure what is wrong with the `ALTER DELETE` query!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19315/comments",
    "author": "OmarBazaraa",
    "comments": [
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-03T15:17:43Z",
        "body": "I even tried to concatenate the fields instead of comparing tuples, but I get the exact same error:\r\n\r\n```sql\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE concat(toString(ProjectId), toString(UserId), toString(SessionId)) NOT IN (\r\n        SELECT concat(toString(ProjectId), toString(UserId), toString(SessionId))\r\n        FROM clarity.page_data_enrich\r\n        GROUP BY ProjectId, UserId, SessionId\r\n        HAVING argMax(IsFavorite, RowVersion)\r\n    );\r\n```\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\nORDER BY create_time DESC\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_16.txt\r\ncommand:                    DELETE WHERE concat(toString(ProjectId), toString(UserId), toString(SessionId)) NOT IN (SELECT concat(toString(ProjectId), toString(UserId), toString(SessionId)) FROM clarity.page_data_enrich GROUP BY ProjectId, UserId, SessionId HAVING argMax(IsFavorite, RowVersion))\r\ncreate_time:                2021-02-03 17:11:40\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [16]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-02-03 17:11:44\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 21.1.2.15 (official build))\r\n```\r\n\r\nIt's stating that the number of columns in section IN doesn't match!\r\n\r\nAny ideas what is going wrong?!"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-03T16:40:32Z",
        "body": "I think mutations are not designed to handle such `where subqueries`\r\n\r\nas a WA I would create a table Engine=Join and inserted into this Join table IDs which should be deleted using `insert select` \r\nthen run delete like this \r\n```\r\nALTER TABLE clarity.page_data DELETE\r\nWHERE joinHas(, , (ProjectId, serId, SessionId) )"
      },
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-04T13:37:39Z",
        "body": "Thanks @den-crane for your suggestion!\r\n\r\nI tried it but it's giving me the same error...\r\n\r\n```sql\r\nSELECT *\r\nFROM system.mutations\r\nORDER BY create_time DESC\r\nLIMIT 1\r\nFORMAT Vertical\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\ndatabase:                   clarity\r\ntable:                      page_data\r\nmutation_id:                mutation_17.txt\r\ncommand:                    DELETE WHERE isNotNull(joinGet('clarity.page_data_retained', 'RowVersion', ProjectId, UserId, SessionId))\r\ncreate_time:                2021-02-04 15:33:34\r\nblock_numbers.partition_id: ['']\r\nblock_numbers.number:       [17]\r\nparts_to_do_names:          ['202101_1_2_3']\r\nparts_to_do:                1\r\nis_done:                    0\r\nlatest_failed_part:         202101_1_2_3\r\nlatest_fail_time:           2021-02-04 15:33:52\r\nlatest_fail_reason:         Code: 20, e.displayText() = DB::Exception: Number of columns in section IN doesn't match. 3 at left, 1 at right. (version 21.1.2.15 (official build))\r\n```\r\n\r\nAny other possible alternatives to retain/TTL records based on values from other tables?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-02-04T14:35:41Z",
        "body": "@OmarBazaraa \r\n\r\nHMm, I think this error from the previous mutations.\r\nTry remove failed mutations first:\r\n\r\n```\r\nkill mutation where not is_done;\r\nALTER TABLE clarity.page_data DELETE WHERE joinHas(, , (ProjectId, serId, SessionId) )\r\n```"
      },
      {
        "user": "OmarBazaraa",
        "created_at": "2021-02-04T14:42:48Z",
        "body": "> Try remove failed mutations first\r\n\r\nThanks @den-crane, it worked!\r\n\r\nAnd what is more interesting now is that my original query is working now too without having to use `Join` table.\r\nAlso, the column `latest_fail_reason` of `system.mutations` table has been cleared, I can no longer find the old error messages."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the ALTER DELETE mutation fails with column mismatch error despite valid SELECT subquery",
      "Resolution for mutation conflicts caused by failed mutation attempts",
      "Working method to delete records based on values from another table",
      "Clarification on supported subquery patterns in ALTER DELETE mutations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:57:51"
    }
  },
  {
    "number": 19223,
    "title": "Clickhouse failed to start, permission denied",
    "created_at": "2021-01-18T03:02:17Z",
    "closed_at": "2021-01-18T06:58:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/19223",
    "body": "Hello\uff1a\r\nCentOS Linux release 7.9\r\nClickHouse server version 20.12.5.14 (official build).\r\nyum installed.\r\n\r\nwhen I use command \"systemctl start clickhouse-server\"   to start clickhouse , and Failed to start , \r\nclickhouse-server.err.log and clickhouse-server.log is empty .\r\n\r\nin /var/log/message\uff0cis\uff1a\r\nclickhouse-server: Processing configuration file '/etc/clickhouse-server/config.xml'.\r\nclickhouse-server: std::exception. Code: 1001, type: std::__1::__fs::filesystem::filesystem_error, e.what() = filesystem error: in posix_stat:  failed to determine attributes for the specified path: Permission denied [/etc/clickhouse-server/config.xml], Stack trace (when copying this message, always include the   \r\nlines below):\r\nclickhouse-server: 0. std::__1::system_error::system_error(std::__1::error_code, std::__1::basic_string<char, std::__1::char_traits<char>, std::    \r\n__1::allocator<char> > const&) @ 0x123f8d83 in ?\r\n\r\nI checked the directory and file permissions\uff0cis OK\r\n-rw-rw---- 1 clickhouse clickhouse 33809 Jan 18 09:06 config.xml\r\n\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 access\r\ndrwxr-x--- 10 clickhouse clickhouse 4096 Jan 18 09:30 data\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 format_schemas\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:30 log\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 tmp\r\ndrwxr-x---  2 clickhouse clickhouse 4096 Jan 18 09:25 user_files\r\n\r\nBut when I use command \u201c/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\u201d to start \uff0cis OK\r\n\r\nCan you take a look for me? Thank you very much.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/19223/comments",
    "author": "Goolen",
    "comments": [
      {
        "user": "zhangjmruc",
        "created_at": "2021-01-18T03:50:37Z",
        "body": "Would you please show us how you use command \u201c/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\u201d to start?\r\nNormally, you should only use clickhouse user to manually start clickhouse server. \r\nsudo -u clickhouse /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml"
      },
      {
        "user": "hexiaoting",
        "created_at": "2021-01-18T04:06:11Z",
        "body": "check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T05:33:22Z",
        "body": "> check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)\r\n\r\n# ll -d /etc/\r\ndrwxr-xr-x. 110 root root 8192 Jan 15 15:13 /etc/\r\n\r\n# ll -d /etc/clickhouse-server\r\ndrw-rw---- 4 clickhouse clickhouse 183 Jan 18 09:06 /etc/clickhouse-server\n\n---\n\n> Would you please show us how you use command \u201c/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\u201d to start?\r\n> Normally, you should only use clickhouse user to manually start clickhouse server.\r\n> sudo -u clickhouse /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml\r\n\r\n# ps aux | grep click\r\nroot     15751  0.0  0.0 112828  2292 pts/0    S+   13:30   0:00 grep --color=auto click\r\n# /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon\r\n# ps aux | grep click\r\nroot     15803  6.0  0.4 804000 133620 ?       DLsl 13:30   0:00 /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon\n\n---\n\n[root@db1074 ~]# ps aux | grep click\r\nroot     15984  0.0  0.0 112828  2196 pts/0    S+   13:32   0:00 grep --color=auto click\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# systemctl start clickhouse-server\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# ps aux | grep click              \r\nroot     16021  0.0  0.0 112828  2244 pts/0    S+   13:32   0:00 grep --color=auto click\n\n---\n\n[root@db1074 ~]# ps aux | grep click\r\nroot     15751  0.0  0.0 112828  2292 pts/0    S+   13:30   0:00 grep --color=auto click\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon\r\n[root@db1074 ~]# \r\n[root@db1074 ~]# ps aux | grep click\r\nroot     15803  6.0  0.4 804000 133620 ?       DLsl 13:30   0:00 /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --daemon\r\nroot     15809  0.0  0.0 112828  2240 pts/0    S+   13:30   0:00 grep --color=auto click\r\n[root@db1074 ~]# "
      },
      {
        "user": "hexiaoting",
        "created_at": "2021-01-18T06:07:10Z",
        "body": "> > check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)\r\n> \r\n> # ll -d /etc/\r\n> drwxr-xr-x. 110 root root 8192 Jan 15 15:13 /etc/\r\n> \r\n> # ll -d /etc/clickhouse-server\r\n> drw-rw---- 4 clickhouse clickhouse 183 Jan 18 09:06 /etc/clickhouse-server\r\n\r\nchown /etc to owner clickhouse, and use clickhouse users to start server"
      },
      {
        "user": "zhangjmruc",
        "created_at": "2021-01-18T06:26:17Z",
        "body": "change the permission for /etc/clickhouse-server to drwxr-xr-x, as blow:\r\ndrwxr-xr-x   4 root                   root                     4096 Oct 26 10:15 clickhouse-server/\r\n\r\nchmod 755 /etc/clickhouse-server\r\n\r\n=== clickhouse-server should be ran with user clickhouse.====\r\n$ systemctl start clickhouse-server\r\n~$ ps -ef | grep clickhouse\r\n**clickho+**  53126      1 48 14:24 ?        00:00:04 /usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --pid-file=/run/clickhouse-server/clickhouse-server.pid\r\n"
      },
      {
        "user": "Goolen",
        "created_at": "2021-01-18T06:55:22Z",
        "body": "> /etc/clickhouse-serv\r\n\r\nthank you first\r\nAfter changing the permissions to 755 , start is OK.\r\n\r\nThere is no permission to change this directory after installation\uff0cI don't know why it became 660.\r\n\r\n\r\n\r\n\r\n\r\n\n\n---\n\n> > > check `/etc` and `/etc/clickhouse-server` directories's permission(mode and owner)\r\n> > \r\n> > \r\n> > # ll -d /etc/\r\n> > drwxr-xr-x. 110 root root 8192 Jan 15 15:13 /etc/\r\n> > # ll -d /etc/clickhouse-server\r\n> > drw-rw---- 4 clickhouse clickhouse 183 Jan 18 09:06 /etc/clickhouse-server\r\n> \r\n> chown /etc to owner clickhouse, and use clickhouse users to start server\r\n\r\nThank you.\r\nThe problem has been solved\uff0creference @zhangjmruc \r\n\r\n"
      },
      {
        "user": "krafter",
        "created_at": "2023-03-26T06:19:22Z",
        "body": "setting WorkingDirectory=/<config_file_dir>/<owned_by_clickhouse_user>/ in systemctl service helped me"
      },
      {
        "user": "amolsr",
        "created_at": "2023-04-04T06:04:37Z",
        "body": "what is the password for clickhouse ubuntu user.? I don't have access to root user of the machine."
      }
    ],
    "satisfaction_conditions": [
      "Identify why systemctl fails to start ClickHouse despite correct file permissions",
      "Resolve directory permission requirements for ClickHouse systemd service",
      "Explain relationship between service user context and file/directory access",
      "Address configuration file accessibility in service runtime environment",
      "Validate service account (clickhouse user) has proper traversal permissions"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:58:01"
    }
  },
  {
    "number": 18855,
    "title": "Array function logical incorrectness.",
    "created_at": "2021-01-08T03:11:07Z",
    "closed_at": "2021-06-27T21:17:44Z",
    "labels": [
      "question",
      "unexpected behaviour"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18855",
    "body": "I'm testing a starting index from an array that is above a certain threshold and then monotonically increases in value.\r\nBut the same judgment logic gives two different results, which is a violation of consistency: A && B == B && A.\r\n```\r\nSELECT \r\n    uuid, \r\n    arrayFirstIndex((a, b) -> ((a >= 5) AND (arraySum(arrayMap(a -> if(a >= 5, 0, 1), arraySlice(data_f, b + 1, 4))) = 0))\r\n        , arraySlice(data_f, 1, length(data_f) - 3), range(toUInt64(length(data_f) - 3))) AS freq_idx\r\nFROM \r\n(\r\n    SELECT \r\n        'id1' AS uuid, \r\n        'p1' AS param, \r\n        [1, 2, 3, 4, 5, 4, 7, 8, 9, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] AS data_f\r\n    FROM system.one\r\n)\r\n\u250c\u2500uuid\u2500\u252c\u2500freq_idx\u2500\u2510\r\n\u2502 id1  \u2502        5 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT \r\n    uuid, \r\n    arrayFirstIndex((a, b) -> ((arraySum(arrayMap(a -> if(a >= 5, 0, 1), arraySlice(data_f, b + 1, 4))) = 0) AND (a >= 5))\r\n        , arraySlice(data_f, 1, length(data_f) - 3), range(toUInt64(length(data_f) - 3))) AS freq_idx\r\nFROM \r\n(\r\n    SELECT \r\n        'id1' AS uuid, \r\n        'p1' AS param, \r\n        [1, 2, 3, 4, 5, 4, 7, 8, 9, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] AS data_f\r\n    FROM system.one\r\n)\r\n\u250c\u2500uuid\u2500\u252c\u2500freq_idx\u2500\u2510\r\n\u2502 id1  \u2502       13 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18855/comments",
    "author": "ClownfishYang",
    "comments": [
      {
        "user": "UnamedRus",
        "created_at": "2021-01-08T04:05:33Z",
        "body": "If we would replace arrayMap(a...) with arrayMap(x...) the result would be consistent.\r\n\r\n```\r\n\r\nSELECT\r\n    uuid,\r\n    arrayFirstIndex((a, b) -> ((arraySum(arrayMap(x -> if(x >= 5, 0, 1), arraySlice(data_f, b + 1, 4))) = 0) AND (a >= 5)), arraySlice(data_f, 1, length(data_f) - 3), range(toUInt64(length(data_f) - 3))) AS freq_idx\r\nFROM\r\n(\r\n    SELECT\r\n        'id1' AS uuid,\r\n        'p1' AS param,\r\n        [1, 2, 3, 4, 5, 4, 7, 8, 9, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] AS data_f\r\n    FROM system.one\r\n)\r\n\r\nQuery id: c05a651e-473f-4961-9a35-f6eec3b8fccf\r\n\r\n\u250c\u2500uuid\u2500\u252c\u2500freq_idx\u2500\u2510\r\n\u2502 id1  \u2502       13 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nSELECT\r\n    uuid,\r\n    arrayFirstIndex((a, b) -> ((a >= 5) AND (arraySum(arrayMap(x -> if(x >= 5, 0, 1), arraySlice(data_f, b + 1, 4))) = 0)), arraySlice(data_f, 1, length(data_f) - 3), range(toUInt64(length(data_f) - 3))) AS freq_idx\r\nFROM\r\n(\r\n    SELECT\r\n        'id1' AS uuid,\r\n        'p1' AS param,\r\n        [1, 2, 3, 4, 5, 4, 7, 8, 9, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] AS data_f\r\n    FROM system.one\r\n)\r\n\r\nQuery id: 1f655d1a-e121-46e1-8abf-e68fcb811268\r\n\r\n\u250c\u2500uuid\u2500\u252c\u2500freq_idx\u2500\u2510\r\n\u2502 id1  \u2502       13 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\nSimpler Example \r\n\r\n```\r\nWITH [0, 1] AS arr\r\nSELECT arrayExists(x -> (x AND (arrayExists(x -> (NOT x), arr) > 0)), arr) AS x\r\n\r\n\u250c\u2500x\u2500\u2510\r\n\u2502 1 \u2502\r\n\u2514\u2500\u2500\u2500\u2518\r\n\r\nWITH [0, 1] AS arr\r\nSELECT arrayExists(x -> ((x = 1) AND (arrayExists(x -> (NOT (x = 1)), arr) > 0)), arr) AS x\r\n\r\n\u250c\u2500x\u2500\u2510\r\n\u2502 0 \u2502\r\n\u2514\u2500\u2500\u2500\u2518\r\n\r\nWITH [0, 1] AS arr\r\nSELECT arrayExists(x -> ((arrayExists(x -> (NOT (x = 1)), arr) > 0) AND (x = 1)), arr) AS x\r\n\r\n\u250c\u2500x\u2500\u2510\r\n\u2502 1 \u2502\r\n\u2514\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIssue happens when clickhouse tries to reuse the result of the first expression (x = 1) the second lambda."
      },
      {
        "user": "ClownfishYang",
        "created_at": "2021-01-08T06:26:48Z",
        "body": "> If we would replace arrayMap(a...) with arrayMap(x...) the result would be consistent.\r\n> \r\n> ```\r\n> \r\n> SELECT\r\n>     uuid,\r\n>     arrayFirstIndex((a, b) -> ((arraySum(arrayMap(x -> if(x >= 5, 0, 1), arraySlice(data_f, b + 1, 4))) = 0) AND (a >= 5)), arraySlice(data_f, 1, length(data_f) - 3), range(toUInt64(length(data_f) - 3))) AS freq_idx\r\n> FROM\r\n> (\r\n>     SELECT\r\n>         'id1' AS uuid,\r\n>         'p1' AS param,\r\n>         [1, 2, 3, 4, 5, 4, 7, 8, 9, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] AS data_f\r\n>     FROM system.one\r\n> )\r\n> \r\n> Query id: c05a651e-473f-4961-9a35-f6eec3b8fccf\r\n> \r\n> \u250c\u2500uuid\u2500\u252c\u2500freq_idx\u2500\u2510\r\n> \u2502 id1  \u2502       13 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> \r\n> SELECT\r\n>     uuid,\r\n>     arrayFirstIndex((a, b) -> ((a >= 5) AND (arraySum(arrayMap(x -> if(x >= 5, 0, 1), arraySlice(data_f, b + 1, 4))) = 0)), arraySlice(data_f, 1, length(data_f) - 3), range(toUInt64(length(data_f) - 3))) AS freq_idx\r\n> FROM\r\n> (\r\n>     SELECT\r\n>         'id1' AS uuid,\r\n>         'p1' AS param,\r\n>         [1, 2, 3, 4, 5, 4, 7, 8, 9, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1] AS data_f\r\n>     FROM system.one\r\n> )\r\n> \r\n> Query id: 1f655d1a-e121-46e1-8abf-e68fcb811268\r\n> \r\n> \u250c\u2500uuid\u2500\u252c\u2500freq_idx\u2500\u2510\r\n> \u2502 id1  \u2502       13 \u2502\r\n> \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n> ```\r\n> \r\n> Simpler Example\r\n> \r\n> ```\r\n> WITH [0, 1] AS arr\r\n> SELECT arrayExists(x -> (x AND (arrayExists(x -> (NOT x), arr) > 0)), arr) AS x\r\n> \r\n> \u250c\u2500x\u2500\u2510\r\n> \u2502 1 \u2502\r\n> \u2514\u2500\u2500\u2500\u2518\r\n> \r\n> WITH [0, 1] AS arr\r\n> SELECT arrayExists(x -> ((x = 1) AND (arrayExists(x -> (NOT (x = 1)), arr) > 0)), arr) AS x\r\n> \r\n> \u250c\u2500x\u2500\u2510\r\n> \u2502 0 \u2502\r\n> \u2514\u2500\u2500\u2500\u2518\r\n> \r\n> WITH [0, 1] AS arr\r\n> SELECT arrayExists(x -> ((arrayExists(x -> (NOT (x = 1)), arr) > 0) AND (x = 1)), arr) AS x\r\n> \r\n> \u250c\u2500x\u2500\u2510\r\n> \u2502 1 \u2502\r\n> \u2514\u2500\u2500\u2500\u2518\r\n> ```\r\n> \r\n> Issue happens when clickhouse tries to reuse the result of the first expression (x = 1) the second lambda.\r\n\r\nYeah, that's right, like you said.Please forgive me for not looking carefully, this is a great design, I will correct it.\r\nNow I happen to have a new problem, can you help me take a look at it?\r\nI need to query for the same starting value in both Arrays and how many times a value will appear after that.\r\n```\r\nSELECT \r\nt1.uuid AS uuid,\r\narrayFirstIndex(c -> t1.data_i[c+1] == 0 AND t2.data_i[c+1] == 0\r\n        AND countEqual(arraySlice(t1.data_i, c+1,40),1) == 1\r\n        AND countEqual(arraySlice(t2.data_i, c+1,40),1) == 1\r\n        ,range(length(t1.data_i))) AS lr_freq_idx\r\n\r\n FROM (\r\nSELECT \r\n    'id1' AS uuid, \r\n    'p1' AS param, \r\n    range(10000) AS data_i\r\nFROM system.one\r\n) t1\r\nLEFT JOIN (\r\nSELECT \r\n    'id1' AS uuid, \r\n    'p2' AS param, \r\n    range(10000) AS data_i\r\nFROM system.one\r\n) t2\r\nON t1.uuid = t2.uuid\r\n\r\n\u250c\u2500uuid\u2500\u252c\u2500lr_freq_idx\u2500\u2510\r\n\u2502 id1  \u2502           1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nIf range(10000), the execution is fine, but if range(100000), the execution is wrong.\r\n```\r\nProgress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) Received exception from server (version 20.2.1):\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 32.00 GiB (attempt to allocate chunk of 34362893096 bytes), maximum: 9.31 GiB:\r\n```\r\nI don't understand why such a simple query consumes so much memory.Is there any way to avoid the use problem, this is just the data I use for testing, development may use more data."
      },
      {
        "user": "den-crane",
        "created_at": "2021-01-08T20:11:11Z",
        "body": ">I need to query for the same starting value in both Arrays and how many times a value will appear after that.\r\n\r\nThis is unclear. Can you make more clear example?\r\n\r\n>Memory limit (for query) exceeded\r\n\r\nThe issue is that you pass arrays into lambda function. In this case CH makes multiple copies of passed arrays.\r\n\r\n```sql\r\nset send_logs_level='debug'\r\n\r\nSELECT arrayFilter(i -> arr1[i]/arr2[i] = arr2[i] * arr2[i], range(length(arr1))) \r\nfrom (select materialize(range(1000000)) arr1, materialize(range(1000000)) arr2)\r\n\r\nMemory limit (for query) exceeded: would use 4.00 TiB\r\n\r\n--the same query without arrays in lamda\r\nSELECT arrayFilter((i, j) -> (i/j = j * j), arr1, arr2) \r\nfrom (select materialize(range(1000000)) arr1, materialize(range(1000000)) arr2)\r\n\r\nPeak memory usage (for query): 20.01 MiB\r\n```"
      },
      {
        "user": "ClownfishYang",
        "created_at": "2021-01-27T07:15:12Z",
        "body": "For example, if I have two Arrays, I want to find that both Arrays are greater than or equal to some value the first time (for example, 60) and then many times in a row (for example, 5 times).\r\n```\r\nSELECT \r\nt1.uuid AS uuid,\r\narrayFirstIndex(i -> t1.data_i[i] >= 60 and t2.data_i[i] >= 60\r\n\tand arraySum(arrayMap(x -> if (x >= 60, 0, 1), arraySlice(t1.data_i, i + 1, 4))) == 0\r\n\tand arraySum(arrayMap(x -> if (x >= 60, 0, 1), arraySlice(t2.data_i, i + 1, 4))) == 0\r\n        ,arrayEnumerate(t1.data_i)) AS lr_freq_idx\r\n\r\n FROM (\r\nSELECT \r\n    'id1' AS uuid, \r\n    'p1' AS param, \r\n    range(10000) AS data_i\r\nFROM system.one\r\n) t1\r\nLEFT JOIN (\r\nSELECT \r\n    'id1' AS uuid, \r\n    'p2' AS param, \r\n    range(10000) AS data_i\r\nFROM system.one\r\n) t2\r\nON t1.uuid = t2.uuid\r\n```\r\nI'm simplifying here, but in fact these arrays may not have the same length, so we need to convert the index value, and there's no quick way to do that (I mean when the array is really long),\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why logical operator order affects array function results in ClickHouse",
      "Clarification of variable scoping rules in nested array functions",
      "Memory optimization strategy for large array operations",
      "Guidance on efficient array element access patterns",
      "Consistent behavior explanation for array function evaluation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:58:25"
    }
  },
  {
    "number": 18854,
    "title": "why select count(*) from numbers(10000000) cannot run in readonly mode?",
    "created_at": "2021-01-08T01:18:28Z",
    "closed_at": "2021-01-08T14:04:31Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18854",
    "body": "Code: 164, e.displayText() = DB::Exception: play: Cannot execute query in readonly mode (version 20.13.1.5552 (official build))\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18854/comments",
    "author": "l1t1",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2021-01-08T01:36:10Z",
        "body": "All table functions require RW privileges by design.\r\nYou can use `select count(*) from system.numbers where number <= 10000000` instead."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the original query requires read-write privileges",
      "Alternative method to achieve the same result without requiring write privileges",
      "Clarification about system table vs table function privileges"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:58:31"
    }
  },
  {
    "number": 18551,
    "title": "[Guidance] Table Migration (due to changing primary key)",
    "created_at": "2020-12-27T13:31:36Z",
    "closed_at": "2020-12-27T22:40:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18551",
    "body": "Working with Clickhouse and all is going well, but today I ran into the problem of needing to a change the name of a primary key on a table. \r\n\r\nAfter some research, it appears that currently a migration of data to a new table is needed to do this. It would be helpful to have some basic syntax and guidelines for how a table migration should happen with Clickhouse. \r\n\r\n- What is the preferred approach for a non-zookeeper managed server?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18551/comments",
    "author": "arpowers",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-27T16:13:19Z",
        "body": "Create a new table.` insert into new select * from old`\r\n"
      },
      {
        "user": "arpowers",
        "created_at": "2020-12-27T16:54:19Z",
        "body": "@den-crane simple enough! thanks"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method for table migration without ZooKeeper dependency",
      "Uses basic SQL operations for data migration",
      "Explains the table replacement process steps",
      "Addresses primary key modification through schema recreation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:58:43"
    }
  },
  {
    "number": 18346,
    "title": "Subquery optimization question",
    "created_at": "2020-12-22T04:08:19Z",
    "closed_at": "2021-01-27T16:27:32Z",
    "labels": [
      "question",
      "comp-optimizers"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18346",
    "body": "I tried to look at the docs and an explain query but couldn't tell... but if I have a query that contains a subquery and the outer query doesn't use all of the fields of the subquery, are those columns ever read?\r\n\r\nFor example:\r\n\r\n```sql\r\nSELECT count(id)\r\nFROM\r\n(\r\n   SELECT id,\r\n    any(country) as country,\r\n    any(state)  as state\r\n  FROM items\r\n  GROUP BY id\r\n) x\r\n```\r\n\r\nWould the country and state fields be read by clickhouse even though the outer query never uses them, i.e. would they be pruned?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18346/comments",
    "author": "mauidude",
    "comments": [
      {
        "user": "mauidude",
        "created_at": "2021-01-24T17:30:19Z",
        "body": "Following up on this... anyone know?"
      },
      {
        "user": "den-crane",
        "created_at": "2021-01-24T21:35:10Z",
        "body": "\r\n\r\n```sql\r\ncreate table items ( id Int64, country String, state String) Engine=MergeTree order by tuple();\r\ninsert into items select number , toString(cityHash64(number)) , \r\ntoString(cityHash64(number)) from numbers(200000000);\r\n\r\nSELECT id,any(country) as country, any(state)  as state \r\nFROM items GROUP BY id format Null  \r\nElapsed: 38.773 sec.\r\n\r\n\r\nSELECT id\r\nFROM items GROUP BY id format Null  \r\nElapsed: 7.238 sec.\r\n\r\n\r\nSELECT count(id)\r\nFROM( SELECT id, any(country) as country, any(state)  as state\r\n    FROM items GROUP BY id\r\n) x format Null  \r\nElapsed: 7.241 sec.\r\n"
      },
      {
        "user": "mauidude",
        "created_at": "2021-01-27T16:27:32Z",
        "body": "thank you! that was helpful!"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether ClickHouse optimizes subqueries by pruning unused columns",
      "Evidence of column pruning validation through measurable performance impact"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:58:51"
    }
  },
  {
    "number": 18001,
    "title": "Columns are from different tables while processing dateDiff",
    "created_at": "2020-12-11T09:58:36Z",
    "closed_at": "2020-12-14T00:24:47Z",
    "labels": [
      "question",
      "comp-joins",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/18001",
    "body": "I'm trying to calc the next day retention login user with ClickHouse.\r\n\r\nThe table structure of `t_user_login` is:\r\n\r\n```\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 user    \u2502 String                    \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2502 log_day \u2502 DateTime('Asia/Shanghai') \u2502              \u2502                    \u2502         \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd the SQL is:\r\n\r\n```sql\r\nSELECT DISTINCT log_day,a.user as user_day0,b.user as user_day1\r\nFROM (\r\n  SELECT min(log_day) as log_day, user\r\n  FROM t_user_login\r\n  GROUP BY user\r\n) a\r\nLEFT JOIN t_user_login b\r\nON dateDiff('day', b.log_day, a.log_day) = 1 AND a.user = b.user;\r\n```\r\n\r\nBut received an exception:\r\n\r\n> Received exception from server (version 20.11.4):\r\nCode: 403. DB::Exception: Received from localhost:9000. DB::Exception: Invalid columns in JOIN ON section. Columns b.log_day and log_day are from different tables.: While processing dateDiff('day', b.log_day, log_day) = 1.\r\n\r\nThis really confused me for a long time. Anyone can help me, thanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/18001/comments",
    "author": "shuizhongyueming",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-11T16:15:06Z",
        "body": "toStartOfDay(b.log_day - interval 1 day) =toStartOfDay(a.log_day)\r\n\r\n```sql\r\nSELECT DISTINCT log_day,a.user as user_day0,b.user as user_day1\r\nFROM (\r\n  SELECT min(log_day) as log_day, user\r\n  FROM t_user_login\r\n  GROUP BY user\r\n) a\r\nLEFT JOIN t_user_login b\r\nON toStartOfDay(b.log_day - interval 1 day) =toStartOfDay(a.log_day) AND a.user = b.user;\r\n\r\n```"
      },
      {
        "user": "shuizhongyueming",
        "created_at": "2020-12-12T06:54:54Z",
        "body": "@den-crane It works! Thank you!\r\nBut I still want ask: did the `dateDiff` can't be use at this context?"
      },
      {
        "user": "filimonov",
        "created_at": "2020-12-13T23:25:41Z",
        "body": "Currently clickhouse supports only equijoins. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why dateDiff() function causes a JOIN error in ClickHouse",
      "Solution must work within ClickHouse's equijoin limitations",
      "Accurate calculation of 1-day difference between login dates"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:59:06"
    }
  },
  {
    "number": 17999,
    "title": "now()    timezone is client timezone not server timezone?",
    "created_at": "2020-12-11T06:37:35Z",
    "closed_at": "2020-12-18T17:40:04Z",
    "labels": [
      "question",
      "comp-datetime"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17999",
    "body": "sql \uff1aselect now(),toString(now())\r\nresult:\r\n|now()|toString(now())|\r\n|-----|---------------|\r\n|2020-12-11 06:37:09|2020-12-10 22:37:09|\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17999/comments",
    "author": "jjtjiang",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-11T16:18:58Z",
        "body": "Yes. What API do you use? JDBC ?\r\n"
      },
      {
        "user": "filimonov",
        "created_at": "2020-12-13T23:27:48Z",
        "body": "`now()` for native clients the number is send to a client and client do formatting to string.\r\n\r\n`toString(now())` server do a formatting and send string. "
      },
      {
        "user": "jjtjiang",
        "created_at": "2020-12-14T06:57:45Z",
        "body": "got it, I use JDBC .thanks @den-crane @filimonov "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of client vs server timezone handling in timestamp processing",
      "Differentiation between timestamp value transmission and string formatting location",
      "Clarification of API/client-specific timezone behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:59:19"
    }
  },
  {
    "number": 17885,
    "title": "how to make groupArray more faster?",
    "created_at": "2020-12-08T06:07:55Z",
    "closed_at": "2020-12-17T05:29:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17885",
    "body": "My query result has 1 million and need to page it,  now implemented by groupArray, but it is slow, is there any way to make it faster ?\r\neg, when click page \"3\" , I need return No. 31~40 row. ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17885/comments",
    "author": "vegastar002",
    "comments": [
      {
        "user": "l1t1",
        "created_at": "2020-12-08T10:49:50Z",
        "body": "plz post the full sql "
      },
      {
        "user": "den-crane",
        "created_at": "2020-12-08T14:41:14Z",
        "body": "Do not use groupArray for this.\r\n\r\n```\r\ncreate table temp(A Int64) Engine=Log;\r\ninsert into temp select * from numbers_mt(100000000);\r\nselect * from temp limit 10 offset 99000000;\r\n10 rows in set. Elapsed: 0.124 sec. Processed 99.08 million rows,\r\n```"
      },
      {
        "user": "vegastar002",
        "created_at": "2020-12-17T05:29:29Z",
        "body": "yes, finally I found can use this way to page"
      }
    ],
    "satisfaction_conditions": [
      "Avoids using groupArray for pagination purposes",
      "Supports efficient pagination of large datasets (1M+ rows) with low latency",
      "Enables direct access to specific row ranges without full dataset processing"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:59:25"
    }
  },
  {
    "number": 17648,
    "title": "Failed to determine user credentials",
    "created_at": "2020-12-01T02:41:01Z",
    "closed_at": "2020-12-01T20:12:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17648",
    "body": "(you don't have to strictly follow this form)\r\n\r\n**Describe the bug**\r\nThe version is : 20.11.4.13\r\nUbuntu: 18.04.5 LTS\r\nWhen run command: sudo /etc/init.d/clickhouse-server start, I got an err message:\r\nDec  1 10:35:24 apps-domain systemd[7878]: clickhouse-server.service: Failed to determine user credentials: No such process\r\nDec  1 10:35:24 apps-domain systemd[7878]: clickhouse-server.service: Failed at step USER spawning /usr/bin/clickhouse-server: No such process\r\n\r\nWhoever has encountered it\uff0ccan reply to the solution\u3002\r\nThank you very much!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17648/comments",
    "author": "gavinju",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-12-01T03:28:35Z",
        "body": "Do not use `/etc/init.d/clickhouse-server start`\r\n\r\nUSE: \r\n`sudo systemctl start clickhouse-server` \r\n`sudo systemctl status clickhouse-server` \r\n`sudo systemctl stop clickhouse-server`\r\n`sudo journalctl -u clickhouse-server.service`"
      },
      {
        "user": "gavinju",
        "created_at": "2020-12-01T10:24:15Z",
        "body": "hi, den-crane\r\nthanks for your replay!  I tried it, but the same error is displayed.\r\nBut, use the following can run correctly:\r\ncoclickhouse-server --config-file=/etc/clickhouse-server/config.xml"
      },
      {
        "user": "den-crane",
        "created_at": "2020-12-01T15:00:21Z",
        "body": "try \r\n\r\nsudo -u clickhouse clickhouse-server --config-file=/etc/clickhouse-server/config.xml\r\n\r\nServices use clickhouse user. "
      }
    ],
    "satisfaction_conditions": [
      "Solution must ensure the ClickHouse service runs under the correct system user context",
      "Answer must address service configuration rather than direct binary execution",
      "Must resolve user credential verification failures during service startup",
      "Solution should validate existence/config of clickhouse system user"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:59:32"
    }
  },
  {
    "number": 17312,
    "title": "PDO integration",
    "created_at": "2020-11-23T14:58:22Z",
    "closed_at": "2020-11-23T19:52:47Z",
    "labels": [
      "question",
      "comp-mysql"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17312",
    "body": "**Describe the bug**\r\n\r\nI can't connect to ClickHouse via the mysql interface using PDO\r\n\r\n**How to reproduce**\r\n* version 20.11.2.1 (official build)\r\n* PHP 7.4.3\r\n\r\n```php\r\n$pdo=new PDO(\"mysql:host=127.0.0.1;port=8123;dbname=test\", \"default\",\"\",[PDO::ATTR_TIMEOUT => 5,PDO::ATTR_ERRMODE => PDO::ERRMODE_EXCEPTION]);\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17312/comments",
    "author": "god1dog",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-23T15:48:12Z",
        "body": "```\r\ncat /etc/clickhouse-server/config.xml|grep mysql\r\n    <mysql_port>9004</mysql_port>\r\n\r\n```\r\nYou should use port 9004 not `port=8123`;"
      },
      {
        "user": "god1dog",
        "created_at": "2020-11-23T19:52:47Z",
        "body": "Thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct port configuration for MySQL interface in ClickHouse",
      "References ClickHouse server configuration verification methods",
      "Differentiates between ClickHouse interface types and their ports"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:59:48"
    }
  },
  {
    "number": 17230,
    "title": "Storing huge amount of log entries",
    "created_at": "2020-11-20T13:30:25Z",
    "closed_at": "2020-11-25T08:59:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/17230",
    "body": "Hello,\r\n\r\nI would like to use ClickHouse for storing huge amount of log entries (sorted without ORDER BY) and after some time dump it to disk and send to the clients. \r\nI have tried to use tables with mergetree table engine but, as the description of mergetree engine says it was not sorted, because not all parts were merged. \r\n\r\nIs there a way how to achieve desired behavior? I would like to have sorted logs without ORDER BY expression. Is it even possible with some engines?\r\nI have look into Log engine but it does not support TTL and replication.\r\n\r\nThank you for help",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/17230/comments",
    "author": "wutchzone",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-20T14:43:38Z",
        "body": "You should use *MergeTree. Other engines is not suggested for permanent data.\r\nCan you show \"create table xxx\" for MergeTree ?\r\nYou can use MergeTree without ORDERBY (`order by tuple()`), but usually it has no sense. At least you can add fake column with insertion date.\r\n\r\n```sql\r\nCREATE TABLE mylogs\r\n(\r\n    `Log` String,\r\n    `created_at` Date MATERIALIZED today()\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(created_at)\r\nORDER BY created_at\r\n```\r\n\r\nCan you show your supposed selects ? "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-11-21T21:06:41Z",
        "body": "> Is there a way how to achieve desired behavior? I would like to have sorted logs without ORDER BY expression. Is it even possible with some engines?\r\n\r\nLogs are typically stored in MergeTree table with ORDER BY time column."
      },
      {
        "user": "wutchzone",
        "created_at": "2020-11-25T08:59:52Z",
        "body": "@den-crane @alexey-milovidov \r\nThank you both for help. Firstly I was very confused when I used the MergeeTree engine, because I tried to dump the data too early. Propably many parts were not merged and it consumed a lot of RAM when used with ORDER BY. When I try to dump after hour when they were inserted, it does not consumed RAM at all and it performs smoothly. I was just afraid that this cannot be solved with mergee tree, but it can. Now everything is working perfectly, thank you once more."
      }
    ],
    "satisfaction_conditions": [
      "Support for sorted log storage without requiring explicit ORDER BY clauses",
      "TTL and replication capabilities",
      "Explanation of merge process impact on data order",
      "Scalable storage for high-volume log ingestion"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:59:52"
    }
  },
  {
    "number": 16799,
    "title": "DB::Exception: Unknown function avg (version 20.9.3.45 (official build))",
    "created_at": "2020-11-09T03:18:18Z",
    "closed_at": "2020-11-09T10:35:02Z",
    "labels": [
      "question",
      "unexpected behaviour"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16799",
    "body": "```sql\r\nSELECT\r\n\t(intDiv(toUInt32(log_time), 1) * 1) * 1000 as t,\r\n\tavg(`request_time`) as a\r\nFROM\r\n\tELB_LOG.api_log\r\nWHERE\r\n\t\"log_time\" >= toDateTime(1604315080)\r\nGROUP BY\r\n\tt,\r\n\ta\r\nORDER BY\r\n\tt\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16799/comments",
    "author": "kim-up",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-11-09T09:18:05Z",
        "body": "Your query is incorrect (you trying to do group by on the column which is an aggregation function). \r\n\r\nBut the error reported is totally misleading and should be improved.\r\n\r\nMinimal testcase:\r\n```\r\nselect number a, avg(number) b from numbers(1) group by a,b;\r\n```\r\n\r\n```\r\n<=20.3:\r\n\r\nReceived exception from server (version 20.3.21):\r\nCode: 47. DB::Exception: Received from localhost:9000. DB::Exception: Unknown identifier (in GROUP BY): avg(number). Stack trace:\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xe193120 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x857071d in /usr/bin/clickhouse\r\n2. ? @ 0xc822394 in /usr/bin/clickhouse\r\n3. DB::ExpressionAnalyzer::ExpressionAnalyzer(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::SyntaxAnalyzerResult const> const&, DB::Context const&, unsigned long, bool, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::SubqueryForSet, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, DB::SubqueryForSet> > >) @ 0xc81e39a in /usr/bin/clickhouse\r\n4. ? @ 0xc658bac in /usr/bin/clickhouse\r\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xc659e09 in /usr/bin/clickhouse\r\n6. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xc65b419 in /usr/bin/clickhouse\r\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xc865e86 in /usr/bin/clickhouse\r\n8. DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::QueryProcessingStage::Enum) @ 0xc5d09c4 in /usr/bin/clickhouse\r\n9. ? @ 0xca74785 in /usr/bin/clickhouse\r\n10. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, bool) @ 0xca77571 in /usr/bin/clickhouse\r\n11. DB::TCPHandler::runImpl() @ 0x86524e9 in /usr/bin/clickhouse\r\n12. DB::TCPHandler::run() @ 0x86534d0 in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerConnection::start() @ 0xd8cebbb in /usr/bin/clickhouse\r\n14. Poco::Net::TCPServerDispatcher::run() @ 0xd8cf03d in /usr/bin/clickhouse\r\n15. Poco::PooledThread::run() @ 0xe2212d7 in /usr/bin/clickhouse\r\n16. Poco::ThreadImpl::runnableEntry(void*) @ 0xe21d0cc in /usr/bin/clickhouse\r\n17. ? @ 0xe21ea6d in /usr/bin/clickhouse\r\n18. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n19. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n\r\n>= 20.4:\r\n\r\nReceived exception from server (version 20.4.9):\r\nCode: 46. DB::Exception: Received from localhost:9000. DB::Exception: Unknown function avg. Stack trace:\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x110e3bb0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x998493d in /usr/bin/clickhouse\r\n2. ? @ 0xd8d5fd6 in /usr/bin/clickhouse\r\n3. DB::FunctionFactory::get(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context const&) const @ 0xd8d5135 in /usr/bin/clickhouse\r\n4. ? @ 0xe198839 in /usr/bin/clickhouse\r\n5. DB::SyntaxAnalyzer::analyzeSelect(std::__1::shared_ptr<DB::IAST>&, DB::SyntaxAnalyzerResult&&, DB::SelectQueryOptions const&, std::__1::vector<DB::TableWithColumnNamesAndTypes, std::__1::allocator<DB::TableWithColumnNamesAndTypes> > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::TableJoin>) const @ 0xe19c84c in /usr/bin/clickhouse\r\n6. ? @ 0xdeeef69 in /usr/bin/clickhouse\r\n7. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xdef24df in /usr/bin/clickhouse\r\n8. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xdef3bd9 in /usr/bin/clickhouse\r\n9. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xe0c8d41 in /usr/bin/clickhouse\r\n10. DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::QueryProcessingStage::Enum) @ 0xde63bbf in /usr/bin/clickhouse\r\n11. ? @ 0xe2134bd in /usr/bin/clickhouse\r\n12. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, bool) @ 0xe216e85 in /usr/bin/clickhouse\r\n13. DB::TCPHandler::runImpl() @ 0x9a8ad68 in /usr/bin/clickhouse\r\n14. DB::TCPHandler::run() @ 0x9a8bd70 in /usr/bin/clickhouse\r\n15. Poco::Net::TCPServerConnection::start() @ 0x10fcf92b in /usr/bin/clickhouse\r\n16. Poco::Net::TCPServerDispatcher::run() @ 0x10fcfdbb in /usr/bin/clickhouse\r\n17. Poco::PooledThread::run() @ 0x1117d986 in /usr/bin/clickhouse\r\n18. Poco::ThreadImpl::runnableEntry(void*) @ 0x11178c40 in /usr/bin/clickhouse\r\n19. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n20. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why grouping by an aggregate function (avg) causes an error",
      "Clarification of ClickHouse's error message behavior differences between versions",
      "Guidance on proper SQL syntax for combining calculated fields and aggregations",
      "Explanation of ClickHouse's aggregation function handling in GROUP BY clauses"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:00:20"
    }
  },
  {
    "number": 16695,
    "title": "how to change default CSV FILE format_csv_delimiter?",
    "created_at": "2020-11-05T07:51:09Z",
    "closed_at": "2020-11-10T11:12:20Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16695",
    "body": "Hello,\r\n\r\n\r\nmy setting:\r\n```\r\n# cat /etc/clickhouse-server/config.d/delimiter.xml\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n\t<profiles>\r\n        <default>\r\n\t\t<format_csv_delimiter>|</format_csv_delimiter>\r\n        </default>\r\n    </profiles>\r\n</yandex>\r\n```\r\n\r\nbut throw exception \r\n```\r\nCode: 27. DB::Exception: Cannot parse input: expected ',' before: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: user_ip,         type: String, parsed text: \"\"\r\nERROR: Line feed found where delimiter (,) is expected. It's like your file has less columns than expected.\r\n```\r\n\r\n    please, how to change default CSV FILE format_csv_delimiter?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16695/comments",
    "author": "trollhe",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-05T14:14:00Z",
        "body": ">cat /etc/clickhouse-server/config.d/delimiter.xml\r\n\r\nWrong folder for user settings.\r\nShould be /etc/clickhouse-server/**users.d**/delimiter.xml\r\n\r\n\r\nconfig.d -- server settings (config.xml)\r\nusers.d -- users settings (user.xml)\r\nconf.d -- any (config.xml and user.xml)"
      },
      {
        "user": "trollhe",
        "created_at": "2020-11-10T11:12:07Z",
        "body": "> > cat /etc/clickhouse-server/config.d/delimiter.xml\r\n> \r\n> Wrong folder for user settings.\r\n> Should be /etc/clickhouse-server/**users.d**/delimiter.xml\r\n> \r\n> config.d -- server settings (config.xml)\r\n> users.d -- users settings (user.xml)\r\n> conf.d -- any (config.xml and user.xml)\r\n\r\nthanks den-crane,\r\n\r\ni'm try to it, file already move from  `/etc/clickhouse-server/config.d/` to `/etc/clickhouse-server/users.d/`. and  restarted to clickhouse-server service.\r\n\r\nbut exception  still.\r\n\r\ntips: i used table function file() to select /data/clickhouse/user_files/*.csv. csv delimiter is  \"|\".\r\n\r\n\n\n---\n\nthanks , Successfully processed. config file move to `/etc/clickhouse-server/users.d/ `"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct configuration directory for user-specific settings in ClickHouse",
      "Ensures configuration changes are properly applied to CSV format settings",
      "Addresses service restart requirements for configuration changes"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:00:28"
    }
  },
  {
    "number": 16690,
    "title": "How can i obtain table comment and column comment",
    "created_at": "2020-11-05T03:40:00Z",
    "closed_at": "2020-11-05T14:28:18Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16690",
    "body": "hi everyone, I want to ask how can I get the table comment and column comment when I writing sql query. \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16690/comments",
    "author": "wmaa0002",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-11-05T04:04:39Z",
        "body": "No table comments, only columns.\r\n\r\n```sql\r\ncreate table x( a Int64 comment 'some comment') Engine=Memory;\r\n\r\n\r\ndesc table x\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500default_type\u2500\u252c\u2500default_expression\u2500\u252c\u2500comment\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500codec_expression\u2500\u252c\u2500ttl_expression\u2500\u2510\r\n\u2502 a    \u2502 Int64 \u2502              \u2502                    \u2502 some comment \u2502                  \u2502                \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\nselect name, type, comment from system.columns where table = 'x' and database = 'default'\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u252c\u2500comment\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 a    \u2502 Int64 \u2502 some comment \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to retrieve column comments through system metadata queries",
      "Clarification about table comment availability in the database system",
      "Method to access metadata without altering existing SQL queries"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:00:36"
    }
  },
  {
    "number": 16633,
    "title": "how to merge 12 sql to 1?",
    "created_at": "2020-11-03T11:41:56Z",
    "closed_at": "2020-11-03T12:22:06Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16633",
    "body": "I need a fine sql. \r\nselect count() from t1 where time1>= toDateTime('2019-01-01 00:00:00')  and time1< toDateTime('2019-02-01 00:00:00');\r\nselect count() from t1 where time1>= toDateTime('2019-02-01 00:00:00')  and time1< toDateTime('2019-03-01 00:00:00');\r\n.......\r\nselect count() from t1 where time1>= toDateTime('2019-12-01 00:00:00')  and time1< toDateTime('2020-01-01 00:00:00');\r\nso, how can I merge the 12 sql to 1?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16633/comments",
    "author": "vegastar002",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-11-03T12:22:06Z",
        "body": "```sql\r\nSELECT \r\n  toStartOfMonth(time1) month,\r\n  count()\r\nFROM t1\r\n  WHERE  time1>= toDateTime('2019-01-01 00:00:00')  and time1< toDateTime('2020-01-01 00:00:00')\r\nGROUP BY month;\r\n```"
      },
      {
        "user": "vegastar002",
        "created_at": "2020-11-04T02:08:07Z",
        "body": "thank u , it's useful.\r\nbut I want use neighbor() to get it.\r\neg:\r\nSELECT \r\n    d2 AS start,\r\n    neighbor(d2, -1) AS end\r\nFROM \r\n(\r\n    WITH toDate('2020-04-01') AS d1\r\n    SELECT toStartOfMonth(d1 - (number * 32)) AS d2\r\n    FROM numbers(16)\r\n)\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500start\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500end\u2500\u2510\r\n\u2502 2020-04-01 \u2502 1970-01-01 \u2502\r\n\u2502 2020-02-01 \u2502 2020-04-01 \u2502\r\n\u2502 2020-01-01 \u2502 2020-02-01 \u2502\r\n\u2502 2019-12-01 \u2502 2020-01-01 \u2502\r\n\u2502 2019-11-01 \u2502 2019-12-01 \u2502\r\n\u2502 2019-10-01 \u2502 2019-11-01 \u2502\r\n\u2502 2019-09-01 \u2502 2019-10-01 \u2502\r\n\u2502 2019-08-01 \u2502 2019-09-01 \u2502\r\n\u2502 2019-07-01 \u2502 2019-08-01 \u2502\r\n\u2502 2019-06-01 \u2502 2019-07-01 \u2502\r\n\u2502 2019-05-01 \u2502 2019-06-01 \u2502\r\n\u2502 2019-04-01 \u2502 2019-05-01 \u2502\r\n\u2502 2019-03-01 \u2502 2019-04-01 \u2502\r\n\u2502 2019-02-01 \u2502 2019-03-01 \u2502\r\n\u2502 2019-01-01 \u2502 2019-02-01 \u2502\r\n\u2502 2018-12-01 \u2502 2019-01-01 \u2502\r\n\r\nselect count() from t1 where time1>= toDateTime( start  ) and time1< toDateTime( end  );\r\n\r\nin this way , how can I instead of start and end ?"
      }
    ],
    "satisfaction_conditions": [
      "Solution must combine 12 monthly count queries into a single query",
      "Must utilize ClickHouse's neighbor() function for time interval generation",
      "Should dynamically generate consecutive month intervals between 2019-01-01 and 2020-01-01",
      "Must associate each generated time interval with its corresponding record count",
      "Should avoid hardcoding individual month boundaries in WHERE clauses"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:00:43"
    }
  },
  {
    "number": 16254,
    "title": "Why CH always restart automatically after killing it?",
    "created_at": "2020-10-22T07:14:27Z",
    "closed_at": "2020-10-22T10:55:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16254",
    "body": "I found that CH server can always restart itself in about half a minute after stopping it, or killing it.\r\nIs there any backgound process to keep the CH server alive?\r\nI tried to remove the file from /etc/init.d/clickhouse-server, but the problem occurs.\r\nActually, It's not a problem, just wondering why...\r\nThanks.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16254/comments",
    "author": "y45398jp123",
    "comments": [
      {
        "user": "MeteHanC",
        "created_at": "2020-10-22T07:53:46Z",
        "body": "I think this is about the unit file of ClickHouse (for ubuntu, filepath is : /etc/systemd/system/clickhouse-server.service)\r\n\r\nUnder the Service section you can see the following lines ;\r\n\r\n```\r\nRestart=always\r\nRestartSec=30\r\n```\r\n\r\nSo this is not actually a ClickHouse specific thing "
      },
      {
        "user": "Inasayang",
        "created_at": "2020-10-22T08:45:28Z",
        "body": "systemd"
      },
      {
        "user": "y45398jp123",
        "created_at": "2020-10-22T10:55:42Z",
        "body": "Got it, thanks a lot."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T00:19:17Z",
        "body": "for initV systems it's a cron-job.\r\n\r\n```\r\n# cat /etc/cron.d/clickhouse-server\r\n*/10 * * * * root (which service > /dev/null 2>&1 && (service clickhouse-server condstart ||:)) || /etc/init.d/clickhouse-server condstart > /dev/null 2>&1\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of the service management mechanism responsible for automatic restarts",
      "Clarification that this is a standard OS-level service management feature",
      "Differentiation between init systems (systemd vs. initV)"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:00:49"
    }
  },
  {
    "number": 16251,
    "title": "ALTER DROP doesn't consider size of a partition correctly",
    "created_at": "2020-10-22T01:27:13Z",
    "closed_at": "2020-10-23T16:37:03Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/16251",
    "body": "Hi. When I try to drop partition for a certain hour:\r\n```alter table db.table drop partition '2020-10-19 18:00:00';```\r\nI sometimes get this error:\r\n```\r\n[2020-10-22 07:21:16] Code: 359, e.displayText() = DB::Exception: Table or Partition in db.table was not dropped.\r\n[2020-10-22 07:21:16] Reason:\r\n[2020-10-22 07:21:16] 1. Size (52.01 GB) is greater than max_[table/partition]_size_to_drop (50.00 GB)\r\n[2020-10-22 07:21:16] 2. File '/var/lib/clickhouse/flags/force_drop_table' intended to force DROP doesn't exist\r\n```\r\nHowever, if I run this:\r\n```select formatReadableSize(sum(bytes_on_disk)) from (select bytes_on_disk from system.parts where table = 'table' and partition = '2020-10-19 18:00:00');```\r\nI see that the size of this partition is much lower: 48.43 GiB. So why does it tell me that I'm dropping too big partition and why does CH allow such big partitions at all if it prohibits dropping them in the end?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/16251/comments",
    "author": "keliss",
    "comments": [
      {
        "user": "keliss",
        "created_at": "2020-10-22T01:41:17Z",
        "body": "Also, the message telling me to \"increase (or set to zero) max_[table/partition]_size_to_drop in server config and restart ClickHouse\" seems to be misleading - I don't have to restart CH for these settings to apply. Or it's just some CH magic that made the partitions smaller so I was able to drop them normally (I've already seen such behaviour but I thought it is impossible for a partition to become smaller without dropping any of its parts)."
      },
      {
        "user": "abyss7",
        "created_at": "2020-10-22T18:10:38Z",
        "body": "Can you provide please the `SHOW CREATE TABLE` result for table in question? And please provide the CH version."
      },
      {
        "user": "keliss",
        "created_at": "2020-10-22T18:17:05Z",
        "body": "Of course:\r\n```\r\nCREATE TABLE db.table (`writeTime` DateTime DEFAULT now(), ...) ENGINE = MergeTree() PARTITION BY toStartOfHour(writeTime) ORDER BY tuple() SETTINGS index_granularity = 8192;\r\n```\r\n20.9.3.45"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T13:01:53Z",
        "body": "> Also, the message telling me to \"increase (or set to zero) max_[table/partition]_size_to_drop in server config and restart ClickHouse\" seems to be misleading - I don't have to restart CH for these settings to apply. Or it's just some CH magic that made the partitions smaller so I was able to drop them normally (I've already seen such behaviour but I thought it is impossible for a partition to become smaller without dropping any of its parts).\r\n\r\nmax_partition_size_to_drop reload/apply without restart was implemented recently and this message should be corrected.\n\n---\n\nAs I understand you just finished to insert data to this partition '2020-10-19 18:00:00'.\r\nIt is possible that when you checked the size by select `inactive` parts were deleted already. "
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T13:27:56Z",
        "body": "No, this partition remained intact for sure, we don't insert data for some past period of time."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T14:12:50Z",
        "body": "Ah, it is two different units Gib vs GB 1024 vs 1000.\r\n\r\n```    \r\n<max_partition_size_to_drop>5000000000</max_partition_size_to_drop> \r\n\r\nSELECT\r\n    formatReadableSize(sum(bytes_on_disk)),\r\n    round(((sum(bytes_on_disk) / 1000) / 1000) / 1000, 2) AS GB\r\nFROM system.parts\r\nWHERE table = 'XX'\r\n\r\n\u250c\u2500formatReadableSize(sum(bytes_on_disk))\u2500\u252c\u2500\u2500\u2500\u2500GB\u2500\u2510\r\n\u2502 9.50 GiB                               \u2502 10.21 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\nalter table XX drop partition tuple();\r\n\r\n1. Size (10.21 GB) is greater than max_[table/partition]_size_to_drop (5.00 GB)\r\n\r\n```"
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T14:37:31Z",
        "body": "But even in this case the exception is triggered by a lower amount of disk space than the limit :)\r\n48.43 GiB * 1024 / 1000 = 49.59 GB."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-23T14:46:14Z",
        "body": "No. The limit is also in GB(1000). Check my message \r\n\r\nmax_partition_size_to_drop = 5000000000\r\n\r\n1. Size (10.21 GB) is greater than max_[table/partition]_size_to_drop (5.00 GB)\n\n---\n\nand \r\n\r\n`48.43 * (1024 * 1024 * 1024) / (1000*1000*1000) = 52.00`\r\n\r\n>1. Size (52.01 GB) is greater than max_[table/partition]_size_to_drop (50.00 GB)"
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T15:03:25Z",
        "body": "Oh, sorry, I calculated incorrectly. Is there any particular reason to keep some limit for DROP queries at all? I can't imagine a use-case for this setting."
      },
      {
        "user": "abyss7",
        "created_at": "2020-10-23T15:40:05Z",
        "body": "> Oh, sorry, I calculated incorrectly. Is there any particular reason to keep some limit for DROP queries at all? I can't imagine a use-case for this setting.\r\n\r\nFrom documentation:\r\n> In many cases mistakes like these will affect all replicas. ClickHouse has built-in safeguards to prevent some types of mistakes \u2014 for example, by default you can\u2019t just drop tables with a MergeTree-like engine containing more than 50 Gb of data."
      },
      {
        "user": "keliss",
        "created_at": "2020-10-23T16:37:03Z",
        "body": "Ok, got it. Thanks for your help."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why ClickHouse reports different size measurements for partition deletion checks versus user queries",
      "Clarification of how max_[table/partition]_size_to_drop configuration works regarding unit basis (base-10 vs base-2)",
      "Explanation of the rationale behind size-based drop restrictions in ClickHouse",
      "Clarification about configuration reload requirements for max_[table/partition]_size_to_drop settings"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:01:01"
    }
  },
  {
    "number": 15851,
    "title": "How to remove default value for column?",
    "created_at": "2020-10-12T08:22:40Z",
    "closed_at": "2020-10-12T13:52:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15851",
    "body": "Hello. Here's a table for example:\r\n```sql\r\nCREATE TABLE test.table1\r\n(\r\n        EventDate Date,\r\n        Id Int32,\r\n        Value String default 'strstrstr'\r\n)\r\nEngine = MergeTree()\r\nPARTITION BY toYYYYMM(EventDate)\r\nORDER BY Id;\r\n```\r\n\r\nI can modify default value for \"Value\" column like: `ALTER TABLE test.table1 MODIFY COLUMN Value DEFAULT 'mystring'`\r\nBut how can I remove this default value? Even if I execute `ALTER TABLE test.table1 MODIFY COLUMN Value DEFAULT ''`, it just defaults to an empty string.\r\nAnd also I cannot do this for types like Int32, because this will throw an error on future SELECT\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15851/comments",
    "author": "MasterGroosha",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-12T13:48:27Z",
        "body": "String type:  MODIFY COLUMN Value DEFAULT ''\r\nInt32 type:  MODIFY COLUMN Value DEFAULT 0\n\n---\n\nAnd starting from CH 20.10\r\n\r\n```sql\r\nalter table table1 \r\n   MODIFY COLUMN Value \r\n   REMOVE DEFAULT;\r\n```"
      },
      {
        "user": "MasterGroosha",
        "created_at": "2020-10-12T13:51:58Z",
        "body": "@den-crane Thank you! Looking forward to installing 20.10 as soon as it is released."
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to completely remove default values from columns rather than replacing them with empty/zero values",
      "Works for different data types (String, Int32 etc.) without type-specific workarounds",
      "Doesn't require maintaining backward-compatible default values",
      "Specifies version requirements if solution depends on specific ClickHouse features"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:01:43"
    }
  },
  {
    "number": 15848,
    "title": "Must there be enough memory to use GROUP BY?",
    "created_at": "2020-10-12T04:30:37Z",
    "closed_at": "2020-10-13T21:42:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15848",
    "body": "The computer used for testing has 16GB of RAM.\r\nThese configurations have been set:\r\n`<max_memory_usage_for_all_queries>12000000000</max_memory_usage_for_all_queries>\r\n            <max_bytes_before_external_group_by>6000000000</max_bytes_before_external_group_by>\r\n            <max_memory_usage>12000000000</max_memory_usage>`\r\nBut still got an error:\r\nDB::Exception: Memory limit (total) exceeded: would use 13.95 GiB (attempt to allocate chunk of 134217760 bytes), maximum: 13.90 GiB: While executing AggregatingTransform.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15848/comments",
    "author": "qinglok",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-12T13:31:34Z",
        "body": "What CH version do you use?\r\n\r\nTry to lower max_memory_usage  to 10G `<max_memory_usage>10000000000` and max_bytes_before_external_group_by to 4G `<max_bytes_before_external_group_by>4000000000`\r\n"
      },
      {
        "user": "qinglok",
        "created_at": "2020-10-12T16:39:43Z",
        "body": "Yes, it seems to solve the problem.\r\n\r\nAnother question about AggregatingMergeTree is why there is no data in the tables of the AggregatingMergeTree engine after the CH service is restarted?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-12T21:01:33Z",
        "body": "What CH version do you use?\r\n\r\n>Another question about AggregatingMergeTree is why there is no data in the \r\n>tables of the AggregatingMergeTree engine after the CH service is restarted?\r\n\r\nDisk corruption? \r\nCheck startup messages **grep table_name /var/log/clickhouse-server/clickhouse-server.log**"
      },
      {
        "user": "qinglok",
        "created_at": "2020-10-13T06:30:15Z",
        "body": "CH version is 20.8.2.3-2.\r\nThe disk is normal. Because I can see that there are still files in the data directory of CH.\r\nHowever, after the CH service is restarted, no data can be found by using SELECT.\r\nBut now I can\u2019t reproduce the same situation.\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how memory configuration parameters interact during GROUP BY operations",
      "Guidance on balancing memory allocation between query execution and spill-to-disk thresholds",
      "Diagnostic methods for verifying data persistence in AggregatingMergeTree tables"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:01:52"
    }
  },
  {
    "number": 15835,
    "title": "tokenbf_v2 index does not drop rows in aggregation query",
    "created_at": "2020-10-11T08:56:05Z",
    "closed_at": "2020-10-13T21:43:12Z",
    "labels": [
      "question",
      "comp-skipidx",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15835",
    "body": "The tokenbf_v2 index does not drop rows when used in an aggregation query. That is, according to the query trace, it is used and rows are said to be dropped, but eventually all rows are scanned anyway as if the index didn't exist.\r\n\r\nIn my case I have a (large) table like:\r\n\r\n```\r\nCREATE TABLE MY_TABLE\r\n(\r\n    `Year` LowCardinality(String),\r\n    `Route` String,\r\n    `Count` Float64\r\n)\r\nENGINE = ReplicatedMergeTree\r\nPARTITION BY Year,\r\nORDER BY (Month, Route, Count),\r\nINDEX route_index (Route) TYPE tokenbf_v1(256, 2, 0) GRANULARITY 1,\r\nSETTINGS index_granularity = 128;\r\n```\r\n\r\nRoute is a comma separated string of id's, like '242341,345223,12341'. There can be hundreds of id's in a Route.\r\n\r\nWhen I query: \r\n\r\n`select Count(*) from MYTABLE where Year = '2020' and hasToken(Route, '12341')`\r\n\r\nthe query trace shows this:\r\n\r\nKey condition: (column 2 in ['2020, '2020']), unknown, and\r\nMinMax index condition: (column 0 in ['2020', '2020']), unknown, and\r\nIndex `route_index` has dropped 254959 granules.\r\nSelected 1 parts by date, 1 parts by key, 17961 marks to read from 7839 ranges\r\nReading approx. 2299008 rows with 2 streams\r\n\r\nwhich looks good, but then it proceeds to read all rows anyway:\r\n\r\nAggregated. 233990 to 1 rows (from 0.225 MiB) in 5.498 sec. (42560.946 rows/sec., 0.041 MiB/sec.)\r\nAggregated. 104612 to 1 rows (from 0.102 MiB) in 5.774 sec. (18119.055 rows/sec., 0.018 MiB/sec.)\r\nAggregated. 130101 to 1 rows (from 0.127 MiB) in 5.775 sec. (22529.594 rows/sec., 0.022 MiB/sec.)\r\nAggregator: Merging aggregated data\r\nexecuteQuery: Read **34925324** rows, 16.27 GiB in 5.966 sec., 5853956 rows/sec., 2.73 GiB/sec.\r\n\r\nActually if I run the query like this, which does not invoke the route_index:\r\n\r\n`select Count(*) from MYTABLE where Year = '2020' and Route LIKE '%12341%'`\r\n\r\nin this case the query will be faster! The reason is it also scans all rows but does not have the extra first step of the tokenbf index.\r\n\r\nExpected behaviour:\r\n\r\nSince the tokenbf_v2 filter was able to skip 34925324 - 2299008 rows, which is 93% of the total number of rows, I expected the hasToken query to be faster than the LIKE query which didn't use any index.\r\nI don't understand why the hasToken query, after initially dropping all those rows, proceeds to scan all rows anyway.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15835/comments",
    "author": "misja-alma",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-10-11T14:18:45Z",
        "body": "What CH version do you use?\n\n---\n\n`route_index` analysis takes 200ms -- and this is expected.\r\n\r\n```sql\r\nCREATE TABLE MY_TABLE\r\n(\r\n    `Year` LowCardinality(String),\r\n    `Route` String,\r\n    `Count` Float64,\r\n    INDEX route_index (Route) TYPE tokenbf_v1(256, 2, 0) GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\nORDER BY tuple()\r\nSETTINGS index_granularity = 128;\r\n\r\ninsert into MY_TABLE select '2020', arrayStringConcat(arrayMap(i-> toString(intHash32(i*number)) ,range(10)),','), number\r\nfrom numbers(100000);\r\n\r\ninsert into MY_TABLE select '2020', '2299008,2299008,2299008', number from numbers(100000000);\r\n\r\nOPTIMIZE TABLE MY_TABLE FINAL\r\n\r\nselect Count(*) from MY_TABLE where hasToken(Route, '3119550599')\r\nIndex `route_index` has dropped 781655 / 782032 granules.\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.219 sec. Processed 48.26 thousand rows, 5.58 MB (220.13 thousand rows/s., 25.46 MB/s.)\r\n\r\n\r\nselect Count(*) from MY_TABLE where Route like '%3119550599%'\r\nIndex `route_index` has dropped 0 / 782032 granules.\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.643 sec. Processed 100.10 million rows, 3.21 GB (155.56 million rows/s., 4.99 GB/s.)\r\n\r\n\r\nselect Count(*) from MY_TABLE where Route like '%,3119550599,%'\r\nIndex `route_index` has dropped 781655 / 782032 granules.\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.213 sec. Processed 48.26 thousand rows, 5.58 MB (226.53 thousand rows/s., 26.20 MB/s.)\r\n\r\nalter table MY_TABLE drop index route_index;\r\nselect Count(*) from MY_TABLE where Route like '%3119550599%'\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.459 sec. Processed 100.10 million rows, 3.21 GB (217.92 million rows/s., 6.99 GB/s.)\r\n\r\n\r\nselect Count(*) from MY_TABLE where hasToken(Route, '3119550599')\r\n\u250c\u2500Count()\u2500\u2510\r\n\u2502       2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.443 sec. Processed 100.10 million rows, 3.21 GB (225.78 million rows/s., 7.24 GB/s.)\r\n\r\n```\r\n\r\nEverything works as expected.\r\n\r\nhasToken === like '%,3119550599,%'\r\n\r\n\r\nThough it's weird that `like '%3119550599%'` -- does not know that `3119550599` is a token but still uses an index.\r\n"
      },
      {
        "user": "misja-alma",
        "created_at": "2020-10-11T17:39:37Z",
        "body": "Thanks, I checked and you are right. I don't know what went wrong the first time but I recreated the table and now things seem to work correctly. Also nice that the index can also be used in LIKE queries, as long as I add the comma's.\r\n\r\nHowever, I noticed that the difference starts to become smaller when there are more partitions filled in my table. In your example there is no partitioning but my table has partitioning by YEAR. When there is only one partition filled, the query is a lot faster with the route_index. But if I add more years, the difference in speed becomes much smaller until it actually becomes much slower to query with index than without.\r\n\r\n(to be clear, querying without index I do like:\r\n\r\n`select Count(*) from MY_TABLE where Year = '2020' and Route like '%3119550599%'`\r\n\r\nand to invoke the index I add the comma's to the like )\r\n\r\nActually I noticed that the difference only appears if I call another optimize after adding the data to my table with the index.\r\nIf I only add the data and don't call optimize, the query with index remains faster."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-11T20:42:38Z",
        "body": "@misja-alma I don't understand what is you mean about partitions. Partition pruning works before index analysis.\r\n\r\n\r\n\r\n```sql\r\nCREATE TABLE MY_TABLE\r\n(\r\n    `Year` LowCardinality(String),\r\n    `Route` String,\r\n    `Count` Float64,\r\n    INDEX route_index (Route) TYPE tokenbf_v1(256, 2, 0) GRANULARITY 1\r\n)\r\nENGINE = MergeTree\r\npartition by Year\r\nORDER BY tuple()\r\nSETTINGS index_granularity = 128;\r\n\r\ninsert into MY_TABLE select '2020', arrayStringConcat(arrayMap(i-> toString(intHash32(i*number)) ,range(10)),','), number\r\nfrom numbers(100000);\r\ninsert into MY_TABLE select '2020', '2299008,2299008,2299008', number from numbers(100000000);\r\n\r\ninsert into MY_TABLE select '2019', '2299008,2299008,2299008', number from numbers(100000000);\r\ninsert into MY_TABLE select '2019', arrayStringConcat(arrayMap(i-> toString(intHash32(i*number)) ,range(10)),','), number\r\nfrom numbers(10000000);\r\n\r\noptimize table MY_TABLE  final ;\r\n\r\nselect Count(*) from MY_TABLE where Year = '2020' and hasToken(Route, '3119550599');\r\n1 rows in set. Elapsed: 0.238 sec. Processed 48.26 thousand rows, 5.63 MB (202.61 thousand rows/s., 23.64 MB/s.)\r\n\r\nselect Count(*) from MY_TABLE where Year = '2020' and  Route like '%3119550599%'\r\n1 rows in set. Elapsed: 0.744 sec. Processed 100.10 million rows, 3.31 GB (134.52 million rows/s., 4.45 GB/s.)\r\n\r\nselect Count(*) from MY_TABLE where Year = '2020' and  Route like '%,3119550599,%'\r\n1 rows in set. Elapsed: 0.233 sec. Processed 48.26 thousand rows, 5.63 MB (207.27 thousand rows/s., 24.18 MB/s.)\r\n\r\n```"
      },
      {
        "user": "misja-alma",
        "created_at": "2020-10-12T06:20:34Z",
        "body": "@den-crane I just reported what I was seeing. I also don't quite understand why the index query slowed down when filling more than one partition. But I have a feeling that it might just be the amount of data that made the index grow so large that it didn't fit into memory anymore.\r\nTo test this I tried making the index_granularity larger and the index queries started to be fast again, also with multiple partitions. This despite the fact that with larger index granularity the bloom filter cannot be as precise in dropping granules."
      },
      {
        "user": "den-crane",
        "created_at": "2020-10-13T00:50:46Z",
        "body": ">This despite the fact that with larger index granularity the bloom filter cannot be as precise in dropping granules.\r\n\r\nAnalyzing skip indexes is a super-slow process and they are heavy and need to read them from disk.\r\nWhen an index granula covers many rows it speeds up the index analyzes but decreases number of dropped rows."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why tokenbf_v2 index usage might still result in full row scans despite granule filtering",
      "Clarification on how partitioning interacts with skip index efficiency",
      "Analysis of memory constraints affecting index performance at scale",
      "Guidance on optimizing index granularity trade-offs"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:02:06"
    }
  },
  {
    "number": 15431,
    "title": "How to convert ColumnPtr object to ColumnUInt64 * object?",
    "created_at": "2020-09-29T07:12:53Z",
    "closed_at": "2020-10-13T22:11:01Z",
    "labels": [
      "question",
      "development",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15431",
    "body": "I define a class A, which has a member `ColumnUInt64 * col` in A.h file. Then, in A.cpp, I initialize the member `col` in the way:\r\n```\r\ncol = ColumnUInt64::create();\r\n```\r\nand I got a error\r\n```\r\ncan not convert COWHelper<DB::ColumnVectorHelper, DB::ColumnVector<long unsigned int> >::MutablePtr {aka \u2018COW<DB::IColumn>::mutable_ptr<DB::ColumnVector<long unsigned int> >\u2019} to non-scalar std::__1::unique_ptr<DB::ColumnVector<long unsigned int> >\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15431/comments",
    "author": "744570676",
    "comments": [
      {
        "user": "Sasasu",
        "created_at": "2020-09-29T07:29:16Z",
        "body": "`ColumnUInt64 *col = ColumnUInt64::create().detach();` if you realy want."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-29T14:47:32Z",
        "body": "@Sasasu @744570676 I'm afraid that's not the best answer.\r\nBecause it will release the ownership of the object.\r\n\r\nJust write `col.get()` to obtain non-owning pointer whille keeping the ownership by MutablePtr object."
      }
    ],
    "satisfaction_conditions": [
      "Proper ownership management when converting smart pointer types to raw pointers",
      "Type-safe conversion between ColumnPtr wrapper and concrete column type",
      "Adherence to ClickHouse's COW (Copy-On-Write) pointer semantics",
      "Preservation of object lifetime guarantees"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:02:22"
    }
  },
  {
    "number": 15295,
    "title": "Can I create MaterializedView over remote table?",
    "created_at": "2020-09-25T09:14:18Z",
    "closed_at": "2020-09-26T22:43:23Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15295",
    "body": "There are 2 clusters.\r\nI create table `base` and table `destination` on `cluster_1`.\r\n```\r\ncreate table base (id Int8, name String) ENGINE=MergeTree() order by id;\r\ncreate table destination (id Int8, cnt Int8) ENGINE=MergeTree() order by id;\r\n```\r\nI create materialized view `view` on `cluster_1` and it's working as expected.\r\n```\r\ncreate MATERIALIZED VIEW view to destination as select id,count(name) as cnt from test;\r\n```\r\n**And here is the problem**. I create table `destination` and materialized view `view` on `cluster_2` which is based on table `test` from `cluster_1`.\r\n```\r\ncreate table destination (id Int8, cnt Int8) ENGINE=MergeTree() order by id;\r\ncreate MATERIALIZED VIEW view to destination as select id,count(name) as cnt from remote('cluster_1',default.test,'default','') group by id;\r\n```\r\nI get exception:\r\n```\r\nReceived exception from server (version 20.4.4):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: Logical error while creating StorageMaterializedView. Could not retrieve table name from select query..\r\n```\r\nIt seems like materializedView source cannot be a remote table. And then I try to create a remote materializedView on `cluster_1`.\r\n```\r\ncreate MATERIALIZED VIEW remote_view to remote('cluster_2',default.destination,'default','') as select id,count(name) as cnt from test\r\n```\r\nFail again:\r\n```\r\nSyntax error: failed at position 48:\r\nExpected one of: CONSTRAINT, identifier, column declaration, INDEX, list of elements, columns or indices declaration list, table property (column, index, constraint) declaration\r\n```\r\n\r\nDo you know how to create MaterializedView over remote tables?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15295/comments",
    "author": "winter7",
    "comments": [
      {
        "user": "winter7",
        "created_at": "2020-09-25T09:45:50Z",
        "body": "The reason I ask this question is that I want to use Kafka Engine. I wonder if I should temporarily add some ClickHouse instances with kafka engine table as consumers when Kafka traffic surges and convert data by materialized View to the original (remote) Clickhouse instances. Is this a good way to deal with the sudden increase in Kafka traffic?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-25T14:25:39Z",
        "body": "MaterializedView is an insert trigger. \r\n\r\nIt gets data from **INSERT**. \r\n\r\nIt **never** reads (selects) from table.\r\n\r\nYou must create MaterializedView at a server where inserts happen. You can re-route MaterializedView output to remote server. `create table xxx as remote(); create Materialized View .... to xxx as select .....` "
      },
      {
        "user": "winter7",
        "created_at": "2020-09-27T02:55:11Z",
        "body": "Thank you! I think `create table xxx as remote();` is what I missed in my second attempt."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to route MaterializedView output to a remote server while maintaining insert trigger functionality",
      "Clarification that MaterializedViews must be created where inserts originate, not where remote data resides",
      "Validation of using temporary Kafka Engine instances with MaterializedViews for traffic scaling",
      "Avoidance of remote table references in MaterializedView source queries"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:02:46"
    }
  },
  {
    "number": 15278,
    "title": "Add summarized column to SummingMergeTree",
    "created_at": "2020-09-25T06:50:26Z",
    "closed_at": "2020-09-26T22:44:07Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15278",
    "body": "Hello guys,I have a SummingMergeTree engine table like \r\n`ENGINE = SummingMergeTree((value_a,value_b))`\r\nNow I would like to add a `value_c` UInt64 to table,is that a way I can add value `value_c` to summarized column array?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15278/comments",
    "author": "byx313",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-25T14:19:18Z",
        "body": "There is no easy way to change this list `(value_a,value_b)`. \r\nUsually it is excessive section and you can create just SummingMergeTree() then Summing will sum all columns except columns in ORDERBY list.\r\n\r\nOptions how to change it:\r\n1. detach table XXX;\r\n   vi /var/lib/clickhouse/metadata/{db}/{table}.sql # edit file manually and change that list.\r\n   attach table XXX;\r\n\r\n2. create a new table YYY with exactly the same structure as XXX but with new ((value_a,value_b)) list . \r\n    alter table YYY attach partition .... from XXX; # for each partition in XXX\r\n    rename table XXX to XXX_old, YYY to XXX;\r\n    drop table XXX_old"
      },
      {
        "user": "byx313",
        "created_at": "2020-09-27T01:27:27Z",
        "body": "thank you den!"
      }
    ],
    "satisfaction_conditions": [
      "Method to modify the list of summed columns in an existing SummingMergeTree table",
      "Solution preserves existing data integrity during schema modification",
      "Clear process for schema evolution without service interruption"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:02:54"
    }
  },
  {
    "number": 15161,
    "title": "Table system.query_log does not exist for few systems",
    "created_at": "2020-09-22T17:32:32Z",
    "closed_at": "2020-09-23T06:59:33Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/15161",
    "body": "Hi,\r\nWe are using clickhouse on our production servers. I have noticed that query_log table sometimes exist on some server and on some it does not exist.\r\nLike I am running clickhouse on two servers with same default configs, no changes except for clickhouse version:\r\n1)Clickhouse version 20..6.3 (here the table exists) \r\n2)Clickhouse version 20..4.6 (here the table does not exist)\r\nQuestion: \r\n1) does query_log is enabled by default in latest version??\r\n2) Does it depend on disk space also?? i mean the table will grow over time and does it get deleted afterwards or something like that?? When does the table flushes??",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/15161/comments",
    "author": "John-belt",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-22T18:34:20Z",
        "body": "It is enabled by default for all queries since version 20.5.\r\nIn older version you can enable it manually with the `log_queries` setting.\r\nIn new version, you can disable it with the same setting...\r\nThat's consistent with your report :)\r\n\r\n> Does it depend on disk space also?? i mean the table will grow over time and does it get deleted afterwards or something like that?? \r\n\r\nNo, there is no cleanup, it will grow indefinitely.\r\nUsually it is quite small nevertheless and there is no issue.\r\nOtherwise you can manually `TRUNCATE`, `ALTER ... DROP PARTITION` or `ALTER MODIFY TTL` to set automatic cleaning.\r\n\r\n> When does the table flushes??\r\n\r\nThe data is flushed from in-memory buffer to the table every 7 seconds by default."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of version-dependent default behavior for query_log activation",
      "Clarification of data retention and growth management mechanisms for query_log",
      "Description of configuration settings controlling query_log behavior",
      "Explanation of data persistence mechanism for query_log entries"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:03:04"
    }
  },
  {
    "number": 14881,
    "title": "clickhouse-local and table with 10K columns",
    "created_at": "2020-09-16T13:29:04Z",
    "closed_at": "2020-09-16T16:46:08Z",
    "labels": [
      "question",
      "comp-cli",
      "question-answered",
      "comp-local"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14881",
    "body": "need to convert TSV to Native but schema is too big for command-line...",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14881/comments",
    "author": "filimonov",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-16T16:46:44Z",
        "body": "```\r\nmilovidov@milovidov-desktop:~/work/tmp$ cat metadata/local/test.sql\r\nATTACH TABLE test (x UInt64 /* long list here */) ENGINE = File(TSV, stdin);\r\n\r\nmilovidov@milovidov-desktop:~/work/tmp$ ls -lR\r\n.:\r\ndrwxrwxr-x 3 milovidov milovidov 4096 \u0441\u0435\u043d 16 19:42 metadata\r\n\r\n./metadata:\r\ndrwxrwxr-x 2 milovidov milovidov 4096 \u0441\u0435\u043d 16 19:43 local\r\n\r\n./metadata/local:\r\n-rw-r--r-- 1 milovidov milovidov 77 \u0441\u0435\u043d 16 19:43 test.sql\r\n\r\nmilovidov@milovidov-desktop:~/work/tmp$ echo 123 | clickhouse-local --query \"SELECT * FROM local.test\" -- --path=.\r\n123\r\n```\n\n---\n\nYou can use `clickhouse-local` on top of predefined catalog as in the example above.\r\nIn this catalog, you can have a table with engine File and arbitrary long list of columns.\n\n---\n\n`-- --path=.`\r\n\r\nCan be also specified with `config.xml` in current directory."
      },
      {
        "user": "filimonov",
        "created_at": "2020-09-16T21:02:10Z",
        "body": "Cool! Didn't know that. BTW - it also means clickhouse-local can produce ready to attach parts. \r\n\r\nJust a side note - may be smth like `--queries-file` (as an alternative for --query) is worth adding both for clickhouse-client and clickhouse-local "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-09-16T22:15:04Z",
        "body": "> it also means clickhouse-local can produce ready to attach parts\r\n\r\nYes.\r\nAlso it can be used for \"maintanence mode\" on server.\r\n\r\n> Just a side note - may be smth like --queries-file (as an alternative for --query) is worth adding both for clickhouse-client and clickhouse-local\r\n\r\nIt's a good feature request, worth doing..."
      }
    ],
    "satisfaction_conditions": [
      "Avoids command-line length limitations for schema definition",
      "Supports external schema definition files",
      "Maintains compatibility with ClickHouse's native format generation",
      "Works with ClickHouse's file-based table engines",
      "Allows flexible path configuration for metadata"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:03:10"
    }
  },
  {
    "number": 14830,
    "title": "Cant' execute grant SQL",
    "created_at": "2020-09-15T02:51:44Z",
    "closed_at": "2020-09-15T09:17:45Z",
    "labels": [
      "question",
      "question-answered",
      "comp-rbac"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14830",
    "body": "Clikchouse Version : 20.3.17.173\r\nProblem of RBAC:\r\nWhen i execute the follow SQL, it's oK.\r\n```\r\nCREATE ROLE devgroup;\r\nGRANT SELECT,INSERT ON *.* TO devgroup;\r\n ````\r\n  \r\nBut when I `GRANT SOURCES` ,don't work;\r\n```\r\njyw-centos7-bd04 :) GRANT SOURCES ON *.* TO devgroup;\r\n\r\nSyntax error: failed at position 15:\r\n\r\nGRANT SOURCES ON *.* TO devgroup;\r\n\r\nExpected one of: EXCEPT, Comma, At, TO, token\r\n```\r\n\r\nHow can i grant privileges  about SOURCES? Thanks for you help.\r\n       \r\n  \r\n   ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14830/comments",
    "author": "spihiker",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-09-15T05:45:38Z",
        "body": "> How can i grant privileges about SOURCES?\r\n\r\nUpdate clickhouse version. 20.3 should not be used for RBAC"
      },
      {
        "user": "spihiker",
        "created_at": "2020-09-15T07:18:17Z",
        "body": "When update 20.6 ,work well .thank you ."
      }
    ],
    "satisfaction_conditions": [
      "Identifies version compatibility as the root cause of the SOURCES privilege syntax error",
      "Provides a path to enable SOURCES privilege functionality",
      "Addresses ClickHouse RBAC feature availability constraints"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:03:33"
    }
  },
  {
    "number": 14687,
    "title": "can clickhouse-copier copy data to another cluster using different timezone?",
    "created_at": "2020-09-10T09:38:37Z",
    "closed_at": "2020-09-10T15:46:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14687",
    "body": "I don't know much about the migration principle of clickhouse-copier.\r\nI have two clusters A B, A uses Asia/Shanghai time zone, and B uses Etc/UTC.\r\nI used clickhouse-copier to migrate the data and found that the Datetime columns were actually eight hours apart.\r\n\r\nBut if I manually import it in the following way, the data is correct.\r\n```\r\nclickhouse-client -udefault -hB --query=\"select * from db.table1\" --format=CSV> table1.csv\r\nclickhouse-client -u default -hA --database=broker --query=\"INSERT INTO db.table1 FORMAT CSV\" <table1.csv\r\n```\r\nI read the log and thought clickhouse-copier was also insert after select, but the result was problematic. So I want to know the difference between clickhouse-copier's  and \"select to csv then insert\"",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14687/comments",
    "author": "Fanduzi",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-10T14:02:00Z",
        "body": "Datetime. The point in time is saved as a Unix timestamp, **regardless of the time zone or daylight saving time**. \r\nAdditionally, the DateTime type can store time zone that is the same for the entire column, that affects how the values of the DateTime type values are **displayed in text format** and how the values specified **as strings are parsed** (\u20182020-01-01 05:00:01\u2019). The time zone is not stored in the rows of the table (or in resultset), but **is stored in the column metadata**.\r\n\r\n\r\nclickhouse-copier copies Datetime value as is. Number of seconds 1599746203 stays as is 1599746203 when you copy data from server A to server B. When you query 1599746203 you see different TZ string representation at A and B.\r\n\r\nWhen you use CSV you convert 1599746203 to a string and parse from a string accordingly the current TZ (for a client/server).\r\n\r\n```\r\n\r\n# TZ=UTC clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n2020-09-10 13:56:43\r\n\r\n# TZ=Asia/Shanghai clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n2020-09-10 21:56:43\r\n```"
      },
      {
        "user": "Fanduzi",
        "created_at": "2020-09-10T14:55:11Z",
        "body": "> Datetime. The point in time is saved as a Unix timestamp, **regardless of the time zone or daylight saving time**.\r\n> Additionally, the DateTime type can store time zone that is the same for the entire column, that affects how the values of the DateTime type values are **displayed in text format** and how the values specified **as strings are parsed** (\u20182020-01-01 05:00:01\u2019). The time zone is not stored in the rows of the table (or in resultset), but **is stored in the column metadata**.\r\n> \r\n> clickhouse-copier copies Datetime value as is. Number of seconds 1599746203 stays as is 1599746203 when you copy data from server A to server B. When you query 1599746203 you see different TZ string representation at A and B.\r\n> \r\n> When you use CSV you convert 1599746203 to a string and parse from a string accordingly the current TZ (for a client/server).\r\n> \r\n> ```\r\n> \r\n> # TZ=UTC clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n> 2020-09-10 13:56:43\r\n> \r\n> # TZ=Asia/Shanghai clickhouse-client -q 'select toDateTime(1599746203)' --use_client_time_zone=1\r\n> 2020-09-10 21:56:43\r\n> ```\r\n\r\nthank you.\r\nDoes this mean I cannot use clickhouse-copier to migrate data between two clusters with different time zone?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-10T15:06:31Z",
        "body": "> \r\n> Does this mean I cannot use clickhouse-copier to migrate data between two clusters with different time zone?\r\n\r\nYou can use clickhouse-copier. You need to configure the client to see results in desired timezone.\r\n\r\nexample : \r\n**TZ=Asia/Shanghai** clickhouse-client -q 'select toDateTime(1599746203)' **--use_client_time_zone=1**"
      },
      {
        "user": "Fanduzi",
        "created_at": "2020-09-10T15:46:29Z",
        "body": "> > Does this mean I cannot use clickhouse-copier to migrate data between two clusters with different time zone?\r\n> \r\n> You can use clickhouse-copier. You need to configure the client to see results in desired timezone.\r\n> \r\n> example :\r\n> **TZ=Asia/Shanghai** clickhouse-client -q 'select toDateTime(1599746203)' **--use_client_time_zone=1**\r\n\r\nThank you very much, I get it"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how time zone metadata affects DateTime values during migration",
      "Clarification on whether time zone conversion is possible during clickhouse-copier migrations",
      "Guidance on configuring client/server time zone handling for timestamp interpretation",
      "Differentiation between timestamp storage vs display in ClickHouse"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:03:44"
    }
  },
  {
    "number": 14543,
    "title": "Dump of create tables sql scripts for database",
    "created_at": "2020-09-07T12:56:13Z",
    "closed_at": "2020-09-07T13:51:14Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14543",
    "body": "Is there a standard way to get a sql script in clickhouse to create all tables in a database, or all tables and databases in general? It would be convenient to have an analogue of pg_dump to get such a script.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14543/comments",
    "author": "horoshenkiy",
    "comments": [
      {
        "user": "amosbird",
        "created_at": "2020-09-07T13:36:13Z",
        "body": "`select create_table_query from system.tables where  <your favorite filters> format TSVRaw`"
      }
    ],
    "satisfaction_conditions": [
      "Method to generate CREATE TABLE statements for all tables in a database",
      "Support for filtering/selecting specific databases or tables",
      "Output formatted as executable SQL script",
      "Native ClickHouse solution without external tools"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:04:06"
    }
  },
  {
    "number": 14419,
    "title": "No data in replicated table",
    "created_at": "2020-09-02T17:41:29Z",
    "closed_at": "2020-09-03T14:21:46Z",
    "labels": [
      "invalid",
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14419",
    "body": "I have two replicas. Data are inserted into table in replica A. Table counterpart in replica B is empty though. Data are not replicated between replicas. I tried recreating table in replica B but it did not trigger the sync.\r\n\r\nReplication for other tables is working just fine.\r\n\r\nWhat can I do to debug the issue? `show create table` gives same result for both replicas.\r\n\r\nClickHouse version 20.6",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14419/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-09-02T17:47:09Z",
        "body": "check (at both nodes) : select * from system.replication_queue \r\n\r\nwrong ZK path? different shards?"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-02T17:50:07Z",
        "body": "I don't have multiple shards.\r\n\r\nreplication queue does not contain any records for the target table\r\n\r\npath is the same (reported by show create table)\r\n\r\n\ud83e\udd14 "
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-02T17:52:25Z",
        "body": "Show `grep tablename clickhouseserver.log`"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-02T17:56:41Z",
        "body": "I tried grepping `cat /var/log/clickhouse-server/clickhouse-server.log` in both replicas but for this particular table contains no logs (it does have some warnings for larger tables though)."
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-02T18:05:33Z",
        "body": "It could be anything. How did you copy/pasted / attached zk path when you created a table?\r\n\r\nFor example these two strings are different. \r\nLatin:    /ccccc/ \r\n\u0421yrrylic: /\u0441\u0441\u0441\u0441\u0441/ \r\n\r\nCheck zookeeper select * from system.zookeeper where path = '/..zk_path/replicas\r\nCheck select * from system.replicas where table = ...\r\n"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-03T13:29:12Z",
        "body": "> select * from system.zookeeper where path = '/..zk_path/replicas\r\nGives result on both replicas. \r\n\r\nBut on the B, there's `replicas\tlast added replica: A` (version: 1)\r\nFor fully replicated tables it's `replicas\tlast added replica: B` (version: 4)\r\n\r\n`system.replicas` says the `total_replicas` is 1. Seems like those tables are disconnected on both replicas. Is there a way to get in in sync somehow?\r\n\r\n> How did you copy/pasted / attached zk path when you created a table?\r\n\r\nI did it a long time ago but It was simple CREATE TABLE ON CLUSTER I believe"
      },
      {
        "user": "den-crane",
        "created_at": "2020-09-03T14:18:37Z",
        "body": "Check at both servers that zhash is identical \r\n```\r\n\r\nselect replica_name, total_replicas, active_replicas, zookeeper_path, \r\n           cityHash64(zookeeper_path) zhash, hex(zookeeper_path) \r\nfrom system.replicas\r\nwhere table = '....'\r\n```"
      },
      {
        "user": "simPod",
        "created_at": "2020-09-03T14:21:46Z",
        "body": "Zookeeper paths differ in a small typo :/ Thank you very much for helping me debug this. I have setup a new monitoring query to catch this in the future."
      }
    ],
    "satisfaction_conditions": [
      "Identifies potential causes of replication failure specific to a single table in ClickHouse",
      "Provides methods to verify ZooKeeper path consistency between replicas",
      "Includes checks for hidden configuration mismatches beyond table schema",
      "Offers monitoring strategies to detect replication issues proactively",
      "Explains how to validate replica synchronization status through system tables"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:04:16"
    }
  },
  {
    "number": 14009,
    "title": "How to set: joined_subquery_requires_alias=0 in config.xml",
    "created_at": "2020-08-24T16:44:56Z",
    "closed_at": "2020-08-24T17:40:55Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/14009",
    "body": "We recently upgraded our Clickhouse server and started getting: \r\n\r\n```\r\nDB::Exception: No alias for subquery or table function in JOIN (set joined_subquery_requires_alias=0 to disable restriction).\r\n```\r\n\r\nI can change this setting in the command line client, but I want to change it in the server's config.xml. I've tried putting it under the `<yandex>` tag, and under the default user profile but neither work. Is there some special tagging that needs to be used around this specific setting? Thanks,\r\n\r\nMatt",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/14009/comments",
    "author": "mvcalder-xbk",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-08-24T17:39:20Z",
        "body": "Not config.xml\r\nThis parameter must be set in users profile in users.xml\r\n\r\nfor example\r\n```\r\n\r\ncat /etc/clickhouse-server/conf.d/any_join_distinct_right_table_keys.xml\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n    <profiles>\r\n        <default>\r\n            <any_join_distinct_right_table_keys>1</any_join_distinct_right_table_keys>\r\n\t    <joined_subquery_requires_alias>0</joined_subquery_requires_alias>\r\n        </default>\r\n    </profiles>\r\n</yandex>\r\n```"
      },
      {
        "user": "mvcalder-xbk",
        "created_at": "2020-08-24T17:40:55Z",
        "body": "@den-crane thanks. "
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct configuration file for server-level parameter changes",
      "Specifies the required XML structure for profile-based settings",
      "Ensures server-wide application without client-side overrides",
      "Confirms persistence across server restarts"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:04:26"
    }
  },
  {
    "number": 13878,
    "title": "How can I using Json-related-format to import multi-level nested Json data?",
    "created_at": "2020-08-19T08:07:01Z",
    "closed_at": "2020-08-20T09:33:46Z",
    "labels": [
      "question",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13878",
    "body": "Each line of my json data looks like:\r\n```\r\n{\r\n    \"id\": 1, \r\n    \"source\": \"china\",\r\n    \"sentences\":[\r\n          { content:\"I loved apples\",\r\n            words: [ {\"content\": \"I\", \"stem\": \"i\", weight: 5}, \r\n                     {\"content\": \"loved\", \"stem\": \"love\", weight: 10}, \r\n                     {\"content\": \"apples\", \"stem\": \"apple\", weight: 1}]}, \r\n         { content:\"My parents have many apples\",\r\n            words: [ {\"content\": \"My\", \"stem\": \"my\", weight: 6}, \r\n                     {\"content\": \"parentes\", \"stem\": \"parent\", weight: 5}, \r\n                     ......\r\n                     {\"content\": \"apples\", \"stem\": \"apple\", weight: 1}]}\r\n    ]\r\n}\r\n```\r\n\r\n\"sentences\" is an array, and \"words\" is an array too. \r\n\r\n\r\nHow can I load this json data to table with JsonEachRow\uff1f\r\nBecause I want to each domain in my json schema like id, source, sentences.content, sentences.words.content, sentences.words.stem, sentences.words.weight is stored separatly. So it will use the benefits of column storage.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13878/comments",
    "author": "hexiaoting",
    "comments": [
      {
        "user": "hczhcz",
        "created_at": "2020-08-19T08:50:23Z",
        "body": "For complex JSON structures, it is good to import them as strings and use JSON functions (JSONExtract-)."
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-19T08:53:14Z",
        "body": "@hczhcz \r\n> For complex JSON structures, it is good to import them as strings and use JSON functions (JSONExtract-).\r\n\r\nYou mean that take the entire json object as a string?\r\n But in the way, it is stored as a single column and we cannot take advantage of column storage in clickhouse?"
      },
      {
        "user": "hczhcz",
        "created_at": "2020-08-19T09:08:19Z",
        "body": "@hexiaoting \r\nSimply materialize what you extract from json."
      },
      {
        "user": "filimonov",
        "created_at": "2020-08-19T10:11:13Z",
        "body": "Import them as JSONAsString, parse them with JSON functions (can be in MV)"
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-20T03:03:27Z",
        "body": "@hczhcz @filimonov \r\n```\r\nclickhouse-client -q \"create table json_as_string(field String) Engine = Memory\"\r\ncat xxx| clickhouse-client -q \"insert into json_as_string format JSONAsString\"\r\n```\r\nnow all my json data is stored in json_as_string table as a string column. \r\n\r\nBut How can I extract the values of \"sentences.content\"  and \"sentences.words.content\" into another table json_data?\r\n```\r\ncreate table json_data {\r\n    id Int32;\r\n    source String;\r\n    sentences_content Array(String);\r\n    sentences_words_content Array(Array(String));\r\n    sentences_words_stem Array(Array(String));\r\n    sentences_words_weight Array(Array(Int32));\r\n}\r\n```\r\n\r\nselect  JSONExtractRaw(field, 'sentences')) from json_as_string; returns a String type not Array. Do I need to transform the String to Array type???\r\nAnd I tried using \"select JSONExtract(field, 'sentences', Array(String))\", but it  returns null."
      },
      {
        "user": "hczhcz",
        "created_at": "2020-08-20T03:44:04Z",
        "body": "```sql\r\nwith\r\n    '{\r\n        \"id\": 1, \r\n        \"source\": \"china\",\r\n        \"sentences\": [\r\n              { \"content\": \"I loved apples\",\r\n                \"words\": [ {\"content\": \"I\", \"stem\": \"i\", \"weight\": 5}, \r\n                         {\"content\": \"loved\", \"stem\": \"love\", \"weight\": 10}, \r\n                         {\"content\": \"apples\", \"stem\": \"apple\", \"weight\": 1}]}, \r\n             { \"content\": \"My parents have many apples\",\r\n                \"words\": [ {\"content\": \"My\", \"stem\": \"my\", \"weight\": 6}, \r\n                         {\"content\": \"parentes\", \"stem\": \"parent\", \"weight\": 5},\r\n                         {\"content\": \"apples\", \"stem\": \"apple\", \"weight\": 1}]}\r\n        ]\r\n    }' as root,\r\n    JSONExtractArrayRaw(root, 'sentences') as sentences,\r\n    arrayMap(s -> JSONExtractArrayRaw(s, 'words'), sentences) as words\r\nselect\r\n    arrayMap(s -> JSONExtractString(s, 'content'), sentences) as sentences_content,\r\n    arrayMap(s -> arrayMap(w -> JSONExtractString(w, 'content'), s), words) as words_content\r\n```\r\n\r\n```sql\r\ncreate table data (\r\n    root String,\r\n    sentences Array(String) alias JSONExtractArrayRaw(root, 'sentences'),\r\n    sentences_content Array(String) materialized arrayMap(s -> JSONExtractString(s, 'content'), sentences),\r\n    ...\r\n)\r\nengine = ...\r\n```\r\n\r\nFYI."
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-20T04:02:51Z",
        "body": "@hczhcz Thanks a lot . That's what I want. ^^"
      },
      {
        "user": "ramazanpolat",
        "created_at": "2020-08-22T19:37:09Z",
        "body": "Bookmarking this for later review.\n\n---\n\n@hexiaoting How did you make it work?\r\n\r\nCan you post your DDL's here? \r\n"
      },
      {
        "user": "hexiaoting",
        "created_at": "2020-08-24T03:28:25Z",
        "body": "@ramazanpolat \r\n```\r\ncreate table json1(\r\n\troot String,\r\n\tid String\r\n\t\tmaterialized JSONExtractString(root, '_id'),\r\n\tsource String\r\n\t\tmaterialized JSONExtractString(root, 'source') ,\r\n\tsentences Array(String)\r\n\t\talias JSONExtractArrayRaw(root, 'sentences'),\r\n\tsentences_content Array(String)\r\n\t\tmaterialized arrayMap(s -> JSONExtractString(s, 'content'), sentences),\r\n\twords Array(Array(String))\r\n\t\tmaterialized arrayMap(s -> JSONExtractArrayRaw(s, 'words'), sentences),\r\n\twords_content Array(Array(String))\r\n\t\tmaterialized arrayMap(s -> (arrayMap(k->JSONExtractString(k, 'content'), s)), words),\r\n\twords_stem Array(Array(String))\r\n\t\tmaterialized arrayMap(s -> (arrayMap(k->JSONExtractString(k, 'stem'), s)), words)\r\n\t) engine = MergeTree order by publish_time;\r\n\r\ncat $file.json | clickhouse-client -q \"insert into json1(root) format JSONAsString \"\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Supports extraction of multi-level nested JSON arrays into separate columns",
      "Handles array structures at multiple nesting levels (sentences array containing words arrays)",
      "Maintains data hierarchy through columnar structure without flattening",
      "Utilizes ClickHouse JSON functions effectively for data transformation",
      "Allows materialization of extracted values during table creation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:04:32"
    }
  },
  {
    "number": 13835,
    "title": "ALTER MODIFY ORDER BY does not work",
    "created_at": "2020-08-17T13:12:30Z",
    "closed_at": "2020-08-17T14:51:39Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13835",
    "body": "Hi guys,\r\n\r\nMy clickhouse version is 20.3.10.75. When altering the table order by expression, I got the exception message as follows:\r\n\r\n```\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: Existing column version is used in the expression that was added to the sorting key. You can add expressions that use only the newly added columns.\r\n```\r\n\r\nThe table is defined as follows:\r\n```\r\nCREATE TABLE default.users_online\r\n(\r\n    `when` DateTime,\r\n    `uid` UInt64,\r\n    `duration` UInt64,\r\n    `version` Int32\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(when)\r\nORDER BY (uid, when)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\nThe ALTER MODIFY ORDER BY command is ```ALTER TABLE users_online MODIFY ORDER BY (uid, when, version)```.\r\n\r\nThe expected behavior is table's order by expression should be modified.\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13835/comments",
    "author": "fastio",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-08-17T13:50:36Z",
        "body": "Code: 36. DB::Exception: Received from localhost:9000. DB::Exception: Existing column version is used in the expression that was added to the sorting key. **You can add expressions that use only the newly added columns.**\r\n\r\n\r\nCREATE TABLE default.users_online\r\n(\r\n    when DateTime,\r\n    uid UInt64,\r\n    duration UInt64\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(when)\r\nORDER BY (uid, when)\r\nSETTINGS index_granularity = 8192\r\n\r\nOk.\r\n\r\n\r\nALTER TABLE default.users_online  **ADD COLUMN version Int32,**  MODIFY ORDER BY (uid, when, version)\r\n\r\nOk.\r\n\r\n\r\n"
      },
      {
        "user": "fastio",
        "created_at": "2020-08-17T14:08:45Z",
        "body": "@den-crane Thank you for reply. I got it. If the ORDER BY expression modified with existing column,  the existing data of table should be re-sorted which will pay a huge cost."
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-17T14:49:52Z",
        "body": "Yes. \r\n\r\nAnd BTW, `MODIFY ORDER BY` does not change primary index, it changes only rows sorting (for new parts).\r\n\r\n\r\nSHOW CREATE TABLE default.users_online\r\n\r\nCREATE TABLE default.users_online\r\n(\r\n    when DateTime,\r\n    uid UInt64,\r\n    duration UInt64,\r\n    version Int32\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(when)\r\n**PRIMARY KEY (uid, when)**\r\n**ORDER BY (uid, when, version)**\r\nSETTINGS index_granularity = 8192 \r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why existing columns cannot be added to the sorting key through MODIFY ORDER BY",
      "Clarification of data reorganization requirements when modifying sorting keys",
      "Differentiation between ORDER BY and PRIMARY KEY behavior in ClickHouse"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:04:44"
    }
  },
  {
    "number": 13543,
    "title": "How could I import kafka message with json object in CH?",
    "created_at": "2020-08-09T07:57:15Z",
    "closed_at": "2020-08-10T05:57:26Z",
    "labels": [
      "question",
      "comp-kafka",
      "comp-formats",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13543",
    "body": "I have a json message in kafka.\r\n```json\r\n{\r\n  \"payload\": {\r\n    \"id\": 3,\r\n    \"name\": \"c\",\r\n    \"__op\": \"c\",\r\n    \"__lsn\": 367383816,\r\n    \"__schema\": \"ec_mall66\",\r\n    \"__deleted\": \"false\"\r\n  }\r\n}\r\n```\r\nCan I import the message in CH?\r\nI saw documents but it seems CH only supports nested json object with [].",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13543/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-08-09T19:36:41Z",
        "body": "Use 20.5 and newer and JSONAsString format. After that parse the string with JSON functions "
      }
    ],
    "satisfaction_conditions": [
      "Support for ingesting JSON with nested objects not using array syntax",
      "Mechanism to parse JSON strings into structured data",
      "Compatibility with ClickHouse's JSON processing capabilities",
      "Solution applicable to Kafka message ingestion workflow"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:04:58"
    }
  },
  {
    "number": 13542,
    "title": "Kafka Connect with Schema Registry",
    "created_at": "2020-08-09T07:24:59Z",
    "closed_at": "2020-08-10T06:02:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13542",
    "body": "Hello :)\r\n\r\nI'm using kafka connect with schema registry and I want to import kafka message to ClickHouse.\r\n\r\nDoes CH support schema registry?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13542/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2020-08-09T19:37:45Z",
        "body": "Check AvroConfluent format"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-08-10T06:02:15Z",
        "body": "thanks. :)"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of ClickHouse's compatibility with Confluent Schema Registry data formats",
      "Identification of supported serialization/deserialization formats related to Schema Registry"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:05:02"
    }
  },
  {
    "number": 13256,
    "title": "\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 CREATE USER",
    "created_at": "2020-08-03T04:05:49Z",
    "closed_at": "2020-08-03T17:11:17Z",
    "labels": [
      "question",
      "question-answered",
      "comp-rbac"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13256",
    "body": "ClickHouse server version 20.5.4 revision 54435\r\n\r\n<access_management>1</access_management>\r\n\r\nCode: 514. DB::Exception: Received from localhost:9000. DB::Exception: Not found a storage to insert user",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13256/comments",
    "author": "handgunman",
    "comments": [
      {
        "user": "handgunman",
        "created_at": "2020-08-03T09:50:57Z",
        "body": "\u0432 \u0442\u0440\u0430\u043a\u0435\u0440\u043e\u0432\u043a\u0435 \u0432\u0440\u043e\u0434\u0435 \u043d\u0438\u0447\u0435\u0433\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0433\u043e\r\nCode: 514. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Not found a storage to insert user `okraina`. Stack trace:\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x11b9acc0 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x9f3e2cd in /usr/bin/clickhouse\r\n2. ? @ 0xe508201 in /usr/bin/clickhouse\r\n3. ? @ 0xe5000c8 in /usr/bin/clickhouse\r\n4. DB::IAccessStorage::insert(std::__1::vector<std::__1::shared_ptr<DB::IAccessEntity const>, std::__1::allocator<std::__1::shared_ptr<DB::IAccessEntity const> > > const&) @ 0xe500e9f in /usr/bin/clickhouse\r\n5. DB::InterpreterCreateUserQuery::execute() @ 0xe9cd24d in /usr/bin/clickhouse\r\n6. ? @ 0xed3c7ed in /usr/bin/clickhouse\r\n7. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xed3fe2a in /usr/bin/clickhouse\r\n8. DB::TCPHandler::runImpl() @ 0xf36443c in /usr/bin/clickhouse\r\n9. DB::TCPHandler::run() @ 0xf365190 in /usr/bin/clickhouse\r\n10. Poco::Net::TCPServerConnection::start() @ 0x11ab8aeb in /usr/bin/clickhouse\r\n11. Poco::Net::TCPServerDispatcher::run() @ 0x11ab8f7b in /usr/bin/clickhouse\r\n12. Poco::PooledThread::run() @ 0x11c37aa6 in /usr/bin/clickhouse\r\n13. Poco::ThreadImpl::runnableEntry(void*) @ 0x11c32ea0 in /usr/bin/clickhouse\r\n14. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n15. clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-08-03T17:09:31Z",
        "body": "Check that you have configured  access_control_path in config.xml\r\n\r\n```\r\n    <!-- Path to folder where users and roles created by SQL commands are stored. -->\r\n    <access_control_path>/var/lib/clickhouse/access/</access_control_path>\r\n```"
      },
      {
        "user": "handgunman",
        "created_at": "2020-08-03T17:11:17Z",
        "body": "\u0421\u043f\u0430\u0441\u0438\u0431\u043e! \u0415\u0433\u043e \u043d\u0435 \u0431\u044b\u043b\u043e \u0432 \u0441\u0442\u0430\u0440\u043e\u043c \u043a\u043e\u043d\u0444\u0438\u0433\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b, \u043f\u043e\u0442\u043e\u043f\u0443 \u043f\u0440\u043e\u0433\u043b\u044f\u0434\u0435\u043b."
      },
      {
        "user": "draev",
        "created_at": "2020-11-06T09:59:27Z",
        "body": "> Check that you have configured access_control_path in config.xml\r\n> \r\n> ```\r\n>     <!-- Path to folder where users and roles created by SQL commands are stored. -->\r\n>     <access_control_path>/var/lib/clickhouse/access/</access_control_path>\r\n> ```\r\n\r\n\u0410 \u0421\u0435\u0440\u0432\u0435\u0440 \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0442\u044c \u043f\u043e\u0441\u043b\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043a\u043e\u043d\u0444\u0438\u0433\u0430 ?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2020-11-06T11:43:51Z",
        "body": "\u0421\u043a\u043e\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e \u0434\u0430."
      }
    ],
    "satisfaction_conditions": [
      "Identifies the root cause of 'Not found a storage to insert user' error",
      "Explains required configuration for SQL-based user management",
      "Specifies server restart requirements after configuration changes",
      "Addresses access control storage initialization"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:05:28"
    }
  },
  {
    "number": 13195,
    "title": "the equal function like MySQL's group_concat",
    "created_at": "2020-07-31T17:26:58Z",
    "closed_at": "2020-08-01T14:51:47Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13195",
    "body": " is there a function as MySQL's group_concat?\r\nthe example show in MySQL:\r\n>create table kpi(emp_no varchar(8),performance varchar(32),month varchar(32));\r\n>insert into kpi values (10,'A','2020-01'),(10,'A','2020-02'),(10,'C','2020-03'),(10,'B','2020-04'),(10,'A','2020-05'),(10,'A','2020-06');\r\n>insert into kpi values (20,'A','2020-01'),(20,'B','2020-02'),(20,'C','2020-03'),(20,'C','2020-04'),(20,'A','2020-05'),(20,'D','2020-06'); \r\n>insert into kpi values (30,'C','2020-03'),(30,'C','2020-04'),(30,'B','2020-05'),(30,'B','2020-06');\r\n\r\n>mysql> select emp_no,group_concat(performance order by month separator '-') kpi_list,group_concat(distinct performance order by month separator '-') kpi_uniq,group_concat(distinct performance order by month desc separator '-') kpi_uniq_desc from kpi group by emp_no;  \r\n>+--------+-------------+----------+---------------+\r\n| emp_no | kpi_list    | kpi_uniq | kpi_uniq_desc |\r\n+--------+-------------+----------+---------------+\r\n| 10     | A-A-C-B-A-A | A-C-B    | B-C-A         |\r\n| 20     | A-B-C-C-A-D | A-B-C-D  | D-C-B-A       |\r\n| 30     | C-C-B-B     | C-B      | B-C           |\r\n+--------+-------------+----------+---------------+\r\n3 rows in set (0.00 sec)\r\n\r\nBy the way i want to get each user's the count of performance level ,for examle emp_no=10 have A 4 times\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13195/comments",
    "author": "vkingnew",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-07-31T20:40:48Z",
        "body": "There is `groupArray` aggregate function, it returns array of all values.\r\nThere is `groupUniqArray` that returns array of all distinct values.\r\n\r\nAnd you can convert resulting array to string with `arrayStringConcat` function."
      },
      {
        "user": "vkingnew",
        "created_at": "2020-08-01T06:25:16Z",
        "body": "ok by your tips,i get it.\r\n>SELECT \r\n    emp_no,\r\n    groupArray(performance) AS kpi_asc,\r\n    arrayStringConcat(kpi_asc, '-') AS kpi_list,\r\n    groupUniqArray(performance) AS kpi_uniq,\r\n    countEqual(kpi_asc, 'A') AS A_cnt,\r\n    countEqual(kpi_asc, 'B') AS B_cnt,\r\n    countEqual(kpi_asc, 'C') AS C_cnt,\r\n    countEqual(kpi_asc, 'D') AS D_cnt\r\nFROM kpi\r\nGROUP BY emp_no\r\nORDER BY emp_no ASC\r\n>\u250c\u2500emp_no\u2500\u252c\u2500kpi_asc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500kpi_list\u2500\u2500\u2500\u2500\u252c\u2500kpi_uniq\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500A_cnt\u2500\u252c\u2500B_cnt\u2500\u252c\u2500C_cnt\u2500\u252c\u2500D_cnt\u2500\u2510\r\n\u2502 10     \u2502 ['A','A','C','B','A','A'] \u2502 A-A-C-B-A-A \u2502 ['B','A','C']     \u2502     4 \u2502     1 \u2502     1 \u2502     0 \u2502\r\n\u2502 20     \u2502 ['A','B','C','C','A','D'] \u2502 A-B-C-C-A-D \u2502 ['B','A','D','C'] \u2502     2 \u2502     1 \u2502     2 \u2502     1 \u2502\r\n\u2502 30     \u2502 ['C','C','B','B']         \u2502 C-C-B-B     \u2502 ['B','C']         \u2502     0 \u2502     2 \u2502     2 \u2502     0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n3 rows in set. Elapsed: 0.004 sec. "
      },
      {
        "user": "fzhedu",
        "created_at": "2021-05-18T14:06:58Z",
        "body": "`groupUniqArray ` and `grougArray ` just accept only only expression, but `group_concat(x,x,x...)` could accept more expressions and order by. So how Clickhouse compeletly support `group_concat(x,x,x...)` of mysql?"
      }
    ],
    "satisfaction_conditions": [
      "Support for concatenating grouped values into a delimited string",
      "Ability to handle distinct values in concatenation results",
      "Support for ordered concatenation based on another column",
      "Capability to count occurrences of specific values in groups"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:05:37"
    }
  },
  {
    "number": 13109,
    "title": "How does concurrency control work?",
    "created_at": "2020-07-30T13:43:56Z",
    "closed_at": "2020-07-30T14:12:17Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/13109",
    "body": "Hi. I'd like to know how **ClickHouse** ensures *concurrency control* at table level. For example, if there's many applications writing in the same table at the same time, won't data get corrupted?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/13109/comments",
    "author": "zergon321",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-07-30T14:07:40Z",
        "body": "MergeTree tables are represented by a set of immutable data parts. Data parts are never modified, only created and deleted. INSERTs create new data parts. SELECTs take snapshot of required data parts and hold the snapshot during the query.\r\nSnapshots are based on reference counting and snapshot acquire operation is very cheap.\r\nBoth SELECTs and INSERTs and even ALTERs can be executed concurrently.\r\n\r\nData parts are merged in background - a new larger data part is created as a result of merge, then source data parts are deleted.\r\nWhen data needs to be modified (ALTER query), we perform copy-on-write - create another part and remove source part.\r\nVersion number and block number are tracked for each part.\r\n\r\nWhen we write new data part, we create temporary data and then perform atomic commit (the data is appeared in a table atomically). The user cannot see partially written data part and no data corruption possible.\r\n\r\nTLDR, it's MVCC.\r\n\r\nPS. Other (rudimentary) table engines like Log, TinyLog, StripeLog are using simple read-write lock.\r\nMemory table engine is using snapshot like MergeTree."
      },
      {
        "user": "zergon321",
        "created_at": "2020-07-30T14:12:17Z",
        "body": "Thank you very much for the answer!"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how concurrent writes are prevented from corrupting data",
      "Description of table-level isolation mechanisms for different operations",
      "Coverage of different table engine behaviors",
      "Clarification of atomicity guarantees for data visibility"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:05:44"
    }
  },
  {
    "number": 12750,
    "title": "Exception with INSERT INTO ",
    "created_at": "2020-07-24T13:12:44Z",
    "closed_at": "2020-07-24T19:07:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/12750",
    "body": "Hello !\r\n\r\n**Describe the bug**\r\nI try to use INSERT INTO  a table (not a view !) using the AggregatingMergeTree() engine, or in a Distributed table created \"on\" the table using the AggregatingMergeTree ; in both case I have the following exception : \r\n> Code: 20. DB::Exception: Received from localhost:9000. DB::Exception: Number of columns doesn't match.\r\n\r\n**How to reproduce**\r\nClickHouse server version 20.5.2 revision 54435\r\nClickHouse client version 20.5.2.7 (official build)\r\n\r\nI didn't change the setting,\r\n\r\n1>  I have created a Distributed table \"visits_distributed_v2\", and filled it.\r\n2> I have created the \"all_visitor\" table :\r\n```\r\nCREATE TABLE poc.all_visitor\r\n(\r\n    `VisitorCode` String,\r\n    `arrayVisitDuration` AggregateFunction(groupArray, Int64)\r\n)\r\nENGINE = AggregatingMergeTree()\r\nPARTITION BY VisitorCode\r\nORDER BY VisitorCode\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\n3> I have created the Distributed table all_visitor_distributed :\r\n```\r\nCREATE TABLE poc.all_visitor_distributed AS poc.all_visitor\r\nENGINE = Distributed(test_shard_localhost, poc, all_visitor, rand())\r\n```\r\n\r\n4> When I try to insert data from the \"visits_distributed_v2\" table in the Distributed Table or the \"source\" one, I have the issue.\r\n```\r\nINSERT INTO poc.all_visitor_distributed SELECT (VisitorCode, groupArrayState(VisitDuration))\r\nFROM poc.visits_distributed_v2\r\nGROUP BY VisitorCode\r\n```\r\n\r\n**Expected behavior**\r\nIt could totally by an error on my side, I already checked the name of the columns or their types, the number of columns seems to match but the error is misleading that's why I need help :)\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/12750/comments",
    "author": "RonanMorgan",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-07-24T15:55:21Z",
        "body": ">SELECT (VisitorCode, groupArrayState(VisitDuration))\r\n\r\nbecause of ( )\r\n\r\ntry `SELECT VisitorCode, groupArrayState(VisitDuration)`\r\n\r\n( ) -- makes a special type - Tuple, syntax sugar for a  function -- tuple\r\n```\r\n\r\nselect 1, 2, tuple(1,2), (1,2)\r\n\u250c\u25001\u2500\u252c\u25002\u2500\u252c\u2500tuple(1, 2)\u2500\u252c\u2500tuple(1, 2)\u2500\u2510\r\n\u2502 1 \u2502 2 \u2502 (1,2)       \u2502 (1,2)       \u2502\r\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n\r\ndesc(select 1, 2, tuple(1,2) y, (1,2) x)\r\n\u250c\u2500name\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\r\n\u2502 1    \u2502 UInt8               \u2502\r\n\u2502 2    \u2502 UInt8               \u2502\r\n\u2502 y    \u2502 Tuple(UInt8, UInt8) \u2502\r\n\u2502 x    \u2502 Tuple(UInt8, UInt8) \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the column count mismatch occurs despite matching column names/types",
      "Clarification of how tuple syntax affects INSERT operations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:05:52"
    }
  },
  {
    "number": 12565,
    "title": "Support configuration hot reload of merge_tree_settings?",
    "created_at": "2020-07-17T14:57:10Z",
    "closed_at": "2020-07-17T15:35:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/12565",
    "body": "Hi team:\r\n    I found there no hot reload ability of merge_tree_settings like max_parts_in_total, parts_to_delay_insert and so on. Are there any ways to implement hot reload?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/12565/comments",
    "author": "kekekedeng",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-07-17T15:03:04Z",
        "body": "you can apply it to the table \r\n```\r\nalter  table foobar modify setting max_parts_in_total  = 6000;\r\n\r\n```"
      },
      {
        "user": "kekekedeng",
        "created_at": "2020-07-17T15:35:59Z",
        "body": "@den-crane It works, thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to dynamically apply merge_tree_settings changes without requiring a service restart",
      "Applies settings at the table level rather than requiring global configuration changes",
      "Works for parameters like max_parts_in_total and parts_to_delay_insert"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:06:13"
    }
  },
  {
    "number": 11888,
    "title": "Multithreading reading the same FD problem.",
    "created_at": "2020-06-23T12:54:56Z",
    "closed_at": "2020-06-24T00:19:03Z",
    "labels": [
      "question",
      "development",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/11888",
    "body": "```seek``` and ```read``` are not atomic operations.I didn't see the lock.Will there be concurrency issues?\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/11888/comments",
    "author": "nicelulu",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-06-23T17:22:44Z",
        "body": "We don't read from one fd from multiple threads.\r\nIf we read from multiple threads, file is opened multiple times."
      },
      {
        "user": "nicelulu",
        "created_at": "2020-06-24T01:46:33Z",
        "body": "> We don't read from one fd from multiple threads.\r\n> If we read from multiple threads, file is opened multiple times.\r\n\r\nThanks for your reply, i get it."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how concurrent access to the same file descriptor (FD) is avoided in multithreaded environments",
      "Clarification of resource management strategy for shared files across threads"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:06:23"
    }
  },
  {
    "number": 10891,
    "title": "How to create table with MATERIALIZED column as select from",
    "created_at": "2020-05-13T14:39:34Z",
    "closed_at": "2020-05-13T14:53:04Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10891",
    "body": "Please? Help me.\r\n\r\nI want to know how create table with MATERIALIZED column from another select with input variables.\r\nI do next:\r\n\r\nCREATE TABLE ANALYST.DATA_TEST1 (\r\n `ids` UInt16,\r\n `timestamp` String,\r\n `code` UInt64,\r\n `id` UUID MATERIALIZED generateUUIDv4(),\r\n `timemoment` DateTime MATERIALIZED CAST(formatDateTime(toDateTime(substring(timestamp, 1, 19)), '%Y-%m-%d %T'), 'DateTime'),\r\n `adate` Date MATERIALIZED toDate(timemoment),\r\n `idate` Date MATERIALIZED toDate(now()),\r\n `moduleid` UInt16 MATERIALIZED (SELECT if((SELECT uniqExact(ANALYST.HOME.moduleid) FROM ANALYST.HOME WHERE ANALYST.HOME.id_soa = ids) > 1, 379, 339))\r\n) ENGINE = MergeTree(adate, (moduleid, timemoment, code), 8192);\r\n\r\nbut execution finshed with error DB::Exception: Missing columns: 'ids' while processing query: 'SELECT uniqExact(ANALYST.HOME.moduleid) FROM ANALYST.HOME WHERE id_soa = ids', required columns: 'ids' 'moduleid' 'id_soa', source columns: 'moduleid' 'description' 'host_min' 'id_soa' 'network' 'host_max'\r\n\r\nMaybe I'm wrong and it's impossible!? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10891/comments",
    "author": "xap9i",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-05-13T14:50:54Z",
        "body": "It's impossible. You can use `select from` in MATERIALIZED column. You can use only `dictGet<Type>`. "
      }
    ],
    "satisfaction_conditions": [
      "Clarification on whether subqueries in MATERIALIZED columns can reference other columns from the same table row",
      "Explanation of allowed query patterns for MATERIALIZED columns in ClickHouse",
      "Alternative approaches for deriving column values from other tables",
      "Guidance on handling row-level context in column defaults"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:06:33"
    }
  },
  {
    "number": 10883,
    "title": "Multiple clusters, same servers",
    "created_at": "2020-05-13T10:26:06Z",
    "closed_at": "2020-05-15T05:00:59Z",
    "labels": [
      "question",
      "comp-distributed"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10883",
    "body": "I have  a distributed table with replication (using zookeeper).\r\n\r\n```\r\n\r\ncreate table s_actions (...)\r\nPARTITION BY toMonday(createdon)\r\nORDER BY\r\n  (createdon, user__id) SAMPLE BY user__id SETTINGS index_granularity = 8192\r\n \r\n```\r\n\r\nI have primary replicas with 3 servers with a lot of memory and cpu . Second replica  has slow ssd ,less cpu and ram and is used for replication backup and then daily backups (FREEZE PARTITION).\r\n\r\nI have distributed table like\r\n\r\n````\r\n CREATE TABLE actions (\r\n....\r\n) ENGINE = Distributed(\r\n  rep,\r\n  actions,\r\n  s_actions,\r\n  cityHash64(toString(user__id))\r\n)\r\n\r\n```` \r\nrep cluster has only one replica for each shard. So If any server from primary replica fails everything will be broken. I want to create rep_write cluster in clickhouse config with secondary replicas to allow writes to secondary or primary replicas . Reads are not needed to be protected. \r\n\r\nProblem is that I'm using hashing function instead of random to optimize performance. Is it safe to define separate clusters with same (by order) servers (with extra replicas) and use distributed tables with same hashing function? \r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10883/comments",
    "author": "thyn",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-05-13T13:44:15Z",
        "body": "Yes. It is safe. If you make a new cluster with the same shards/servers order it will have the same shard numbers. \r\nYou can verify it in `select * from system.clusters`"
      },
      {
        "user": "thyn",
        "created_at": "2020-05-15T05:00:59Z",
        "body": "It's great, thank you"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that cluster configuration with identical server order preserves shard numbering",
      "Assurance of write redundancy without breaking hash-based distribution"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:06:43"
    }
  },
  {
    "number": 10062,
    "title": "Duplicated primary key in materialized view",
    "created_at": "2020-04-06T09:16:00Z",
    "closed_at": "2020-04-06T18:29:41Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/10062",
    "body": "I have a table\r\n```sql\r\nCREATE TABLE user_video_view (\r\n\tuser_id\t\t\t\tUInt64,\r\n\tvideo_id\t\t\tUInt64,\r\n\tvisitor_session_id\tUUID,\r\n\tvisitor_id\t\t\tUInt64,\r\n\tvisitor_ip\t\t\tString,\r\n\tvisitor_user_agent\tString,\r\n\tcreated_at\t\t\tDateTime\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY (toYYYYMM(created_at), user_id, video_id)\r\nORDER BY (created_at, user_id, video_id, visitor_session_id)\r\n```\r\nand materialized view based on it\r\n```sql\r\nCREATE MATERIALIZED VIEW grouped_user_video_view\r\nENGINE = SummingMergeTree()\r\nPARTITION BY week\r\nORDER BY (user_id, week)\r\nPOPULATE\r\nAS SELECT\r\n\tintDiv(toRelativeWeekNum(created_at) - toRelativeWeekNum(toDateTime('2020-03-23 00:00:00')), 2) AS week,\r\n\tuser_id,\r\n\tcount() AS view_count\r\nFROM user_video_view\r\nGROUP BY user_id, week\r\n```\r\n\r\nFor some combination of `user_id` and `week` I have duplicated rows in response:\r\n```\r\n\u250c\u2500week\u2500\u252c\u2500user_id\u2500\u252c\u2500view_count\u2500\u2510\r\n\u2502    0 \u2502     159 \u2502          1 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\u250c\u2500week\u2500\u252c\u2500user_id\u2500\u252c\u2500view_count\u2500\u2510\r\n\u2502    0 \u2502       5 \u2502          2 \u2502\r\n\u2502    0 \u2502      15 \u2502          5 \u2502\r\n\u2502    0 \u2502      16 \u2502          4 \u2502\r\n\u2502    0 \u2502      17 \u2502          1 \u2502\r\n\u2502    0 \u2502      42 \u2502          2 \u2502\r\n\u2502    0 \u2502      45 \u2502          3 \u2502\r\n\u2502    0 \u2502     159 \u2502          2 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nIs this an expected behavior?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/10062/comments",
    "author": "grachov",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-04-06T12:41:43Z",
        "body": "Yes. SumminngMT collapses rows during the Merge. \r\nSummingMT and AggregatingMT expect that select query will do final aggregation using group by. Merges are eventual and may never happen.\r\n\r\nSo CH expects that all queries will do final summing using\r\n```\r\nselect sum(view_count) ,  ...\r\nfrom grouped_user_video_view\r\ngroup by ...\r\n```"
      },
      {
        "user": "grachov",
        "created_at": "2020-04-06T13:34:18Z",
        "body": "Thanks for explanation! Does it mean that other view (not materialized) can be created on top of it to apply grouping?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-04-06T14:01:34Z",
        "body": "> Thanks for explanation! Does it mean that other view (not materialized) can be created on top of it to apply grouping?\r\n\r\nyes.\r\n\r\n\r\n\r\nBe aware that double groupping and reading excessive columns may slow-down your queries up to 10 times.\r\n\r\n```\r\nselect sum(view_count), user_id\r\nfrom grouped_user_video_view\r\ngroup by user_id\r\n```\r\n\r\nVS\r\n```\r\n\r\nselect sum(view_count), user_id\r\nfrom ( ---  view\r\n              select sum(view_count), user_id, week\r\n              from grouped_user_video_view \r\n              group by user_id, week \r\n         ) \r\ngroup by user_id\r\n\r\n```"
      },
      {
        "user": "grachov",
        "created_at": "2020-04-06T18:23:56Z",
        "body": "Thanks again! \ud83d\udc4d "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why duplicate primary keys occur in SummingMergeTree materialized views",
      "Clear guidance on proper aggregation techniques for SummingMergeTree results",
      "Performance considerations for aggregation strategies",
      "Clarification of eventual consistency in ClickHouse merges"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:06:49"
    }
  },
  {
    "number": 9541,
    "title": "Materialized View with targeting past data",
    "created_at": "2020-03-06T16:06:16Z",
    "closed_at": "2020-03-17T02:52:31Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9541",
    "body": "I am having an issue with a Materialized View which targets the past data. I know that MV works like a trigger for inserts to a table. \r\n\r\nI need a Materialized View only for yesterday. I have the following MV:\r\n\r\n```\r\nCREATE MATERIALIZED VIEW default.chart_yesterday\r\nENGINE = ReplicatedSummingMergeTree(\r\n     '/clickhouse/tables/{shard}/default/chart_yesterday',\r\n     '{replica}')\r\n     PARTITION BY toYYYYMM(date)\r\n     ORDER BY (date, hour, cityHash64(organization_id)\r\n)\r\nSAMPLE BY cityHash64(organization_id)\r\nSETTINGS index_granularity = 8192\r\nPOPULATE AS\r\nSELECT\r\n     SUM(rejected) AS clr,\r\n     (count() - clr) AS cla,\r\n     toDate(request_time) AS date,\r\n     toHour(request_time) as hour,\r\n     organization_id\r\nFROM mytable_sharded\r\nWHERE date = yesterday()\r\nGROUP BY date, hour, organization_id\r\nORDER BY hour;\r\n```\r\n\r\nAfter creating the VM, I have data only for yesterday, everything fine. but after a day, the VM has no data\r\n\r\nDoes it mean that since there is no trigger for yesterday's data, so VM doesn't get triggered, so no data?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9541/comments",
    "author": "hatrena",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-03-06T21:57:52Z",
        "body": ">WHERE date = yesterday()\r\n\r\nYou MV will get a new data only if `insert into mytable_sharded` will insert `date = yesterday()`\r\n\r\nYou can create MV without this condition `WHERE date = yesterday()`\r\nWith daily partitioning ` PARTITION BY toYYYYMMDD(date)`\r\nAnd remove data older than yesterday by `drop partition` or by `table TTL`\r\nthen your MV will have data only for yesterday and today."
      },
      {
        "user": "hatrena",
        "created_at": "2020-03-25T15:21:36Z",
        "body": "I used `PARTITION BY toYYYYMMDD(date)` with `table TTL`, and it worked perfectly. thanks"
      }
    ],
    "satisfaction_conditions": [
      "Solution must retain yesterday's data in the materialized view beyond the initial population day",
      "Implementation must automatically manage data expiration for older partitions",
      "Approach should work with continuous data ingestion without relying on insert triggers for specific dates",
      "Must maintain daily partitioning scheme for efficient data management"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:07:26"
    }
  },
  {
    "number": 9504,
    "title": "Error in system.replication_queue ",
    "created_at": "2020-03-04T00:38:39Z",
    "closed_at": "2020-03-04T01:20:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9504",
    "body": "version  20.2.1\r\n\r\nThere is  an error in table system.replication_queue  .\r\n\r\n Not executing log entry for part 20200213_2040_2040_1_2016 because another log entry for the same part is being processed. This shouldn't happen often.\r\n\r\n\r\n\r\n\u250c\u2500database\u2500\u252c\u2500table\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500replica_name\u2500\u252c\u2500position\u2500\u252c\u2500node_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500type\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500create_time\u2500\u252c\u2500required_quorum\u2500\u252c\u2500source_replica\u2500\u252c\u2500new_part_name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500parts_to_merge\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500is_detach\u2500\u252c\u2500is_currently_executing\u2500\u252c\u2500num_tries\u2500\u252c\u2500last_exception\u2500\u252c\u2500\u2500\u2500last_attempt_time\u2500\u252c\u2500num_postponed\u2500\u252c\u2500postpone_reason\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500last_postpone_time\u2500\u2510\r\n\u2502 my_sdap  \u2502 dm_user_behavior_events \u2502 replica-001  \u2502        0 \u2502 queue-0000104788 \u2502 MERGE_PARTS \u2502 2020-03-04 07:46:52 \u2502               0 \u2502 replica-001    \u2502 20200213_2040_2040_1_2016 \u2502 ['20200213_2040_2040_0_2016'] \u2502         0 \u2502                      0 \u2502         0 \u2502                \u2502 0000-00-00 00:00:00 \u2502          3801 \u2502 Not executing log entry for part 20200213_2040_2040_1_2016 because another log entry for the same part is being processed. This shouldn't happen often. \u2502 2020-03-04 08:36:43 \u2502",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9504/comments",
    "author": "onine007",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2020-03-04T01:19:07Z",
        "body": "This is normal system behaviour.\n\n---\n\nAnd this is not an error but `postpone_reason`."
      },
      {
        "user": "onine007",
        "created_at": "2020-03-04T01:20:42Z",
        "body": "OK  ,thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that the message represents normal system behavior rather than an error",
      "Clarification about the difference between error messages and system status notifications"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:07:35"
    }
  },
  {
    "number": 9115,
    "title": "How to update using Join with 2 join condition",
    "created_at": "2020-02-14T10:03:02Z",
    "closed_at": "2020-02-15T09:03:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9115",
    "body": "I want to make below query as clickhouse query.\r\n```sql\r\nupdate table1 \r\nset table1.col4 = table2.col4\r\nfrom table2 \r\nwhere table1.col1 = table2.col1 and table1.col2 = table2.col2 and table2.col3='2020-01-02';\r\n```\r\n\r\nI made a query like below, But, I got error and don`t know how to make 2 join condition.\r\n\r\n```sql\r\n\r\nCREATE TABLE test1\r\n(\r\n    `col1` Int8, \r\n    `col2` String, \r\n    `col3` Date, \r\n    `col4` UInt16\r\n)\r\nENGINE = Log\r\n\r\nINSERT INTO test1 VALUES(1,'001','2020-01-01', 1)(1,'002','2020-01-01', 1)(2,'001','2020-01-01', 2)(2,'002','2020-01-02', 3)(2,'003','2020-01-04', 5);\r\n\r\n-- create join engine\r\nCREATE TABLE test_join AS test1\r\nENGINE = Join(ANY, LEFT, col1, col2)\r\n\r\nOk.\r\n\r\nINSERT INTO test_join SELECT *\r\nFROM test1\r\nWHERE col3 = '2020-01-02'\r\n\r\n-- update\r\n:) ALTER TABLE test1 UPDATE col4 = joinGet('test_join', 'col4', col1, col2);\r\n\r\nSyntax error: failed at position 73 (end of query):\r\n\r\nALTER TABLE test1 UPDATE col4 = joinGet('test_join', 'col4', col1, col2);\r\n\r\nExpected one of: AND, OR, token, WHERE, NOT, BETWEEN, LIKE, IS, NOT LIKE, IN, NOT IN, GLOBAL IN, GLOBAL NOT IN, Comma, QuestionMark\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9115/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "4ertus2",
        "created_at": "2020-02-14T12:33:46Z",
        "body": "You've made something very strange.\r\n\r\n1. ALTER UPDATE is special operation. Do not use it in general ETL logic.\r\n2. Engine JOIN is a kind of optimisation of JOINs with joinGet extension. Do not use it for JOINs if general JOIN doesn't work. Do not use it in dictionary-like scenario if dictGet doesn't work.\r\n3. It's not clear in docs but engine JOIN do not support complex keys yet.\r\n\r\nYou're trying to combine several special extensions in totally unexpected dangerous way. Nobody helps you if something goes wrong.\r\nCreate intermediate table and update via it."
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-15T00:52:26Z",
        "body": "ext.dictionaries\r\n\r\n```\r\nCREATE TABLE test1\r\n(\r\n    `col1` Int8, \r\n    `col2` String, \r\n    `col3` Date, \r\n    `col4` UInt16\r\n)\r\nENGINE = MergeTree order by tuple();\r\n\r\nINSERT INTO test1 VALUES(1,'001','2020-01-01', 1)(1,'002','2020-01-01', 1)\r\n(2,'001','2020-01-01', 2)(2,'002','2020-01-02', 3)(2,'003','2020-01-04', 5);\r\n\r\nCREATE TABLE test_join AS test1 ENGINE = MergeTree order by tuple();\r\n\r\nINSERT INTO test_join SELECT * FROM test1 WHERE col3 = '2020-01-02';\r\n\r\nCREATE DICTIONARY test_join_dict (`col1` Int8, `col2` String, `col3` Date, `col4` UInt16)\r\nPRIMARY KEY col1,col2 \r\nSOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE test_join DB 'default' USER 'default')) \r\nLIFETIME(MIN 0 MAX 0) LAYOUT(COMPLEX_KEY_HASHED());\r\n\r\nALTER TABLE test1 UPDATE col4 = dictGet('default.test_join_dict', 'col4', tuple(col1, col2))\r\nwhere dictHas('default.test_join_dict', tuple(col1, col2))\r\n\r\n```\r\n"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-15T09:03:01Z",
        "body": "Thank for help. Thanks."
      }
    ],
    "satisfaction_conditions": [
      "Supports composite key joins (multiple conditions)",
      "Uses ClickHouse-recommended patterns for updates",
      "Provides working key-based lookup mechanism",
      "Avoids engine-specific limitations",
      "Maintains data integrity during updates"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:07:43"
    }
  },
  {
    "number": 9044,
    "title": "Import tsv exception: Cannot parse input: expected \\t ",
    "created_at": "2020-02-07T08:49:13Z",
    "closed_at": "2020-02-07T15:05:31Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/9044",
    "body": "I imported tsv file to CH, but, I got an error.\r\nHow can I fix this?\r\n\r\ntsv file for import\r\n```tsv\r\n2010-01-01\tabc\t1\t\t\t2020-02-07\r\n2020-01-02\t\t2\t\t\t\r\n2020-01-03\taaa\t\t\t\t2020-02-04\r\n```\r\n\r\ntable\r\n```sql\r\nCREATE TABLE default.test3\r\n(\r\n    `EventDate` Date, \r\n    `CounterID` Nullable(String), \r\n    `UserID` Nullable(UInt32), \r\n    `day1` Nullable(Date), \r\n    `day2` Nullable(Date), \r\n    `day3` Nullable(Date)\r\n)\r\nENGINE = MergeTree()\r\nORDER BY EventDate\r\n```\r\n\r\nError\r\n```\r\n# cat test.tsv | clickhouse-client --query=\"INSERT INTO test3 FORMAT TSV\"\r\nCode: 27, e.displayText() = DB::Exception: Cannot parse input: expected \\t before: -07\\n2020-01-02\\t\\t2\\t\\t\\t\\n2020-01-03\\taaa\\t\\t\\t\\t2020-02-04\\n: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: EventDate, type: Date,             parsed text: \"2010-01-01\"\r\nColumn 1,   name: CounterID, type: Nullable(String), parsed text: \"abc\"\r\nColumn 2,   name: UserID,    type: Nullable(UInt32), parsed text: \"1\"\r\nColumn 3,   name: day1,      type: Nullable(Date),   parsed text: \"<TAB><TAB>2020-02\"\r\nERROR: garbage after Nullable(Date): \"-07<LINE FEED>2020-0\"\r\n\r\n, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. 0xbc31d9c Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n1. 0x4f6ccd9 DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int)  in /usr/bin/clickhouse\r\n2. 0x496bab9 ?  in /usr/bin/clickhouse\r\n3. 0x92ed647 DB::TabSeparatedRowInputFormat::readRow(std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, DB::RowReadExtension&)  in /usr/bin/clickhouse\r\n4. 0x97e5f69 DB::IRowInputFormat::generate()  in /usr/bin/clickhouse\r\n5. 0x91a4c27 DB::ISource::work()  in /usr/bin/clickhouse\r\n6. 0x9169435 DB::InputStreamFromInputFormat::readImpl()  in /usr/bin/clickhouse\r\n7. 0x8a6d32f DB::IBlockInputStream::read()  in /usr/bin/clickhouse\r\n8. 0x94eb632 DB::ParallelParsingBlockInputStream::parserThreadFunction(unsigned long)  in /usr/bin/clickhouse\r\n9. 0x4fa4657 ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>)  in /usr/bin/clickhouse\r\n10. 0x4fa4c84 ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() const  in /usr/bin/clickhouse\r\n11. 0x4fa3b77 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)  in /usr/bin/clickhouse\r\n12. 0x4fa212f ?  in /usr/bin/clickhouse\r\n13. 0x7e25 start_thread  in /usr/lib64/libpthread-2.17.so\r\n14. 0xfebad clone  in /usr/lib64/libc-2.17.so\r\n (version 20.1.2.4 (official build))\r\nCode: 27. DB::Exception: Cannot parse input: expected \\t before: -07\\n2020-01-02\\t\\t2\\t\\t\\t\\n2020-01-03\\taaa\\t\\t\\t\\t2020-02-04\\n: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: EventDate, type: Date,             parsed text: \"2010-01-01\"\r\nColumn 1,   name: CounterID, type: Nullable(String), parsed text: \"abc\"\r\nColumn 2,   name: UserID,    type: Nullable(UInt32), parsed text: \"1\"\r\nColumn 3,   name: day1,      type: Nullable(Date),   parsed text: \"<TAB><TAB>2020-02\"\r\nERROR: garbage after Nullable(Date): \"-07<LINE FEED>2020-0\"\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/9044/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-07T14:59:43Z",
        "body": "By default CH expects in TSV Nulls encoded as \\N\r\n\r\nYou can use `--input_format_tsv_empty_as_default arg   Treat empty fields in TSV input as default values.`\r\n\r\n```\r\ncat test.tsv | clickhouse-client --input_format_tsv_empty_as_default=1 --query=\"INSERT INTO default.test3 FORMAT TSV\"\r\n\r\nSELECT *\r\nFROM default.test3\r\n\r\n\u250c\u2500\u2500EventDate\u2500\u252c\u2500CounterID\u2500\u252c\u2500UserID\u2500\u252c\u2500day1\u2500\u252c\u2500day2\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500day3\u2500\u2510\r\n\u2502 2010-01-01 \u2502 abc       \u2502      1 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 2020-02-07 \u2502\r\n\u2502 2020-01-02 \u2502 \u1d3a\u1d41\u1d38\u1d38      \u2502      2 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502       \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2502 2020-01-03 \u2502 aaa       \u2502   \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502 2020-02-04 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "chu1070y",
        "created_at": "2020-02-07T15:05:29Z",
        "body": "Thanks a lot. It worked"
      }
    ],
    "satisfaction_conditions": [
      "Handling of empty fields in TSV input for Nullable columns"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:07:55"
    }
  },
  {
    "number": 8999,
    "title": "Create MATERIALIZED VIEW against ReplicatedMergeTree and Distributed tables",
    "created_at": "2020-02-04T17:48:51Z",
    "closed_at": "2020-02-11T17:38:12Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8999",
    "body": "I am trying to create VM for my cluster. before getting to the point here is the details:\r\n\r\nI have 2 shards and 2 replicas in each.\r\n\r\nDetails:\r\n\r\ncluster name: _clicks_cluster_\r\n\r\nI have a replicated table:\r\n```\r\nCREATE TABLE default.clicks_replicated\r\n(\r\n    ...\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/clicks_replicated', '{replica}')\r\nPARTITION BY (...)\r\nORDER BY (...)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n\r\nThen I created the distributed from it:\r\n```\r\nCREATE TABLE IF NOT EXISTS default.clicks_distributed AS default.clicks_replicated\r\nENGINE = Distributed(clicks_cluster, default, clicks_replicated, cityHash64(my_column));\r\n```\r\n\r\nNow I want to create a VM.:\r\n\r\nBut I found out I don't get the new data if I create it against `clicks_distributed`  with `ENGINE = SummingMergeTree` \r\n\r\nAlso creating it against `clicks_replicated` will lead to incomplete data per replica. \r\n\r\nwhat would be the query for creating VM in this case?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8999/comments",
    "author": "hatrena",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-02-04T18:55:37Z",
        "body": "Materialized View is an insert trigger. It gets data from INSERT. It never reads/selects a source table.\r\n\r\nThe most used schema is to create at **all nodes** the **same set of tables / MVs**. MV's also **replicated** Engine.\r\n\r\n\r\nCREATE TABLE default.clicks_replicated\r\n(\r\n)\r\nENGINE = ReplicatedMergeTree\r\n\r\nCREATE MATERIALIZED VIEW default.clicks_replicatedMV \r\nEngine= **ReplicatedSummingMergeTree**\r\nas select .... **from default.clicks_replicated**\r\n\r\n....\r\nCREATE TABLE IF NOT EXISTS default.clicks_distributed AS default.clicks_replicated\r\nENGINE = Distributed(clicks_cluster, default, clicks_sharded, cityHash64(my_column));"
      },
      {
        "user": "hatrena",
        "created_at": "2020-02-04T20:32:30Z",
        "body": "ReplicatedSummingMergeTree requires 2 to 3 parameters. \r\n\r\nAnd if I do: \r\n\r\n`Engine = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/clicks_replicated', '{replica}')` \r\n\r\n\r\nI get this error: \r\n\r\n`Existing table metadata in ZooKeeper differs in mode of merge operation. Stored in ZooKeeper: 0, local: 2`"
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-04T20:50:40Z",
        "body": "SummingMT completely another table. ZK Path should be different\r\n\r\nReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/clicks_replicated_sumXXXMyFirstMV', '{replica}')"
      },
      {
        "user": "hatrena",
        "created_at": "2020-02-10T17:53:12Z",
        "body": "Damn, I totally missed that part. my bad. Now it works perfectly fine. Thanks a lot for your quick response. \ud83d\udc4d \n\n---\n\nI tested the MV in our staging where we have 1 shard and 2 replicas. everything is fine. \r\n\r\nThen I have tested it in our production where we have 2 shards and 2 replicas in each. we also have 4 Kubernetes pods. \r\n\r\nI have checked in all 4 pods and the MV does exist in all of them, but the result of the same query is different on each pod. data is increasing, but only in a very specific pod. \r\n\r\n```\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS default.vm_click_line_chart\r\nENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/vm_click_line_chart', '{replica}')\r\nORDER BY date POPULATE AS\r\nSELECT\r\n    count() AS clicks,\r\n    toDate(request_time) AS date,\r\n    organization_id\r\nFROM default.clicks_sharded\r\nWHERE (today() - toDate(request_time)) <= 180\r\nGROUP BY\r\n    date,\r\n    organization_id;\r\n```\r\n\r\nAny clue of such behavior?"
      },
      {
        "user": "den-crane",
        "created_at": "2020-02-10T18:48:12Z",
        "body": "> but the result of the same query is different on each pod\r\n\r\nWhat query?"
      },
      {
        "user": "hatrena",
        "created_at": "2020-02-10T19:44:19Z",
        "body": "a simple query for checking stats like\r\n`SELECT SUM(clicks), date FROM default.vm_click_line_chart WHERE date > '2020-02-01' AND organization_id = 'XXX' group by date`\r\n\r\nThe result of that query differs in every pod."
      },
      {
        "user": "mirajgodha",
        "created_at": "2021-01-08T05:59:52Z",
        "body": "@hatrena  Were you able to get the same results on all the pods, if yes how?"
      },
      {
        "user": "hatrena",
        "created_at": "2021-01-08T11:48:59Z",
        "body": "@mirajgodha , as @den-crane said\r\n> The most used schema is to create at all nodes the same set of tables / MVs. MV's also replicated Engine."
      },
      {
        "user": "mazensibai",
        "created_at": "2022-11-14T06:22:56Z",
        "body": "> I tested the MV in our staging where we have 1 shard and 2 replicas. everything is fine.\r\n> \r\n> Then I have tested it in our production where we have 2 shards and 2 replicas in each. we also have 4 Kubernetes pods.\r\n> \r\n> I have checked in all 4 pods and the MV does exist in all of them, but the result of the same query is different on each pod. data is increasing, but only in a very specific pod.\r\n> \r\n> ```\r\n> CREATE MATERIALIZED VIEW IF NOT EXISTS default.vm_click_line_chart\r\n> ENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/default/vm_click_line_chart', '{replica}')\r\n> ORDER BY date POPULATE AS\r\n> SELECT\r\n>     count() AS clicks,\r\n>     toDate(request_time) AS date,\r\n>     organization_id\r\n> FROM default.clicks_sharded\r\n> WHERE (today() - toDate(request_time)) <= 180\r\n> GROUP BY\r\n>     date,\r\n>     organization_id;\r\n> ```\r\n> \r\n> Any clue of such behavior?\r\n\r\nthe mistake that you did is that you have created the MV as sharded .. that is why you are getting different result ..."
      }
    ],
    "satisfaction_conditions": [
      "Ensures materialized view works with distributed tables in a multi-shard/replica cluster",
      "Provides consistent aggregated results across all cluster nodes",
      "Handles replication mechanics properly for materialized views",
      "Maintains data completeness across shards and replicas",
      "Explains relationship between materialized views and underlying table engines"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:08:04"
    }
  },
  {
    "number": 8686,
    "title": "New installation on Ubuntu VM: Password required for user default. ",
    "created_at": "2020-01-16T15:05:24Z",
    "closed_at": "2020-01-16T15:42:11Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8686",
    "body": "I can't get the initial setup to work on my newly created Ubuntu 18.04.3 LTS virtual machine.\r\nI followed the instructions on the website by executing the following terminal commands:\r\n\r\n```\r\nsudo apt-get install dirmngr    # optional\r\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4    # optional\r\nsudo apt-get update\r\nsudo apt-get install clickhouse-client clickhouse-server\r\n```\r\n\r\nThis should suffice, right? I then start the server, try to start the client and this is what happens:\r\n```\r\ntaxel@taxel-VirtualBox:~$ sudo service clickhouse-server start\r\ntaxel@taxel-VirtualBox:~$ clickhouse-client\r\nClickHouse client version 19.17.6.36 (official build).\r\nConnecting to localhost:9000 as user default.\r\nCode: 194. DB::Exception: Received from localhost:9000. DB::Exception: Password required for user default. \r\n```\r\nI also tried changing the default password from `<password></password>` to `<password>123</password>` and logging in via `clickhouse-client --password=123` but it outputs that the password is wrong (and yes, I have ensured the xml file is saved and the server is restarted)\r\n\r\nAny help would be much appreciated.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8686/comments",
    "author": "Taxel",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-16T15:34:21Z",
        "body": "On install CH asked for a default password for default user and placed it to /etc/clickhouse-server/users.d/default-password.xml \r\nYou can change this password or remove this file to empty password."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to properly configure ClickHouse authentication for the default user",
      "Identification of correct configuration file locations and their precedence",
      "Clarification on password persistence requirements after installation",
      "Resolution path for conflicting authentication settings"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:08:11"
    }
  },
  {
    "number": 8556,
    "title": "Creating index on an existing table ",
    "created_at": "2020-01-07T09:17:46Z",
    "closed_at": "2020-01-08T05:22:50Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8556",
    "body": "Hi \r\n\r\nI am trying to add index on an existing table , with the below syntax.\r\n\r\ncreate table contact_in.....\r\n(\r\n.......\r\n.....\r\n......\r\n) ENGINE = MergeTree PARTITION BY category\r\nORDER BY\r\n  (topic, domain) SETTINGS index_granularity = 8192\r\n\r\n1. I want to create an index on the topic column (granularity is 6020)\r\n\r\n2. tried syntax from the documentation but unable to understand since there is no examples explaining the fields in it.\r\n\r\n3. Tried the below \r\nalter table contact_in add index inx1 topic TYPE minmax granularity 1\r\nnot sure how it works (need a better understanding on this).\r\n\r\nCan somebody quickly help me with this please.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8556/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-07T15:30:03Z",
        "body": "a. **You already have an index** on topic `ORDER BY(topic, domain)` -- as a prefix of a primary index.\r\n\r\nb. You can add only skip indexes -- this a special type of indexes and they work by another way, minmax index stores only min max values of an index granula, so for example with  6020, this minmax index will have one minmax values for each 6020*8192 = 49315840 rows and will allow to skip this granula if a required value out of minmax range.\r\n\r\nc. (granularity of skip index = 6020) -- this is nonsense. try 1 or 2.\r\n\r\n"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2020-01-08T05:22:50Z",
        "body": "ok got it, thanks."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of primary index vs skip index functionality",
      "Clarification of skip index use cases and limitations",
      "Guidance on appropriate granularity values"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:08:51"
    }
  },
  {
    "number": 8531,
    "title": "About deleting new values every day affects performance",
    "created_at": "2020-01-05T09:02:19Z",
    "closed_at": "2020-01-06T01:46:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8531",
    "body": "\r\nA multi-million data table needs to delete a part of the data and add a new part every day. Will this affect the performance of ck query?\r\n\r\n ENGINE = MergeTree() ORDER BA_MONTH",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8531/comments",
    "author": "samz406",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2020-01-05T16:27:51Z",
        "body": "It's unclear how are you going to `delete a part`. \r\n`alter table ... drop partition`  does not affect performance."
      },
      {
        "user": "samz406",
        "created_at": "2020-01-06T01:19:42Z",
        "body": "no partition, first use ALTER TABLE table DELETE WHERE BA_MONTH='xxx', and insert data,Will this affect the performance  query?\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2020-01-06T01:38:45Z",
        "body": "`ALTER TABLE table DELETE` is very heavy operation in comparison with `drop partition` .\r\n`ALTER TABLE table DELETE` causes a huge I/O and CPU usage. After it finishes it does not affect performance.\r\n\r\nConsider to use `alter table ... drop partition` and table with monthly / daily partitioning. \r\n` drop partition` much more faster and more reliable operation. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how ALTER TABLE DELETE operations compare to partition-based deletion in terms of performance impact",
      "Clarification of how data deletion/insertion patterns affect query performance over time",
      "Guidance on optimal table partitioning strategies for frequent data rotation",
      "Analysis of resource utilization tradeoffs between different deletion methods"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:08:59"
    }
  },
  {
    "number": 8228,
    "title": "mysql connection in clickhouse",
    "created_at": "2019-12-16T07:32:32Z",
    "closed_at": "2019-12-23T18:47:54Z",
    "labels": [
      "question",
      "comp-foreign-db"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8228",
    "body": "I'm using clickhouse for a while now. I have inserted 1 million records so far and I intend to add to it to about 100 billion. It's blazing fast, and I like how it compresses data. \r\n\r\nThe problem is that it keeps throwing an error every now and then, for example when I just login to clickhouse client:\r\n\r\nCannot load data for command line suggestions: Code: 1000, e.displayText() = DB::Exception: Received from localhost:9000. DB::Exception: mysqlxx::ConnectionFailed: Unknown MySQL server host 'host' (-2) ((nullptr):0). (version 19.17.5.18 (official build))\r\n\r\nFor doing ordinary tasks it seems to not affect the performance, but the main problem is that when I want to get partitions using command:\r\n\r\n`SELECT partition FROM system.parts WHERE table='bars'`\r\n\r\nagain it throws the same exception. I went through the documentation, but I couldn't find a solution.\r\n\r\nAny help would be appreciated...\r\n\r\nPS: I used: Engine = MergeTree() Partition by isin Order by time primary key time",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8228/comments",
    "author": "ashkank66",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2019-12-16T13:39:58Z",
        "body": "It probably means that you have table with `MySQL` engine which can't connect to MySQL.\r\nIt also strange that we have `nullptr` in error message. May be a misconfiguration. \r\n\r\nCan you please check that all you MySQL configurations are correct?\r\nAnd also find full stacktrace after this error in logs?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-16T19:17:50Z",
        "body": "> Unknown MySQL server host 'host' (-2) ((nullptr):0)\r\n\r\nProbably you have erroneously specified `host` as hostname for MySQL server, like this:\r\n`<host>host</host>`\r\n\r\nThe `(nullptr):0` part is Ok - it's what we have as the error message from the library."
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-17T07:17:18Z",
        "body": "I actually haven't configured MySQL on my clickhouse, and to be honest, I have to admit I tried to find a configuration for MySQL but I couldn't.\r\nCould you tell me where should I configure it?\n\n---\n\n2019.12.17 10:46:30.000314 [ 44 ] {} <Information> Application: MYSQL: Connecting to database@host:0 as user user\r\n2019.12.17 10:46:30.001630 [ 44 ] {} <Error> Application: mysqlxx::ConnectionFailed\r\n2019.12.17 10:46:30.001943 [ 44 ] {} <Error> void DB::AsynchronousMetrics::run(): Poco::Exception. Code: 1000, e.code() = 2005, e.displayText() = mysqlxx::ConnectionFailed: Unknown MySQL server host 'host' (-2) ((nullptr):0) (version 19.17.5.18 (official build)\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-17T16:52:08Z",
        "body": "It looks like you have configured MySQL table actually.\r\n\r\n```\r\ngrep -r -i mysql /etc/clickhouse-server/\r\ngrep -i mysql /etc/metrika.xml\r\ngrep -r -i mysql /var/lib/clickhouse/\r\n```"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-18T10:00:29Z",
        "body": "This is all the responses:\r\n\r\nroot@ashkanPC:/home/ashkan# grep -r -i mysql /etc/clickhouse-server/\r\n/etc/clickhouse-server/users.xml:                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).\r\nroot@ashkanPC:/home/ashkan# grep -i mysql /etc/metrika.xml\r\ngrep: /etc/metrika.xml: No such file or directory\r\nroot@ashkanPC:/home/ashkan# grep -r -i mysql /var/lib/clickhouse/\r\n/var/lib/clickhouse/preprocessed_configs/users.xml:                 Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019).\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:       /etc/clickhouse-server/mysql_dictionary.xml      -->\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:    <comment>This dictionary is set to connect clickhouse to mysql</comment>\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:\t  <mysql>\r\n/var/lib/clickhouse/preprocessed_configs/mysql_dictionary.xml:\t  </mysql>\r\n/var/lib/clickhouse/metadata/db_name.sql:ENGINE = MySQL('host:port', 'database', 'user', 'password')\r\n\n\n---\n\nI created a file mysql_dictionary in hope of getting rid of the error, but no proper result, so I deleted it later"
      },
      {
        "user": "KochetovNicolai",
        "created_at": "2019-12-18T10:51:56Z",
        "body": "> /var/lib/clickhouse/metadata/db_name.sql:ENGINE = MySQL('host:port', 'database', 'user', 'password')\r\n\r\nThat means that you have `MySQL` database with name `db_name`, which has incorrect configuration (instead of `'host:port', 'database', 'user', 'password'` must be real values). And this database can't connect to MySql server.\r\n\r\nYou can just run `DROP DATABASE db_name` to remove it.\r\n"
      },
      {
        "user": "ashkank66",
        "created_at": "2019-12-21T05:25:32Z",
        "body": "That's right, thank you."
      }
    ],
    "satisfaction_conditions": [
      "Identify the source of unintended MySQL configuration in ClickHouse setup",
      "Locate and address misconfigured MySQL engine definitions in metadata",
      "Provide method to remove/disable erroneous MySQL-related configurations",
      "Ensure solution preserves existing ClickHouse data and functionality"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:09:12"
    }
  },
  {
    "number": 8121,
    "title": "\"Too many open files\" while loading data into table",
    "created_at": "2019-12-10T13:19:59Z",
    "closed_at": "2020-05-17T15:54:31Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/8121",
    "body": "Am getting the below error while loading data and only half of the data is being loaded into the table \r\n\r\nDB::Exception: Cannot open file /t-3tb-data/clickhouse/data/database/table/tmp_insert_0c87b3bf0c31a7766299a14d202c8da9_648_648_0/TI_verification_status.mrk, errno: 24, strerror: Too many open files.\r\n\r\nCan someone help me quickly here.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/8121/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "byx313",
        "created_at": "2019-12-10T14:25:32Z",
        "body": "> Am getting the below error while loading data and only half of the data is being loaded into the table\r\n> \r\n> DB::Exception: Cannot open file /t-3tb-data/clickhouse/data/database/table/tmp_insert_0c87b3bf0c31a7766299a14d202c8da9_648_648_0/TI_verification_status.mrk, errno: 24, strerror: Too many open files.\r\n> \r\n> Can someone help me quickly here.\r\n\r\nYou got too many files in OS.\r\nMethod 1,increase open files limit\r\ncheck open files \r\n> ulimit -a\r\n\r\nincrease open files \r\n> ulimit -n 65536\r\n\r\nMethod 2,increase messege count in one batch/one insert operation.\r\n"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T14:51:36Z",
        "body": "@byx313 \r\nI tried the first option but the problem is still same :(\r\nMethod2: You mean to say single insert will do than multiple inserts ?"
      },
      {
        "user": "byx313",
        "created_at": "2019-12-10T14:55:41Z",
        "body": "> @byx313\r\n> I tried the first option but the problem is still same :(\r\n> Method2: You mean to say single insert will do than multiple inserts ?\r\n\r\nDo 'ulimit -a' again to check whether the operation work.\r\n\r\n> Method2: You mean to say single insert will do than multiple inserts ?\r\n\r\nYes.10w message a batch a insert is better than 1w message * 10 concurrent insert"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T14:58:12Z",
        "body": "@byx313\r\nyes, I did ulimit -a to check and yes the change is reflected."
      },
      {
        "user": "byx313",
        "created_at": "2019-12-10T15:00:03Z",
        "body": "> @byx313\r\n> yes, I did ulimit -a to check and yes the change is reflected.\r\n\r\nmay be you should try to change you insert frequency.What's the frequency now?"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-10T15:06:40Z",
        "body": "@byx313\r\nam loading one file after the other , once the first file is loaded starting with the next one."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-12-10T20:09:16Z",
        "body": "This happens because you are using too granular partition key in a table.\r\nSolution: do not use `PARTITION BY`."
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-12-11T10:03:48Z",
        "body": "@alexey-milovidov  I have partitioned the table on state code which has some 60 values \r\n\r\nSo if I don't use the PARTITION BY  it doesn't have impact on queries??\n\n---\n\n@alexey-milovidov Yes, I removed the PARTITION BY and without any error I could load the data :)\r\nMy worry is query returning time. \r\nThank you."
      },
      {
        "user": "filimonov",
        "created_at": "2019-12-12T00:23:19Z",
        "body": "> @byx313\r\n> yes, I did ulimit -a to check and yes the change is reflected.\r\n\r\nAlso for clickhouse user? What is your OS? How did you install/run clickhouse?\r\n\r\nI'm asking because official packages should extend that limit during installation, and 9fficial docker readme mentions how to increase max number of opened files for clickhouse. "
      }
    ],
    "satisfaction_conditions": [
      "Addresses the root cause of excessive file handles during data loading",
      "Provides a sustainable way to manage ClickHouse file operations",
      "Maintains acceptable query performance after changes",
      "Considers ClickHouse-specific configuration requirements"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:09:36"
    }
  },
  {
    "number": 7865,
    "title": "Escape double quote sign in CSV",
    "created_at": "2019-11-20T16:39:48Z",
    "closed_at": "2019-11-21T13:36:11Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7865",
    "body": "I have CSV file with comma as separator sign, but without quotes for column entries.\r\n\r\nIssue I have is with double quote sign existing in string value for column,\r\nwhen I try to escape it with another double quote ( \"\" ) for command line clickhouse-client\r\nI got error and insert fails.\r\n\r\nWhen I escape one double quote with backslash ( \\\" ) then it works but got backslash and quote in entry in column.\r\n\r\nWhat is the proper way to escape double quote in CSV structure like I have ?\r\n\r\nClickhouse server version is 19.11.12\r\n\r\nhere is example for structures and data:\r\n```\r\n# testdata.csv\r\n749a2c8c-3682-4745-aefe-c21b3164bade,name,\"\"MY COMPANY\"\" COM\r\n749a2c8c-3682-4745-aefe-c21b3164bade,hash,67FF87AF9E9E4BA9E4C03FAC4A23F21C\r\n\r\n# table structure\r\nCREATE TABLE temp.events (`event_id` String, `property_name` String, `property_value` String) ENGINE = MergeTree() PARTITION BY tuple() ORDER BY event_id SETTINGS index_granularity = 8192\r\n\r\n#shell script to insert data\r\ncat testdata.csv | clickhouse-client --host=localhost --query='INSERT INTO temp.events (event_id, property_name, property_value) FORMAT CSV'\r\n\r\nCode: 117. DB::Exception: Expected end of line: (at row 1)\r\n\r\nRow 1:\r\nColumn 0,   name: event_id,       type: String, parsed text: \"749a2c8c-3682-4745-aefe-c21b3164bade\"\r\nColumn 1,   name: property_name,  type: String, parsed text: \"name\"\r\nColumn 2,   name: property_value, type: String, parsed text: \"<DOUBLE QUOTE><DOUBLE QUOTE>\"\r\nERROR: There is no line feed. \"M\" found instead.\r\n It's like your file has more columns than expected.\r\nAnd if your file have right number of columns, maybe it have unquoted string value with comma.\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7865/comments",
    "author": "goranc",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-20T18:31:08Z",
        "body": "--format_csv_allow_double_quotes=0\r\n```\r\ncat testdata.csv | clickhouse-client --format_csv_allow_double_quotes=0 --host=localhost --query='INSERT INTO events (event_id, property_name, property_value) FORMAT CSV'\r\n```\n\n---\n\nAh, double quote escaping works only inside quoted string.\r\n```\r\n749a2c8c-3682-4745-aefe-c21b3164bade,name,\"\"\"Y COMPANY\"\" COM\"\r\n749a2c8c-3682-4745-aefe-c21b3164bade,hash,67FF87AF9E9E4BA9E4C03FAC4A23F21C\r\n```\r\n\r\n-----------------------\r\nSo yeah CH supports only double quote escaping by double quote.\r\n\r\nThis string `[,\"MY COMPANY\" COM]` needs escaping because it starts with \"\r\nThis string `[,\u0445\u0430\u0445\u0430 \"MY COMPANY\" COM]` does not need escaping.\r\nThis string `[,\\\"MY COMPANY\\\" COM]` does not need escaping and \\ -- is not escaping, but usual symbol and it works because the string starts with \\ not with \".\r\n\r\n\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-20T21:31:15Z",
        "body": "@den-crane backslash escaping is for TSV and escaping by doubling quotes is for CSV."
      },
      {
        "user": "goranc",
        "created_at": "2019-11-21T10:01:14Z",
        "body": "Thanks for info, sometimes is hard to find proper parameter.\r\n\r\nParameter \"format_csv_allow_double_quotes\" resolve the problem, and there is no need to escape double quote at all.\r\nOther special characters should be escaped as usual.\r\n"
      },
      {
        "user": "inkrement",
        "created_at": "2022-10-05T06:45:21Z",
        "body": "I have a related question: How would you handle a mixture between CSV & TSV (i.e., CSV with escaping instead of quotes)? TSV does not allow changing the delimiter, CSV hates the escapes, and I was unable to get CustomSeparated to work (although I set the comma as separator it detects the full row as single column)."
      }
    ],
    "satisfaction_conditions": [
      "Allows proper handling of double quotes in CSV data without causing parsing errors",
      "Works with ClickHouse's CSV parser requirements",
      "Avoids introducing unintended characters in final data",
      "Maintains CSV structure integrity with comma separators"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:09:41"
    }
  },
  {
    "number": 7849,
    "title": "Avoid `Too many partitions for single INSERT block` in Kafka Engine",
    "created_at": "2019-11-19T17:58:02Z",
    "closed_at": "2019-11-19T18:30:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7849",
    "body": "Hi! I'm using Kafka engine and getting this error: ```DB::Exception: Too many partitions for single INSERT block (more than 100). The limit is controlled by 'max_partitions_per_insert_block' setting. ...```\r\nI understand why this error can occur in small inserts by hand (using clickhouse-client or http interface), but in case with Kafka engine documentation says: \r\n```\r\nTo improve performance, received messages are grouped into blocks the size of max_insert_block_size. If the block wasn't formed within stream_flush_interval_ms milliseconds, the data will be flushed to the table regardless of the completeness of the block.\r\n```\r\nSo if I understand correctly Kafka engine should merge blocks before insert to increase performance and avoid this error. Also I can suspect the root of that error in my case is that in every kafka message is one `row` of data, so I suppose that blocks are merged but this does not reduce parts count. Is there a way to overcome this? Is it a bug? I'm hoping to avoid writing middleware pre-batching service, since Kafka engine does almost all needed things\r\n\r\nI'm using version 19.15.5.18. Thanks in advance!\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7849/comments",
    "author": "LizardWizzard",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-19T18:10:48Z",
        "body": ">I understand why this error can occur in small inserts by hand\r\n\r\nNo. It's not about this.\r\nIt's about partitioning key in your MV. One insert into kafka (into MV) tries to create more than 100 partitions.\r\n\r\nFor example \r\n```\r\n\r\ncreate table A( D Date) Engine=MergeTree partition by D order by D;\r\ninsert into A select number from numbers(365);\r\nCode: 252. DB::Exception: Received from localhost:9000. DB::Exception: Too many partitions for single INSERT block (more than 100).\r\n```\r\n\r\nThe error is because this insert tries to create 365 partitions\r\n\r\n```\r\ncreate table A( D Date) Engine=MergeTree partition by toYYYYMM(D) order by D;\r\ninsert into A select number from numbers(365);\r\nOK.\r\n```\r\nNo error. Because this insert creates only 12 partitions."
      },
      {
        "user": "LizardWizzard",
        "created_at": "2019-11-19T18:30:26Z",
        "body": "Thank you so much for explanation! Got it :) Changed partition expression and error is gone"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how partitioning keys affect partition creation during Kafka engine ingestion",
      "Guidance on optimizing partition grouping without middleware pre-batching",
      "Solution must maintain Kafka engine's performance benefits of block merging",
      "Addresses relationship between data structure and partition limits"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:09:53"
    }
  },
  {
    "number": 7765,
    "title": "Drop in writes on high number of selects.",
    "created_at": "2019-11-13T23:16:06Z",
    "closed_at": "2019-11-14T06:04:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7765",
    "body": "Version: ClickHouse 19.13.1.11\r\nServer has 400GB RAM with 48 CPU cores. and 3.2 TB of HDD.\r\nInsert batch size is 20K records (~40MB). Each insert is taking ~1.5secs.\r\nServer has 12.5B records.\r\nWe are running parallel select queries of ~500. Selects include some count queries, some aggregations and group bys.\r\nCPU and RAM are at their minimals (like 10% of CPU and 10GB of RAM used).\r\nDuring this time, inserts are dropped. Noticed \"TimeoutExceptions\" on client side (Using Http Client for inserts)\r\nGet below exception at high rate. (very few select queries went through)\r\n\r\nCode: 202, e.displayText() = DB::Exception: Too many simultaneous queries. Maximum: 100 (version 19.13.1.11 (official build))\r\n\r\nTried to increase max simultaneous queries in config.xml (to 1000).\r\n <max_concurrent_queries>1000</max_concurrent_queries>\r\n\r\nEven after increasing, exception still says Maximun: 100\r\nCode: 202, e.displayText() = DB::Exception: Too many simultaneous queries. Maximum: 100 (version 19.13.1.11 (official build))\r\n\r\nSeems my change is not taking effect.\r\n\r\nHow can I make sure that writes are not impacted due to reads?\r\nWhy cant I increase simultaneous queries in spite of having RAM and MEMORY available?\r\nHow can we support more selects? (we need little higher selects to build dashboards, aggregations, and for AD purposes)",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7765/comments",
    "author": "SreekanthMannari",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-11-13T23:22:46Z",
        "body": "Did you restart CH after setting max_concurrent_queries ?"
      },
      {
        "user": "SreekanthMannari",
        "created_at": "2019-11-13T23:27:03Z",
        "body": "No. I haven't restarted.\r\nBut config.xml in \"preprocessed_configs\" folder shows my change."
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-13T23:28:59Z",
        "body": ">But config.xml in \"preprocessed_configs\" folder shows my change.\r\n\r\nIt does not matter. All parameters from config.xml (except cluster & dictionaries configurations) require CH reboot to apply. "
      },
      {
        "user": "SreekanthMannari",
        "created_at": "2019-11-14T01:00:32Z",
        "body": "Thanks. Setting worked after CH reboot. \r\nDoes reboot needed even for User.xml changes like max memory settings?"
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-14T01:27:11Z",
        "body": ">Does reboot needed even for User.xml changes like max memory settings?\r\n\r\nNo, changes in user.xml does not need reboot."
      },
      {
        "user": "haiertashu",
        "created_at": "2021-06-03T06:25:08Z",
        "body": "how to make sure the server settings modified such as 'max_concurrent_queries' taking effect \uff1f where/which system table can show the latest changes\uff1f or any other way"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of configuration change requirements for ClickHouse settings",
      "Differentiation between settings requiring restart vs. live reload",
      "Method to verify active configuration settings"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:10:00"
    }
  },
  {
    "number": 7711,
    "title": "cannot parse CSV with '\\x7F' as delmiter. \\x7F is ascii code.",
    "created_at": "2019-11-11T09:27:56Z",
    "closed_at": "2019-11-11T10:45:54Z",
    "labels": [
      "invalid",
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7711",
    "body": "$clickhouse-client --host dbt20 --port 9000 --format_csv_delimiter='\\x7F' --query=\"INSERT INTO dwzc.twb_m_top_organization FORMAT CSV\" < /data/test_data/test.txt \r\nCode: 19, e.displayText() = DB::Exception: A setting's value string has to be an exactly one character long, Stack trace:\r\n\r\n0. 0x563fac3bd7b0 StackTrace::StackTrace() /usr/bin/clickhouse\r\n1. 0x563fac3bd585 DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) /usr/bin/clickhouse\r\n2. 0x563fac0ba051 ? /usr/bin/clickhouse\r\n3. 0x563fb03d2c5d boost::program_options::variables_map::notify() /usr/bin/clickhouse\r\n4. 0x563fac48413c DB::Client::init(int, char**) /usr/bin/clickhouse\r\n5. 0x563fac46feef mainEntryClickHouseClient(int, char**) /usr/bin/clickhouse\r\n6. 0x563fac2f9fed main /usr/bin/clickhouse\r\n7. 0x7f1c99a2e3d5 __libc_start_main /usr/lib64/libc-2.17.so\r\n8. 0x563fac3632ea _start /usr/bin/clickhouse\r\n (version 19.16.2.2 (official build))\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7711/comments",
    "author": "xuxudede",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-11T10:01:00Z",
        "body": "You have specified four characters: \\\\, x, 7, F, as a delimiter, because escape sequence is not interpreted by your shell interpreter.\r\n\r\nWrite\r\n```\r\nclickhouse-client --host dbt20 --port 9000 --format_csv_delimiter=$'\\x7F' --query=\"INSERT INTO dwzc.twb_m_top_organization FORMAT CSV\" < /data/test_data/test.txt\r\n```\r\ninstead (if you use `bash` as a shell interpreter).\r\nNote the dollar sign before quote.\n\n---\n\n`man bash`, `QUOTING` section."
      },
      {
        "user": "xuxudede",
        "created_at": "2019-11-11T10:45:04Z",
        "body": "> You have specified four characters: \\, x, 7, F, as a delimiter, because escape sequence is not interpreted by your shell interpreter.\r\n> \r\n> Write\r\n> \r\n> ```\r\n> clickhouse-client --host dbt20 --port 9000 --format_csv_delimiter=$'\\x7F' --query=\"INSERT INTO dwzc.twb_m_top_organization FORMAT CSV\" < /data/test_data/test.txt\r\n> ```\r\n> \r\n> instead (if you use `bash` as a shell interpreter).\r\n> Note the dollar sign before quote.\r\n\r\nTKS"
      }
    ],
    "satisfaction_conditions": [
      "Proper shell interpretation of escape sequences for delimiter specification",
      "Validation of delimiter length before processing",
      "Clear distinction between shell interpretation and ClickHouse configuration"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:10:09"
    }
  },
  {
    "number": 7700,
    "title": "Net work days function",
    "created_at": "2019-11-10T15:53:57Z",
    "closed_at": "2019-11-11T08:18:12Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7700",
    "body": "Hi clickhouse team,\r\n\r\nI would like to know if there is any built in function to find the net work days between to given dates. I would provide the holiday calendar and I would expect clickhouse to find the number of working days (excluding holidays + weekends)\r\n\r\nThanks!\r\nCaue",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7700/comments",
    "author": "caueteixeira",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-10T19:17:10Z",
        "body": "Here is an example:\r\n\r\n```\r\nWITH \r\n    toDate('2019-01-01') AS d1, \r\n    today() AS d2\r\nSELECT length(arrayFilter(x -> ((toDayOfWeek(d1 + x) <= 5) AND ((d1 + x) NOT IN ('2019-01-01', '2019-01-02'))), range(toUInt64(d2 - d1)))) AS num_work_days\r\n\r\n\u250c\u2500num_work_days\u2500\u2510\r\n\u2502           222 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nHere I create an array spanning all days between two dates, filter weekends and a list of public holidays (after NOT IN), then calculate the length.\r\n\r\nBut it's neither cute nor optimal."
      },
      {
        "user": "den-crane",
        "created_at": "2019-11-11T01:54:56Z",
        "body": "```\r\ncreate table calendar (Day Date, is_holiday Int8) Engine=MergeTree order by Day;\r\ninsert into calendar select today()+number, number%7 from numbers(1000);\r\n\r\nselect count() from calendar where Day between '2019-12-30' and '2020-12-30' and not is_holiday;\r\n```"
      },
      {
        "user": "caueteixeira",
        "created_at": "2019-11-11T08:15:44Z",
        "body": "Thanks guys!"
      },
      {
        "user": "zhikeke",
        "created_at": "2022-06-17T06:59:40Z",
        "body": "SELECT\r\n'2022-06-18 10:10:10' AS d1,\r\n'2022-06-18 20:10:10' AS d2,\r\ntoDate(d1) AS d1_date,\r\ntoDate(d2) AS d2_date,\r\naddDays(d1_date, 1) AS d1_next_day,\r\ntoInt8('1') AS week_start_day,\r\ntoInt8('5') AS week_end_day,\r\narrayMap(x -> (x + d1_next_day), arrayFilter(x -> (week_start_day > toDayOfWeek(addDays(d1_next_day, x)) OR toDayOfWeek(addDays(d1_next_day, x)) > week_end_day),\r\nCASE WHEN (d2_date - d1_next_day) < 0 THEN [] ELSE range(abs(d2_date - d1_next_day)) END\r\n)) as nonworkdays,\r\ndateDiff('minute', toDateTime(d1), toDateTime(d2)) - 24 * 60 * length(nonworkdays) AS all_minture,\r\nCASE WHEN ((week_start_day > toDayOfWeek(d1_date) OR toDayOfWeek(d1_date) > week_end_day) AND d1_date != d2_date) THEN dateDiff('minute', toDateTime(d1), addDays(toStartOfDay(d1_date), 1)) ELSE 0 END AS d1_minture,\r\nCASE WHEN ((week_start_day > toDayOfWeek(d2_date) OR toDayOfWeek(d2_date) > week_end_day) AND d1_date != d2_date) THEN dateDiff('minute', toStartOfDay(d2_date), toDateTime(d2)) ELSE 0 END AS d2_minture,\r\nCASE WHEN ((week_start_day > toDayOfWeek(d2_date) OR toDayOfWeek(d2_date) > week_end_day) AND d1_date == d2_date) THEN dateDiff('minute', toDateTime(d1), toDateTime(d2)) ELSE 0 END AS d1_d2_between_minture,\r\nall_minture - d1_minture - d2_minture - d1_d2_between_minture AS work_minture\r\n\r\n\r\n\r\n"
      },
      {
        "user": "Spratty59",
        "created_at": "2024-04-18T08:37:31Z",
        "body": "> Here is an example:\r\n> \r\n> ```\r\n> WITH \r\n>     toDate('2019-01-01') AS d1, \r\n>     today() AS d2\r\n> SELECT length(arrayFilter(x -> ((toDayOfWeek(d1 + x) <= 5) AND ((d1 + x) NOT IN ('2019-01-01', '2019-01-02'))), range(toUInt64(d2 - d1)))) AS num_work_days\r\n> \r\n> Here I create an array spanning all days between two dates, filter weekends and a list of public holidays (after NOT IN), then calculate the length.\r\n> \r\n\r\nHey, when I try to use this (working with a table) I get an error on the 'range' section where is states that there are illegal types: DateTime64(3), any idea how to resolve this?"
      }
    ],
    "satisfaction_conditions": [
      "Supports exclusion of weekends (non-working days) in date range calculations",
      "Allows integration of custom holiday calendars for exclusion",
      "Provides accurate count of business days between two dates",
      "Works with ClickHouse date/time functions and data types",
      "Handles date ranges without requiring hardcoded values"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:10:14"
    }
  },
  {
    "number": 7503,
    "title": "How to shutdown clickhouse instance ?",
    "created_at": "2019-10-28T08:26:21Z",
    "closed_at": "2019-10-28T09:45:22Z",
    "labels": [
      "question",
      "operations"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7503",
    "body": "First time to use clickhouse, how can i close or shutdown clickhouse ? Can only use \"kill -9 pid\" ?\r\nThx.  ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7503/comments",
    "author": "Myshiner",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-10-28T09:39:33Z",
        "body": "Sending SIGKILL (9) with `kill` command will work but recommended way is \r\n```\r\nsystemctl stop clickhouse-server\r\n```\r\n\r\nor  \r\n```\r\n/etc/init.d/clickhouse-server stop\r\n``` "
      }
    ],
    "satisfaction_conditions": [
      "Identifies a graceful shutdown method that avoids data corruption",
      "Provides standard service management approaches rather than process-level commands",
      "Explains recommended practices for ClickHouse instance management",
      "Differentiates between emergency termination and normal shutdown workflows"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:10:27"
    }
  },
  {
    "number": 7499,
    "title": "How does the Clickhouse find other column data?",
    "created_at": "2019-10-28T05:31:51Z",
    "closed_at": "2019-10-28T07:24:56Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7499",
    "body": "When I run a query with where statement like `select * from tbl1 where col1 = 'CH' `, Clickhouse find other columns which correspond with `col1 = 'CH'`.\r\n\r\nUnlike row-base DBMS, ClickHouse is column-base DBMS that saves data by column. Then How CH find other column data with same row?\r\n\r\nCould you explain a process step by step that the ClickHouse finds other columns data?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7499/comments",
    "author": "chu1070y",
    "comments": [
      {
        "user": "hczhcz",
        "created_at": "2019-10-28T06:30:46Z",
        "body": "The expression `col1 = 'CH'` will yield a \"condition column\" of boolean values. Physically, it is one or more memory blocks containing UInt8 values.\r\nThen, we can perform a for-loop within the range of the row numbers. Let the row number to be \"i\", the i-th element in the condition column will indicate whether the i-th row is selected. If so, we will visit each column, grab the i-th element, and append it to the corresponding result column."
      },
      {
        "user": "chu1070y",
        "created_at": "2019-10-28T07:24:56Z",
        "body": "It`s very helpful. Thanks."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how ClickHouse aligns column data by row index during query execution",
      "Description of the process for applying WHERE conditions to identify relevant row positions",
      "Clarification of data retrieval mechanics across multiple columns after row selection",
      "Comparison to row-based DBMS architectures without requiring technical implementation specifics"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:10:34"
    }
  },
  {
    "number": 7489,
    "title": "Strange Null literal handling behavior?",
    "created_at": "2019-10-25T13:00:26Z",
    "closed_at": "2019-10-25T13:21:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7489",
    "body": "I am observing this counterintuitive behavior with special `Null` value:\r\n```\r\nSELECT isNull(CAST('Null', 'Nullable(String)'))\r\n```\r\nreturns `0`\r\n\r\nWhereas:\r\n```\r\nSELECT isNull(CAST(Null, 'Nullable(String)'))\r\n```\r\nreturns `1`\r\n\r\nand\r\n```\r\nSELECT isNull(CAST('Null', 'Nullable(Int32)'))\r\n```\r\nreturns `1`\r\n\r\nN.B.: single quotes and their absence around `Null`.\r\n\r\nIs this a bug or a feature?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7489/comments",
    "author": "traceon",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-10-25T13:16:11Z",
        "body": "CAST('Null', 'Nullable(String)') -- it's a string with text Null.\r\n\r\n```\r\n\r\nSELECT\r\n    CAST('Null', 'Nullable(String)'),\r\n    NULL\r\n\r\n\u250c\u2500CAST('Null', 'Nullable(String)')\u2500\u252c\u2500NULL\u2500\u2510\r\n\u2502 Null                             \u2502 \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nit's how cast works from strings to Int32\r\n```\r\nSELECT CAST('aaaaaaa', 'Nullable(Int32)')\r\n\r\n\u250c\u2500CAST('aaaaaaa', 'Nullable(Int32)')\u2500\u2510\r\n\u2502                               \u1d3a\u1d41\u1d38\u1d38 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      },
      {
        "user": "traceon",
        "created_at": "2019-10-25T13:21:13Z",
        "body": "I see, that makes sense."
      }
    ],
    "satisfaction_conditions": [
      "Clarify the distinction between string literals containing 'Null' and actual NULL literals in type conversion",
      "Explain type conversion failure behavior for incompatible values",
      "Differentiate between NULL storage and string representations in type systems"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:10:43"
    }
  },
  {
    "number": 7443,
    "title": "Is there any way to predaggregate uniqState?",
    "created_at": "2019-10-23T02:12:23Z",
    "closed_at": "2019-10-23T13:38:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7443",
    "body": "There is no function RunningDifference for uniqState.\r\n\r\nBut two uniqStates can be summed by +\r\n\r\nSo RunningDifference can be emulated like this\r\n```\r\ncreate table z(d Date, z String, u String)\r\nEngine=MergeTree partition by tuple() order by tuple();\r\n\r\nCREATE MATERIALIZED VIEW mvz\r\nENGINE = AggregatingMergeTree order by (z,d) settings index_granularity = 8 \r\nas select d, z,uniqState(u) as us from z group by z,d;\r\n\r\ninsert into z select today()+1, 'g1' , toString(number) from numbers(1000);\r\ninsert into z select today()+2, 'g1' , toString(number+100) from numbers(1000);\r\ninsert into z select today()+3, 'g1' , toString(number+200) from numbers(1000);\r\ninsert into z select today()+4, 'g1' , toString(number+200) from numbers(1000);\r\ninsert into z select today()+5, 'g1' , toString(number+300) from numbers(1000);\r\n\r\nselect m1, m2 from (\r\nSELECT\r\n        groupArray(d) AS gd,\r\n        arrayMap(x -> toString(gd[x+1])||' - '||toString(gd[x+2]), range(toUInt64(length(gd)-1))) m1,\r\n        groupArray(us) AS gus,\r\n        arrayMap(x -> (arrayReduce('uniqMerge', [gus[x+1]+gus[x+2]]) - arrayReduce('uniqMerge', [gus[x+2]])) , range(toUInt64(length(gd)-1))) m2\r\n          from (select d, us FROM mvz  order by d ) )\r\n    Array Join m1, m2\r\n\r\n\u250c\u2500m1\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500m2\u2500\u2510\r\n\u2502 2019-10-24 - 2019-10-25 \u2502 100 \u2502\r\n\u2502 2019-10-25 - 2019-10-26 \u2502 100 \u2502\r\n\u2502 2019-10-26 - 2019-10-27 \u2502   0 \u2502\r\n\u2502 2019-10-27 - 2019-10-28 \u2502 100 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThere is only problem that it's working only if state fully aggregated by date.\r\nAnd there is no function partialUniqMerge to merge by date but leave states.\r\n`select d, partialUniqMerge(us) FROM mvz group by d order by d`\r\n\r\nBut probably such function exists internally because Distributed gets such data from shards.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7443/comments",
    "author": "den-crane",
    "comments": [
      {
        "user": "vpanfilov",
        "created_at": "2019-10-23T08:03:25Z",
        "body": "Have you tried `-MergeState` modifier?\r\n\r\n```\r\nselect d, uniqMergeState(us) FROM mvz group by d order by d\r\n```"
      },
      {
        "user": "den-crane",
        "created_at": "2019-10-23T13:38:38Z",
        "body": "@vpanfilov ha, that what I asked for. \r\nuniqMergeState works. \r\n\r\nGreat. Thank you."
      }
    ],
    "satisfaction_conditions": [
      "Provides a way to merge partial aggregation states by date while preserving their intermediate form",
      "Supports incremental aggregation maintenance for time-series analysis",
      "Enables state preservation for subsequent differential calculations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:11:04"
    }
  },
  {
    "number": 7312,
    "title": "What is \"active\" znode means in Zookeeper?",
    "created_at": "2019-10-14T12:13:18Z",
    "closed_at": "2019-11-03T22:36:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7312",
    "body": "When I check my zk, I found a 'actived' znode in the clickhouse path.I check the document but nothing found out.\r\nso what's this znode means?\r\nbyw,when I use replicated table,every insert operation one znode would be created,should I need to delete znode manually after some days like a month to prevent so many znodes in zk? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7312/comments",
    "author": "byx313",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-11-03T22:36:02Z",
        "body": "> so what's this znode means?\r\n\r\nReplica has a session with ZooKeeper and can serve INSERT queries.\r\n\r\n> byw,when I use replicated table,every insert operation one znode would be created,should I need to delete znode manually after some days like a month to prevent so many znodes in zk?\r\n\r\nNo, you don't need to do anything manually with ZK nodes."
      },
      {
        "user": "byx313",
        "created_at": "2019-11-05T06:29:58Z",
        "body": "> > so what's this znode means?\r\n> \r\n> Replica has a session with ZooKeeper and can serve INSERT queries.\r\n> \r\n> > byw,when I use replicated table,every insert operation one znode would be created,should I need to delete znode manually after some days like a month to prevent so many znodes in zk?\r\n> \r\n> No, you don't need to do anything manually with ZK nodes.\r\n\r\nthx\uff01"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of the 'actived' znode's purpose in ClickHouse/ZooKeeper operations",
      "Confirmation of automatic ZooKeeper node lifecycle management for replicated tables"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:11:14"
    }
  },
  {
    "number": 7147,
    "title": "I think sth is wrong with arrayDifference",
    "created_at": "2019-09-30T07:58:53Z",
    "closed_at": "2019-10-06T23:07:15Z",
    "labels": [
      "question",
      "comp-arrays"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/7147",
    "body": "THIS IS FINE \r\n\r\n```\r\nSELECT arrayDifference([1, 2, 3, 4])\r\n\r\n\u250c\u2500arrayDifference([1, 2, 3, 4])\u2500\u2510\r\n\u2502 [0,1,1,1]                     \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nBUT what `-1` mean ?\r\n```\r\nSELECT arrayDifference([1, 2, 2, 3, 3, 2])\r\n\r\n\u250c\u2500arrayDifference([1, 2, 2, 3, 3, 2])\u2500\u2510\r\n\u2502 [0,1,0,1,0,-1]                      \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAnd this is not reasonable:\r\n```\r\nSELECT arrayDifference(['a', 'b'])\r\n\r\nReceived exception from server (version 19.5.3):\r\nCode: 43. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: arrayDifference cannot process values of type String.\r\n```\r\narray(T) takes T type, but a function can not support String type......",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/7147/comments",
    "author": "Tasselmi",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-09-30T08:36:44Z",
        "body": "> BUT what -1 mean ?\r\n\r\n`-1` means that last element of array (`2`) minus previous one (`3`) equals `2 - 3 = -1`\r\n\r\nIsn't it the expected behaviour?\r\n\r\n> array(T) takes T type, but a function can not support String type......\r\n\r\nDoes strings support substraction? What is the result of 'Hello' minus 'world!' ?"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-09-30T08:42:31Z",
        "body": "> > BUT what -1 mean ?\r\n> \r\n> `-1` means that last element of array (`2`) minus previous one (`3`) equals `2 - 3 = -1`\r\n> \r\n> Isn't it the expected behaviour?\r\n> \r\n> > array(T) takes T type, but a function can not support String type......\r\n> \r\n> Does strings support substraction? What is the result of 'Hello' minus 'world!' ?\r\n\r\nI thought arrayDifference means if they are not equal........ 1 means true and 0 means false\r\n......\r\nSo I misunderstood..\n\n---\n\n> > BUT what -1 mean ?\r\n> \r\n> `-1` means that last element of array (`2`) minus previous one (`3`) equals `2 - 3 = -1`\r\n> \r\n> Isn't it the expected behaviour?\r\n> \r\n> > array(T) takes T type, but a function can not support String type......\r\n> \r\n> Does strings support substraction? What is the result of 'Hello' minus 'world!' ?\r\n\r\nI want to make [1, 2, 2, 3, 3, 2, 2]  -> [1, 2, 3, 2]\r\nI am doing a program computing user-behaviour-path ~~~"
      },
      {
        "user": "den-crane",
        "created_at": "2019-09-30T14:05:52Z",
        "body": "select arrayConcat([arr[1]], arrayFilter(x,y -> x = y, arraySlice(arr, 2), arraySlice(arr, 1, -1))) from (  select [1, 2, 2, 3, 3, 2, 2] arr )\r\n\r\nSELECT arrayConcat([arr[1]], arrayFilter(x,y -> y!=0, arr ,arrayDifference(arr))) from (select [1, 2, 2, 3, 3, 2] arr)\r\n\r\n\r\n"
      },
      {
        "user": "Tasselmi",
        "created_at": "2019-10-07T11:48:48Z",
        "body": "> select arrayConcat([arr[1]], arrayFilter(x,y -> x = y, arraySlice(arr, 2), arraySlice(arr, 1, -1))) from ( select [1, 2, 2, 3, 3, 2, 2] arr )\r\n> \r\n> SELECT arrayConcat([arr[1]], arrayFilter(x,y -> y!=0, arr ,arrayDifference(arr))) from (select [1, 2, 2, 3, 3, 2] arr)\r\n\r\nThanks for your help ~\n\n---\n\n> select arrayConcat([arr[1]], arrayFilter(x,y -> x = y, arraySlice(arr, 2), arraySlice(arr, 1, -1))) from ( select [1, 2, 2, 3, 3, 2, 2] arr )\r\n> \r\n> SELECT arrayConcat([arr[1]], arrayFilter(x,y -> y!=0, arr ,arrayDifference(arr))) from (select [1, 2, 2, 3, 3, 2] arr)\r\n\r\n\r\n\r\nThe first method is more universal.  It takes parameters of any type, but arrayDifference can only take numeric type.\r\n\r\n```\r\nSELECT arrayConcat(array(event_list[1]), arrayFilter((x, y) -> (x != y), arraySlice(event_list, 2), arraySlice(event_list, 1, -1)))\r\nFROM ( SELECT [1, 2, 2, 3, 2, 2] AS event_list ) m\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of arrayDifference's actual functionality vs user's initial assumption",
      "Solution for removing consecutive duplicates in arrays of any type",
      "Approach that works with non-numeric array elements",
      "Clarification of array function limitations and alternatives"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:11:46"
    }
  },
  {
    "number": 6973,
    "title": "Data written because of max_bytes_before_external_*",
    "created_at": "2019-09-18T11:33:22Z",
    "closed_at": "2019-10-24T22:50:42Z",
    "labels": [
      "question",
      "st-need-info"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6973",
    "body": "Hello.\r\n\r\nIs it possible to see how much data was written to disk during query because of max_bytes_before_external_* ?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6973/comments",
    "author": "Tri0L",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2019-09-18T15:45:14Z",
        "body": "Hi. This information you can find in `system.query_log` table (you need to enable query_log in config and set `log_queries=1`). Then if you know query id, run query like\r\n```\r\nSELECT \r\n    ProfileEvents.Names, \r\n    ProfileEvents.Values\r\nFROM system.query_log\r\nARRAY JOIN ProfileEvents\r\nWHERE query_id = '<query_id>' and ProfileEvents.Names = '<event_name>'\r\n```\r\n\r\nFor external aggregation `<event_name>` is `ExternalAggregationCompressedBytes` or `ExternalAggregationUncompressedBytes`.\r\nFor external sorting there is no special event, but you can use `WriteBufferFromFileDescriptorWriteBytes` to get the number of written bytes in total for query."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T09:26:02Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "blinkov",
        "created_at": "2019-10-20T10:00:44Z",
        "body": "@Tri0L do you have any further questions?"
      },
      {
        "user": "Tri0L",
        "created_at": "2019-10-24T22:50:42Z",
        "body": "Nope, thanks a lot!"
      }
    ],
    "satisfaction_conditions": [
      "A method to retrieve metrics about disk writes caused by external aggregation/sorting thresholds",
      "Differentiation between external aggregation and sorting operations in metrics",
      "Query-level granularity for the metrics",
      "Mechanism to access historical data about past queries"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:11:57"
    }
  },
  {
    "number": 6484,
    "title": " DB::Exception: Bad get: has UInt64, requested String.",
    "created_at": "2019-08-14T05:44:51Z",
    "closed_at": "2019-08-19T11:18:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6484",
    "body": "When I set up the distributed tables, there was such a mistake.\r\n`node01 :) create table dummy (p Date, k UInt64, d String) ENGINE = MergeTree(p, k, 8192)\r\n\r\nCREATE TABLE dummy\r\n(\r\n    `p` Date, \r\n    `k` UInt64, \r\n    `d` String\r\n)\r\nENGINE = MergeTree(p, k, 8192)\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.020 sec. \r\nnode01 :) create table distributed (p Date, k UInt64, d String) ENGINE = Distributed(cluster-1, 'default', 'dummy')\r\n\r\nCREATE TABLE distributed\r\n(\r\n    `p` Date, \r\n    `k` UInt64, \r\n    `d` String\r\n)\r\nENGINE = Distributed(cluster - 1, 'default', 'dummy')\r\n\r\nReceived exception from server (version 19.9.5):\r\nCode: 170. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Bad get: has UInt64, requested String. \r\n\r\n0 rows in set. Elapsed: 0.001 sec. \r\n`\r\n I don't know why. I tried many times.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6484/comments",
    "author": "TomatoBoy90",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-08-14T16:57:31Z",
        "body": ">Distributed(cluster - 1\r\n\r\n-1 ? It should be a cluster name. \r\nFor example `test_shard_localhost` : `ENGINE = Distributed(test_shard_localhost, default, dummy)`\r\n\r\ncheck for available clusters `select distinct cluster from system.clusters`\r\n\r\n\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-08-19T11:36:14Z",
        "body": "@JonLeeCSDN \r\n`cluster-1` is interpreted as arithmetic expression. You have to put in in backticks: \r\n```\r\n`cluster-1`\r\n```\r\n or use different cluster name."
      },
      {
        "user": "TomatoBoy90",
        "created_at": "2019-08-21T07:38:06Z",
        "body": "> @JonLeeCSDN\r\n> `cluster-1` is interpreted as arithmetic expression. You have to put in in backticks:\r\n> \r\n> ```\r\n> `cluster-1`\r\n> ```\r\n> \r\n> or use different cluster name.\r\n\r\nthank you ,in fact ,code as follow will be true:\r\n`node03 :) CREATE TABLE ontime_all AS ontime_local_2 ENGINE = Distributed('cluster-1', 'h2', 'ontime_local_2', 100);\r\n\r\nCREATE TABLE ontime_all AS ontime_local_2\r\nENGINE = Distributed('cluster-1', 'h2', 'ontime_local_2', 100)\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.005 sec"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the root cause of the cluster name parsing error",
      "Explains proper cluster name specification syntax",
      "Addresses ClickHouse's expression parsing behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:12:06"
    }
  },
  {
    "number": 6461,
    "title": "How much data can the Set engine store?",
    "created_at": "2019-08-13T06:30:05Z",
    "closed_at": "2019-08-21T03:49:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6461",
    "body": "",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6461/comments",
    "author": "heneyin",
    "comments": [
      {
        "user": "4ertus2",
        "created_at": "2019-08-20T10:31:12Z",
        "body": "It's limited by query memory limit. So you are able to use as much data there as you can allocate in your query context.\r\n\r\n```\r\ncreate table test.t (x Int64) engine = Set;\r\ninsert into test.t select number from system.numbers;\r\n```\r\n\r\n```\r\n\u2198 Progress: 33.55 million rows, 268.44 MB (2.48 million rows/s., 19.82 MB/s.) Received exception from server (version 19.14.1):\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 1.02 GiB (attempt to allocate chunk of 535822408 bytes), maximum: 953.67 M\r\n```"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-08-21T03:49:24Z",
        "body": "@4ertus2 You can continue inserting data into Set engine with subsequent queries until the server will hit OOM."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of memory constraints affecting Set engine capacity",
      "Clarification of relationship between query context and storage limits",
      "Description of server-wide resource boundaries",
      "Differentiation between per-query limits and total engine capacity"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:12:15"
    }
  },
  {
    "number": 6353,
    "title": "Using runningDifference() and quantilesExactWeighted() correctly on time series data",
    "created_at": "2019-08-05T20:47:57Z",
    "closed_at": "2019-08-06T07:31:38Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6353",
    "body": "We need to calculate exact value quantiles on large non-uniformly sampled time series data. The schema is (String sensor_path, DateTime timestamp, Float64 value). \r\n\r\nIt would be perfect if we could use something like \u201eSELECT quantilesExactWeighted(...)(value, delta)...\u201c,\r\nwhere \u201edelta\u201c is the result of \u201erunningDifference(timestamp)\u201c. \r\nUnfortunately, this does not work, because for each row, delta is the time difference between the previous and the current row instead of the time difference between the current and the next row, as needed for the weights parameter of quantilesExactWeighted(). \r\n\r\nSelf-joining the time series to shift delta forward one row will probably not work when the time series does not fit into memory, right?\r\n\r\nIs there maybe another, more efficient solution?\r\n\r\nI\u2019d be glad to try to submit a patch with a new variant of runningDifference() if there is no other solution. \r\n\r\nMany thanks for your excellent work on ClickHouse!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6353/comments",
    "author": "oflasch",
    "comments": [
      {
        "user": "zombiemonkey",
        "created_at": "2019-08-06T05:06:20Z",
        "body": "You can use arrays to get a series per key, sort by timestamp then calculate pretty much anything on the series. Performance on array vectors is certainly not the same as column vectors however if you consider the fact you have to sort to get things in order you end up with performance that roughly matches the new optimize order by primary key functionality. GROUP BY can also spill to disk which can help the memory issue.\r\n\r\nExample assuming positive values only - generate 1000 individual series and some metrics\r\n\r\n```\r\nDROP TABLE IF EXISTS timeseries;\r\n\r\nCREATE TABLE timeseries\r\nENGINE = MergeTree\r\nPARTITION BY toStartOfDay(ts)\r\nORDER BY (key, ts) AS\r\nSELECT \r\n    concat('device-', toString(number % 1000)) AS key, \r\n    toDateTime('2019-01-01 00:00:00') + toIntervalSecond(rand(number) % 1000) AS ts, \r\n    any(rand(number + 100000)) AS value\r\nFROM numbers(1000000)\r\nGROUP BY key, ts;\r\n```\r\n\r\nCalculate the delta and convert back to column\r\n\r\n```\r\nselect\r\n    key,\r\n    quantilesExactWeighted(0.5)(_value, _delta) AS q\r\nfrom (\r\n    with\r\n        arrayMap(i -> (_s[i].1, _s[i].2, abs(_s[i+1].2 - _s[i].2)), arrayEnumerate(arraySort(x -> x.1, groupArray((ts, value))) AS _s)) AS _d\r\n    select\r\n        key,\r\n        (arrayJoin(_d) AS series).1 AS _ts,\r\n        series.2 AS _value,\r\n        series.3 AS _delta\r\n    from \r\n        timeseries\r\n    group by key\r\n) group by key\r\n```\r\n\r\nCalculate the delta and use -Array combiner on the array without conversion.\r\n\r\n```\r\nselect\r\n    key,\r\n    quantilesExactWeightedArray(0.5)(_d.2, _d.3) AS q\r\nfrom (\r\n    select\r\n        key,\r\n        arrayMap(i -> (_s[i].1, _s[i].2, abs(_s[i+1].2 - _s[i].2)), arrayEnumerate(arraySort(x -> x.1, groupArray((ts, value))) AS _s)) AS _d\r\n    from timeseries\r\n    group by key\r\n) group by key\r\n```"
      },
      {
        "user": "oflasch",
        "created_at": "2019-08-06T07:28:27Z",
        "body": "Many thanks for the detailed solution, looks very good. I did not know about the effect of GROUP BY on memory usage. Neat optimization!\r\n\r\nYour observation that the series must be sorted anyway brought another possible solution to mind that I wanted to document for others perhaps facing the same task:\r\nWe can simply sort the series by timestamp in descending order before calculating runningDifference(), then apply abs() to the resulting negative deltas, which should give the desired \u201cforward_delta\u201d:\r\n\r\n```\r\nSELECT \r\n    *, \r\n    abs(runningDifference(q)) AS forward_delta\r\nFROM \r\n(\r\n    SELECT \r\n        number AS n, \r\n        n * n AS q\r\n    FROM numbers(15)\r\n    ORDER BY n DESC\r\n)\r\nORDER BY n ASC\r\n\r\n\u250c\u2500\u2500n\u2500\u252c\u2500\u2500\u2500q\u2500\u252c\u2500forward_delta\u2500\u2510\r\n\u2502  0 \u2502   0 \u2502             1 \u2502\r\n\u2502  1 \u2502   1 \u2502             3 \u2502\r\n\u2502  2 \u2502   4 \u2502             5 \u2502\r\n\u2502  3 \u2502   9 \u2502             7 \u2502\r\n\u2502  4 \u2502  16 \u2502             9 \u2502\r\n\u2502  5 \u2502  25 \u2502            11 \u2502\r\n\u2502  6 \u2502  36 \u2502            13 \u2502\r\n\u2502  7 \u2502  49 \u2502            15 \u2502\r\n\u2502  8 \u2502  64 \u2502            17 \u2502\r\n\u2502  9 \u2502  81 \u2502            19 \u2502\r\n\u2502 10 \u2502 100 \u2502            21 \u2502\r\n\u2502 11 \u2502 121 \u2502            23 \u2502\r\n\u2502 12 \u2502 144 \u2502            25 \u2502\r\n\u2502 13 \u2502 169 \u2502            27 \u2502\r\n\u2502 14 \u2502 196 \u2502             0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Solution must calculate time deltas between current and next timestamps for quantile weights",
      "Approach must work efficiently on large time series that may not fit in memory",
      "Method must preserve exact quantile calculation requirements",
      "Solution should avoid row-order dependency through proper sorting",
      "Approach must handle non-uniform sampling of time series data"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:12:34"
    }
  },
  {
    "number": 6215,
    "title": "sumMap for record but not aggregate",
    "created_at": "2019-07-30T08:53:05Z",
    "closed_at": "2020-04-03T02:17:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6215",
    "body": "Hi\r\n  I have a table named test,the data like below\r\n```\r\nid payMode.code  payMode.fee name ...\r\n1  [100,100,200]  [10,20,30] name1 ...\r\n2  [100,200,100]  [10,20,30] name2 ...\r\n```\r\nnow I want get the result \r\n```\r\nid payMode.code  payMode.fee name ...\r\n1  [100,200]  [30,30] name1 ...\r\n2  [100,200]  [40,20] name2 ...\r\n```\r\n\r\nI want to sumMap the payMode.code for every record using \r\n```\r\nselect summap(payMode.code,payMode.fee) as payMode,id,name,... from test\r\n```\r\nbut it get the exception id,name is not in aggregate, is there any way sumMap for every record but not aggregate.\r\nI know I can use sumMap and group by to complete it ,but it's more complicated",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6215/comments",
    "author": "peaksnail",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-07-31T15:56:27Z",
        "body": "Alternaitve is smth like that:\r\n```\r\nselect ..., arrayReduce( 'sumMap', [payMode.code],[payMode.fee] ), ... \r\n```\r\n\r\nWill work properly on real table, but please be careful - i've found that there is some bug in processing constant parameters for that type of function call combination (see #6242)."
      },
      {
        "user": "peaksnail",
        "created_at": "2019-08-08T09:51:40Z",
        "body": "\ud83d\udc4d\n\n---\n\nHi\r\nI find that when payMode.fee equals [0], it will get the empty array\r\n\r\nsql like\r\n```\r\nselect arrayReduce('sumMap', array([100]), array([0])) \r\n```\r\nreturn \r\n```\r\n([],[]) \r\n```\r\n\r\nbut I want get the result \r\n```\r\n([100],[0]) \r\n```"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T13:25:27Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "blinkov",
        "created_at": "2020-04-01T16:53:56Z",
        "body": "@peaksnail, do you have any further questions?"
      }
    ],
    "satisfaction_conditions": [
      "Per-record processing without aggregation",
      "Preservation of zero values in fee arrays",
      "Row-level context preservation",
      "Avoidance of complex grouping"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:12:39"
    }
  },
  {
    "number": 6064,
    "title": "Clickhouse count is not working",
    "created_at": "2019-07-19T07:31:10Z",
    "closed_at": "2019-07-19T10:22:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6064",
    "body": "Hi,\r\n\r\nI have some issues when trying to count the number of rows of a table using a simple query like:\r\n`SELECT count()\r\nFROM mop3 \r\nWHERE (key = category) AND (value = lips)`\r\n\r\nThe table is\r\n`CREATE TABLE mop3\r\n(\r\n    customer_id Int32,\r\n    order_id Int64,\r\n    order_date_created DateTime,\r\n    key String,\r\n    value String,\r\n    quantity Int32,\r\n    unit_amount Decimal32(4),\r\n    total_amount Decimal32(4)\r\n) ENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(order_date_created)\r\nORDER BY (key, value)\r\n`\r\n\r\nThank you very much\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6064/comments",
    "author": "masosky",
    "comments": [
      {
        "user": "amosbird",
        "created_at": "2019-07-19T07:35:13Z",
        "body": "What's the issue, and what version do you use?"
      },
      {
        "user": "masosky",
        "created_at": "2019-07-19T08:09:11Z",
        "body": "I am using last version released\r\n`ClickHouse client version 19.9.2.4.\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 19.9.2 revision 54421.`\r\n\r\nError:\r\n`Received exception from server (version 19.9.2):\r\nCode: 47. DB::Exception: Received from localhost:9000, ::1. DB::Exception: Missing columns: 'category' 'lips' while processing query: 'SELECT count() FROM mop3  WHERE (key = category) AND (value = lips)', required columns: 'key' 'category' 'value' 'lips', source columns: 'quantity' 'unit_amount' 'value' 'customer_id' 'order_id' 'total_amount' 'order_date_created' 'key'.`"
      },
      {
        "user": "vasyaabr",
        "created_at": "2019-07-19T08:13:59Z",
        "body": "Use\r\n`SELECT count() FROM mop3 WHERE (key = 'category') AND (value = 'lips')`"
      },
      {
        "user": "masosky",
        "created_at": "2019-07-19T08:16:19Z",
        "body": "Oops, I forgot the single quotes.\r\nBut I do not understand I tried that previously and I didn't work.\r\nBut now it is working!\r\n\r\nSorry for the inconvenience and thanks for the fast answer."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why string literals require quotes in ClickHouse WHERE clauses",
      "Clarification on how ClickHouse interprets unquoted values in comparisons",
      "Guidance on proper string literal formatting in SQL queries for ClickHouse"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:12:46"
    }
  },
  {
    "number": 6050,
    "title": "19.11-*-stable dictionaries loading failed",
    "created_at": "2019-07-18T12:47:34Z",
    "closed_at": "2019-07-18T19:54:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6050",
    "body": "After upgrading to 19.11 dictionaries loading fail with error:\r\n\r\n```\r\n2019.07.18 13:46:47.501892 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'interserver_http_host', expected 'dictionary'\r\n2019.07.18 13:46:47.502034 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host', expected 'dictionary'\r\n2019.07.18 13:46:47.502177 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host[1]', expected 'dictionary'\r\n2019.07.18 13:46:47.502256 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'listen_host[2]', expected 'dictionary'\r\n2019.07.18 13:46:47.502324 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'logger', expected 'dictionary'\r\n2019.07.18 13:46:47.502388 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'profiles', expected 'dictionary'\r\n2019.07.18 13:46:47.502472 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'max_concurrent_queries', expected 'dictionary'\r\n2019.07.18 13:46:47.502539 [ 57 ] {} <Warning> ExternalDictionaries: /etc/clickhouse-server/dnl_dictionary.xml: file contains unknown node 'zookeeper-servers', expected 'dictionary'\r\n\r\n```\r\n\r\n cat /etc/clickhouse-server/dnl_dictionary.xml\r\n\r\n```\r\n<?xml version=\"1.0\"?>\r\n<yandex>\r\n  <dictionary>\r\n    <name>hosts</name>\r\n    <source>\r\n      <odbc>\r\n        <connection_string>DSN=PostgreSQLCHglobal</connection_string>\r\n        <table>hosts</table>\r\n      </odbc>\r\n......\r\n......\r\n......\r\n</yandex>\r\n\r\n```\r\nWorks on 19.9.4.34 and older.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6050/comments",
    "author": "mikeeremin",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-18T13:45:06Z",
        "body": "Dictionaries will load successfully regardless to this warning.\r\n\r\nThis warning present, because some config elements get substituted into `/etc/clickhouse-server/dnl_dictionary.xml`. You can check it in `/etc/clickhouse-server/preprocessed/...` directory.\r\n\r\nExtraneous config elements are substituted from `conf.d` directory.\r\nYou can override configuration with files from `conf.d` or _config_name_.d directory, for example `config.d`, `users.d` or `dnl_dictionary.d`. Overrides from `conf.d` directory will be substituted into every configuration file while overrides from _config_name_.d directories will be substituted only to the corresponding configuration file."
      },
      {
        "user": "mikeeremin",
        "created_at": "2019-07-18T14:00:11Z",
        "body": "```\r\nclickhouse-test-node1 :) system reload dictionaries;\r\n\r\nSYSTEM RELOAD DICTIONARIES\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.005 sec.\r\n\r\nclickhouse-test-node1 :) select * from system.dictionaries\\G\r\n\r\nSELECT *\r\nFROM system.dictionaries\r\n\r\nRow 1:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\r\nname:               programreleasefiles\r\nstatus:             NOT_LOADED\r\norigin:             /etc/clickhouse-server/dnl_dictionary.xml\r\ntype:\r\nkey:\r\nattribute.names:    []\r\nattribute.types:    []\r\nbytes_allocated:    0\r\nquery_count:        0\r\nhit_rate:           0\r\nelement_count:      0\r\nload_factor:        0\r\nsource:\r\nloading_start_time: 0000-00-00 00:00:00\r\nloading_duration:   0\r\nlast_exception:\r\n\r\n\r\n```"
      },
      {
        "user": "vitlibar",
        "created_at": "2019-07-18T15:43:06Z",
        "body": "`SYSTEM RELOAD DICTIONARIES` reloads only those dictionaries which have been loaded before.\r\nClickhouse has not loaded `programreleasefiles` before so the command `SYSTEM RELOAD DICTIONARIES` doesn't reload it.\r\n\r\nIf you want `programreleasefiles` to be loaded use\r\n\r\n```\r\nSYSTEM RELOAD DICTIONARY programreleasefiles\r\n```\r\n\r\nor just start using this dictionary\r\n```\r\nSELECT dictGetUInt64('programreleasefiles', 'a', 1)\r\n```\r\n"
      },
      {
        "user": "mikeeremin",
        "created_at": "2019-07-18T19:54:16Z",
        "body": "Worked, thanks.\r\n"
      },
      {
        "user": "fessmage",
        "created_at": "2019-09-05T18:56:39Z",
        "body": "I got NOT_LOADED state for every dictionary after update of clickhouse-server from 18.* to 19.* and only thing that worked for me - advice from @vitlibar. `system reload dictionary <dictionary_name>` for every dictionary - fixes problem."
      },
      {
        "user": "filimonov",
        "created_at": "2019-09-06T00:26:55Z",
        "body": "> I got NOT_LOADED state for every dictionary after update of clickhouse-server from 18.* to 19.* and only thing that worked for me - advice from @vitlibar. `system reload dictionary <dictionary_name>` for every dictionary - fixes problem.\r\n\r\nTry disabling lazy loading of dictionaries if you need them preinitialized. "
      },
      {
        "user": "fessmage",
        "created_at": "2019-09-06T06:45:08Z",
        "body": "Oh, thanks, i understood reason now. On server start i have problem with `clickhouse-odbc-bridge` - forgot to update it with rest packages. And with setting `dictionaries_lazy_load` defaults true - because of that error with odbc dictionaries, it resulted in all dictionaries not loaded."
      },
      {
        "user": "Naveen071110",
        "created_at": "2020-04-27T14:20:22Z",
        "body": "when i type system reload dictionary dictionary_name\r\nThen it shows that connection to all replicas failed!\r\nCan you explain why?\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why dictionaries remain in NOT_LOADED state despite successful configuration parsing",
      "Clear distinction between global configuration warnings and dictionary-specific configuration validity",
      "Working solution for forcing dictionary initialization after upgrade",
      "Guidance on troubleshooting dependency-related loading failures",
      "Explanation of version-specific behavior changes in dictionary loading"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:12:53"
    }
  },
  {
    "number": 6030,
    "title": "order by with formatReadableSize orders alphabetically instead of numerically",
    "created_at": "2019-07-17T01:37:49Z",
    "closed_at": "2019-07-17T10:50:04Z",
    "labels": [
      "question",
      "usability"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6030",
    "body": "This is probably due to the `formatReadableSize` being applied before the results get displayed, but it would be super nice if order by still ordered by the original values.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6030/comments",
    "author": "abraithwaite",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-07-17T02:04:29Z",
        "body": "this is contradicts SQL.\r\n\r\nyou can use \r\nselect formatReadableSize(size)\r\n...\r\norder by size"
      },
      {
        "user": "filimonov",
        "created_at": "2019-07-17T10:50:04Z",
        "body": "It is not a bug. `formatReadableSize` return strings, so they are ordered as strings.\r\n\r\nUse the approach shown in @den-crane 's answer,  "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-17T10:51:14Z",
        "body": "@abraithwaite \r\n\r\nConsider more simple example:\r\n\r\n```\r\nSELECT toString(number) AS x FROM numbers(100) ORDER BY x\r\n```\r\n\r\nIt's obvious that this query sort data alphabetically (by String value). And it is the only way it should work.\r\n\r\n`formatReadableSize` is similar. This function returns `String` data type, not some `String but sort numerically` magic.\r\n"
      },
      {
        "user": "abraithwaite",
        "created_at": "2019-07-17T16:15:54Z",
        "body": "Thanks!  I was aware that it was being converted to a string but didn't realize I could order by the original field.  Works great!"
      }
    ],
    "satisfaction_conditions": [
      "Preserve numerical ordering capabilities when displaying formatted human-readable values",
      "Enable sorting by raw data values while displaying processed/transformed versions",
      "Avoid implicit expectations about type-based sorting behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:13:02"
    }
  },
  {
    "number": 6013,
    "title": "Table info  isn't show on system.parts ",
    "created_at": "2019-07-15T15:57:24Z",
    "closed_at": "2019-07-16T02:15:18Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/6013",
    "body": "I created table (for example XX), queried successfully. \r\nHow ever I can't see any info of table XX when I query info from system.parts. \r\nWhen I run `Select table from system.parts` that didn't display XX. It displayed 4 names: cpu, cpu, cpu, tags.\r\n\r\nCould I find the information of a table I created in database (such name, size,..)?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/6013/comments",
    "author": "ngoanpv",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-07-15T19:08:25Z",
        "body": "system.parts has information about parts of MergeTree tables.\r\nIf XX is empty it has no parts, and the size of the X is 0.\r\n\r\nFor engines *Log you can use OS utilities only [du -sh /var/lib/clickhouse/data/db/XX]\r\nFor engine Memory no way at all."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the created table doesn't appear in system.parts",
      "Identification of alternative methods to retrieve table metadata for different engine types"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:13:10"
    }
  },
  {
    "number": 5853,
    "title": "How to pretty print a generated RowBinary file?",
    "created_at": "2019-07-03T14:20:07Z",
    "closed_at": "2019-07-05T14:07:06Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5853",
    "body": "Hello all,\r\n\r\nGiven a valid ClickHouse RowBinary file, how I can prettyprint the row data within the file?\r\n\r\nThank you",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5853/comments",
    "author": "Jack012a",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-03T14:31:31Z",
        "body": "Sure! It's very easy to do with `clickhouse-local`:\r\n\r\n```\r\nclickhouse-local --structure \"x UInt8, y String, ...\" --query \"SELECT * FROM table\" --input-format RowBinary --output-format TSV < file\r\n```\r\n\r\nYou have to know correct data structure (`--structure` parameter).\r\n\r\nYou can use `clickhouse-local` tool for converting between various formats and for data processing without a server.\n\n---\n\nIf you want **pretty**print, use `--output-format Pretty`."
      }
    ],
    "satisfaction_conditions": [
      "Supports conversion of RowBinary format to human-readable output",
      "Allows specification of data structure/schema",
      "Provides formatted output (e.g., tabular, aligned columns)",
      "Works without requiring a running ClickHouse server"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:13:17"
    }
  },
  {
    "number": 5512,
    "title": "Is there accumulate for array?",
    "created_at": "2019-06-02T09:10:53Z",
    "closed_at": "2019-06-05T13:45:26Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5512",
    "body": "data : [1, 4, 6, 8]\r\n\r\nresult : 1 * 4 * 6 * 8 = 192",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5512/comments",
    "author": "lzy305",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-06-04T15:49:53Z",
        "body": "```\r\n\r\nSELECT exp2(arraySum(x -> log2(x), [1, 4, 6, 8]))\r\n\r\n\u250c\u2500exp2(arraySum(lambda(tuple(x), log2(x)), [1, 4, 6, 8]))\u2500\u2510\r\n\u2502                                                     192 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Solution must compute the product of all elements in an array",
      "Approach must work with array data structures",
      "Method should use available built-in functions or standard operations",
      "Solution must avoid explicit loop constructs"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:13:22"
    }
  },
  {
    "number": 5351,
    "title": "Clickhouse Add date column to new table with specific date",
    "created_at": "2019-05-20T18:07:04Z",
    "closed_at": "2019-05-20T18:34:22Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5351",
    "body": "table 1----- without date column\r\ntable 2----- want to insert date column with a specific date\r\n\r\nQuery--- insert into table 2\r\n               select a,b,c,d,NOW() from table 1 \r\n\r\nNOW() will insert current date but I want to insert a specific date  into table 2\r\n\r\nEX: 2019-12-09\r\n\r\nPlease help me with the query as early as possible\r\n\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5351/comments",
    "author": "Crazylearner30",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-05-20T18:22:01Z",
        "body": "```\r\ninsert into table2\r\nselect a,b,c,d,toDate('2019-12-09') from table1\r\n```"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-05-20T18:22:50Z",
        "body": "i tried this and its not working :("
      },
      {
        "user": "den-crane",
        "created_at": "2019-05-20T18:26:43Z",
        "body": "> i tried this and its not working :(\r\n\r\nwith what error ???"
      },
      {
        "user": "Crazylearner30",
        "created_at": "2019-05-20T18:34:21Z",
        "body": "Hi sorry i was using todate instead of toDate('YYYY-MM-DD')\r\n\r\nIts working and thanks for the quick reply :)"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to insert a static date value instead of a dynamic timestamp",
      "Uses ClickHouse-compatible date conversion syntax",
      "Maintains column structure during insertion",
      "Validates date format compatibility"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:13:27"
    }
  },
  {
    "number": 5199,
    "title": "clickhouse default hash algorithm",
    "created_at": "2019-05-06T04:03:08Z",
    "closed_at": "2019-05-07T03:45:17Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/5199",
    "body": "After applying sharding expression, clickhouse computes the reminder of the returned results to distribute data.\r\n\r\nAs Jump consistent hashing algorithm says that using this algorithm can greatly reduce network communication while resharding the data in cluster expansion, why don't we use this algothrim as the default hashing algorithm.  Is there any special consideration?\r\n\r\nThanks.\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/5199/comments",
    "author": "etah000",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-05-06T23:35:31Z",
        "body": "What resharding? CH does not have resharding."
      },
      {
        "user": "filimonov",
        "created_at": "2019-05-06T23:55:38Z",
        "body": "ClickHouse supports 3 consistent hash algorithms: \r\n\r\n```\r\nSELECT *\r\nFROM system.functions \r\nWHERE name LIKE '%Consi%'\r\n\r\n\r\n\u250c\u2500name\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500is_aggregate\u2500\u252c\u2500case_insensitive\u2500\u252c\u2500alias_to\u2500\u2510\r\n\u2502 sumburConsistentHash \u2502            0 \u2502                0 \u2502          \u2502\r\n\u2502 jumpConsistentHash   \u2502            0 \u2502                0 \u2502          \u2502\r\n\u2502 yandexConsistentHash \u2502            0 \u2502                0 \u2502          \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n```\r\n\r\n\r\nAnd of course you can use them in sharding expression, to limit the network communication when cluster is reconfigured.\r\n```\r\nCREATE TABLE distributed AS test_table ENGINE = Distributed(cluster, db, test_table, jumpConsistentHash(field1, 2))\r\n```\r\n\r\nQuite often people prefer just to distribute data randomly. It doesn't make sense to calculate consistent hashes from random values. \r\n\r\nAnd resharding is not a common task. So why should we enforce some defaults?"
      },
      {
        "user": "etah000",
        "created_at": "2019-05-07T03:45:13Z",
        "body": "I see. The computation of the reminder does not influence the data distribution generated by \"jumpConsistentHash\". In order to follow the data distribution weight specified by shard configuration parameter, the \"bucket number\" should be the total weight of all the shards.\r\n\r\nThanks."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why ClickHouse does not enforce jumpConsistentHash as the default despite its resharding benefits",
      "Comparison of tradeoffs between different consistent hash algorithms in ClickHouse's context",
      "Identification of scenarios where default hash algorithm selection matters"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:13:34"
    }
  },
  {
    "number": 4959,
    "title": "DB::Exception: Cannot read all data. Bytes read: 2538.",
    "created_at": "2019-04-10T13:01:07Z",
    "closed_at": "2019-07-17T20:33:06Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4959",
    "body": "I have table \r\n```\r\nCREATE TABLE IF NOT EXISTS orgs_stats_views (\r\n            date  Date,\r\n            id_item UInt64,\r\n            id_user Nullable(UInt64),\r\n            platform UInt8,\r\n            is_app UInt8,\r\n            id_country Nullable(UInt8),\r\n            id_region Nullable(UInt8),\r\n            id_city Nullable(UInt8)\r\n        ) engine=MergeTree(date, (id_item, platform, is_app), 8192)\r\n```\r\nquery\r\n```\r\nSELECT count() as hit, date\r\nFROM (SELECT * FROM orgs_stats_views WHERE (id_item = 85660) AND (date BETWEEN '2019-01-01' AND '2019-12-31')) t\r\nGROUP BY date\r\nORDER BY date\r\n```\r\nerror\r\n```\r\nCode: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 2538. Bytes expected: 33102.: (while reading column date): (while reading from part /var/lib/clickhouse/data/xxxx/orgs_stats_views/20190101_20190118_9742_9747_1/ from mark 14 with max_rows_to_read = 8192)\r\n```\r\nHow i can resolve this problem?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4959/comments",
    "author": "free6k",
    "comments": [
      {
        "user": "free6k",
        "created_at": "2019-04-10T13:04:52Z",
        "body": "if execute query for this part\r\n```\r\nselect * from orgs_stats_views where toDate(date) = toDate('2019-01-18 01:01:01') ORDER BY date desc;\r\n```\r\ngot error\r\n```\r\nCode: 49, e.displayText() = DB::Exception: Can't adjust last granule because it has 8192rows, but try to subtract 40960 rows.\r\n```"
      },
      {
        "user": "alesapin",
        "created_at": "2019-04-10T13:31:30Z",
        "body": "Do you use setting `min_bytes_to_use_direct_io`?"
      },
      {
        "user": "free6k",
        "created_at": "2019-04-10T13:33:57Z",
        "body": "@alesapin what is it? I have default config"
      },
      {
        "user": "alesapin",
        "created_at": "2019-04-10T13:36:40Z",
        "body": "Ok, which version of clickhouse do you use?"
      },
      {
        "user": "free6k",
        "created_at": "2019-04-10T13:39:04Z",
        "body": "@alesapin ClickHouse server version 19.4.2.7."
      },
      {
        "user": "alesapin",
        "created_at": "2019-04-10T13:43:21Z",
        "body": "Maybe you can provide some data to reproduce the error?"
      },
      {
        "user": "free6k",
        "created_at": "2019-04-10T13:45:22Z",
        "body": "How? dump?"
      },
      {
        "user": "alesapin",
        "created_at": "2019-04-10T13:47:36Z",
        "body": "You can compress partitions stored on disk or use `SELECT xxx from yyy INTO OUTFILE 'xxx.tmp' Format TSV`. Can we communicate in Telegram?"
      },
      {
        "user": "free6k",
        "created_at": "2019-04-10T14:29:58Z",
        "body": "yes, free6k nickname\n\n---\n\n```\r\n2019.04.10 17:25:35.532828 [ 29 ] {15d0a921-c04d-4e10-be4a-765d4a263233} <Debug> MemoryTracker: Peak memory usage (total): 530.70 KiB.\r\n2019.04.10 17:25:35.532910 [ 29 ] {15d0a921-c04d-4e10-be4a-765d4a263233} <Debug> MemoryTracker: Peak memory usage (for query): 3.52 MiB.\r\n2019.04.10 17:25:35.532966 [ 29 ] {15d0a921-c04d-4e10-be4a-765d4a263233} <Error> HTTPHandler: Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 2538. Bytes expected: 33102.: (while reading column date): (while reading from part /var/lib/clickhouse//data/xxxx/orgs_stats_views/20190101_20190118_9742_9747_1/ from mark 14 with max_rows_to_read = 8192), Stack trace:\r\n\r\n0. /usr/bin/clickhouse-server(StackTrace::StackTrace()+0x16) [0x6f72af6]\r\n1. /usr/bin/clickhouse-server(DB::Exception::Exception(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int)+0x22) [0x37128c2]\r\n2. /usr/bin/clickhouse-server(DB::CompressedReadBufferBase::readCompressedData(unsigned long&, unsigned long&)+0x868) [0x6b811d8]\r\n3. /usr/bin/clickhouse-server(DB::CompressedReadBufferFromFile::nextImpl()+0x37) [0x6b829a7]\r\n4. /usr/bin/clickhouse-server(DB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)+0x79) [0x6b837a9]\r\n5. /usr/bin/clickhouse-server(DB::MergeTreeReaderStream::seekToMark(unsigned long)+0x83) [0x674d123]\r\n6. /usr/bin/clickhouse-server() [0x6a8dcd0]\r\n7. /usr/bin/clickhouse-server(DB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const+0x28) [0x3890928]\r\n8. /usr/bin/clickhouse-server(DB::MergeTreeReader::readData(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)+0x2da) [0x6a8e93a]\r\n9. /usr/bin/clickhouse-server(DB::MergeTreeReader::readRows(unsigned long, bool, unsigned long, DB::Block&)+0x29c) [0x6a8eeac]\r\n10. /usr/bin/clickhouse-server(DB::MergeTreeRangeReader::DelayedStream::finalize(DB::Block&)+0x166) [0x6a8aaa6]\r\n11. /usr/bin/clickhouse-server(DB::MergeTreeRangeReader::startReadingChain(unsigned long, std::vector<DB::MarkRange, std::allocator<DB::MarkRange> >&)+0x1d5) [0x6a8b865]\r\n12. /usr/bin/clickhouse-server(DB::MergeTreeRangeReader::read(unsigned long, std::vector<DB::MarkRange, std::allocator<DB::MarkRange> >&)+0x739) [0x6a8d259]\r\n13. /usr/bin/clickhouse-server(DB::MergeTreeRangeReader::read(unsigned long, std::vector<DB::MarkRange, std::allocator<DB::MarkRange> >&)+0xba) [0x6a8cbda]\r\n14. /usr/bin/clickhouse-server(DB::MergeTreeBaseSelectBlockInputStream::readFromPart()+0x651) [0x6a860c1]\r\n15. /usr/bin/clickhouse-server(DB::MergeTreeBaseSelectBlockInputStream::readImpl()+0xbe) [0x6a8772e]\r\n16. /usr/bin/clickhouse-server(DB::IBlockInputStream::read()+0x178) [0x62ed6a8]\r\n17. /usr/bin/clickhouse-server(DB::FilterBlockInputStream::readImpl()+0xd9) [0x68aa919]\r\n18. /usr/bin/clickhouse-server(DB::IBlockInputStream::read()+0x178) [0x62ed6a8]\r\n19. /usr/bin/clickhouse-server(DB::ExpressionBlockInputStream::readImpl()+0x2d) [0x68a9c9d]\r\n20. /usr/bin/clickhouse-server(DB::IBlockInputStream::read()+0x178) [0x62ed6a8]\r\n21. /usr/bin/clickhouse-server(DB::ExpressionBlockInputStream::readImpl()+0x2d) [0x68a9c9d]\r\n22. /usr/bin/clickhouse-server(DB::IBlockInputStream::read()+0x178) [0x62ed6a8]\r\n23. /usr/bin/clickhouse-server(DB::ExpressionBlockInputStream::readImpl()+0x2d) [0x68a9c9d]\r\n24. /usr/bin/clickhouse-server(DB::IBlockInputStream::read()+0x178) [0x62ed6a8]\r\n25. /usr/bin/clickhouse-server(DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::thread(std::shared_ptr<DB::ThreadGroupStatus>, unsigned long)+0x2e2) [0x68d53c2]\r\n26. /usr/bin/clickhouse-server(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::process()::{lambda()#1}>(DB::ParallelInputsProcessor<DB::ParallelAggregatingBlockInputStream::Handler>::process()::{lambda()#1}&&)::{lambda()#1}::operator()() const+0x71) [0x68d5db1]\r\n27. /usr/bin/clickhouse-server(ThreadPoolImpl<std::thread>::worker(std::_List_iterator<std::thread>)+0x1ab) [0x6f78a8b]\r\n28. /usr/bin/clickhouse-server() [0xadee95f]\r\n29. /lib/x86_64-linux-gnu/libpthread.so.0(+0x74a4) [0x7f54237314a4]\r\n30. /lib/x86_64-linux-gnu/libc.so.6(clone+0x3f) [0x7f5422d63d0f]\r\n```"
      },
      {
        "user": "AlexAkulov",
        "created_at": "2019-07-10T08:35:22Z",
        "body": "Any progress? "
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-07-17T20:33:40Z",
        "body": "@free6k said that the issue no longer reproduces.\n\n---\n\nIf you experience this issue, consider updating to 19.11."
      },
      {
        "user": "jackpgao",
        "created_at": "2019-11-07T07:42:52Z",
        "body": "```\r\n\u250c\u2500version()\u2500\u2500\u2510\r\n\u2502 19.13.3.26 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n\r\n\r\n```\r\n2019.11.07 15:35:47.483092 [ 97 ] {97a4ed62-edb7-44ac-92eb-293657785370} <Error> executeQuery: Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 293. Bytes expected: 1070. (version 19.13.3.26)\r\n```\r\n"
      },
      {
        "user": "dbaops-com",
        "created_at": "2019-11-10T09:20:40Z",
        "body": "ClickHouse server version 19.14.6:\r\nwhen i use mysql cilent, get this error:\r\n2019.11.10 17:15:35.159193 [ 183 ] {} <Error> MySQLHandler: DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 3."
      },
      {
        "user": "simonhou",
        "created_at": "2020-04-23T07:17:04Z",
        "body": "ClickHouse version 19.11.9.52, I met the same error:\r\n\r\n```\r\n2020.04.23 14:48:43.994496 [ 48 ] {02a71eb2-5843-41b9-8d0a-8ffcd5de70d1} <Error> executeQuery: Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 19854. Bytes expected: 211790. (version 19.11.9.52) (from 10.62.184.177:29845) (in query: INSERT INTO...\r\n\r\n0. clickhouse-server(StackTrace::StackTrace()+0x30) [0x6d84db0]\r\n1. clickhouse-server(DB::Exception::Exception(std::string const&, int)+0x1f) [0x312baaf]\r\n2. clickhouse-server(DB::CompressedReadBufferBase::readCompressedData(unsigned long&, unsigned long&)+0x1688) [0x5bb9d08]\r\n3. clickhouse-server(DB::CompressedReadBuffer::nextImpl()+0x28) [0x5bb5888]\r\n4. clickhouse-server(DB::NativeBlockInputStream::readImpl()+0x1bd9) [0x5bdcce9]\r\n5. clickhouse-server(DB::IBlockInputStream::read()+0x238) [0x5bd0228]\r\n6. clickhouse-server(DB::TCPHandler::receiveData()+0x4f) [0x31346ff]\r\n7. clickhouse-server(DB::TCPHandler::receivePacket()+0x85) [0x3135785]\r\n8. clickhouse-server(DB::TCPHandler::readData(DB::Settings const&)+0x19b) [0x3135c3b]\r\n9. clickhouse-server(DB::TCPHandler::processInsertQuery(DB::Settings const&)+0x20b) [0x313612b]\r\n10. clickhouse-server(DB::TCPHandler::runImpl()+0xa37) [0x3136d97]\r\n11. clickhouse-server(DB::TCPHandler::run()+0x1c) [0x3137aac]\r\n12. clickhouse-server(Poco::Net::TCPServerConnection::start()+0xf) [0x68d057f]\r\n13. clickhouse-server(Poco::Net::TCPServerDispatcher::run()+0x166) [0x68d0946]\r\n14. clickhouse-server(Poco::PooledThread::run()+0x77) [0x6f59a17]\r\n15. clickhouse-server(Poco::ThreadImpl::runnableEntry(void*)+0x38) [0x6f55bd8]\r\n16. clickhouse-server() [0x75275df]\r\n17. /lib64/libpthread.so.0(+0x7e25) [0x7f45a4f50e25]\r\n18. /lib64/libc.so.6(clone+0x6d) [0x7f45a497c34d]\r\n\r\n```\n\n---\n\n@alexey-milovidov Can you help to verify this exception?"
      },
      {
        "user": "xmabul",
        "created_at": "2020-04-30T09:49:30Z",
        "body": "> ClickHouse server version 19.14.6:\r\n> when i use mysql cilent, get this error:\r\n> 2019.11.10 17:15:35.159193 [ 183 ] {} MySQLHandler: DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 3.\r\n\r\n@dbaops-com hello,I met the same problem.Do u have solved it?"
      },
      {
        "user": "muligolan",
        "created_at": "2020-09-09T13:27:25Z",
        "body": "I'm experiencing the same problem:\r\n<Error> executeQuery: Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 93974. Bytes expected: 764646. (version 19.11.4.24) (from 10.10.200.208:29780) (in query: INSERT INTO {table_name}, Stack trace:\r\n\r\n0. /usr/bin/clickhouse-server(StackTrace::StackTrace()+0x30) [0x6d0c010]\r\n1. /usr/bin/clickhouse-server(DB::Exception::Exception(std::string const&, int)+0x1f) [0x30ee99f]\r\n2. /usr/bin/clickhouse-server(DB::CompressedReadBufferBase::readCompressedData(unsigned long&, unsigned long&)+0x1706) [0x5b46aa6]\r\n3. /usr/bin/clickhouse-server(DB::CompressedReadBuffer::nextImpl()+0x28) [0x5b425a8]\r\n4. /usr/bin/clickhouse-server(DB::NativeBlockInputStream::readImpl()+0x1bbc) [0x5b6989c]\r\n5. /usr/bin/clickhouse-server(DB::IBlockInputStream::read()+0x238) [0x5b5ce78]\r\n6. /usr/bin/clickhouse-server(DB::TCPHandler::receiveData()+0x4f) [0x30f75ff]\r\n7. /usr/bin/clickhouse-server(DB::TCPHandler::receivePacket()+0x85) [0x30f8685]\r\n8. /usr/bin/clickhouse-server(DB::TCPHandler::readData(DB::Settings const&)+0x19b) [0x30f8b3b]\r\n9. /usr/bin/clickhouse-server(DB::TCPHandler::processInsertQuery(DB::Settings const&)+0x20b) [0x30f902b]\r\n10. /usr/bin/clickhouse-server(DB::TCPHandler::runImpl()+0x61c) [0x30f987c]\r\n11. /usr/bin/clickhouse-server(DB::TCPHandler::run()+0x1c) [0x30fa9bc]\r\n12. /usr/bin/clickhouse-server(Poco::Net::TCPServerConnection::start()+0xf) [0x68577df]\r\n13. /usr/bin/clickhouse-server(Poco::Net::TCPServerDispatcher::run()+0x166) [0x6857ba6]\r\n14. /usr/bin/clickhouse-server(Poco::PooledThread::run()+0x77) [0x6ee0c77]\r\n15. /usr/bin/clickhouse-server(Poco::ThreadImpl::runnableEntry(void*)+0x38) [0x6edce38]\r\n16. /usr/bin/clickhouse-server() [0x74ae81f]\r\n17. /lib64/libpthread.so.0(+0x7dd5) [0x7f6a61e2cdd5]\r\n18. /lib64/libc.so.6(clone+0x6d) [0x7f6a61853ead]\r\n\r\nlooks like during replication because 10.10.200.208 is the instance with the replica"
      },
      {
        "user": "tantnat",
        "created_at": "2020-09-16T09:36:04Z",
        "body": "Hello! Meet the same error.\r\n\r\n``` \r\nru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 33, host: {HOST}, port: {PORT}; Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 1. Bytes expected: 2. (version 20.4.9.110 (official build))\r\n```"
      },
      {
        "user": "stufently",
        "created_at": "2020-12-03T07:49:58Z",
        "body": "I have same problem in latest release clickhouse-server                    20.11.4.13\r\nI try fix it run \r\ncheck table events; \r\nbut after clickhouse move part to detached , this part replicate from other replica with same problem.\r\nseems this part broken on all replicas\r\n\r\n`2020.12.02 12:27:29.607426 [ 144664 ] {} <Warning> default.events (ReplicatedMergeTreePartCheckThread): Checking data of part 20201130_20201130_499695_499707_2.\r\n2020.12.02 12:27:30.317326 [ 144657 ] {} <Warning> default.events (ReplicatedMergeTreePartCheckThread): Checking part 20201130_20201130_499695_499707_2\r\n2020.12.02 12:27:30.322286 [ 144623 ] {} <Error> default.events (8005f254-5c8e-45ed-9a03-8ba6d4b05bce): auto DB::StorageReplicatedMergeTree::queueTask()::(anonymous class)::operator()(DB::StorageReplicatedMergeTree::LogEntryPtr &) const: Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 15729662. Bytes expected: 268435455.: (while reading column BillingURL): (while reading from part /var/lib/clickhouse/store/800/8005f254-5c8e-45ed-9a03-8ba6d4b05bce/20201130_20201130_499695_499707_2/ from mark 14 with max_rows_to_read = 8192): While executing MergeTreeSequentialSource: Cannot fetch required block. Stream PipelineExecuting, part 2, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::ReadBuffer::readStrict(char*, unsigned long) @ 0x7b6db35 in /usr/bin/clickhouse\r\n1. ? @ 0xd5a8ba1 in /usr/bin/clickhouse\r\n2. DB::DataTypeString::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const @ 0xd5a81f8 in /usr/bin/clickhouse\r\n3. DB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool) @ 0xe1c2c79 in /usr/bin/clickhouse\r\n4. DB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&) @ 0xe1c1f90 in /usr/bin/clickhouse\r\n5. DB::MergeTreeSequentialSource::generate() @ 0xe1d36c9 in /usr/bin/clickhouse\r\n6. DB::ISource::work() @ 0xe36459a in /usr/bin/clickhouse\r\n7. DB::SourceWithProgress::work() @ 0xe4c927a in /usr/bin/clickhouse\r\n8. ? @ 0xe39d37c in /usr/bin/clickhouse\r\n9. DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0xe39a4a7 in /usr/bin/clickhouse\r\n10. DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) @ 0xe39918c in /usr/bin/clickhouse\r\n11. DB::PullingPipelineExecutor::pull(DB::Chunk&) @ 0xe3a5ac8 in /usr/bin/clickhouse\r\n12. DB::PullingPipelineExecutor::pull(DB::Block&) @ 0xe3a5d10 in /usr/bin/clickhouse\r\n13. DB::PipelineExecutingBlockInputStream::readImpl() @ 0xe395444 in /usr/bin/clickhouse\r\n14. DB::IBlockInputStream::read() @ 0xd4f2625 in /usr/bin/clickhouse\r\n15. DB::ColumnGathererStream::fetchNewBlock(DB::ColumnGathererStream::Source&, unsigned long) @ 0xdce085f in /usr/bin/clickhouse\r\n16. void DB::ColumnGathererStream::gather<DB::ColumnString>(DB::ColumnString&) @ 0xdd39a11 in /usr/bin/clickhouse\r\n17. DB::ColumnGathererStream::readImpl() @ 0xdce0650 in /usr/bin/clickhouse\r\n18. DB::IBlockInputStream::read() @ 0xd4f2625 in /usr/bin/clickhouse\r\n19. DB::MergeTreeDataMergerMutator::mergePartsToTemporaryPart(DB::FutureMergedMutatedPart const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::BackgroundProcessListEntry<DB::MergeListElement, DB::MergeInfo>&, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, long, DB::Context const&, std::__1::unique_ptr<DB::IReservation, std::__1::default_delete<DB::IReservation> > const&, bool) @ 0xe0fb24b in /usr/bin/clickhouse\r\n20. DB::StorageReplicatedMergeTree::tryExecuteMerge(DB::ReplicatedMergeTreeLogEntry const&) @ 0xdef4c04 in /usr/bin/clickhouse\r\n21. DB::StorageReplicatedMergeTree::executeLogEntry(DB::ReplicatedMergeTreeLogEntry&) @ 0xdee6d6c in /usr/bin/clickhouse\r\n22. ? @ 0xdf5af7c in /usr/bin/clickhouse\r\n23. DB::ReplicatedMergeTreeQueue::processEntry(std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()>, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&, std::__1::function<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>) @ 0xe23f2b5 in /usr/bin/clickhouse\r\n\r\n`\r\n"
      },
      {
        "user": "thinhnp-it",
        "created_at": "2021-03-09T13:09:31Z",
        "body": "Any solution for this, guys ?  I have same problem in latest release clickhouse-server 21.2.5.5"
      },
      {
        "user": "stufently",
        "created_at": "2021-03-09T13:32:34Z",
        "body": "> Any solution for this, guys ? I have same problem in latest release clickhouse-server 21.2.5.5\r\n\r\nTry change server, update clickhouse dont help with this issue."
      },
      {
        "user": "thinhnp-it",
        "created_at": "2021-03-09T13:51:47Z",
        "body": "> Try change server, update clickhouse dont help with this issue.\r\n\r\nWhich settings need to be changed? Or you meant reinstall CH cluster ? Thanks."
      },
      {
        "user": "stufently",
        "created_at": "2021-03-09T13:57:11Z",
        "body": "I mean real server for replica with this issue"
      },
      {
        "user": "schedin",
        "created_at": "2021-08-26T08:42:58Z",
        "body": "I had the problem/error message in a different context compared to @free6k. I use Java code and clickhouse-jdbc to insert many rows with the ClickHouse stream writer/ClickHouseStreamCallback. I had a misalignment of the number of columns in my Java stream code compared to the table in the database (I had one column too many in the database). When I aligned the number of columns everything worked (I used clickhouse-server-21.1.9.41)."
      },
      {
        "user": "nblagodarnyi",
        "created_at": "2021-09-22T12:02:47Z",
        "body": "@schedin i got the same issue with correct alignment/order. But table column is of type **Int16** while my method uses \r\n`case IntegerType => stream.writeInt32(row.getInt(colIndex))`\r\n\r\n"
      },
      {
        "user": "jun0tpyrc",
        "created_at": "2021-11-30T13:40:27Z",
        "body": "hitting this too"
      },
      {
        "user": "TvoroG",
        "created_at": "2022-04-28T14:34:05Z",
        "body": "Solved my case. For me it was that i've tried to restore old backup for mv table with changed schema (order by)."
      },
      {
        "user": "anatoly-scherbakov",
        "created_at": "2022-12-19T09:05:17Z",
        "body": "In my case, I was `INSERT`-ing rows into a Clickhouse table in a rather large chunk. Clickhouse log says (where \u2026 are mine):\r\n\r\n```\r\n2022.12.19 06:38:56.300588 [ 314 ] {02ea534c-d8bf-4cd3-8b25-4deb5a9d3bf0} <Error> executeQuery: Code: 241. DB::Exception: Memory limit (for query) exceeded: would use 16.01 GiB (attempt to allocate chunk of 8589934592 bytes), maximum: 9.31 GiB. (MEMORY_LIMIT_EXCEEDED) (version 21.12.3.32 (official build)) (from 10.0.0.82:34700) (in query: INSERT INTO \u2026 (\u2026) VALUES), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xa21959a in /usr/bin/clickhouse\r\n\u2026\r\n15. DB::TCPHandler::run() @ 0x13fec4b9 in /usr/bin/clickhouse\r\n16. Poco::Net::TCPServerConnection::start() @ 0x16f3d6af in /usr/bin/clickhouse\r\n17. Poco::Net::TCPServerDispatcher::run() @ 0x16f3fb01 in /usr/bin/clickhouse\r\n18. Poco::PooledThread::run() @ 0x1704e889 in /usr/bin/clickhouse\r\n19. Poco::ThreadImpl::runnableEntry(void*) @ 0x1704bf80 in /usr/bin/clickhouse\r\n20. ? @ 0x7f9f490d7609 in ?\r\n21. __clone @ 0x7f9f48ffe293 in ?\r\n\r\n2022.12.19 06:38:56.300678 [ 314 ] {02ea534c-d8bf-4cd3-8b25-4deb5a9d3bf0} <Error> TCPHandler: Code: 241. DB::Exception: Memory limit (for query) exceeded: would use 16.01 GiB (attempt to allocate chunk of 8589934592 bytes), maximum: 9.31 GiB. (MEMORY_LIMIT_EXCEEDED), Stack trace (when copying this message, always include the lines below):\r\n\r\n\u2026\r\n\r\n2022.12.19 06:38:56.301087 [ 314 ] {} <Error> ServerErrorHandler: Code: 99. DB::Exception: Unknown packet 73 from client. (UNKNOWN_PACKET_FROM_CLIENT), Stack trace (when copying this message, always include the lines below):\r\n\r\n\u2026\r\n2022.12.19 06:39:53.970475 [ 314 ] {932f40ea-35ed-4349-a157-b1dd2f03cf64} <Error> executeQuery: Code: 33. DB::Exception: Cannot read all data. Bytes read: 160398. Bytes expected: 200076. (CANNOT_READ_ALL_DATA) (version 21.12.3.32 (official build)) (from 10.0.0.82:44234) (in query: INSERT INTO \u2026 (\u2026) VALUES), Stack trace (when copying this message, always include the lines below):\r\n\u2026\r\n```\r\n\r\nI used the chunk size of 100 000 rows when inserting. Switched to 50 000 rows and everything seems fine so far."
      }
    ],
    "satisfaction_conditions": [
      "Identify and resolve data corruption in ClickHouse table parts",
      "Address version-specific ClickHouse bugs affecting data integrity",
      "Ensure compatibility between data schema and query patterns",
      "Handle memory constraints during large operations",
      "Provide recovery mechanisms for corrupted partitions"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:14:07"
    }
  },
  {
    "number": 4957,
    "title": "How to join tables by range of numbers",
    "created_at": "2019-04-10T10:55:48Z",
    "closed_at": "2019-04-10T11:14:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4957",
    "body": "Hi, is there a way that we can do join query like this with Clickhouse?\r\n\r\n`select * from tableA a `\r\n`left join tableB b on a.mynumber between b.rangefrom and b.rangeto`\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4957/comments",
    "author": "alvin85",
    "comments": [
      {
        "user": "blinkov",
        "created_at": "2019-04-10T11:03:45Z",
        "body": "At the moment only via CROSS JOIN + WHERE"
      },
      {
        "user": "alvin85",
        "created_at": "2019-04-10T11:14:42Z",
        "body": "Good idea! thanks."
      }
    ],
    "satisfaction_conditions": [
      "Supports range-based table joins where a value falls between two columns",
      "Works within ClickHouse's SQL capabilities",
      "Provides a workaround when direct range joins aren't supported"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:14:15"
    }
  },
  {
    "number": 4737,
    "title": "Confused about server binaries size (deb)",
    "created_at": "2019-03-20T14:20:05Z",
    "closed_at": "2019-03-21T08:25:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4737",
    "body": "Hello. \r\nI'm a little confused about clickhouse binaries size. I was going to upgrade version from 18 to 19 and noticed deb packets size: \r\n\r\n    clickhouse-server-base_18.16.1_amd64.deb                339M\r\n    clickhouse-server-base_19.4.1.3_amd64.deb               734M\r\n\r\nTwo questions:\r\n1) Is it correct that size doubles for a major version?\r\n2) What is included in a distribution that takes ~1.8G for a server binary (/usr/bin/clickhouse unarchived) in 19.4.1.3 version? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4737/comments",
    "author": "nitso",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-03-20T15:19:56Z",
        "body": "you need only 3 debs\r\n\r\n```\r\n 103M Mar 18 12:57 clickhouse-common-static_19.5.1.122_amd64.deb\r\n 12K Mar 18 12:57 clickhouse-client_19.5.1.122_all.deb\r\n 24K Mar 18 12:57 clickhouse-server_19.5.1.122_all.deb\r\n```\r\n\r\n242M  /usr/bin/clickhouse"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-03-20T21:49:20Z",
        "body": "`\u0441lickhouse-server-base` is an obsolete package that has embedded debug info.\r\nMost of its size is debug information. There is a lot of debug info due to large number of C++ template instantiations. You can obtain debug info separately from `clickhouse-common-static-dbg` package.\r\n\r\nThe size of `clickhouse-common-static` package (the main package with `clickhouse` binary) has increased from 69M (for 18.16) to 102M (for 19.4). This is Ok and caused by addition of new features (mostly for HDFS and Parquet integration).\n\n---\n\nPS. JFYI, number of commits since version 18.16 was increased by 14% and number of LOC in `dbms` directory - by 9%."
      },
      {
        "user": "nitso",
        "created_at": "2019-03-21T08:25:25Z",
        "body": "Thanks a lot for explanation. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of whether a major version upgrade typically justifies doubling binary size",
      "Breakdown of components contributing to server binary size",
      "Clarification about debug information vs core functionality in packages",
      "Identification of essential vs optional packages for installation",
      "Context about feature development impacting binary size"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:14:45"
    }
  },
  {
    "number": 4716,
    "title": "Question around set indexes",
    "created_at": "2019-03-17T22:20:10Z",
    "closed_at": "2019-03-18T09:48:30Z",
    "labels": [
      "question",
      "comp-skipidx"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4716",
    "body": "Hi there,\r\n\r\nI'm a bit confused about set indexes and where they kick in. I've tried the examples in the tests folder and they work as expected however if I try to apply them in my case granules are never dropped. I've tried with larger values and more partitions (use toStartOfHour(ts) below) but set indexes never kick in. Am I mistaken in their use/purpose and have to wait for bloom indices or am I using them incorrectly?\r\n\r\nThanks!\r\n\r\n```\r\nSET allow_experimental_data_skipping_indices = 1;\r\n\r\nDROP TABLE IF EXISTS test.idx_test;\r\n\r\nCREATE TABLE test.idx_test (\r\n                s_key UInt64,\r\n                id UInt32,\r\n                ts DateTime,\r\n                value UInt64,\r\n                INDEX s_key_idx (s_key) TYPE set(0) GRANULARITY 1000\r\n) ENGINE = MergeTree\r\nORDER BY (id, ts)\r\nSETTINGS index_granularity = 32;\r\n\r\nINSERT INTO test.idx_test\r\nSELECT\r\n                cityHash64(id) AS s_key,\r\n                number AS id,\r\n                toDateTime(1551398400 + rand(1)%86400) AS ts,\r\n                rand(2) AS value\r\nFROM system.numbers LIMIT 100000;\r\n\r\nSET send_logs_level = 'debug';\r\n\r\nselect * from test.idx_test where id = 3000 format PrettySpace;\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.564572 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> executeQuery: (from 127.0.0.1:34334) select * from test.idx_test where id = 3000 format PrettySpace;\r\n[clickhouse-demo] 2019.03.17 22:01:10.566056 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> test.idx_test (SelectExecutor): Key condition: (column 0 in [3000, 3000])\r\n[clickhouse-demo] 2019.03.17 22:01:10.566108 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> test.idx_test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 1 marks to read from 1 ranges\r\n[clickhouse-demo] 2019.03.17 22:01:10.566366 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> executeQuery: Query pipeline:\r\n\r\nExpression\r\nExpression\r\n  Filter\r\n   MergeTreeThread\r\n\r\n               s_key     id                    ts        value\r\n\r\n16286406272394286119   3000   2019-03-01 13:11:22   3080386888\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.568391 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Information> executeQuery: Read 32 rows, 768.00 B in 0.004 sec., 8682 rows/sec., 203.51 KiB/sec.\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:10.568446 {44b0be5f-2a11-47f2-9ca5-abe862c3ab79} [ 55 ] <Debug> MemoryTracker: Peak memory usage (for query): 1.44 MiB.\r\n\r\n1 rows in set. Elapsed: 0.004 sec.\r\n\r\nselect * from test.idx_test where s_key = 16286406272394286119 format PrettySpace;\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:35.857723 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> executeQuery: (from 127.0.0.1:34334) select * from test.idx_test where s_key = 16286406272394286119 format PrettySpace;\r\n[clickhouse-demo] 2019.03.17 22:01:35.858430 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"s_key = 16286406272394286119\" moved to PREWHERE\r\n[clickhouse-demo] 2019.03.17 22:01:35.858879 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Key condition: unknown\r\n[clickhouse-demo] 2019.03.17 22:01:35.859056 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Index `s_key_idx` has dropped 0 granules.\r\n[clickhouse-demo] 2019.03.17 22:01:35.859096 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> test.idx_test (SelectExecutor): Selected 1 parts by date, 1 parts by key, 3125 marks to read from 1 ranges\r\n[clickhouse-demo] 2019.03.17 22:01:35.859269 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> executeQuery: Query pipeline:\r\nExpression\r\nExpression\r\n  MergeTreeThread\r\n\r\n               s_key     id                    ts        value\r\n16286406272394286119   3000   2019-03-01 13:11:22   3080386888\r\n\r\n[clickhouse-demo] 2019.03.17 22:01:35.864492 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Information> executeQuery: Read 100000 rows, 781.27 KiB in 0.007 sec., 14970210 rows/sec., 114.22 MiB/sec.\r\n[clickhouse-demo] 2019.03.17 22:01:35.864554 {2a2f2182-1abd-4037-834a-bb963b58afa6} [ 55 ] <Debug> MemoryTracker: Peak memory usage (for query): 3.99 MiB.\r\n\r\n1 rows in set. Elapsed: 0.008 sec. Processed 100.00 thousand rows, 800.02 KB (12.81 million rows/s., 102.50 MB/s.)\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4716/comments",
    "author": "zombiemonkey",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2019-03-18T08:52:41Z",
        "body": "I think the reason is that `set(0)` is an empty set, so empty index was created. Try to use, for example, `set(100)`. 100 here is the max set size which is created per granule (so, if granule has more than 100 distinct values, set won't be created, and granule will never be dropped)."
      },
      {
        "user": "zombiemonkey",
        "created_at": "2019-03-18T09:48:30Z",
        "body": "Ah ok - thanks Nicolai. Have it working now. The docs say that set(0) means no limit but that must be incorrect. Cheers!"
      },
      {
        "user": "KochetovNicolai",
        "created_at": "2019-03-18T10:42:52Z",
        "body": "Well, it seems that set(0) should mean no limit, and it is fixed in #4640. Docs was correct."
      },
      {
        "user": "den-crane",
        "created_at": "2019-03-18T16:04:40Z",
        "body": "IMHO skip `INDEX s_key_idx (s_key) TYPE set(0) GRANULARITY 1000` just has no sense.\r\nIt makes a query slower and doubles the column stored data.\r\n\r\n\r\n```\r\ncreate table BX(\r\nI Int64, \r\nS String,\r\nINDEX Sx S TYPE set(0) GRANULARITY 1000\r\n) Engine=MergeTree order by I;\r\n\r\ninsert into BX select number, toString(rand()) from numbers(10000000);\r\ninsert into BX values(45645645, '666');\r\nselect * from BX where S = '666'\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.235 sec.\r\n```\r\n\r\n```\r\nalter table BX drop index Sx;\r\nselect * from BX where S = '666'\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500I\u2500\u252c\u2500S\u2500\u2500\u2500\u2510\r\n\u2502 45645645 \u2502 666 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\r\n1 rows in set. Elapsed: 0.070 sec. Processed 10.00 million rows, 267.41 MB (143.10 million rows/s., 3.83 GB/s.)\r\n```\r\nAnd this is an expected behaviour."
      },
      {
        "user": "zombiemonkey",
        "created_at": "2019-03-30T09:23:39Z",
        "body": "@den-crane - I know :) I think you misunderstood that this was seeking clarity as to why set(0) was not working as documented. As KochetovNicolai pointed out - there was a bug fixed in #4640. 1000 was simply one of the values that was used for testing from a range of values and not the problem/issue WRT the behavior of set(0)."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how set index parameters affect index creation and granule skipping",
      "Clarification of relationship between index type, granularity settings, and query patterns",
      "Guidance on verifying index usage through query analysis/logs",
      "Explanation of version-specific behaviors affecting index functionality"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:14:55"
    }
  },
  {
    "number": 4628,
    "title": "Top N elements group by time",
    "created_at": "2019-03-08T16:14:16Z",
    "closed_at": "2020-04-05T15:54:15Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4628",
    "body": "Following is a sample schema\r\n\r\n```\r\n{\r\n    name String,\r\n    dateTime String,\r\n    ....\r\n    value UInt32\r\n}\r\n```\r\n\r\nWe would like to group them by timeseries. However, we are interested only in the top 5 elements with the highest value sum. \r\n\r\n```\r\nSELECT any(name),\r\n       sum(value)\r\nFROM TABLE\r\nWHERE name IN\r\n    (SELECT name\r\n     FROM TABLE\r\n     GROUP BY name\r\n     ORDER BY sum(value) DESC\r\n     LIMIT 5)\r\nGROUP BY name,\r\n         toRelativeMinuteNum(dateTime)\r\n```\r\n\r\nThe above gives the top 5 elements in the `IN` call and which are then used to again be grouped by name with the time bucket.\r\n\r\nHowever, we would also like to get the calculated internal `sum(value)` also separately. So\r\n\r\n```\r\nSELECT any(name),\r\n       sum(value),\r\n  (SELECT name\r\n   FROM TABLE\r\n   GROUP BY name\r\n   ORDER BY sum(value) DESC\r\n   LIMIT 5) AS topSumNames\r\nFROM TABLE\r\nWHERE name IN topSumNames\r\nGROUP BY name,\r\n         toRelativeMinuteNum(dateTime)\r\n```\r\n\r\nThis above fails with the error \r\n```\r\nDB::Exception: Scalar subquery returned more than one row\r\n```\r\n\r\nQuestions:\r\n\r\n1. How to get the internal sub query value also in the select output\r\n2. Is there a better way to perform this top 5 elements\r\n\r\nThe `LIMIT BY clause` is not working, when we put in the `dateTime` in the grouping.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4628/comments",
    "author": "sundarv85",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-03-08T16:37:18Z",
        "body": "bad\r\n```\r\nSELECT name, toRelativeMinuteNum(dateTime) r,  ss, sum(value)\r\nFROM TABLE inner join (SELECT name, sum(value) ss FROM TABLE GROUP BY name ORDER BY ss DESC LIMIT 5) using name\r\nGROUP BY name, ss, r\r\n```\r\n\r\nbetter\r\n\r\n```\r\ncreate temporary table TX as \r\nSELECT name, sum(value) ss FROM TABLE GROUP BY name ORDER BY ss DESC LIMIT 5;\r\n\r\nSELECT name, toRelativeMinuteNum(dateTime) r,  ss, sum(value)\r\nFROM TABLE inner join (SELECT name, sum(value) ss FROM TX) using name\r\nWHERE name in (select name from TX)\r\nGROUP BY name,ss, r;\r\n\r\n```"
      },
      {
        "user": "sundarv85",
        "created_at": "2019-03-08T20:28:50Z",
        "body": "Thanks @den-crane. The INNER JOIN did the trick.\r\n\r\nCould you explain more the difference between having a direct `SELECT` in the inner join, vs the `temporary table`.  Why the former is `bad` while the other one is `better`\r\n\r\nThe issue is - this `sum(value)` and order changes based on the time range (1d or 1M) that is being queried. So we could not create a table that could cover various time ranges. With the `temporary table`, do you suggest to create the temporary table everytime a query is about to be executed? Also, how to make this temporary table work for parallel queries for different time ranges.."
      },
      {
        "user": "den-crane",
        "created_at": "2019-03-08T22:04:29Z",
        "body": "> Could you explain more the difference between having a direct `SELECT` in the inner join, vs the `temporary table`. Why the former is `bad` while the other one is `better`\r\n\r\nI had an idea that a filter `WHERE name in (select name from TX)` makes the query faster but now I am not so sure, because perhaps `using name` do the same in the *bad* query.\r\nThough if you have `index` by name column it can be much faster with `WHERE name in `.\r\nCheck the speed and processed rows statistics on real data. "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T20:25:04Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      },
      {
        "user": "blinkov",
        "created_at": "2020-04-01T16:46:57Z",
        "body": "@sundarv85, do you have any further questions?"
      },
      {
        "user": "sundarv85",
        "created_at": "2020-04-05T15:54:15Z",
        "body": "Thanks @blinkov. No questions anymore. It worked. "
      }
    ],
    "satisfaction_conditions": [
      "Solution must allow inclusion of aggregated subquery results (like top 5 sums) in the main query output without scalar errors",
      "Approach must handle dynamic time ranges without requiring precomputed tables",
      "Method should explain performance considerations between subquery approaches",
      "Solution must maintain ability to group by time intervals while filtering to top N elements",
      "Approach should work with existing indexes on name column"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:15:09"
    }
  },
  {
    "number": 4544,
    "title": "Should Select be optimized for MATERIALIZED VIEW?",
    "created_at": "2019-03-01T10:10:30Z",
    "closed_at": "2019-03-02T15:57:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4544",
    "body": "When using materialized view as a trigger, that said used for inserting data to table A when data is inserted to table B:\r\n\r\n```sql\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS tableBToTableA TO tableA\r\nAS\r\nSELECT\r\n    some_fields\r\nFROM tableB\r\nGROUP BY some_fields\r\n```\r\n\r\nIs it also required to optimize the MV select so it uses partitioning key and sorting keys of `tableB`? Or it doesn't matter?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4544/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "den-crane",
        "created_at": "2019-03-02T15:54:24Z",
        "body": "Does not matter.\r\nMV does not read real tableB (except at a populate stage).\r\nMV gets inserted buffer (from insert query) and evaluates select over this buffer."
      }
    ],
    "satisfaction_conditions": [
      "Clarify how materialized views process data during inserts",
      "Explain the relationship between MV processing and source table indexes",
      "Address performance implications of MV data ingestion method",
      "Confirm whether MV behavior differs between initial population and ongoing inserts"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:15:17"
    }
  },
  {
    "number": 4414,
    "title": "What is a \"granule\"?",
    "created_at": "2019-02-15T21:31:37Z",
    "closed_at": "2019-11-01T18:20:35Z",
    "labels": [
      "question",
      "comp-documentation",
      "comp-skipidx"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/4414",
    "body": "The documentation on data skipping indexes states:\r\n\r\n> These indices aggregate some information about the specified expression on blocks, which consist of granularity_value granules, then these aggregates are used in SELECT queries for reducing the amount of data to read from the disk by skipping big blocks of data where where query cannot be satisfied.\r\n\r\nWhat exactly is a granule? Is it a row?\r\n\r\nAs a related question: are there plans for an index type similar to btree/hash secondary indexes of traditional RDBMS so a WHERE could efficiently look up rows without needing to be part of a prefix of the primary key or scanning all rows for the given column?\r\nAs I understand it, the current data skipping indexes basically allow only to answer the question \"does this block of rows contain the value that I am looking for?\" instead of \"which rows in this block contain the value that I am looking for\".",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/4414/comments",
    "author": "arctica",
    "comments": [
      {
        "user": "alesapin",
        "created_at": "2019-02-18T16:32:26Z",
        "body": "> What exactly is a granule? Is it a row?\r\n\r\nGranule is a batch of rows of fixed size which addresses with primary key. Term make sense only for MergeTree* engine family. It can be set with setting `index_granularity=N`, default value is 8192 rows per batch. So if you use default value, you will have index per each 8192 row.\r\n\r\n> As I understand it, the current data skipping indexes basically allow only to answer the question \"does this block of rows contain the value that I am looking for?\" instead of \"which rows in this block contain the value that I am looking for\".\r\n\r\nYes, you understood correctly. This way (sparse index) of indexing is very efficient. Index is very small so it can be placed in memory. Sequential processing of group of small granules is also very fast. \r\nYou can set `index_granularity=1` (primary key per each row) and also set `GRANULARITY=1` if you want to get index per each row, but this will require a lot of memory."
      },
      {
        "user": "arctica",
        "created_at": "2019-02-19T10:43:11Z",
        "body": "Thank you for the explanation. Maybe a small piece of text could be added to the documentation like \"(a granule is one block of primary key containing `index_granularity` rows)?\r\n\r\nI see now how this index can be properly used. It only makes sense when the value being filtered for is very sparse or one needs very fine grained primary keys.\r\n\r\nAs I now understand it, the data skipping index is tied to the primary key. E.g. If I have index_granularity=8192 and GRANULARITY=1, then each 8192 rows, the index contains say the minmax for the Nth primary key.\r\n\r\nIs there an advantage to tieing the data skipping index to the primary key or would it make sense to make it its own stand-alone index which could have its own granularity defined by rows? If I had a data skipping index with GRANULARITY=4096rows then one could easily compute which primary key the current data skipping index batch belongs to since the number of rows is always fixed. That way one could have a finer grained data skipping index if filtering just by that column. It would also make for easier understanding of the index.\r\n"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2019-02-19T16:51:33Z",
        "body": "> As I now understand it, the data skipping index is tied to the primary key. E.g. If I have index_granularity=8192 and GRANULARITY=1, then each 8192 rows, the index contains say the minmax for the Nth primary key.\r\n\r\nCorrect.\r\n\r\n> Is there an advantage to tieing the data skipping index to the primary key or would it make sense to make it its own stand-alone index which could have its own granularity defined by rows?\r\n\r\nEvery column has the .mrk file along with .bin (data) file. These files store \"marks\" - offsets in data file, that allow to read or skip data for specific granules. These marks have primary key index granularity.\r\n\r\nIf you have different granularity for secondary keys, you either:\r\n- cannot skip data efficiently (you'll have to read and throw off data instead of seek);\r\n- have to store secondary .mrk files for every column."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T18:25:23Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ],
    "satisfaction_conditions": [
      "Clear definition of 'granule' in relation to primary key structure",
      "Explanation of relationship between data skipping indexes and primary key granularity",
      "Comparison of sparse index efficiency vs traditional secondary indexes",
      "Design rationale for index granularity alignment with data marks"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:15:24"
    }
  },
  {
    "number": 3978,
    "title": "Question:How to kill last executed select query which cost more time than expected through http interface?",
    "created_at": "2019-01-03T02:10:56Z",
    "closed_at": "2019-03-11T07:17:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3978",
    "body": "When querying clickhouse through http interface, if the query time beyond the expectation, we want to kill this query and save the server's resources. Is there any way for this purpose?",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3978/comments",
    "author": "AlexanderJLiu",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2019-01-03T11:04:38Z",
        "body": "Use `max_execution_time` setting or increase timeouts for http connection on your client.\r\n( Related: #1403 )"
      },
      {
        "user": "AlexanderJLiu",
        "created_at": "2019-01-03T12:23:46Z",
        "body": "@filimonov Yes, it works, thanks. \ud83d\udc4d \r\nI use this setting via passing HTTP CGI parameters: `URL?max_execution_time=1`, not in config file. By this way, querying from the clickhouse console client with the same user can avoid timeout limit.\r\n\r\n---\r\n**Another question:** If querying a distributed table with `max_execution_time` setting, the query in remote server will stop as well according to my test.  But the query error says `Code: 159, Message: Timeout exceeded: elapsed 4.386062641 seconds, maximum: 1`, elapsed not 1 but 4, how to explain?"
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to programmatically enforce query execution time limits",
      "Clarification of timeout behavior in distributed query environments",
      "Solution must work through HTTP interface without affecting other clients"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:15:33"
    }
  },
  {
    "number": 3971,
    "title": "ALL JOIN inflating numbers",
    "created_at": "2018-12-30T13:55:37Z",
    "closed_at": "2018-12-31T17:13:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3971",
    "body": "Hi there,\r\nimagine a simple query with a join like the following. When using an ANY join I get the same result as without a join but as soon as I use ALL, the result from the local fields (impressions, value..) is inflated. Usually this inflation is just 1-4% but in some cases it is 10-100 times. What would be the best course of action here?\r\n\r\n```\r\nSELECT\r\n    category,\r\n    count() AS impressions,\r\n    uniq(sessionId) as sessions,\r\n    sum(value) as value,\r\n    sum(visible) AS visible,\r\n    sum(engaged) AS engaged\r\nFROM impressions ALL LEFT JOIN\r\n(\r\n    SELECT\r\n        CounterID,\r\n        engaged,\r\n        visible\r\n    FROM visits\r\n    GROUP BY CounterID\r\n) USING CounterID\r\nGROUP BY category\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3971/comments",
    "author": "Slind14",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-12-30T19:58:38Z",
        "body": "If I understand your question correctly, this is how ALL JOIN is expected to behave.\r\n(And ALL JOIN is the default behaviour of JOIN in other relational DBMSs)\r\nIf there are multiple rows in the right table with corresponding CounterID, it will create multiple rows in the result."
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-30T20:52:52Z",
        "body": ">Usually this inflation is just 1-4% but in some cases it is 10-100 times. \r\n\r\nCH always gives exact numbers. If you experience some inflation, even 0.00001% it means something wrong with design or it's some unknown CH's bug.\r\n\r\n`GROUP BY CounterID ) USING CounterID`\r\nmeans that the right table has only one row with each CounterID, so it should not be any difference with ALL vs ANY.\r\n\r\nCan you show exact table DDLs and exact SQL, because your example is too vague and wrong for the right table _engaged_ and _visible_ are not a SUMS and they are not in GROUP BY section."
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T05:08:58Z",
        "body": "Here is a real query and the result:\r\n\r\nBoth tables are a simple MergeTree.\r\n\r\n```\r\nSELECT\r\n    date,\r\n    count() AS impressions,\r\n    uniq(sessionUUID) as sessions,\r\n    sum(value) as value,\r\n    sum(visible) AS visible,\r\n    sum(engaged) AS engaged\r\nFROM `impressions` ANY LEFT JOIN\r\n(\r\n    SELECT\r\n        impressionUUID,\r\n        visible,\r\n        engaged\r\n    FROM `meta`\r\n    WHERE date = yesterday()\r\n) USING impressionUUID\r\nWHERE date = yesterday()\r\nGROUP BY date;\r\n```\r\n\r\n## NO JOIN\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500impressions\u2500\u252c\u2500sessions\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500value\u2500\u252c\u2500visible\u2500\u252c\u2500engaged\u2500\u2510\r\n\u2502 2018-12-30 \u2502     4353169 \u2502   123935 \u2502 5636888.545389 \u2502         \u2502         \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n## ANY LEFT JOIN\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500impressions\u2500\u252c\u2500sessions\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500value\u2500\u252c\u2500visible\u2500\u252c\u2500engaged\u2500\u2510\r\n\u2502 2018-12-30 \u2502     4353169 \u2502   123935 \u2502 5636888.545389 \u2502 2662372 \u2502    2274 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\n## ALL LEFT JOIN\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500impressions\u2500\u252c\u2500sessions\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500value\u2500\u252c\u2500\u2500visible\u2500\u252c\u2500engaged\u2500\u2510\r\n\u2502 2018-12-30 \u2502    41745815 \u2502   123935 \u2502 17641794.240708 \u2502 39908334 \u2502  148958 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nThe Meta table can contain an impressionUUID twice because visible and engaged both have their own entry/row). Here is the total count vs uniq impressions and the amount of \"duplicated\" impressionUUIDs:\r\n\r\n```\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500date\u2500\u252c\u2500\u2500\u2500count\u2500\u252c\u2500impressions\u2500\u252c\u2500duplicates\u2500\u2510\r\n\u2502 2018-12-30 \u2502 2772833 \u2502     2646950 \u2502     125883 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T13:55:53Z",
        "body": "So probably meta has duplicates for some reason\r\n\r\nSELECT      impressionUUID, count() cnt\r\n    FROM `meta` WHERE date = yesterday() group by impressionUUID having cnt > 1\r\n\r\nWhat table engine is used by meta ? Replacing ? "
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T13:59:42Z",
        "body": "> Both tables are a simple MergeTree.\r\n\r\nYour query returns: `13 rows in set. Elapsed: 0.485 sec.` all with a cnt of 2\r\n"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T14:15:15Z",
        "body": "OK. Is it valid to have duplicates?\r\nThe observed difference is because these duplicates multiply select's result rows with ALL and does not with ANY."
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T14:21:57Z",
        "body": "Yes, these are not really duplicates, visible and engaged are treated separately. So there is one row for engaged and one for visible. \r\nWhy would this inflate them so much. I'm don't think this ever happened with mysql."
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T15:03:51Z",
        "body": ">Yes, these are not really duplicates, visible and engaged are treated separately. \r\n\r\nso are they flags?\r\n\r\nvisible 0  engaged  1\r\nvisible 1  engaged  1\r\n\r\nor numbers?\r\n\r\nWhat CH version do you use?\r\n"
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T15:06:24Z",
        "body": "They are boolean (numbers which can only be 0 and 1).\r\nIt is not possible that one row has visible and engaged set. It is always\r\nimpressionA visible 1 engaged 0\r\nimpressionA visible 0 engaged 1\r\n\r\n`18.16.1 revision 54412`"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T15:22:41Z",
        "body": "OK. So is any difference in result \r\n```\r\n\r\nSELECT\r\n    date,\r\n    count() AS impressions,\r\n    uniq(sessionUUID) as sessions,\r\n    sum(value) as value,\r\n    sum(visible) AS visible,\r\n    sum(engaged) AS engaged\r\nFROM `impressions` ANY LEFT JOIN\r\n(\r\n    SELECT\r\n        impressionUUID,\r\n        sum(visible) visible,\r\n        sum(engaged) engaged\r\n    FROM `meta`\r\n    WHERE date = yesterday()\r\n    GROUP BY impressionUUID\r\n) USING impressionUUID\r\nWHERE date = yesterday()\r\nGROUP BY date;\r\n\r\n```\r\n\r\nthe same with ALL LEFT JOIN\n\n---\n\n>18.16.1 revision 54412\r\n\r\nand check any difference if you execute \r\nset compile_expressions = 0;\r\nbefore query"
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T15:56:17Z",
        "body": "You are right, it works with the group by. My mistake, sorry. \r\n\r\nWhat I still don't get is why this usually results in an inflation of 1-4% and only on this one database of 10 times. The type of data is the same and the duplicates are not really different in those other databases.\r\n\r\nBtw. is there any way to tell `ANY` to pick the latest record? (go through it staring with the latest)"
      },
      {
        "user": "den-crane",
        "created_at": "2018-12-31T17:10:54Z",
        "body": "It's in your data. Just check what is going on for one of these (of 13) impressionUUID.\r\nThere is no mystery or magic or inflation.\r\n\r\n>Btw. is there any way to tell ANY to pick the latest record? (go through it staring with the latest)\r\n\r\nNo.\r\nIt will be something like that\r\n```\r\n\r\nselect mx.1 impressionUUID, sum(mx.2) visible, sum(mx.3) engaged from\r\n( select argMax((impressionUUID,visible,engaged),date) mx \r\n  from `meta`\r\n  group by impressionUUID, visible, engaged)\r\ngroup by impressionUUID\r\n```"
      },
      {
        "user": "Slind14",
        "created_at": "2018-12-31T17:13:58Z",
        "body": "I see. Thank you."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why ALL JOIN causes data inflation compared to ANY JOIN in this context",
      "Guidance on handling duplicate keys in joined tables without causing metric inflation",
      "Method to ensure 1:1 relationship between joined tables when required",
      "Best practices for aggregating boolean flags across multiple rows",
      "Explanation of ClickHouse's join behavior differences from MySQL"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:15:41"
    }
  },
  {
    "number": 3784,
    "title": "Question: How to load fast big flat files ?",
    "created_at": "2018-12-07T13:47:32Z",
    "closed_at": "2018-12-08T06:25:40Z",
    "labels": [
      "question",
      "performance"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3784",
    "body": "The best method I found is to use the table function `file(path, format, structure)`. It takes as an input parameter the relative path to the file from the setting `user_files_path`. One can change this setting in `/etc/clickhouse-server/config.xml` \r\n\r\n**Question:** Is it possible to change `user_files_path` in a clickhouse-client session with an `sql` command ?\r\n\r\nI suppose an alternative method instead of copying/placing the flat-file under `user_files_path` is to pipe the flat-file to command line client (`clickhouse-client`) but that requires access to the file system and the command has to be invoked from my python application.\r\n\r\nIs there another method to load fast big flat-files (millions of rows) ?\r\n\r\n**Clarification:** I want to load the data from flat-files to a temporary clickhouse table engine e.g. merge tree, log, memory, so that I can read and process column data fast and use these as an input to my TRIADB clickhouse table engines.",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3784/comments",
    "author": "healiseu",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-12-07T19:57:02Z",
        "body": "> Question: Is it possible to change user_files_path in a clickhouse-client session with an sql command ?\r\n\r\nNo, because the setting is security limit: to allow to read only restricted subset of files from server's filesystem. But you can specify `user_files_path` as `/` (and use path relative to filesystem root) if you really don't care.\r\n\r\n> I suppose an alternative method instead of copying/placing the flat-file under user_files_path is to pipe the flat-file to command line client (clickhouse-client) but that requires access to the file system and the command has to be invoked from my python application.\r\n\r\nIf your files are on client side and you need to transfer it over the network, better to use `clickhouse-client` (than to transfer by other tools). If your files are already on server's filesystem, better to use `file` table function.\r\n\r\nNote: sometimes using clickhouse-client may be faster, because it use \"double buffering\". It will parse next chunk of data while waiting for server to insert previous chunk of data. But this should not be very significant.\r\n\r\nIf you have multiple files, you can load them in parallel.\r\n\r\n> Is there another method to load fast big flat-files (millions of rows) ?\r\n\r\nThere are also some advanced solutions, like to prepare a partition for MergeTree table, then move it to the server and attach. It should not be faster unless you can do data preparation on separate cluster in parallel.\r\n\r\n"
      },
      {
        "user": "healiseu",
        "created_at": "2018-12-08T06:25:32Z",
        "body": "Hi @alexey-milovidov, thank you for your answers, you covered me sufficiently for the moment."
      }
    ],
    "satisfaction_conditions": [
      "Alternative methods for loading large flat files without requiring file placement in user_files_path",
      "Support for client-side file loading over network without server filesystem access",
      "Methods compatible with temporary table engines (MergeTree, Log, Memory)",
      "Solutions enabling parallel data loading",
      "Approaches maintaining security constraints while providing flexibility"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:15:47"
    }
  },
  {
    "number": 3659,
    "title": "Pushing WHERE conditions from the view to underlying table ",
    "created_at": "2018-11-25T01:00:09Z",
    "closed_at": "2020-01-14T20:27:10Z",
    "labels": [
      "question",
      "st-need-info",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/3659",
    "body": "Can you add parameters to the view\uff0c If there are no parameters, then every request must be queried before filtering, resulting in unnecessary waste of computing resources. In addition, JDBC query avoids transferring a large amount of SQL code.\r\nLook forward to your reply\r\nthanks ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/3659/comments",
    "author": "754154377",
    "comments": [
      {
        "user": "KochetovNicolai",
        "created_at": "2018-11-26T09:58:41Z",
        "body": "Do you mean that you what to push down WHERE predicate from main query to VIEW? There is a setting `enable_optimize_predicate_expression`. Try to enable it and check if it works for your case."
      },
      {
        "user": "754154377",
        "created_at": "2018-11-26T13:51:54Z",
        "body": "> Do you mean that you what to push down WHERE predicate from main query to VIEW? There is a setting `enable_optimize_predicate_expression`. Try to enable it and check if it works for your case.\r\n\r\nFor Example: To express my thoughts, I have fabricated the following functions, which do not actually exist.\r\ncreate table shop_sale (event_date Date, shop_id String, goods_id String, sale_amt Float32) ENGINE = MergeTree(event_date , (shop_id), 8192);  \r\n\r\ninsert into shop_sale VALUES('2000-01-01', 'AB01', 'A', 11201), ('2000-01-01', 'AB02', 'B', 11301), ('2000-01-01', 'AB02', 'C'. 12301);\r\n\r\ncreate view view_shop_sale (event_date Date, shop_id String, sale_amt Float32)  as select event_date, shop_id, sale_amt from shop_sale where event_date = ::eventDate:: and shop_id = ::shopId::\r\n;\r\n\r\nselect *\r\nfrom view_shop_sale \r\nwhere eventDate= '2000-01-01' and shopId= 'AB01'"
      },
      {
        "user": "den-crane",
        "created_at": "2018-11-26T20:12:30Z",
        "body": "You don't need parameters, it works out the box\r\n\r\nIf you create view like \r\ncreate view view_shop_sale  as **select event_date, shop_id, sale_amt from shop_sale**\r\n\r\n**enable_optimize_predicate_expression = 0**\r\nselect * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\nwill be executed as \r\nselect * from (**select event_date, shop_id, sale_amt from shop_sale**) where eventDate= '2000-01-01' and shopId= 'AB01'\r\n\r\nenable_optimize_predicate_expression = 1\r\nselect * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\nwill be re-written and executed as \r\nselect * from **shop_sale** where eventDate= '2000-01-01' and shopId= 'AB01'\r\n"
      },
      {
        "user": "754154377",
        "created_at": "2018-11-27T01:36:30Z",
        "body": "> You don't need parameters, it works out the box\r\n> \r\n> If you create view like\r\n> create view view_shop_sale as **select event_date, shop_id, sale_amt from shop_sale**\r\n> \r\n> **enable_optimize_predicate_expression = 0**\r\n> select * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\n> will be executed as\r\n> select * from (**select event_date, shop_id, sale_amt from shop_sale**) where eventDate= '2000-01-01' and shopId= 'AB01'\r\n> \r\n> enable_optimize_predicate_expression = 1\r\n> select * from view_shop_sale where eventDate= '2000-01-01' and shopId= 'AB01'\r\n> will be re-written and executed as\r\n> select * from **shop_sale** where eventDate= '2000-01-01' and shopId= 'AB01'\r\n\r\nthanks \uff01 \r\nIn addition, can replicated tables support views?\r\nFor example:\r\ncreate view view_shop_sale ON CLUSTER xxx_3replicas as select event_date, shop_id, sale_amt from shop_sale ?"
      },
      {
        "user": "KochetovNicolai",
        "created_at": "2018-11-27T10:04:56Z",
        "body": "Yes, you can create view on any replica (or on cluster itself).\r\nIf you need several shards you can also create distributed table over views."
      },
      {
        "user": "blinkov",
        "created_at": "2019-03-26T11:10:02Z",
        "body": "@754154377 do you have any further questions?"
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-10-20T19:25:26Z",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n"
      }
    ],
    "satisfaction_conditions": [
      "Predicate pushdown optimization for views must be supported",
      "Solution must work without view parameters",
      "Compatibility with replicated tables and cluster environments",
      "Reduction of unnecessary computation and data transfer",
      "Clear explanation of optimization behavior"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:15:57"
    }
  },
  {
    "number": 2897,
    "title": "[Question] ALTER DELETE in Materialized Views",
    "created_at": "2018-08-20T15:28:41Z",
    "closed_at": "2018-08-20T16:32:06Z",
    "labels": [
      "question",
      "comp-matview",
      "comp-mutations"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2897",
    "body": "Hello, will alter delete work for materialized views as well? Thanks",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2897/comments",
    "author": "simPod",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T16:26:44Z",
        "body": "You have to apply it to `.inner.` table (that you can find in `system.tables`) and mutations will work if the underlying storage is of MergeTree family."
      },
      {
        "user": "simPod",
        "created_at": "2018-08-20T16:32:06Z",
        "body": "Thank's for the reply! I also see you implememented query forwarding to the underlying table \ud83d\udc4d "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to apply mutations (like ALTER DELETE) to materialized views through their underlying storage mechanism",
      "Clarification about storage engine requirements for mutation operations",
      "Description of how materialized view interactions propagate to underlying tables"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:16:16"
    }
  },
  {
    "number": 2892,
    "title": "Ways to include per user configuration from external files",
    "created_at": "2018-08-19T11:34:32Z",
    "closed_at": "2018-08-20T09:12:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2892",
    "body": "Hello!\r\n\r\nI'm working on some ways to simplify user management for Clickhouse.\r\n\r\nGlobal user configuration file \"/etc/clickhouse-server/users.xml\" does not work for me very well. Because it requires complicated logic when we add/remove users.\r\n\r\nI have two options to maintain it properly:\r\n- Regenerate this user.xml file each time when we add new user external source (JSON/YAML). But it requires external code to generate it and increases complexity.\r\n- Read content of existing file, add new section, write changes. But it also involves pretty tricky XML processing and can break something.\r\n\r\nI'm interested in extracting this information to separate files:\r\n```\r\n<admin>\r\n  <password>new_password</password>\r\n  <networks incl=\"networks\" replace=\"replace\">\r\n    <ip>::/0</ip>\r\n  </networks>\r\n  <profile>default</profile>\r\n  <quota>default</quota>\r\n</admin>\r\n```\r\n\r\nIs there is any way to extract configuration for each use in separate file? \r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2892/comments",
    "author": "pavel-odintsov",
    "comments": [
      {
        "user": "alexey-milovidov",
        "created_at": "2018-08-20T03:39:59Z",
        "body": "Sure. You can add files for each user inside `/etc/clickhouse-server/users.d` directory.\r\n\n\n---\n\n```\r\n$ cat /etc/clickhouse-server/users.d/alice.xml\r\n<yandex>\r\n    <users>\r\n      <alice>\r\n          <profile>analytics</profile>\r\n            <networks>\r\n                  <ip>::/0</ip>\r\n            </networks>\r\n          <password_sha256_hex>...</password_sha256_hex>\r\n          <quota>analytics</quota>\r\n      </alice>\r\n    </users>\r\n</yandex>\r\n```\n\n---\n\nThe list of external files is tracked and updated on the fly."
      },
      {
        "user": "pavel-odintsov",
        "created_at": "2018-08-20T09:12:01Z",
        "body": "Hello!\r\n\r\nWow! That's awesome! It works for me! Thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Supports per-user configuration in separate files",
      "Allows dynamic updates without service restarts",
      "Eliminates need for XML file manipulation",
      "Maintains native ClickHouse compatibility"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:16:22"
    }
  },
  {
    "number": 2746,
    "title": "What is the difference between version v18.x.x and v1.1.xxxx?",
    "created_at": "2018-07-28T03:41:02Z",
    "closed_at": "2018-07-28T04:03:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/2746",
    "body": "I notice there is a naming pattern change on ClickHouse version code.\r\nI wonder whether both are compatible, and I am safe to upgrade from v1.1.xxx to v18.xx? ",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/2746/comments",
    "author": "vnnw",
    "comments": [
      {
        "user": "vnnw",
        "created_at": "2018-07-28T04:03:10Z",
        "body": "Sorry. The CHANGELOG.md already explains my question."
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2018-07-28T15:09:49Z",
        "body": "TLDR: \r\nVersions 1 and 18 are totally compatible."
      }
    ],
    "satisfaction_conditions": [
      "Clarification of compatibility between version numbering schemes",
      "Reference to official compatibility guarantees"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:16:31"
    }
  },
  {
    "number": 1501,
    "title": "New function like runningDifference with correct processing of counter reset",
    "created_at": "2017-11-16T11:32:19Z",
    "closed_at": "2017-11-16T13:51:13Z",
    "labels": [
      "question",
      "question-answered"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/1501",
    "body": "Is there any chance to have a function linke ```runningDifference()``` which can correctly process counter reset.\r\n\r\nLets say I have a counter with possible values 0 to 99. Now, when I run runningDifference() on values ```94, 96, 3, 8, 14``` I will have result ```2, -93, 5, 6```, while in fact it should be ```2, 7, 5, 6```.\r\nSo new function should have additional parameter - ```max_value``` and with it equal to 100 difference should be calculated like this:\r\n```V2 - V1 < 0 ? max_value - V1 + V2 : V2 - V1```\r\n",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/1501/comments",
    "author": "michep",
    "comments": [
      {
        "user": "filimonov",
        "created_at": "2017-11-16T12:15:07Z",
        "body": "For those type of problems modulo calculation is usually used (formula `(x + max_value) % max_value`)\r\n\r\nYour sample:\r\n```\r\n:) select x, runningDifference(x) from\r\n  (select x from system.one array join [94, 96, 3, 8, 14] as x);\r\n\u250c\u2500\u2500x\u2500\u252c\u2500runningDifference(x)\u2500\u2510\r\n\u2502 94 \u2502                    0 \u2502\r\n\u2502 96 \u2502                    2 \u2502\r\n\u2502  3 \u2502                  -93 \u2502\r\n\u2502  8 \u2502                    5 \u2502\r\n\u2502 14 \u2502                    6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\nWith modulo\r\n```\r\n:) select x, (runningDifference(x) + 100) % 100 from\r\n  (select x from system.one array join [94, 96, 3, 8, 14] as x);\r\n\u250c\u2500\u2500x\u2500\u252c\u2500modulo(plus(runningDifference(x), 100), 100)\u2500\u2510\r\n\u2502 94 \u2502                                            0 \u2502\r\n\u2502 96 \u2502                                            2 \u2502\r\n\u2502  3 \u2502                                            7 \u2502\r\n\u2502  8 \u2502                                            5 \u2502\r\n\u2502 14 \u2502                                            6 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\n\n---\n\nAnd that requirement looks very specific, and it's very easy to solve that problem without modifications of runningDifference (as i showed), so i don't think that adding that possibility to runningDifference is really needed."
      }
    ],
    "satisfaction_conditions": [
      "Handles counter resets when calculating differences between consecutive values",
      "Produces correct incremental differences across counter boundaries",
      "Works as a built-in function rather than requiring manual post-processing"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:16:36"
    }
  },
  {
    "number": 12405,
    "title": "ORDER BY WITH FILL + DateTime64",
    "created_at": "2020-07-10T14:48:58Z",
    "closed_at": "2021-05-12T11:45:30Z",
    "labels": [
      "help wanted",
      "unfinished code",
      "comp-datetime"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/12405",
    "body": "**Describe the bug**\r\nException when try to use `toDateTime64` and `ORDER BY ... WITH FILL`\r\n**How to reproduce**\r\n```sql\r\n SELECT n, source FROM (    \r\n   SELECT toDateTime64(number*1000, 3) AS n, 'original' AS source    \r\n   FROM numbers(10) WHERE number % 3 = 1 \r\n) \r\nORDER BY n WITH FILL STEP 1000;\r\n```\r\n\r\n* Which ClickHouse server version to use\r\n20.5.2.7\r\n\r\n**Expected behavior**\r\nfill with step 1000 nanoseconds?\r\n\r\n**Error message and/or stacktrace**\r\n```\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: Add different decimal fields.\r\n```",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/12405/comments",
    "author": "Slach",
    "comments": [
      {
        "user": "Slach",
        "created_at": "2020-07-10T14:49:57Z",
        "body": "@CurtizJ could you look to this issue?"
      },
      {
        "user": "alexey-milovidov",
        "created_at": "2021-02-22T14:17:47Z",
        "body": "CC @Enmk \n\n---\n\nIt also does not work when I try to convert types:\r\n\r\n```\r\nmilovidov-desktop :) SELECT n, source FROM (\u3000   SELECT toDateTime64(number*1000, 3) AS n, 'original' AS source\u3000   FROM numbers(10) WHERE number % 3 = 1\u3000)\u3000ORDER BY n WITH FILL STEP toDateTime64(1000, 3);\r\n\r\nSELECT\r\n    n,\r\n    source\r\nFROM \r\n(\r\n    SELECT\r\n        toDateTime64(number * 1000, 3) AS n,\r\n        'original' AS source\r\n    FROM numbers(10)\r\n    WHERE (number % 3) = 1\r\n)\r\nORDER BY n ASC WITH FILL STEP toDateTime64(1000, 3)\r\n\r\nQuery id: fcab6c79-58af-4d53-8065-818a5b37a0b1\r\n\r\n\r\n0 rows in set. Elapsed: 0.012 sec. \r\n\r\nReceived exception from server (version 21.3.1):\r\nCode: 475. DB::Exception: Received from localhost:9000. DB::Exception: Incompatible types of WITH FILL expression values with column type DateTime64(3).\r\n```"
      },
      {
        "user": "CurtizJ",
        "created_at": "2021-05-12T11:45:30Z",
        "body": "Implemented in #24016."
      },
      {
        "user": "Slach",
        "created_at": "2021-05-12T12:44:46Z",
        "body": "@MaxWk thanks  a lot for your PR"
      }
    ],
    "satisfaction_conditions": [
      "Compatibility between DateTime64 data type and WITH FILL STEP value type",
      "Correct handling of temporal intervals with DateTime64 in WITH FILL clause",
      "Support for explicit type conversion in WITH FILL STEP expressions",
      "Consistent behavior between numeric and temporal fill steps"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:17:29"
    }
  },
  {
    "number": 503,
    "title": "How to copy big data  to ClickHouse",
    "created_at": "2017-02-17T05:56:57Z",
    "closed_at": "2017-02-17T18:22:10Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/ClickHouse/ClickHouse/issues/503",
    "body": "I used tpch make 20G data in my OS.\r\nI know used command\r\n ```\r\n  time clickhouse-client --query=\"INSERT INTO NATION FORMAT CSV\" < some.csv\r\n```\r\ntpch make   data format is |  ,I read document can't find customer define  format .\r\n\r\nhow to copy this data to ClickHouse?\r\n\r\nThanks....",
    "comments_url": "https://api.github.com/repos/ClickHouse/ClickHouse/issues/503/comments",
    "author": "sangli00",
    "comments": [
      {
        "user": "ludv1x",
        "created_at": "2017-02-17T11:12:51Z",
        "body": "Could you provide example of data (or its format description) which you try to load?"
      },
      {
        "user": "sangli00",
        "created_at": "2017-02-17T12:47:44Z",
        "body": "1  copy this data to postgres database \r\n2 copy postgres database from to directory\r\n3 use ClickHouse-client copy to ClickHouse \r\nIs very trouble\r\n\r\n![Uploading 5784F93A-C638-4F3A-A79B-D653845DAFD6.png\u2026]()\r\n"
      },
      {
        "user": "ludv1x",
        "created_at": "2017-02-17T14:27:06Z",
        "body": "I downloaded `TPCH_Tools_v2.17.1.zip` and loaded data from `customer.tbl.150000` into ClickHouse.\r\n\r\nYou just need to remove last `|` in each line and replace `|` to `\\t`.\r\nAfter you can import data into ClickHouse using TabSeparated FORMAT.\r\n\r\n```\r\nsed 's/|$//g' customer.tbl.150000 | tr \"|\" \"\\t\" | clickhouse-client -q \"INSERT INTO tpc FORMAT TSV\"\r\n```\r\n"
      },
      {
        "user": "sangli00",
        "created_at": "2017-02-17T14:29:16Z",
        "body": "yes, I remove last ```|```\r\nbut I can't replace ```|``` to ```\\t```\r\n\r\nused TabSeparated FORMAT is OK.\r\nThanks.\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Solution must handle pipe-delimited data format with trailing separator",
      "Method must convert data to a ClickHouse-supported format without intermediate databases",
      "Process must work with large (20GB+) datasets efficiently",
      "Solution must use standard ClickHouse client capabilities"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:17:35"
    }
  }
]