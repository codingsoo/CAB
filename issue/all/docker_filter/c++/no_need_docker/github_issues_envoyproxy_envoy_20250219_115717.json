[
  {
    "number": 32004,
    "title": "FQDN Filter",
    "created_at": "2024-01-24T12:01:04Z",
    "closed_at": "2024-04-15T12:02:35Z",
    "labels": [
      "question",
      "stale",
      "area/tcp_proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32004",
    "body": "*FQDN Filter*:\r\n\r\n*Description*:\r\nHello everyone,\r\nThe following question: Is it possible to implement a listener with an FQDN? I want Envoy to only forward traffic when Test.test.io is called. Currently, Envoy Proxy forwards all traffic that comes to port 443.\r\n\r\n```\r\n    -   connect_timeout: 5s\r\n        load_assignment:\r\n            cluster_name: ingress_https\r\n            endpoints:\r\n            -   lb_endpoints:\r\n                -   endpoint:\r\n                        address:\r\n                            socket_address:\r\n                                address: bla.bla.bla.io\r\n                                port_value: 443\r\n                                \r\n         name: ingress_https\r\n        per_connection_buffer_limit_bytes: 32768\r\n        type: strict_dns\r\n```\r\n\r\n```\r\n    -   address:\r\n            socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 443\r\n        filter_chains:\r\n        -   filters:\r\n            -   name: envoy.filters.network.tcp_proxy\r\n                typed_config:\r\n                    '@type': type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                    access_log:\r\n                    -   name: envoy.access_loggers.file\r\n                        typed_config:\r\n                            '@type': type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                            path: /var/log/envoy/ingress_https_access.log\r\n                    cluster: ingress_https\r\n                    stat_prefix: ingress_https\r\n        name: listener_ingress_https\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32004/comments",
    "author": "eliassteiner",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2024-01-25T18:11:19Z",
        "body": "I'm not sure I understand your question.\r\n\r\nYour config has the listener accept connections on port 443 for any IPv4 assigned to the host (that's what 0.0.0.0 implies). A specific IP can be configured (e.g. 10.0.0.1), which would cause Envoy to only listen on that IP address (not, for example, 127.0.0.1). But most hosts only have 1 IP and localhost, and your DNS name presumably points at the IP address.\r\n"
      },
      {
        "user": "jewertow",
        "created_at": "2024-01-25T21:02:59Z",
        "body": "@eliassteiner you need tls_inspector in you listener:\r\n```\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    listener_filters:\r\n    - name: \"envoy.filters.listener.tls_inspector\"\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\r\n    filter_chains:\r\n    - filter_chain_match:\r\n        server_names: [\"test.test.io\"]\r\n      filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n          ...\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-25T00:03:45Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "eliassteiner",
        "created_at": "2024-02-25T06:36:18Z",
        "body": "perfect thank you. but do this need a ssl certificate? i will check this option thank you"
      },
      {
        "user": "jewertow",
        "created_at": "2024-03-09T09:59:08Z",
        "body": "> but do this need a ssl certificate?\r\n\r\nNo"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-15T12:02:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to filter traffic based on FQDN (Test.test.io) at the listener level",
      "Configuration that works with TLS traffic without requiring SSL certificates",
      "Use of TLS inspection to extract SNI (Server Name Indication) information",
      "Filter chain matching based on server name"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:24:24"
    }
  },
  {
    "number": 29814,
    "title": "`RESPONSE_CODE` is always zero when added as a response header",
    "created_at": "2023-09-26T17:33:40Z",
    "closed_at": "2023-09-28T14:49:07Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29814",
    "body": "*Title*: `RESPONSE_CODE` is always zero when added as a response header\r\n\r\n*Description*:\r\nI am trying to add `RESPONSE_CODE` to the header of calls going through envoy, but have not been successful. This may sound like an odd request because the call already returns status, but we have a few microservices that are responsible for communicating with third-parties and proxying the response, and we want to be 100% sure the issue is not inside the microservice.\r\n\r\n*Repro steps*:\r\nI have crafted what I think is the simplest possible example where envoy is doing a direct response, and the response code header is still 0. I have tons of other examples but this is the smallest.\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - name: reverse_proxy\r\n      address:\r\n        socket_address:\r\n          protocol: TCP\r\n          address: 0.0.0.0\r\n          port_value: 10005\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  response_headers_to_add:\r\n                    - header:\r\n                        key: \"response-code\"\r\n                        value: \"%RESPONSE_CODE%\"\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          direct_response:\r\n                            status: 200\r\n                            body:\r\n                              inline_string: \"{true}\"\r\n                http_filters:\r\n                  - name: envoy.filters.http.router\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n\r\nAt this point I am thinking its just an edge-case problem, or I am missing something small but critical.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29814/comments",
    "author": "inssein",
    "comments": [
      {
        "user": "inssein",
        "created_at": "2023-09-26T21:52:32Z",
        "body": "I ended up getting it working via lua, but would be nice to keep it simpler :)\r\n\r\n```\r\nhttp_filters:\r\n  - name: lua_response_code\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n      default_source_code:\r\n        inline_string: |\r\n          function envoy_on_response(response_handle)\r\n            response_handle:headers():add(\"response-code\", response_handle:headers():get(\":status\"))\r\n          end\r\n```"
      },
      {
        "user": "StarryVae",
        "created_at": "2023-09-27T08:21:32Z",
        "body": "i think `RESPONSE_CODE` in `response_headers_to_add` is only supported after this patch #29028 , maybe you can update your Envoy version and try again."
      },
      {
        "user": "alyssawilk",
        "created_at": "2023-09-27T12:26:39Z",
        "body": "If this works with modern versions of Envoy and you think it merits backports, let us know!"
      },
      {
        "user": "inssein",
        "created_at": "2023-09-27T16:09:31Z",
        "body": "I just tried the latest dev build and it works! Not a huge rush for us and looks like this will make it out on the next release (2023/10/16)."
      }
    ],
    "satisfaction_conditions": [
      "Identifies why RESPONSE_CODE placeholder returns zero in response headers",
      "Provides a solution compatible with Envoy's header manipulation capabilities",
      "Confirms version compatibility for RESPONSE_CODE header functionality",
      "Addresses direct_response route configuration limitations",
      "Maintains response code integrity for third-party proxy verification"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:24:32"
    }
  },
  {
    "number": 29315,
    "title": "Is it necessary for go extension route config to store the configuration of different plugins",
    "created_at": "2023-08-29T09:59:35Z",
    "closed_at": "2023-10-07T12:01:41Z",
    "labels": [
      "question",
      "stale",
      "area/golang"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29315",
    "body": "*Title*: *Is it necessary for go extension route config to store the configuration of different plugins?*\r\n\r\n*Description*:\r\n```\r\nmessage ConfigsPerRoute {\r\n  // Configuration of the Go plugin at the per-router or per-virtualhost level,\r\n  // keyed on the :ref:`plugin_name <envoy_v3_api_field_extensions.filters.http.golang.v3alpha.Config.plugin_name>`\r\n  // of the Go plugin.\r\n  //\r\n  map<string, RouterPlugin> plugins_config = 1;\r\n}\r\n```\r\nSince now getting per filter config by the http filter config name is supported, is it necessary for go extension route config to  store the configuration of different go filters? For example, if we have two go filters `go-basic-auth` and `go-rate-limit`, for `http_filters` and `typed_per_filter_config`, the config name can be the same, so the go filter can get its own route config.\r\n\r\n```\r\n          http_filters:\r\n          - name: go-basic-auth\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              ......\r\n          - name: go-rate-limit\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              ......\r\n\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n              - name: host-one\r\n                domains:\r\n                  - \"*\"\r\n                routes:\r\n                  - match:\r\n                      prefix: \"/\"\r\n                    route:\r\n                      cluster: httpbin\r\n                    typed_per_filter_config:\r\n                      go-basic-auth:\r\n                        \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.ConfigsPerRoute\r\n                        config:\r\n                        ......\r\n                      go-rate-limit:\r\n                        \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.ConfigsPerRoute\r\n                        config:\r\n                        ......\r\n```\r\n\r\nIn this way, the go filter will just be like a c++ filter, every go filter has its own route config, which maybe simple for control plane.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29315/comments",
    "author": "StarryVae",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2023-08-29T10:00:00Z",
        "body": "cc @doujiang24 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T00:50:34Z",
        "body": "@StarryVae Sorry, I may not understand you correctly. For now, every go filter has its own per route config is already supported. Now, as you said `ConfigsPerRoute` store per route configs for each golang filters, indexed by their plugin name, and each golang filter could get this own per route config by the plugin name.\r\n\r\nCould you please describe the changes that you sugguested? I haven't see what do we need to change. maybe a little design doc? Thanks."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T02:09:43Z",
        "body": "Ah, what i mean is that golang filter route config may not need store per route configs for each golang filters, it could only be its own route config just like c++ filter:\r\n\r\n```\r\nmessage ConfigPerRoute {\r\n  oneof override {\r\n    option (validate.required) = true;\r\n\r\n    // [#not-implemented-hide:]\r\n    // Disable the filter for this particular vhost or route.\r\n    // If disabled is specified in multiple per-filter-configs, the most specific one will be used.\r\n    bool disabled = 1 [(validate.rules).bool = {const: true}];\r\n\r\n    // The config field is used for setting per-route and per-virtualhost plugin config.\r\n    google.protobuf.Any config = 2;\r\n  }\r\n}\r\n```"
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T03:59:07Z",
        "body": "Oh, I see, your suggestion is not about lack of capability, but about implementation of capability.\r\n\r\nWe have also considered the way that you suggested, which is also a attractive solution, it's simper.\r\nHowever, we finally chose the current implementation way, since we do think it is more controllable for the golang plugin ecosystem, for example, the plugin name does not need to care about the conflicting about the existing name."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T06:26:41Z",
        "body": "`for example, the plugin name does not need to care about the conflicting about the existing name.`\r\n\r\ndoes the plugin name refer to http filter name `go-basic-auth` above? if so, there may be conflicts on route config, but it can be avoided by control plane. Anyway, the design now is also fine, just a littile bit complex. \ud83d\ude04 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T11:00:02Z",
        "body": "nope, each golang filter/plugin has its own plugin name, which is defined in `Golang.Config` proto:\r\n```\r\n  string plugin_name = 3 [(validate.rules).string = {min_len: 1}];\r\n```\r\nalso golang source code register golang plugin with that name, see: `RegisterHttpFilterConfigFactoryAndParser`.\r\nit's another name compare to the http filter name, so that we do not need to care about the confliction.\r\n\r\nyep, they both should works, we choose to introduce the new plugin name, since we think it might be more controllable, as said above."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T11:27:15Z",
        "body": "emm,,, if the plugin name refers to `plugin_name` in `Golang.Config`, then i am confused about what confliction it can solve? \ud83d\ude15 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T13:59:17Z",
        "body": "after discussed in wechat, summary a bit here:\r\nin the current implementation, introduce a new `plugin_name`, so that the per route config won't rely on the http filter name.\r\neven two golang plugins/filters in the http filter chain, has the same http filter name, the two plugins still have their own per route plugin config, since the keys in the `ConfigPerRoute. plugins_config ` are different.\r\n\r\non the other hand, rely on the http filter name, is also a workable solution, and it is simpler."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-31T07:13:34Z",
        "body": "yes, but how can we ensure the keys in the `ConfigPerRoute. plugins_config`  are different? \ud83d\ude15 \n\n---\n\nas we discussed in wechat, the `plugin name` should be better suited for resolving potential routing configuration conflicts, thanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-07T12:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why a separate plugin_name-based configuration storage is necessary despite potential complexity",
      "Clarification on how configuration conflicts are prevented between multiple Go filters",
      "Comparison of tradeoffs between http filter name-based vs plugin_name-based configuration approaches",
      "Demonstration that each Go filter can maintain isolated route configurations without interference"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:24:39"
    }
  },
  {
    "number": 29260,
    "title": "should go extension support header operation in body phase?",
    "created_at": "2023-08-25T09:40:15Z",
    "closed_at": "2023-08-28T06:52:15Z",
    "labels": [
      "enhancement",
      "question",
      "area/go"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29260",
    "body": "*Title*: *should go extension support header operation in body phase?*\r\n\r\n*Description*:\r\ninspired by #28587 , should go extension also support header operation in body phase? since some filters may need to change headers according to some content from body.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29260/comments",
    "author": "StarryVae",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2023-08-25T09:40:37Z",
        "body": "cc @doujiang24 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-26T01:18:28Z",
        "body": "yep, I do think the golang extension already support it.\r\nGo could return `StopAndBufferWatermark` in `DecodeHeaders`, then Go will receive data in `DecodeData` and could still write headers before invoke `Continue`.\r\nGolang extension is designed to be able do the same thing(almostly) align to C++."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-28T06:51:46Z",
        "body": "oh, i see, we can store the headers in the filter object just like what C++ do \ud83e\udd23 ."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that header operations in the body phase are supported in the Go extension",
      "Demonstration of parity with C++ filter capabilities for header-body interaction",
      "Mechanism to modify headers after accessing body data"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:24:46"
    }
  },
  {
    "number": 28386,
    "title": "Connection draining on SDS update",
    "created_at": "2023-07-13T15:57:15Z",
    "closed_at": "2023-07-17T10:09:08Z",
    "labels": [
      "question",
      "api"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28386",
    "body": "In case I'm using LDS, where some of the filter chains in the listener config have SDS config (downstream transport TLS that uses SDS to fetch secrets). After a while, the certificate is refreshed from the SDS. Does it cause connections that currently use the filter chain to drain? Will only new connections get the new certificate?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28386/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-07-14T17:04:59Z",
        "body": "cc @adisuissa "
      },
      {
        "user": "soulxu",
        "created_at": "2023-07-17T09:51:52Z",
        "body": "there is no draining, only the new connection will use the new certificate"
      },
      {
        "user": "ohadvano",
        "created_at": "2023-07-17T10:09:08Z",
        "body": "Thanks"
      }
    ],
    "satisfaction_conditions": [
      "Clarify whether existing connections are terminated or allowed to complete when SDS-updated certificates are applied",
      "Confirm whether certificate updates are applied only to new connections",
      "Address the relationship between SDS updates and connection lifecycle management"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:24:53"
    }
  },
  {
    "number": 28293,
    "title": "Is dispatcher thread-local?",
    "created_at": "2023-07-08T03:26:32Z",
    "closed_at": "2023-08-18T04:01:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28293",
    "body": "That is, could I call `dispatcher.post()` from another thread? I need to put a task into the worker thread from another thread.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28293/comments",
    "author": "kingluo",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-07-08T20:51:39Z",
        "body": "Yes, that's a normal way how thread local storage works, it posts from main thread to worker threads with an updated value."
      },
      {
        "user": "kingluo",
        "created_at": "2023-07-09T04:14:14Z",
        "body": "> Yes, that's a normal way how thread local storage works, it posts from main thread to worker threads with an updated value.\r\n\r\nThanks for your reply. And I just confirmed this fact in my coding. BTW, you mean the main thread posts xds update to the worker thread, right?\r\n\r\nAnother side question is whether the post is done by a locked queue and eventfd in Linux, and the worker thread will handle the post callbacks in batch, right?"
      },
      {
        "user": "kyessenov",
        "created_at": "2023-07-11T21:31:24Z",
        "body": "I think it's edge triggered. It's using libevent for event management, but I don't recall the details."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that dispatcher.post() can be safely called across threads",
      "Explanation of the synchronization mechanism between threads",
      "Description of event management architecture"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:24:58"
    }
  },
  {
    "number": 25938,
    "title": "Safe Regex not working for External Authorization Filter..",
    "created_at": "2023-03-06T17:39:16Z",
    "closed_at": "2023-03-07T05:35:45Z",
    "labels": [
      "question",
      "area/matching"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25938",
    "body": "I do not want to apply external authorization filter for routes starting with /css, /img, /assets. While it is working fine if I put 3 entries using prefix but its not working with safe_regex.\r\n```\r\nstatic_resources:\r\n\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 10000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n          - name: envoy.access_loggers.stdout\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              typed_per_filter_config:\r\n                envoy.filters.http.ext_authz:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute\r\n                  check_settings:\r\n                    context_extensions:\r\n                      virtual_host: local_service\r\n              routes:\r\n              - match:\r\n                  safe_regex:\r\n                    google_re2: {}\r\n                    regex: \"^/(css|img|assets)/\"\r\n                route:\r\n                  host_rewrite_literal: www.envoyproxy.io\r\n                  cluster: service_envoyproxy_io\r\n                typed_per_filter_config:\r\n                  envoy.filters.http.ext_authz:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute\r\n                    disabled: true\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  host_rewrite_literal: www.envoyproxy.io\r\n                  cluster: service_envoyproxy_io    \r\n          http_filters:\r\n          - name: envoy.filters.http.ext_authz\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\r\n              grpc_service:\r\n                envoy_grpc:\r\n                  cluster_name: ext_authz-grpc-service\r\n                timeout: 0.250s\r\n              transport_api_version: V3\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n\r\n  clusters:\r\n  - name: service_envoyproxy_io\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: service_envoyproxy_io\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.envoyproxy.io\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.envoyproxy.io\r\n\r\n  - name: ext_authz-grpc-service\r\n    type: STRICT_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        explicit_http_config:\r\n          http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: ext_authz-grpc-service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 7058\r\n```    ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25938/comments",
    "author": "rakesh-eltropy",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-03-07T03:09:57Z",
        "body": "cc @rakesh-eltropy full match is used by the safe_regex matching here. So, may be `\"^/(css|img|assets)/.*\"` should be used here?"
      },
      {
        "user": "rakesh-eltropy",
        "created_at": "2023-03-07T05:35:45Z",
        "body": "Thanks @wbpcode. I was not aware that full match is used by safe_regex."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how Envoy's safe_regex matching requires full path matching",
      "Regex pattern that matches entire paths starting with /css, /img, or /assets",
      "Clarification on path matching semantics for route exclusions in Envoy filters"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:25:23"
    }
  },
  {
    "number": 23016,
    "title": "ECDS config source from path - discovery response format for resource ",
    "created_at": "2022-09-07T12:30:04Z",
    "closed_at": "2022-10-15T16:01:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23016",
    "body": "Hi folks,\r\nI'm also trying to implement ECDS but the config should come from a file.\r\nI'm struggling to make it work... please help me with how to define the contents of the yaml file.\r\n\r\nthis config:\r\n```\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\nfails with:\r\n`Filesystem config update rejected: Unable to unpack as envoy.config.core.v3.TypedExtensionConfig: [type.googleapis.com/envoy.extensions.filters.http.router.v3.Router] `\r\n\r\n```\r\n          http_filters:\r\n          - name: router\r\n            config_discovery:\r\n              type_urls: [\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"]\r\n              config_source:\r\n                path: /usr/local/bin/test-ecds-v1.yml\r\n              default_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23016/comments",
    "author": "pxpnetworks",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-09-07T16:18:05Z",
        "body": "The issue is that you need to wrap `Router` message into `TypedExtensionConfig` message. That means something like this:\r\n```yaml\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.core.v3.TypedExtensionConfig\r\n  name: router\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```"
      },
      {
        "user": "pxpnetworks",
        "created_at": "2022-09-08T08:27:09Z",
        "body": "Thank you @kyessenov , it is accepted now however i tried to add a second http filter (rbac) before router and still get errors loading both rbac and router.\r\nCan you help me with this once again? Thanks!\r\n\r\ncode:\r\n```\r\n- name: envoy.filters.http.rbac\r\n  typed_config:\r\n    \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\"\r\n    shadow_rules:\r\n      action: LOG\r\n      policies:\r\n        \"log\":\r\n          permissions: {any: true}\r\n          principals: {any: true}\r\n```\r\n```\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.core.v3.TypedExtensionConfig\r\n  name: router\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n```\r\nhttp_filters:\r\n- name: router\r\n  config_discovery:\r\n    type_urls:\r\n    - \"type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\"\r\n    - \"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"\r\n    config_source:\r\n      path: /usr/local/bin/test-ecds-v1.yml\r\n    default_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n\r\nBR,\r\nStoyan\n\n---\n\nFigured out I need a separate config_source file for each HTTP filter in the chain, hope that is how it is supposed to work.\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Clear explanation of how to properly structure ECDS configuration resources in YAML format",
      "Guidance on handling multiple HTTP filters in discovery configuration",
      "Explanation of resource separation requirements for different filter types"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:25:46"
    }
  },
  {
    "number": 21121,
    "title": "How envoy queues requests?",
    "created_at": "2022-05-03T06:06:46Z",
    "closed_at": "2024-06-26T13:38:23Z",
    "labels": [
      "question",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21121",
    "body": "In my case, there are multiple grpc endpoints, each of which can only process one request at a time. It may take several seconds to several minutes to process one request.\r\n\r\nWhat I need:\r\n\r\nEnvoy takes a bunch of requests, assigns one to each endpoint, and enqueues the rest. Once an endpoint finishes, envoy assigns the next in queue to this endpoint.\r\n\r\nI have looked into \"Circuit Breakers\", but it just fails the requests beyond max_requests. \r\n\r\n```\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 5\r\n          max_pending_requests: 20\r\n          max_requests: 5\r\n```\r\nUsing the config above, I send 10 requests, only first 5 are successful.\r\n\r\nI have also checked \"connection pool\" and tested max_concurrent_streams. It seems not relevant.\r\n\r\nI am new to envoy. Thanks if anyone could give a hint.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21121/comments",
    "author": "exhau",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-05-03T14:26:57Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-05-04T16:16:41Z",
        "body": "so the circuit breakers will hard fail if you go over the request limit.\r\nI _believe_ if you go over the connection limit it'll queue.\r\nso I think if you configure your connect limit at 1, and max concurrent streams at 1 you'd get the behavior you wanted.  Please give it a shot and let me know if it doesn't work."
      },
      {
        "user": "exhau",
        "created_at": "2022-05-04T17:10:48Z",
        "body": "thanks very much @alyssawilk. Your config works.\r\n\r\nmax_connections: 1\r\nmax_concurrent_streams: 1\r\n\r\nBut it seems, envoy assigns requests when they come, not assign when one endpoint becomes available. "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-05-04T20:46:47Z",
        "body": "Yeah, the queuing is done in the connection pool, not at the cluster level, which is sub-optimal for your use case.  I think your use case was one Envoy wasn't really designed for, but I think we'd welcome changes if you're game for queuing at a higher level."
      },
      {
        "user": "exhau",
        "created_at": "2024-06-23T12:02:36Z",
        "body": "got it. thanks!\n\n---\n\n``` \r\n    http2_protocol_options: \r\n      max_concurrent_streams: 1\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 1\r\n```\r\n \r\nIf \"one\" client sends a lot of requests simultaneously, this config works as expected. Each backend processes one request at a time. \r\n \r\nBut when a second client starts sending requests, the backend may process two requests at the same time.\r\n \r\nI wonder if it is by designed. Is there a way to achieve processing request one by one for each endpoint, when multiple clients are sending multiple request?\r\n\r\nThanks~"
      },
      {
        "user": "alyssawilk",
        "created_at": "2024-06-24T13:07:06Z",
        "body": "Do you perhaps have multiple worker threads?  These limits apply to each worker thread so my suspicion is Envoy is working as intended but you need to limit worker threads if you want to rate limit so much"
      },
      {
        "user": "exhau",
        "created_at": "2024-06-26T13:38:23Z",
        "body": "--concurrency 1\r\nsolved it. thank you again!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable request queuing without hard failures when endpoints are busy",
      "Configuration must enforce strict 1-request-at-a-time processing per endpoint",
      "Mechanism must work across multiple clients/connections",
      "Solution must account for Envoy's threading model in resource allocation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:26:20"
    }
  },
  {
    "number": 18891,
    "title": "Can xds control plane be dynamically discovered after bootstrap?",
    "created_at": "2021-11-04T02:42:39Z",
    "closed_at": "2021-11-19T06:45:50Z",
    "labels": [
      "question",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18891",
    "body": "*Title*: *Can xds control plane be dynamically discovered after initial bootstrap?*\r\n\r\n*Description*:\r\nAll the Envoy config today define xds control plane cluster as static resource. Usually it is accessed by resolving a DNS name. Is it possible after the initial bootstrapping process, the xds control plane just push down its own endpoints to proxies? That is to dynamically override xds control plane's static cluster configuration. \r\n\r\nThanks.  ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18891/comments",
    "author": "manlinl",
    "comments": [
      {
        "user": "rojkov",
        "created_at": "2021-11-15T13:59:29Z",
        "body": "/cc @markdroth @htuch "
      },
      {
        "user": "htuch",
        "created_at": "2021-11-16T05:50:54Z",
        "body": "I have deja vu on this topic of dynamic clusters overriding static ones. I think we don't allow it due to complexity and confusion potential, @mattklein123 or @snowp  might remember better (it's come up a few times)."
      },
      {
        "user": "mattklein123",
        "created_at": "2021-11-16T15:28:02Z",
        "body": "> I have deja vu on this topic of dynamic clusters overriding static ones. I think we don't allow it due to complexity and confusion potential, @mattklein123 or @snowp might remember better (it's come up a few times).\r\n\r\nYeah agreed this has been discussed before, and I also agree that the complexity seems dubiously worth it. Is there some reason DNS is not a workable solution for you?"
      },
      {
        "user": "manlinl",
        "created_at": "2021-11-19T06:45:50Z",
        "body": "Thanks for the insight. DNS works for us. I was curious if current setup allows making XDS control plane part of the mesh itself.  "
      }
    ],
    "satisfaction_conditions": [
      "Clarification of whether Envoy allows dynamic updates to xDS control plane cluster configuration after bootstrap",
      "Explanation of architectural constraints preventing dynamic control plane discovery",
      "Identification of supported alternatives to achieve control plane availability goals",
      "Addressing the underlying need for control plane to be part of the mesh"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:26:51"
    }
  },
  {
    "number": 18172,
    "title": "Question: Can envoy mirror tcp traffic when acting as a tcp proxy?",
    "created_at": "2021-09-18T12:01:51Z",
    "closed_at": "2021-09-22T04:49:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18172",
    "body": "I'm running envoy in tcp proxy mode. I know envoy can mirror traffic when acting in http mode, but not sure if that is possible in tcp proxy mode. I don't see any documentation which suggests  mirroring is supported in tcp mode, but I still wanted to ask.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18172/comments",
    "author": "samitpal",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-09-20T14:04:24Z",
        "body": "As far as I know that's not possible today."
      },
      {
        "user": "samitpal",
        "created_at": "2021-09-22T04:49:20Z",
        "body": "Ok, thanks! Closing the issue."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether Envoy supports traffic mirroring in TCP proxy mode"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:26:57"
    }
  },
  {
    "number": 17540,
    "title": "Context management in request-response cycle",
    "created_at": "2021-07-29T18:42:11Z",
    "closed_at": "2021-08-09T17:21:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17540",
    "body": "*Title*: *Context management in request-response cycle*\r\n\r\n*Description*:\r\n\r\n> We are currently testing out Envoy's new ExternalProcessor filter type.  We are using this filter to mutate request headers and request bodies before they reach our downstream applications.  We are removing some of this data from the request and then hoping to re-access it on response.\r\n> \r\n> In development, our filter containers (we have tested with 3 containers running a Go application as our cluster) seems to be maintaining a single context object across the entire request-response lifecycle, which is not what we expected to have happen.  Though this makes our lives easier in some ways (we don't need an external cache to hold onto this data if it can be held in memory of a container), we are concerned that this is not expected behavior and could change in the future.  Since Go's Context library is not doing anything fancy behind the scenes, we have a sneaking suspicion that this peculiar behavior is Envoy-related.\r\n> \r\n> Is maintaining this context expected behavior for Envoy?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17540/comments",
    "author": "mdettelson",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-08-06T22:01:36Z",
        "body": "cc @gbrail "
      },
      {
        "user": "gbrail",
        "created_at": "2021-08-07T00:49:44Z",
        "body": "ext_proc starts a bidirectional gRPC stream for each HTTP request/response. (It's basically a long-running gRPC with data going back in forth in two ways.) \r\n\r\nIn go, you'd receive a single call to \"Process,\" and in there you read and write to the stream. Envoy doesn't know anything about Go contexts, but I assume that the Go gRPC code creates a single context and you'll use it as you interact with the stream.\r\n\r\nThis is on purpose, and it is indeed supposed to make things easier for implementers of external processors, since you can maintain state with the gRPC stream. (In Go you handle the whole stream from a single function, running in a single goroutine, so it's particularly easy.) In most cases you shouldn't need a separate state table or anything like that."
      },
      {
        "user": "mdettelson",
        "created_at": "2021-08-09T17:21:27Z",
        "body": "Awesome!  Thank you for clarifying that this behavior is expected :)"
      },
      {
        "user": "liu-cong",
        "created_at": "2024-09-26T19:17:20Z",
        "body": "Had the same question and found this super useful. Thanks!\r\n\r\nBTW is this documented anywhere? It will help future developers :)"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether Envoy's ExternalProcessor filter is designed to maintain a single context across the entire request-response lifecycle",
      "Explanation of the relationship between gRPC stream handling and context persistence in Envoy",
      "Assurance about the stability of this behavior in future Envoy versions",
      "Clarification of state management requirements for external processors"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:04"
    }
  },
  {
    "number": 17476,
    "title": "Is there a command to view the configuration of eds?",
    "created_at": "2021-07-24T02:30:22Z",
    "closed_at": "2021-07-29T07:06:53Z",
    "labels": [
      "question",
      "area/xds",
      "area/admin"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17476",
    "body": "The configuration of endpoints cannot be found using `127.0.0.1:15000/config_dump`. Is there a command to see the configuration issued by eds?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17476/comments",
    "author": "zhangzerui20",
    "comments": [
      {
        "user": "ramaraochavali",
        "created_at": "2021-07-25T09:59:51Z",
        "body": "config_dump?include_eds will give eds details"
      },
      {
        "user": "htuch",
        "created_at": "2021-07-25T18:10:28Z",
        "body": "Also some of this information is available on the `/clusters` admin endpoint."
      },
      {
        "user": "zhangzerui20",
        "created_at": "2021-07-29T07:06:53Z",
        "body": " I got what I want through `config_dump?include_eds`"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to retrieve EDS-specific configuration details",
      "Works with existing admin interface endpoints",
      "Includes cluster-level endpoint discovery information",
      "Allows programmatic access to configuration data"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:11"
    }
  },
  {
    "number": 17353,
    "title": "Relationship between grpc service definition timeout and cluster definition connect_timeout",
    "created_at": "2021-07-14T22:12:20Z",
    "closed_at": "2021-07-16T15:36:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17353",
    "body": "In the configuration shown below there is timeout: 1s of grpc_service which is defined to call cluster: ext_authz which also has connect_timeout: 5s. Is there a relationship between those two timeouts? The grpc_service timeout is defined as \"the timeout for a specific request\", vs connect_timeout as \"The timeout for new network connections to hosts in the cluster\", so in case the grpc_service timeout is 'started' first, should it be larger or at least, equal to that of the cluster connect_timeout? \r\n\r\nIs there a best practice advice I can follow? For now, I have set both to the same value of 5s for testing, to be set via an environment variable to 3s in prod.\r\n\r\nthanks in advance,\r\nswav\r\n\r\n\"http_filters\": [\r\n             {\r\n              \"name\": \"envoy.filters.http.ext_authz\",\r\n              \"typed_config\": {\r\n               \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\",\r\n               \"grpc_service\": {\r\n                \"envoy_grpc\": {\r\n                 \"cluster_name\": \"ext_authz\"\r\n                },\r\n                **\"timeout\": \"1s\"**\r\n               },\r\n               \"transport_api_version\": \"V3\"\r\n              }\r\n             },\r\n...\r\n\"dynamic_active_clusters\": [\r\n{\r\n     \"version_info\": \"1626296116754330624\",\r\n     \"cluster\": {\r\n      \"@type\": \"type.googleapis.com/envoy.config.cluster.v3.Cluster\",\r\n      \"name\": \"ext_authz\",\r\n      \"type\": \"LOGICAL_DNS\",\r\n      **\"connect_timeout\": \"5s\",**\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17353/comments",
    "author": "swav",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-07-16T13:56:02Z",
        "body": "The connect timeout is how long to wait for the connection to be established, while the timeout is how long to wait for the response for a given request. It probably makes sense to have timeout > connect_timeout"
      },
      {
        "user": "swav",
        "created_at": "2021-07-16T15:36:01Z",
        "body": "@snowp  Thank you very much for clarifying this. :)"
      }
    ],
    "satisfaction_conditions": [
      "Clarify the relationship between gRPC service request timeout and cluster connection timeout",
      "Provide guidance on relative timeout duration configuration",
      "Offer best practices for timeout configuration in this context"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:19"
    }
  },
  {
    "number": 17012,
    "title": "Question: How to embed XDS client in Envoy",
    "created_at": "2021-06-16T16:25:07Z",
    "closed_at": "2021-07-24T00:01:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17012",
    "body": "*Title*: *How to embed XDS client in Envoy*\r\n\r\n*Description*:\r\nWe are currently exploring possibilities of a file system based configuration reader which does the following:\r\n- Reads any updates to configuration from a  file/directory \r\n- Translates the configuration changes (whole or delta) to XDS\r\n- Speaks XDS to Envoy's XDS listener.\r\n\r\nHow can we achieve this within Envoy? One possibility is to launch a new thread for this which can setup a listener for file system. But this would be intrusive and change the threading model.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17012/comments",
    "author": "conqerAtapple",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-06-16T20:02:06Z",
        "body": "Couldn't you just use file based xDS and have some other process write the config out?"
      },
      {
        "user": "conqerAtapple",
        "created_at": "2021-06-16T20:23:55Z",
        "body": "> Couldn't you just use file based xDS and have some other process write the config out?\r\n\r\nAbsolutely makes sense to have another process write the xDS config.  We were exploring if the client/user could be saved from writing the error prone configurations and let the translator do that. But i think this idea might bring in other issues. \r\nThanks for the answer. "
      }
    ],
    "satisfaction_conditions": [
      "Avoids modifying Envoy's internal threading model or core architecture",
      "Provides a way to automatically translate configuration changes to xDS format",
      "Maintains separation between configuration translation and Envoy's runtime"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:25"
    }
  },
  {
    "number": 15869,
    "title": "Connecting with IP when the listener is configured with SNI",
    "created_at": "2021-04-07T13:10:16Z",
    "closed_at": "2021-04-20T01:59:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15869",
    "body": "We are PoC-ing envoy to use it as our load balancer, we are trying to utilize the SNI feature and it works perfectky fine.\r\n\r\nBut when we try to connect to IP of the listener that is configured with SNI, we get `no matching filter chain found` and the request fails.\r\n\r\nWe use curl with option `-k` to do this.\r\n\r\nBelow is the minimal configuration to do this\r\n\r\n```\r\n- \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n  name: demo-https\r\n  address:\r\n    socket_address:\r\n      address: 100.x.x.x\r\n      port_value: 443\r\n  listener_filters:\r\n  - name: \"envoy.filters.listener.tls_inspector\"\r\n    typed_config: {}\r\n  filter_chains:\r\n          #- use_proxy_proto: true\r\n  - filter_chain_match:\r\n      server_names: [\"*.example.net\", \"example.net\"]\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n        cluster: demo-https-cluster\r\n        common_tls_context:\r\n          tls_certificates:\r\n          - certificate_chain: { filename: \"/etc/certs/asterisk.example.net.chain\" }\r\n            private_key: { filename: \"/etc/certs/asterisk.example.net.key\" }\r\n    filters:\r\n    - name: envoy.filters.network.http_connection_manager\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n        stat_prefix: http\r\n        cluster: demo-https-cluster\r\n        rds:\r\n          route_config_name: demo-rds\r\n          config_source:\r\n            path: \"/etc/envoy/rds/demo-rds.yaml\"\r\n        access_log:\r\n        - name: log\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n            path: \"/var/log/envoy.log\"\r\n            typed_json_format: *json_Format\r\n        http_filters:\r\n        - name: envoy.router\r\n          config: {}\r\n```\r\nMy questions are : \r\n\r\n1. Will connecting with IP work if SNI is enabled and specified?\r\n2. If it does not support, any suggestions to use multiple certificates like haproxy supports?\r\n\r\nThank you !",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15869/comments",
    "author": "VigneshSP94",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-04-08T17:11:29Z",
        "body": "I believe you'd either need to include the ip as one of the SNIs (assuming the client sets the IP as the SNI) or use a second filter chain that doesn't try to match on SNI."
      },
      {
        "user": "lambdai",
        "created_at": "2021-04-08T21:12:41Z",
        "body": "Or you can create another filter chain with empty `server_names` to expclity accept either no-SNI or `unknown-SNI`"
      },
      {
        "user": "VigneshSP94",
        "created_at": "2021-04-13T15:54:22Z",
        "body": "> Or you can create another filter chain with empty `server_names` to expclity accept either no-SNI or `unknown-SNI`\n\nThis worked, thanks for your humble help !"
      }
    ],
    "satisfaction_conditions": [
      "Explains how to handle client connections that don't provide SNI or provide an unrecognized SNI",
      "Provides a method to support multiple certificates without strict SNI matching requirements",
      "Clarifies the relationship between IP-based connections and SNI configuration in Envoy"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:32"
    }
  },
  {
    "number": 15071,
    "title": "round robin load balancing issue on TCP_Proxy with envoy.filters.network.sni_cluster ",
    "created_at": "2021-02-17T08:54:27Z",
    "closed_at": "2021-04-29T08:01:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15071",
    "body": "Currently I am using Istio to form a service mesh on 2 k8s clusters, say, clusterA and clusterB.  15443 port is used for cross cluster communication. i.e. in clusterA, we can access the service in clusterB through the mtls port 15443 on the istio ingressgateway of clusterB.   The problem is the traffic is not evenly distributed to the work load of the service in clusterB. \r\n\r\ne.g. \r\n\r\n kubectl logs test-deploy-6df899c68d-fm7h6  -c istio-proxy  --context dev-svc-cluster | grep \"GET\" | wc -l\r\n11659\r\n kubectl logs test-deploy-6df899c68d-sbswr  -c istio-proxy  --context dev-svc-cluster | grep \"GET\" | wc -l\r\n19837\r\n\r\nMay I know there is anything I can do to make  round robin load balancing work in my case?  Thanks.\r\n\r\n\r\nhere is the listener configuration for port 15443 of istio ingressgateway of clusterB:\r\n\r\n    {\r\n        \"name\": \"0.0.0.0_15443\",\r\n        \"address\": {\r\n            \"socketAddress\": {\r\n                \"address\": \"0.0.0.0\",\r\n                \"portValue\": 15443\r\n            }\r\n        },\r\n        \"filterChains\": [\r\n            {\r\n                \"filterChainMatch\": {\r\n                    \"serverNames\": [\r\n                        \"*.local\"\r\n                    ]\r\n                },\r\n                \"filters\": [\r\n                    {\r\n                        \"name\": \"envoy.filters.network.sni_cluster\"\r\n                    },\r\n                    {\r\n                        \"name\": \"envoy.filters.network.rbac\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.rbac.v3.RBAC\",\r\n                            \"rules\": {\r\n                                \"policies\": {\r\n                                    \"ns[istio-system]-policy[allow-ingress-gateway]-rule[0]\": {\r\n                                        \"permissions\": [\r\n                                            {\r\n                                                \"andRules\": {\r\n                                                    \"rules\": [\r\n                                                        {\r\n                                                            \"any\": true\r\n                                                        }\r\n                                                    ]\r\n                                                }\r\n                                            }\r\n                                        ],\r\n                                        \"principals\": [\r\n                                            {\r\n                                                \"andIds\": {\r\n                                                    \"ids\": [\r\n                                                        {\r\n                                                            \"any\": true\r\n                                                        }\r\n                                                    ]\r\n                                                }\r\n                                            }\r\n                                        ]\r\n                                    }\r\n                                }\r\n                            },\r\n                            \"statPrefix\": \"tcp.\"\r\n                        }\r\n                    },\r\n                    {\r\n                        \"name\": \"istio.stats\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                            \"typeUrl\": \"type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm\",\r\n                            \"value\": {\r\n                                \"config\": {\r\n                                    \"configuration\": {\r\n                                        \"@type\": \"type.googleapis.com/google.protobuf.StringValue\",\r\n                                        \"value\": \"{\\n  \\\"metrics\\\": [\\n    {\\n      \\\"dimensions\\\": {\\n        \\\"source_cluster\\\": \\\"node.metadata['CLUSTER_ID']\\\",\\n        \\\"destination_cluster\\\": \\\"upstream_peer.cluster_id\\\"\\n      }\\n    }\\n  ]\\n}\\n\"\r\n                                    },\r\n                                    \"root_id\": \"stats_outbound\",\r\n                                    \"vm_config\": {\r\n                                        \"code\": {\r\n                                            \"local\": {\r\n                                                \"inline_string\": \"envoy.wasm.stats\"\r\n                                            }\r\n                                        },\r\n                                        \"runtime\": \"envoy.wasm.runtime.null\",\r\n                                        \"vm_id\": \"tcp_stats_outbound\"\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                    },\r\n                    {\r\n                        \"name\": \"envoy.filters.network.tcp_proxy\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\r\n                            \"statPrefix\": \"BlackHoleCluster\",\r\n                            \"cluster\": \"BlackHoleCluster\",\r\n                            \"accessLog\": [\r\n                                {\r\n                                    \"name\": \"envoy.access_loggers.file\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n                                        \"path\": \"/dev/stdout\",\r\n                                        \"logFormat\": {\r\n                                            \"textFormat\": \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\"\r\n                                        }\r\n                                    }\r\n                                }\r\n                            ]\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\n        ],\r\n        \"listenerFilters\": [\r\n            {\r\n                \"name\": \"envoy.filters.listener.tls_inspector\",\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\"\r\n                }\r\n            }\r\n        ],\r\n        \"trafficDirection\": \"OUTBOUND\",\r\n        \"accessLog\": [\r\n            {\r\n                \"name\": \"envoy.access_loggers.file\",\r\n                \"filter\": {\r\n                    \"responseFlagFilter\": {\r\n                        \"flags\": [\r\n                            \"NR\"\r\n                        ]\r\n                    }\r\n                },\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n                    \"path\": \"/dev/stdout\",\r\n                    \"logFormat\": {\r\n                        \"textFormat\": \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    },\r\n\r\nhere is cluster config\r\n\r\n    {\r\n        \"name\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\",\r\n        \"type\": \"EDS\",\r\n        \"edsClusterConfig\": {\r\n            \"edsConfig\": {\r\n                \"ads\": {},\r\n                \"resourceApiVersion\": \"V3\"\r\n            },\r\n            \"serviceName\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\"\r\n        },\r\n        \"connectTimeout\": \"10s\",\r\n        \"circuitBreakers\": {\r\n            \"thresholds\": [\r\n                {\r\n                    \"maxConnections\": 4294967295,\r\n                    \"maxPendingRequests\": 4294967295,\r\n                    \"maxRequests\": 4294967295,\r\n                    \"maxRetries\": 4294967295\r\n                }\r\n            ]\r\n        },\r\n        \"metadata\": {\r\n            \"filterMetadata\": {\r\n                \"istio\": {\r\n                    \"default_original_port\": 8000,\r\n                    \"services\": [\r\n                        {\r\n                            \"host\": \"test-svc.default.svc.cluster.local\",\r\n                            \"name\": \" test-svc\",\r\n                            \"namespace\": \"default\"\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        },\r\n        \"filters\": [\r\n            {\r\n                \"name\": \"istio.metadata_exchange\",\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                    \"typeUrl\": \"type.googleapis.com/envoy.tcp.metadataexchange.config.MetadataExchange\",\r\n                    \"value\": {\r\n                        \"protocol\": \"istio-peer-exchange\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    },\r\n\r\nhere is the end points configuration:\r\n\r\n    {\r\n        \"name\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\",\r\n        \"addedViaApi\": true,\r\n        \"hostStatuses\": [\r\n            {\r\n                \"address\": {\r\n                    \"socketAddress\": {\r\n                        \"address\": \"192.168.241.194\",\r\n                        \"portValue\": 8000\r\n                    }\r\n                },\r\n                \"stats\": [\r\n                    {\r\n                        \"name\": \"cx_connect_fail\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"cx_total\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_error\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_success\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_timeout\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"rq_total\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"31\",\r\n                        \"name\": \"cx_active\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"31\",\r\n                        \"name\": \"rq_active\"\r\n                    }\r\n                ],\r\n                \"healthStatus\": {\r\n                    \"edsHealthStatus\": \"HEALTHY\"\r\n                },\r\n                \"weight\": 1,\r\n                \"locality\": {}\r\n            },\r\n            {\r\n                \"address\": {\r\n                    \"socketAddress\": {\r\n                        \"address\": \"192.168.249.65\",\r\n                        \"portValue\": 8000\r\n                    }\r\n                },\r\n                \"stats\": [\r\n                    {\r\n                        \"name\": \"cx_connect_fail\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"cx_total\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_error\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_success\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_timeout\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"rq_total\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"30\",\r\n                        \"name\": \"cx_active\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"30\",\r\n                        \"name\": \"rq_active\"\r\n                    }\r\n                ],\r\n                \"healthStatus\": {\r\n                    \"edsHealthStatus\": \"HEALTHY\"\r\n                },\r\n                \"weight\": 1,\r\n                \"locality\": {}\r\n            }\r\n        ],\r\n        \"circuitBreakers\": {\r\n            \"thresholds\": [\r\n                {\r\n                    \"maxConnections\": 4294967295,\r\n                    \"maxPendingRequests\": 4294967295,\r\n                     \"maxRequests\": 4294967295,\r\n                    \"maxRetries\": 4294967295\r\n                },\r\n                {\r\n                    \"priority\": \"HIGH\",\r\n                    \"maxConnections\": 1024,\r\n                    \"maxPendingRequests\": 1024,\r\n                    \"maxRequests\": 1024,\r\n                    \"maxRetries\": 3\r\n                }\r\n            ]\r\n        }\r\n    },\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15071/comments",
    "author": "debbyku",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-02-18T05:41:45Z",
        "body": "This config looks good.\r\ngrep GET at log file is vague. Is there any metric, graph or access log that can drill down to \"gateway - 192.168.241.194\" and \"gateway - 192.168.249.65\" ? "
      },
      {
        "user": "debbyku",
        "created_at": "2021-02-18T06:27:25Z",
        "body": "grep GET is to return the access log like this \r\n\r\n[2021-02-18T06:16:33.248Z] \"GET /rt0/v1/api/racingtouch/ping HTTP/1.1\" 200 - \"-\" 0 18 1 1 \"192.168.177.192\" \"Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.7\" \"82bf70f0-7b2c-9b69-aae0-94e3e963c989\" \"istio-ingressgateway.istio-system\" \"127.0.0.1:8000\" inbound|8000|| 127.0.0.1:53962 192.168.53.7:8000 192.168.177.192:0 outbound_.8000_._.test-svc.default.svc.cluster.local default\r\n[2021-02-18T06:16:33.249Z] \"GET /rt0/v1/api/racingtouch/ping HTTP/1.1\" 200 - \"-\" 0 18 1 0 \"192.168.98.64\" \"Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.7\" \"b28ff319-1dc2-994e-aca3-26f88881f40b\" \"istio-ingressgateway.istio-system\" \"127.0.0.1:8000\" inbound|8000|| 127.0.0.1:53922 192.168.53.7:8000 192.168.98.64:0 outbound_.8000_._.test-svc.default.svc.cluster.local default\r\n\r\nit is to count how many requests going to the pod\r\nsorry, the ip changed as I restarted the pod many times.\n\n---\n\nThe stats in the endpoints do not count correctly.  The rq_total should be much larger than it and I rarely find the access log for 15443 in the istio ingressgateway."
      },
      {
        "user": "lambdai",
        "created_at": "2021-02-18T07:46:58Z",
        "body": "> The stats in the endpoints do not count correctly. The rq_total should be much larger than it and I rarely find the access log for 15443 in the istio ingressgateway.\r\n\r\nThe request in istio-gateway is loadbalanced per `tcp connection` since you use sni_cluster with tcp_proxy filter at istio-ingressgateway. This config doesn't guarantee http request is balanced.\r\n\r\nAt an extreme case, if your siege client use only 1 tcp connection during the your load test, you will see only 1 endpoint handle all the http request. You are right at the beginning: SNI cluster + tcp_proxy doesn't well load balancing http request. \r\n\r\nYou can either switch to another http benchmark tool with max-request-per-connection to give istio-ingressgateway more chances to load balance."
      },
      {
        "user": "debbyku",
        "created_at": "2021-02-18T13:32:46Z",
        "body": "Hi @lambdai,  thanks for your advice.\r\nAs the request is from siege -> clusterA isto-ingressgateway -> clusterB istio-ingressgateway(15443) -> service, in clusterA istio-ingressgateway, we set the max-request-per-connection to 10 in order to max. the no. of connections to clusterB 15443, it seems the load balancing performance is much better.  For 4xxxx requests, the difference of number of requests to the service pods is reduced within 100.  \r\n\r\nMay I ask, if max-request-per-connection is set to 10, is there any adverse effect to the overall performance, i.e. it takes more time to create connections. etc?  Originally there is no setting for it.   Thanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-20T16:05:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "lambdai",
        "created_at": "2021-03-23T02:07:10Z",
        "body": "Sorry I missed this one.\r\nFor light weight request the major cost will be tls handshake. I usually use 5ms cpu time to estimate. YMMV"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-29T08:01:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how TCP connection reuse affects HTTP request distribution in SNI-based load balancing",
      "Guidance on balancing HTTP request-level distribution over TCP connections",
      "Analysis of performance implications for connection cycling settings",
      "Verification methodology for load balancing effectiveness"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:46"
    }
  },
  {
    "number": 14440,
    "title": "question: how to fetch the remote IP address in WASM",
    "created_at": "2020-12-16T14:03:30Z",
    "closed_at": "2020-12-23T11:20:12Z",
    "labels": [
      "question",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14440",
    "body": "I do not find any doc about how to fetch the remote IP address in WASM.\r\n\r\nMany thx for your help.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14440/comments",
    "author": "membphis",
    "comments": [
      {
        "user": "alandiegosantos",
        "created_at": "2020-12-18T23:15:56Z",
        "body": "It is possible to get the upstream IP address by getting the property _upstream.address_.\r\nI am working with WASM filters written in Rust, so the code looks like: \r\n```\r\nuse log::error;\r\nuse proxy_wasm::traits::*;\r\nuse proxy_wasm::types::*;\r\nuse std::str;\r\n\r\n#[no_mangle]\r\npub fn _start() {\r\n    proxy_wasm::set_log_level(LogLevel::Info);\r\n    proxy_wasm::set_http_context(|_, _| -> Box<dyn HttpContext> { Box::new(HttpFilter) });\r\n}\r\n\r\nstruct HttpFilter;\r\n\r\nimpl Context for HttpFilter{}\r\n\r\nimpl HttpContext for HttpFilter {\r\n    fn on_http_response_headers(&mut self, _: usize) -> Action {\r\n        // Add a header on the response.\r\n        let prop = self.get_property([\"upstream\", \"address\"].to_vec()).unwrap();\r\n        let addr = match str::from_utf8(&prop) {\r\n            Ok(v) => v,\r\n            Err(_e) => \"\",\r\n        };\r\n        error!(\"upstream address {}\",addr);\r\n        Action::Continue\r\n    }\r\n}\r\n```\r\nPS: Do not use that as production code. It is only an example.\r\n\r\nI would like to create the docs about these properties, if possible."
      },
      {
        "user": "membphis",
        "created_at": "2020-12-23T11:20:12Z",
        "body": "ok, got it. many thx"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct property path to retrieve upstream network information in WASM context",
      "Explains how to access network context properties through WASM host environment",
      "Works within proxy-WASM filter architecture constraints"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:51"
    }
  },
  {
    "number": 14075,
    "title": "Questions re OAuth2 plugin behaviour",
    "created_at": "2020-11-18T11:07:50Z",
    "closed_at": "2021-01-30T04:06:58Z",
    "labels": [
      "question",
      "stale",
      "area/oauth"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14075",
    "body": "In the docs for the OAuth2 plugin it contains the following bullet point:-\r\n\r\n> - Upon receiving an access token, the filter sets cookies so that subseqeuent requests can skip the full flow. These cookies are calculated using the hmac_secret to assist in encoding.\r\n\r\nI'm wondering what this means in practical terms, e.g. is the access token sent back to the client as part of the cookie content? Or, is it an 'opaque' cookie that is sent and when subsequent requests are received from the client the access token is retrieved from some internal cache and added to the upstream requests?\r\n\r\nThis actually leads to another question, and, again quoting from the docs:\r\n\r\n> When the authn server validates the client and returns an authorization token back to the OAuth filter, no matter what format that token is, if forward_bearer_token is set to true the filter will send over a cookie named BearerToken to the upstream. Additionally, the Authorization header will be populated with the same value.\r\n\r\nIf the access token is returned as part of the cookie content, the client will not then populate the `Authorization` header value, it will simply send the cookie in all subsequent requests. In this instance what is the plugins behaviour? Does it extract the value from the BearerToken cookie and insert it as the `Authorization` header, or does this header value remain unset?\r\n\r\nMy final questions relate to cookie expiry, is it a simple session cookie that expires when the browser is closed? Also, if there is any kind of session cache in the plugin, is there a built in expiration based on inactivity, and if so what is the inactive period?\r\n\r\nApologies for asking these questions rather than simply testing the scenarios for myself but I'm not currently able to test things out.\r\n\r\nThanks in advance!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14075/comments",
    "author": "andye2004",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2020-11-19T01:35:35Z",
        "body": "@williamsfu99 perhaps you can address these questions?"
      },
      {
        "user": "williamsfu99",
        "created_at": "2020-11-19T02:40:39Z",
        "body": "@junr03 Thanks for the tag.\r\n\r\n@andye2004 \r\n> e.g. is the access token sent back to the client as part of the cookie content? Or, is it an 'opaque' cookie that is sent and when subsequent requests are received from the client the access token is retrieved from some internal cache and added to the upstream requests?\r\n\r\nAfter the OAuth flow completely finishes, the client receives a response with SetCookie headers for an HMAC value and an Expiry epoch. The HMAC value is calculated by concatenating the domain, expiration, and token together before hashing it using the hmac-sha256(hmac_secret, data) function. Consequently, when that same user visits the domain again, the filter will first inspect these Cookie values, independently perform the same hash function, and compare these two values to validate the session; if this succeeds, the filter verifies that the current epoch is not larger than the Expiry cookie. As you may realize, the filter is stateless and the ability to auto-skip authentication is powered entirely by the exact contents of these explicit cookies. No internal cache.\r\n\r\n> If the access token is returned as part of the cookie content, the client will not then populate the Authorization header value, it will simply send the cookie in all subsequent requests. In this instance what is the plugins behaviour? Does it extract the value from the BearerToken cookie and insert it as the Authorization header, or does this header value remain unset?\r\n\r\nThe first behavior - it will extract the value from the BearerToken cookie and insert it into the Authorization header. This is an unconventional pattern but its purpose is to help the upstream expect consistently populated Authorization headers (when the boolean `forward_bearer_token` is flipped to true).\r\n\r\n> My final questions relate to cookie expiry, is it a simple session cookie that expires when the browser is closed? Also, if there is any kind of session cache in the plugin, is there a built in expiration based on inactivity, and if so what is the inactive period?\r\n\r\nThe cookies do not expire when the browser is necessarily closed - they are your standard HTTP Cookie headers with the `secure` and `httpOnly` flags enabled. The expiration epoch is determined from the `expires_in` field within the JSON response body received from the token_endpoint. Unless you clear these cookies in your browser, they will continue to be sent up until expiration.\r\n\r\nHope this helps - I do recognize that the filter needs some work before it can be widely adopted. We had to prune the filter substantially to omit proprietary interactions and the outcome is an oversimplified authentication plugin. Some improvements we need:\r\n* The filter should perform relevant data fetches - such as username - using the extracted token and pass them upstream for you, instead of expecting the upstream to do so.\r\n* Token signing from the token server should be supported, so that clients cannot arbitrary send invalid bearer tokens."
      },
      {
        "user": "juanvasquezreyes",
        "created_at": "2020-11-23T14:35:46Z",
        "body": "> Hope this helps - I do recognize that the filter needs some work before it can be widely adopted. We had to prune the filter substantially to omit proprietary interactions and the outcome is an oversimplified authentication plugin. Some improvements we need:\r\n> \r\n> * The filter should perform relevant data fetches - such as username - using the extracted token and pass them upstream for you, instead of expecting the upstream to do so.\r\n> * Token signing from the token server should be supported, so that clients cannot arbitrary send invalid bearer tokens.\r\n\r\nAnother enhancement is to add authorization mechanism such as group validations or roles validation and based on that deny or approved the request and forward to the cluster"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-23T16:14:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "andye2004",
        "created_at": "2020-12-23T23:12:16Z",
        "body": "I meant to respond to this before now but other things got in the way and I forgot, apologies. I just wanted to thank everyone, especially @williamsfu99, for commenting and clearing up some things. I'm sure in the longer term this will prove to be a very valuable plugin, but, for the moment at least, we will look at alternative methods of implementing oauth/oidc at the proxy level."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-30T04:06:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Clarify whether the access token is stored in client-side cookies or handled via HMAC validation without exposing the token",
      "Explain how Authorization headers are populated when cookies are present",
      "Detail cookie expiration behavior relative to token validity periods",
      "Confirm stateless session validation mechanism"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:27:56"
    }
  },
  {
    "number": 13992,
    "title": "[Question] prefix_rewrite based on selected host in the upstream cluster",
    "created_at": "2020-11-12T07:30:38Z",
    "closed_at": "2020-11-14T16:44:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13992",
    "body": "*Title*: prefix_rewrite based on selected upstream cluster\r\n\r\n*Description*:\r\nIs there anyway possible to rewrite the prefix based on the selected host in a cluster?\r\nI have a static cluster of 4 hosts - 1,2,3,4 and when I receive a request with prefix `/msg.host_x/`, I want the `x` to be replaced by 1,2,3 or 4 based on whichever host is selected in the cluster.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13992/comments",
    "author": "rkbalgi",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2020-11-14T09:36:06Z",
        "body": "quoting @antoniovicente on slack channel:\r\n\r\n> No, that's not possible.  One of the things that makes this kind of rewrite difficult is that the proxy would need to rewrite the request again if a different host is selected on retry.\r\n"
      },
      {
        "user": "rkbalgi",
        "created_at": "2020-11-14T16:44:10Z",
        "body": "Closing as this is not possible at the moment. Thanks guys."
      }
    ],
    "satisfaction_conditions": [
      "Dynamic path rewriting based on upstream host selection",
      "Handling retries with different hosts",
      "Cluster-aware request processing"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:28:19"
    }
  },
  {
    "number": 12675,
    "title": "Stats filtering inclusion list is not working for server level metrics",
    "created_at": "2020-08-17T05:16:20Z",
    "closed_at": "2020-08-19T04:34:35Z",
    "labels": [
      "question",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12675",
    "body": "We have the following configuration in envoy\r\n\r\n```yml\r\n    stats_config:\r\n      stats_matcher:\r\n        inclusion_list:\r\n          patterns:\r\n          - suffix: upstream_cx_total\r\n          - exact: envoy_server_state\r\n```\r\nIn this case, the metrics with suffix upstream_cx_total is emitted properly, but the metric envoy_server_state is never emitted. Please check if this a bug, we have a large config and metrics is slowing envoy down, hence wanted to include only the required metrics which is a combination of cluster and server level metrics.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12675/comments",
    "author": "shyamradhakrishnan",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2020-08-18T11:03:00Z",
        "body": "@shyamradhakrishnan I believe what you want is `exact: server.state`, the stats matcher is based on their canonical name, names in Prometheus exporter are normalized to Prometheus naming convention (with envoy prefix)."
      },
      {
        "user": "shyamradhakrishnan",
        "created_at": "2020-08-19T04:34:35Z",
        "body": "Thanks @lizan , that was the issue."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of Envoy's metric naming conventions for stats matcher inclusion lists",
      "Clarification on how server-level metrics differ from cluster-level metrics in naming patterns",
      "Guidance on proper pattern matching syntax for Envoy's stats configuration"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:28:33"
    }
  },
  {
    "number": 11012,
    "title": "Question: Envoy configured to use V3 connects to /v2/discovery:clusters",
    "created_at": "2020-04-30T12:53:22Z",
    "closed_at": "2020-05-01T10:20:21Z",
    "labels": [
      "question",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11012",
    "body": "Envoy 1.14.1 is configured to use transport_api_version: V3 for REST xDS. However it still sends requests to \"/v2/discovery:routes\" and \"/v2/discovery:clusters\". Am I missing something obvious here ?\r\n\r\n\r\nConfig:\r\n```\r\ndynamic_resources:\r\n  cds_config:\r\n    api_config_source:\r\n      api_type: REST\r\n      cluster_names: [xds_cluster]\r\n      refresh_delay: 5s\r\n      transport_api_version: V3\r\n\r\n...\r\n          rds:\r\n            route_config_name: Route_configuration\r\n            config_source:\r\n              api_config_source:\r\n                api_type: REST\r\n                cluster_names: [xds_cluster]\r\n                refresh_delay: 5s\r\n                transport_api_version: V3\r\n\r\n```\r\n\r\nLog:\r\n\r\nRoutes:\r\n```\r\n[2020-04-30 12:41:44.722][6][debug][router] [source/common/router/router.cc:477] [C0][S2429303885158800883] cluster 'xds_cluster' match for URL '/v2/discovery:routes'\r\n[2020-04-30 12:41:44.722][6][debug][router] [source/common/router/router.cc:634] [C0][S2429303885158800883] router decoding headers:\r\n':method', 'POST'\r\n':path', '/v2/discovery:routes'\r\n':authority', 'xds_cluster'\r\n':scheme', 'http'\r\n'content-type', 'application/json'\r\n'content-length', '11287'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.2'\r\n'x-envoy-expected-rq-timeout-ms', '1000'\r\n\r\n```\r\nClusters:\r\n```\r\n[2020-04-30 12:41:37.982][6][debug][config] [source/common/config/http_subscription_impl.cc:68] Sending REST request for /v2/discovery:clusters\r\n[2020-04-30 12:41:37.983][6][debug][router] [source/common/router/router.cc:477] [C0][S7534484415178178826] cluster 'xds_cluster' match for URL '/v2/discovery:clusters'\r\n[2020-04-30 12:41:37.983][6][debug][router] [source/common/router/router.cc:634] [C0][S7534484415178178826] router decoding headers:\r\n':method', 'POST'\r\n':path', '/v2/discovery:clusters'\r\n':authority', 'xds_cluster'\r\n':scheme', 'http'\r\n'content-type', 'application/json'\r\n'content-length', '11246'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.2'\r\n'x-envoy-expected-rq-timeout-ms', '1000'\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11012/comments",
    "author": "andrewtikhonov",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-04-30T16:30:44Z",
        "body": "cc @htuch @Shikugawa "
      },
      {
        "user": "htuch",
        "created_at": "2020-05-01T01:06:22Z",
        "body": "I think this is definitely not the correct behavior Envoy side, but curious what happens if you set `resource_api_version` to v3 as well?"
      },
      {
        "user": "andrewtikhonov",
        "created_at": "2020-05-01T10:02:50Z",
        "body": "Thanks. `resource_api_version` enabled it.\r\n\r\n```\r\ndynamic_resources:\r\n  cds_config:\r\n    resource_api_version: V3\r\n    api_config_source:\r\n      api_type: REST\r\n      cluster_names: [xds_cluster]\r\n      refresh_delay: 5s\r\n\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of the relationship between transport_api_version and resource_api_version in Envoy configuration",
      "Clarification of required configuration parameters for V3 xDS API usage",
      "Identification of all version-related configuration settings affecting REST endpoint paths",
      "Documentation of version compatibility between different Envoy configuration components"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:29:12"
    }
  },
  {
    "number": 10967,
    "title": "Clarifications on upstream_rq_time and downstream_rq_time",
    "created_at": "2020-04-27T18:48:00Z",
    "closed_at": "2020-06-06T09:10:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10967",
    "body": "Hi, I am trying to understand better what `upstream_rq_time` and `downstream_rq_time` exactly measure. I referred to the documentation but it wasn't clear to me. For ex. consider  `request: service A -> Envoy A -> Envoy B -> Service B; response: Service B -> Envoy B -> Envoy A -> Service A`, what do `upstream_rq_time` and `downstream_rq_time` mean here? How is the total RTT calculated?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10967/comments",
    "author": "shashankram",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-04-27T19:27:39Z",
        "body": "Envoy A's downstream_rq_time measures the time elapsed from when Envoy A starts handling Service A's request until the entire response has sent to Service A.\r\n\r\nEnvoy A's upstream_rq_time measures the time elapsed from the point where Service A's entire request has been received by the HTTP router filter until the entire upstream response from Envoy B has been received.\r\n\r\nThe same is true for Envoy B, except the downstream is Envoy A's request/response and the upstream is Service B.\r\n\r\nSo Envoy A's downstream_rq_time > Envoy A's upstream_rq_time > Envoy B's downtream_rq_time > Envoy B's upstream_rq_time.\r\n\r\nAssuming there are no blocking filters in use (e.g. ext_auth) I expect the times to be reasonably close together."
      },
      {
        "user": "shashankram",
        "created_at": "2020-04-28T00:30:45Z",
        "body": "Thanks for clarifying!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-06T09:10:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ],
    "satisfaction_conditions": [
      "Clear definition of the start and end points for upstream_rq_time and downstream_rq_time in a multi-Envoy service chain",
      "Explanation of how these metrics relate hierarchically between Envoy instances",
      "Description of how total RTT can be derived from the combination of these metrics",
      "Clarification of the directionality (request vs response flow) captured by each metric"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:29:17"
    }
  },
  {
    "number": 9794,
    "title": "Multiple validation_context's",
    "created_at": "2020-01-23T08:18:12Z",
    "closed_at": "2020-01-29T02:11:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9794",
    "body": "Is there a way to setup multiple validation_context's with Envoy, particularly for TCP traffic not HTTP?\r\n\r\nWe'd like to provide a single URL, and use mTLS w/ ~3-10 other external systems. We can't influence the other systems to set unique headers, domains, SNI, or anything else.\r\n\r\nWe could associate their CA certs with their hostname/IP to match on based on that, or we could eat the cost of trying every cert till one matched.\r\n\r\nOr Is this possible to do at the moment leveraging filter chains?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9794/comments",
    "author": "steeling",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-01-26T19:03:15Z",
        "body": "As long as you can match on hostname/IP I think you can do this with filter chains? cc @lambdai @PiotrSikora "
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-26T23:16:39Z",
        "body": "Yes, you could use multiple filter chains to do that, but the cost of matching to a particular certificate is negligible (it's just a quick comparison of hashes), so unless you need/want to segregate traffic from those external systems for some reason (it doesn't sound like you do), the best and easiest solution would be to simply provide `trusted_ca` that contains all the trusted CA certificates or provide multiple valid values in `match_subject_alt_names`."
      },
      {
        "user": "steeling",
        "created_at": "2020-01-28T03:03:49Z",
        "body": "can we match on client IP or hostname?\r\n\r\nAll of the traffic will come to a single hostname. At the moment we don't have any influence on enforcing a specific CA to use, so we first request their CA cert with us, and we add it to a list we will verify against"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-28T03:17:07Z",
        "body": "Yes, you can match on the client (source) IP address or subnet.\r\n\r\nYou cannot match on the client's hostname."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-01-28T16:49:19Z",
        "body": "> You cannot match on the client's hostname.\r\n\r\nJust to clarify, \"hostname\" is an L7/HTTP concept. You can match on SNI."
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-28T17:00:29Z",
        "body": "> Just to clarify, \"hostname\" is an L7/HTTP concept. You can match on SNI.\r\n\r\nHe means client's hostname (reverse DNS of client's IP address), not SNI / `Host` / `:authority`."
      },
      {
        "user": "steeling",
        "created_at": "2020-01-29T02:11:08Z",
        "body": "Thanks all, that helps!"
      }
    ],
    "satisfaction_conditions": [
      "Support validation of multiple CA certificates for mTLS without requiring client-side headers/SNI/domain customization",
      "Enable traffic segregation based on client IP addresses or certificate attributes",
      "Handle TCP traffic without relying on HTTP-layer features",
      "Provide certificate validation efficiency when handling multiple CAs"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:29:59"
    }
  },
  {
    "number": 9689,
    "title": "Leak of information about cluster member",
    "created_at": "2020-01-15T15:44:24Z",
    "closed_at": "2020-01-17T14:47:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9689",
    "body": "Please, how do I retrieve the `envoy_cluster_internal_upstream_rq_xx` by cluster members?\r\n\r\nI have a cluster with 2 members... (load_assignment/lb_endpoints)... but I can't find any information to get errors from each cluster nodes.\r\n\r\nOne of these cluster members is buggy, but I can't find which node. No tip found.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9689/comments",
    "author": "hakuno",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2020-01-15T17:16:15Z",
        "body": "@hakuno does the `/clusters` endpoint provide you the information you need?"
      },
      {
        "user": "hakuno",
        "created_at": "2020-01-15T18:19:22Z",
        "body": "So I can read `rq_error` as bad HTTP status code. Nice!\r\n\r\n```\r\ncluster::node1::rq_active::0\r\ncluster::node1::rq_error::0\r\ncluster::node1::rq_success::94\r\ncluster::node1::rq_timeout::0\r\ncluster::node1::rq_total::94\r\n```\r\n\r\nHow do I read that with the Prometheus/Grafana?"
      },
      {
        "user": "htuch",
        "created_at": "2020-01-15T19:57:44Z",
        "body": "@hakuno I think that question is outside my domain knowledge, so I'll leave this open for others to comment on for the normal question period of time (2 weeks). "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-01-16T01:19:33Z",
        "body": "Per-host stats are not currently exported to the stats sinks, due to cardinality issues."
      },
      {
        "user": "hakuno",
        "created_at": "2020-01-17T14:47:35Z",
        "body": "All right. Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Ability to identify error metrics per individual cluster member",
      "Integration with monitoring tools (Prometheus/Grafana)",
      "Access to per-host upstream request statistics",
      "Solution addresses cardinality concerns in metrics collection"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:30:06"
    }
  },
  {
    "number": 9423,
    "title": "Ability to force restart of envoy upon certain trigger?",
    "created_at": "2019-12-19T18:05:52Z",
    "closed_at": "2019-12-20T20:53:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9423",
    "body": "*Title*: *Force server restart upon triggered condition?*\r\n\r\n*Description*:\r\nWhen setting up Envoy with a central control plane, the static cluster that serves up the xDS interface is vital to the functioning of the server.  If that cluster isn't up/reachable when Envoy is trying to connect, it goes through it's retries and eventually gets into a state where it's never gonna get updates, but also doesn't shut down.\r\n\r\nThis makes sense from the perspective that the static cluster for xDS is just another cluster, and envoy should stay up.  But now the proxy is in an unrecoverable state and since the server didn't shut down the orchestration tool in use (k8s for instance) won't restart the pod automatically.  \r\n\r\nIs there anything like this in Envoy, or do I need to relegate this to readiness probes in the orchestration tool?  I didn't see anything that could force a shutdown in the docs, but wanted to ask here before I gave up.  \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9423/comments",
    "author": "justincely",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-20T16:15:41Z",
        "body": "> Is there anything like this in Envoy, or do I need to relegate this to readiness probes in the orchestration tool? I didn't see anything that could force a shutdown in the docs, but wanted to ask here before I gave up.\r\n\r\nCorrect, there is nothing like this today and we suggest defining readiness probes that do what you want. We have quite a few different admin endpoints that help with writing such probes."
      },
      {
        "user": "justincely",
        "created_at": "2019-12-20T20:43:34Z",
        "body": "thanks @mattklein123, i suspected as much, but appreciate the quick reply.  Happy to have this closed, as my question is answered.  "
      }
    ],
    "satisfaction_conditions": [
      "A mechanism to detect Envoy's unrecoverable state when xDS control plane connectivity is lost",
      "Integration with orchestration systems to trigger automated restarts",
      "Clear guidance on monitoring xDS cluster health status",
      "Solution that works without requiring Envoy self-termination capability"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:30:12"
    }
  },
  {
    "number": 9417,
    "title": "any reason on \"no healthy host for HTTP connection pool\"",
    "created_at": "2019-12-19T13:49:47Z",
    "closed_at": "2019-12-23T01:58:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9417",
    "body": "Hello,\r\n\r\nWe have seen many logs like \"no healthy host for HTTP connection pool\". Envoy failed to establish gRPC to peer side. Could someone shed some light on it?\r\n\r\nThanks a lot!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9417/comments",
    "author": "mailzyok",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T06:27:27Z",
        "body": "@mailzyok\r\n\r\n\"no healthy\" is generally because the health check is configured and the upstream machine's health status is unhealth, so when there is a request, there is no way to find a healthy host."
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-20T06:38:36Z",
        "body": "@zyfjeff  Thanks for you comment.\r\n\r\nHere is additional log which indicated the gRPC cannot be established. It was a response from upstream. It is strange the status code is 200. \r\nI am still not clear on what the reason for unhealth, because the upstream service is up and running in K8S. Is there any other log i can check?\r\n\r\n> [2019-12-20 02:39:18.786][18][debug][http] [external/envoy/source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=true):\r\n> ':status', '200'\r\n> 'content-type', 'application/grpc'\r\n> 'grpc-status', '14'\r\n> 'grpc-message', 'no healthy upstream'\r\n> \r\n\r\nThanks."
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T06:45:11Z",
        "body": "@mailzyok can you upload the complete log and config_dump files?"
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-20T06:55:29Z",
        "body": "@zyfjeff Thanks. I cannot upload a complete config_dump log here, could you let me know which part you want to see for config_dump, i will copy it here.\r\n\r\nHere is the log for complete log when the problem happens\r\n\r\n> [2019-12-20 02:39:18.784][18][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:74] cm init: adding: cluster=xds-grpc primary=1 secondary=0\r\n> [2019-12-20 02:39:18.784][18][debug][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:42] Establishing new gRPC bidi stream for rpc StreamAggregatedResources(stream .envoy.api.v2.DiscoveryRequest) returns (stream .envoy.api.v2.DiscoveryResponse);\r\n> \r\n> [2019-12-20 02:39:18.784][18][debug][router] [external/envoy/source/common/router/router.cc:332] [C0][S5272412103284310368] cluster 'xds-grpc' match for URL '/envoy.service.discovery.v2.AggregatedDiscoveryService/StreamAggregatedResources'\r\n> [2019-12-20 02:39:18.784][18][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:1124] no healthy host for HTTP connection pool\r\n> [2019-12-20 02:39:18.786][18][debug][http] [external/envoy/source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=true):\r\n> ':status', '200'\r\n> 'content-type', 'application/grpc'\r\n> 'grpc-status', '14'\r\n> 'grpc-message', 'no healthy upstream'\r\n> \r\n> [2019-12-20 02:39:18.786][18][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:86] gRPC config stream closed: 14, no healthy upstream\r\n> [2019-12-20 02:39:18.786][18][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:49] Unable to establish new stream\r\n> \r\n\r\nHere is the cluster info in config_dump\r\n\r\n>      \"clusters\": [\r\n>       {\r\n>        \"name\": \"prometheus_stats\",\r\n>        \"type\": \"STATIC\",\r\n>        \"connect_timeout\": \"0.250s\",\r\n>        \"hosts\": [\r\n>         {\r\n>          \"socket_address\": {\r\n>           \"address\": \"::1\",\r\n>           \"port_value\": 15000\r\n>          }\r\n>         }\r\n>        ]\r\n>       },\r\n>       {\r\n>        \"name\": \"xds-grpc\",\r\n>        \"type\": \"STRICT_DNS\",\r\n>        \"connect_timeout\": \"10s\",\r\n>        \"hosts\": [\r\n>         {\r\n>          \"socket_address\": {\r\n>           \"address\": \"istio-pilot\",\r\n>           \"port_value\": 15010\r\n>          }\r\n>         }\r\n>        ],\r\n>        \"circuit_breakers\": {\r\n>         \"thresholds\": [\r\n>          {\r\n>           \"max_connections\": 100000,\r\n>           \"max_pending_requests\": 100000,\r\n>           \"max_requests\": 100000\r\n>          },\r\n>          {\r\n>           \"priority\": \"HIGH\",\r\n>           \"max_connections\": 100000,\r\n>           \"max_pending_requests\": 100000,\r\n>           \"max_requests\": 100000\r\n>          }\r\n>         ]\r\n>        },\r\n>        \"http2_protocol_options\": {},\r\n>        \"dns_refresh_rate\": \"300s\",\r\n>        \"upstream_connection_options\": {\r\n>         \"tcp_keepalive\": {\r\n>          \"keepalive_time\": 300\r\n>         }\r\n>        }\r\n>       }\r\n>      ]\r\n> "
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T07:44:28Z",
        "body": "@mailzyok \r\nAccording to your logs, it should be that you cannot connect to the` istio-pilot` to cause \" no health upstream\", you can confirm whether `istio-pilot.istio-system` can resolve the IP"
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-23T01:58:13Z",
        "body": "@zyfjeff  Thanks a lot, seems DNS resolution problem, see logs below:\r\n\r\nWhen envoy starting resolution for istio-pilot, it takes 150s no response, i think it is a timeout, but i didn't find the default timeout values,\r\n\r\nThen after 300s, envoy start a new resolution, i think 300s is configured as dns_refresh_rate in static cluster configuration. This time the resolution succeeded.\r\n\r\nI will ask the admin to check the issue of DNS resolution. Thanks again and i would take this opportunity to wish you a Merry Xmas and Happy new Year.\r\n\r\n> [2019-12-20 08:32:22.245][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:88] starting async DNS resolution for istio-pilot\r\n> [2019-12-20 08:34:52.249][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:95] async DNS resolution complete for istio-pilot \r\n> [2019-12-20 08:39:52.252][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:88] starting async DNS resolution for istio-pilot\r\n> [2019-12-20 08:39:52.253][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:95] async DNS resolution complete for istio-pilot\r\n> [2019-12-20 08:39:52.253][20][debug][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:117] DNS hosts have changed for istio-pilot \n\n---\n\nThank you all for the support. The trace log is useful, we figured out the timeout values finally."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Envoy cannot establish a connection to the upstream service despite it being reported as running in Kubernetes",
      "Identification of specific log types or diagnostic methods to investigate connection failures between Envoy and istio-pilot",
      "Clarification of how DNS resolution timeouts interact with Envoy's cluster configuration",
      "Guidance on validating network connectivity between Envoy and upstream services in Kubernetes environments"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:30:17"
    }
  },
  {
    "number": 9138,
    "title": "Browser getting static files from rewritten URLs instead of matched route",
    "created_at": "2019-11-26T05:17:37Z",
    "closed_at": "2019-11-27T06:54:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9138",
    "body": "*Title*\r\nQuestion: Browser getting static files from rewritten URLs instead of matched route\r\n\r\n*Description*:\r\nCurrently I want Envoy to route to a backend cluster using this endpoint: <some-domain>.com/cluster1.\r\n\r\nThe virtual host config contains the following:\r\n`\r\n              - match:\r\n                  prefix: \"/cluster1/\"\r\n                route:\r\n                  cluster: cluster1\r\n                  prefix_rewrite: \"/\"\r\n`\r\n\r\nThis routes browser's requests to /cluster1 as expected. However, all the static files (js, css...) from this backend is now resolved to root URL. Ex: <some-domain>.com/index.js instead of <some-domain>.com/cluster1/index.js, resulting in 404 error.\r\n\r\nI want to ask if there is a way to configure Envoy to resolve this issue?\r\n\r\nMany thanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9138/comments",
    "author": "RisingSun777",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2019-11-26T21:37:07Z",
        "body": "Envoy doesn't support rewriting responses.\r\n\r\nIt sounds like the responses from cluster1 contains URLs based on the path of requests sent to cluster1. Perhaps that application could use relative URLs?"
      },
      {
        "user": "RisingSun777",
        "created_at": "2019-11-27T06:54:27Z",
        "body": "Yeah changing to using relative URLs from the application side works.\r\n\r\nThanks for your support."
      }
    ],
    "satisfaction_conditions": [
      "Ensures static resource URLs maintain the original path prefix (/cluster1/)",
      "Works within Envoy's limitations of not supporting response rewriting",
      "Preserves routing functionality to backend cluster while maintaining resource accessibility",
      "Does not require absolute URL generation from backend application"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:30:25"
    }
  },
  {
    "number": 8774,
    "title": "Propagate Cluster.alt_stat_name to RouteEntry",
    "created_at": "2019-10-25T22:44:30Z",
    "closed_at": "2019-10-26T04:23:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8774",
    "body": "We use custom `PassThroughEncoderFilter` which emits some cluster-specific stats.\r\nCurrently we use `streamInfo.routeEntry()->clusterName()` for this purpose, but since Cluster.alt_stat_name is not propagated to RouteEntry we can't achieve consistent logging across different code paths.\r\nWould it make sense to provide cluster's alt_stat_name via RouteEntry?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8774/comments",
    "author": "veshij",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-10-25T23:08:22Z",
        "body": "You can look up the cluster and then get the alt stat name from cluster info right?"
      },
      {
        "user": "veshij",
        "created_at": "2019-10-26T04:23:32Z",
        "body": "Yep, that should work, sorry for bothering."
      }
    ],
    "satisfaction_conditions": [
      "Maintains compatibility with existing stat emission patterns",
      "Ensures cluster configuration data is available where routing decisions are implemented"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:31:23"
    }
  },
  {
    "number": 8123,
    "title": "Question : Custom filter to split PUT in several multipart requests",
    "created_at": "2019-09-03T08:02:02Z",
    "closed_at": "2019-09-03T17:38:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8123",
    "body": "Let's say I want to upload a large file to Amazon S3 using their multipart API in which the file is sent in several pieces, but I want to do that in a single upload.\r\n\r\nWould it be possible to use Envoy for that ? i.e. the client makes a single PUT request, Envoy splits the request in several pieces (and thus several requests to the upstream), all that in full streaming (no buffering whatsoever).\r\n\r\nI'm guessing it could be feasible if I code a custom filter, but before investing a lot of time in this I want to make sure I'm not missing some constraints that would make this impossible.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8123/comments",
    "author": "s-vivien",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-09-03T15:08:54Z",
        "body": "Yeah you could do this with a custom filter."
      },
      {
        "user": "htuch",
        "created_at": "2019-09-03T15:23:21Z",
        "body": "@s-vivien does this answer your question? If so, can we close out this issue?"
      },
      {
        "user": "s-vivien",
        "created_at": "2019-09-03T15:37:41Z",
        "body": "It does answer my question. Thanks."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that Envoy's architecture supports splitting a single PUT request into multiple upstream multipart requests",
      "Capability to handle full streaming without buffering during request splitting",
      "Validation that custom filters can manipulate request bodies in a chunked/streaming manner",
      "Absence of architectural limitations preventing request transformation during streaming"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:31:31"
    }
  },
  {
    "number": 6930,
    "title": "Envoy doesn't execute automatic retries using 5xx Envoy retry policy ",
    "created_at": "2019-05-14T10:40:44Z",
    "closed_at": "2019-06-20T20:45:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6930",
    "body": "**Envoy doesn't execute automatic retries using 5xx Envoy retry policy**\r\n\r\n*Description*:\r\n\r\nI'm interested in some envoy behavior. I implemented email sending service that is used as cluster in envoy configuration.  I want to retry requests using envoy, if each of 500, 502, 503, 504 status codes return back from service to envoy. \r\n\r\n**Bug Template**\r\n\r\n*Description*:\r\n\r\nIn the first case I'm sending several request in parallel with binary file to service through envoy and get 500 status code in response from service or 503 service code, if the service is shut down. I except that envoy automatically retry requests, but retries are not occur.\r\n\r\nIn the second case I'am implement POST request with empty data and in this case I get 500 status code and retries are successfully happen.\r\n\r\n*Repro steps*:\r\n> There are 20 parallel requests in the first case with data binary and in the second case with empty data\r\n\r\n```\r\nfile=$1\r\ncurl --data-binary @${file}.post  -L --post301 --connect-timeout 300 $envoy-url \r\n```\r\n*Admin and Stats Output*:\r\n\r\n/stats/prometheus | grep email-common\r\n>The first case: sending data binary file\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 4183272\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 8380\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 40\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 0\r\n\r\n```\r\n>The second case: sending request with empty data\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 26950\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 30\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 23045\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 13\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_retry_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 55\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 31\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 3771\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 9\r\n```\r\n*Config*:\r\nEnvoy version is v1.9.0, pulled from docker hub\r\n\r\n```\r\nadmin:\r\n  access_log_path: /app/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 8081 }\r\nstatic_resources:\r\n  listeners:\r\n  - name: https_listener\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 8080 }\r\n    filter_chains:\r\n      - filters:\r\n        - name: envoy.http_connection_manager\r\n          config:\r\n            codec_type: AUTO\r\n            stat_prefix: ingress\r\n            route_config:\r\n              name: router\r\n              virtual_hosts:\r\n              - name: common\r\n                domains: [\"*\"]\r\n                routes:\r\n                - match: { prefix: \"/\" }\r\n                  route:\r\n                    cluster: email-common\r\n                    auto_host_rewrite: true\r\n                    timeout: 50s\r\n                    retry_policy:\r\n                      retry_on: \"5xx\"\r\n                      num_retries: 5\r\n                      per_try_timeout: 10s\r\n            http_filters:\r\n            - name: envoy.router\r\n              config: { deprecated_v1: true }\r\n  clusters:\r\n    name: email-common\r\n    type: LOGICAL_DNS\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /app/data/fullchain.pem\r\n    hosts:\r\n      - socket_address: { address: email-service, port_value: 443 }\r\n```\r\n\r\n\r\n*Logs*:\r\n>Logs of requests for requests with data binary:\r\n\r\n```\r\n[2019-05-14 10:25:28.054][000036][debug][router] [source/common/router/router.cc:1023] [C782][S9375733934444584087] pool ready\r\n--\r\n\u00a0 | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:615] [C782][S9375733934444584087] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:968] [C782][S9375733934444584087] resetting pool request\r\n\u00a0 | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:270] [C784][S6563403434288646526] cluster 'email-common' match for URL '/email/'\r\n\u00a0 | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:328] [C784][S6563403434288646526] router decoding headers:\r\n\u00a0 | ':authority', 'envoy-proxy'\r\n\u00a0 | ':path', '/email/'\r\n\u00a0 | ':method', 'POST'\r\n\u00a0 | ':scheme', 'http'\r\n\u00a0 | 'content-length', '2266446'\r\n\u00a0 | 'user-agent', 'curl/7.64.0'\r\n\u00a0 | 'accept', '*/*'\r\n\u00a0 | 'authorization', 'none'\r\n\u00a0 | 'content-type', 'application/json'\r\n\u00a0 | 'x-request-id', '693db9a5-46ee-4bd7-9bb3-0766fbe90ed4'\r\n\u00a0 | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n\u00a0 | 'x-forwarded-host', 'envoy-proxy.host.ru;'\r\n\u00a0 | 'x-forwarded-port', '80'\r\n\u00a0 | 'x-forwarded-proto', 'http'\r\n\u00a0 | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;;proto=http'\r\n\u00a0 | 'x-forwarded-for', '10.233.53.119'\r\n\u00a0 | 'x-envoy-internal', 'true'\r\n```\r\n>Logs of requests for requests with empty data:\r\n```\r\n[2019-05-14 10:10:08.241][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n--\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.248][000040][debug][router] [source/common/router/router.cc:1023] [C254][S6944810802321297265] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:615] [C254][S6944810802321297265] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:779] [C254][S6944810802321297265] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:968] [C254][S6944810802321297265] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.260][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.271][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.276][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:270] [C263][S2736688878384099272] cluster 'email-common' match for URL '/email/'\r\n\u00a0 | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:328] [C263][S2736688878384099272] router decoding headers:\r\n\u00a0 | ':authority', 'envoy-proxy'\r\n\u00a0 | ':path', '/email/'\r\n\u00a0 | ':method', 'POST'\r\n\u00a0 | ':scheme', 'http'\r\n\u00a0 | 'content-length', '0'\r\n\u00a0 | 'user-agent', 'curl/7.64.0'\r\n\u00a0 | 'accept', '*/*'\r\n\u00a0 | 'authorization', 'none'\r\n\u00a0 | 'content-type', 'application/json'\r\n\u00a0 | 'x-request-id', 'f627a2c5-a91d-4c71-aa56-61c04163eb88'\r\n\u00a0 | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n\u00a0 | 'x-forwarded-host', 'envoy-proxy.host.ru'\r\n\u00a0 | 'x-forwarded-port', '80'\r\n\u00a0 | 'x-forwarded-proto', 'http'\r\n\u00a0 | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;proto=http'\r\n\u00a0 | 'x-forwarded-for', '10.233.53.119'\r\n\u00a0 | 'x-envoy-internal', 'true'\r\n```\r\n\r\nQuestions:\r\n\r\n1. Why envoy don't execute retries, if I send to it some data binary in requests? The target service return 500 error code, and the envoy can executing retries according '5xx', for example for requests with empty data.\r\n\r\n1. What it can be changed to retry occurring for requests with binary data? Can it is due with some retry policy parameters, such as 'per_try_timeout' or some clusters parameters like type or connect timeout? .\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6930/comments",
    "author": "PetrovMikhail",
    "comments": [
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T13:21:34Z",
        "body": "Size of sending file in request is in the first case ~2 MB."
      },
      {
        "user": "snowp",
        "created_at": "2019-05-14T13:27:21Z",
        "body": "I suspect you're running into the limitation that Envoy is unable to retry requests that are too large to fit into its buffer, evident by the fact that `envoy_cluster_retry_or_shadow_abandoned` is non-zero. Can you try setting `per_connection_buffer_limit_bytes` on the cluster to something higher? The default is 1MiB."
      },
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T20:29:23Z",
        "body": "Yes, I think that your advise in right way, thank you! but I increased this value, and it did not work yet, I think that i have to increase limit file value in another parameters of Envoy too. Do you where it is may be? \n\n---\n\n@snowp , thank you very match, this was a right solution. Besides envoy we have nginx in series, and after increasing `client_max_body_size`  it works fine.\n\n---\n\nThis is my folder, I didn't understood correct results of tests, which I describe in two previous comments. I already change `per_connection_buffer_limit_bytes` to 30 000 000 value, which just about equal 30 MiB, but when I was testing envoy behavior next, I discovered that envoy still doesn't retry request with files > 1 MiB. Maybe another parameter is occurs which changes buffer size of cluster and so on? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-20T20:45:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "feature-id",
        "created_at": "2021-03-15T12:01:49Z",
        "body": "For all, who got the same problem: there's also per_connection_buffer_limit_bytes option on a listener level, which is 1MB by default."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Envoy cannot buffer large request bodies for retries",
      "Identification of buffer-related configuration parameters affecting retry behavior",
      "Clarification of Envoy's retry requirements for different request types",
      "Guidance on configuring buffer limits at multiple levels (listener, cluster, upstream)",
      "Explanation of relationship between request body size and retry capability"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:33:13"
    }
  },
  {
    "number": 6633,
    "title": "Using request_headers_to_ad to set Host header, the port value is removed!?",
    "created_at": "2019-04-18T09:13:31Z",
    "closed_at": "2019-04-19T16:00:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6633",
    "body": "I'm trying to perform this configuration:\r\n`        request_headers_to_add:\r\n            - header:\r\n                key: \"Host\"\r\n                value: \"127.0.0.1:10000\"\r\n              append: false\r\n`\r\nBut on the http request the \"Host\" header lost the value port:\r\n\"Host\": \"127.0.0.1\"\r\nIs this an intentional Envoy behavior? Or I'm using a wrong configuration?\r\nThanks in advance.\r\nAlessandro",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6633/comments",
    "author": "alessandro-vincelli",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-04-18T12:19:48Z",
        "body": "I think you probably want to use `host_rewrite` instead of `request_headers_to_add`. The `host` header is special and has a bunch of interactions with HTTP connection manager and the codecs."
      },
      {
        "user": "alessandro-vincelli",
        "created_at": "2019-04-19T16:00:33Z",
        "body": "Thanks hutch, Evoy it works correctly, my fault, the header was override by another system."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Host header manipulation through request_headers_to_add might not work as expected",
      "Identification of the correct configuration approach for Host header modifications",
      "Clarification of header override mechanisms in Envoy",
      "Differentiation between standard header manipulation and Host header special cases"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:33:22"
    }
  },
  {
    "number": 6471,
    "title": "Help with gRPC HTTP / 1.1 reverse bridge: grpc-message mapping.",
    "created_at": "2019-04-03T13:45:12Z",
    "closed_at": "2019-05-11T14:14:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6471",
    "body": "**Help with gRPC HTTP / 1.1 reverse bridge.**\r\n\r\n*Title*: *Help with gRPC HTTP / 1.1 reverse bridge: grpc-message mapping.*\r\n\r\n*Description*:\r\nI'm using the gRPC HTTP / 1.1 reverse bridge and I have the next doubt:\r\n\r\nIn case of error, (besides the **grpc-status** \"auto\" map) is there any way to map an error message to the trailer **grpc-message** when my upstream does not understand any gRPC semantics? \r\n\r\nI would like to avoid passing errors in the body and use the standard way.\r\n\r\nMany thanks in advance and excuse my ignorance.\r\n\r\n*Config*:\r\nThis is the config I'm using it and working correctly:\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          protocol: TCP\r\n          address: 0.0.0.0\r\n          port_value: 10000\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                codec_type: AUTO\r\n                route_config:\r\n                  name: backend\r\n                  virtual_hosts:\r\n                    - name: backend\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match: { prefix: \"/\" }\r\n                          route: { host_rewrite: nginx, cluster: backend, timeout: 59.99s }\r\n                http_filters:\r\n                  - name: envoy.filters.http.grpc_http1_reverse_bridge\r\n                    config:\r\n                      content_type: application/grpc+proto\r\n                      withhold_grpc_frames: true\r\n                  - name: envoy.router\r\n                    typed_config: {}\r\n  clusters:\r\n  - name: backend\r\n    connect_timeout: 59.99s\r\n    type: logical_dns\r\n    dns_lookup_family: v4_only\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: backend\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: nginx\r\n                    port_value: 80\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6471/comments",
    "author": "sp-manuel-jurado",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-04-03T14:30:13Z",
        "body": "The filter already injects error messages into grpc-message when it receives an unsupported response. Are you talking about being able to respond with a custom message?"
      },
      {
        "user": "sp-manuel-jurado",
        "created_at": "2019-04-03T14:45:23Z",
        "body": "@snowp yes, I was referring to custom message error (even the possibility to customize grpc-status too).\r\n\r\nNote: my upstream does not understand any gRPC semantics, I'm using PHP."
      },
      {
        "user": "snowp",
        "created_at": "2019-04-03T15:21:03Z",
        "body": "To send a custom message you can just send a header only response (no body) with a header named `grpc-message` with your message. As log as the content-type matches what the filter expects it should just pass through the `grpc-message`.\r\n\r\nFor `grpc-status` we'd have to make some code changes. Making the filter just pass through the status if the upstream provided one sounds reasonable, would that work for you?"
      },
      {
        "user": "sp-manuel-jurado",
        "created_at": "2019-04-04T10:54:33Z",
        "body": "Hi @snowp \r\nI've checked grpc-message pass through using your directions and works as expected.\r\n\r\nFor example:\r\n\r\nHTTP/1.1 request/response (within content-type, content-length: 0, grpc-message headers set)\r\n```\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 10000 (#0)\r\n> POST /TestAPI/TestMethod HTTP/1.1\r\n> Host: localhost:10000\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 400 Bad Request\r\n< server: envoy\r\n< content-type: application/grpc+proto\r\n< content-length: 0\r\n< x-powered-by: PHP/7.2.15\r\n< grpc-message: custom message for 400 http status code\r\n< cache-control: no-cache, private\r\n< date: Thu, 04 Apr 2019 10:19:12 GMT\r\n< access-control-allow-origin:\r\n< x-envoy-upstream-service-time: 966\r\n<\r\n* Connection #0 to host localhost left intact\r\n```\r\n\r\ngRPC response status (as PHP array, sorry for that. I'm testing with PHP client too)\r\n```\r\n(\r\n    [metadata] => Array\r\n        (\r\n            [:status] => Array\r\n                (\r\n                    [0] => 400\r\n                )\r\n\r\n            [server] => Array\r\n                (\r\n                    [0] => envoy\r\n                )\r\n\r\n            [content-type] => Array\r\n                (\r\n                    [0] => application/grpc\r\n                )\r\n\r\n            [content-length] => Array\r\n                (\r\n                    [0] => 5\r\n                )\r\n\r\n            [x-powered-by] => Array\r\n                (\r\n                    [0] => PHP/7.2.15\r\n                )\r\n\r\n            [grpc-message] => Array\r\n                (\r\n                    [0] => custom message for 400 http status code\r\n                )\r\n\r\n            [cache-control] => Array\r\n                (\r\n                    [0] => no-cache, private\r\n                )\r\n\r\n            [date] => Array\r\n                (\r\n                    [0] => Thu, 04 Apr 2019 10:22:16 GMT\r\n                )\r\n\r\n            [access-control-allow-origin] => Array\r\n                (\r\n                    [0] =>\r\n                )\r\n\r\n            [x-envoy-upstream-service-time] => Array\r\n                (\r\n                    [0] => 1255\r\n                )\r\n\r\n        )\r\n\r\n    [code] => 1\r\n    [details] => Received http2 header with status: 400\r\n)\r\n```\r\n\r\nI can see:\r\n- http2 status (:status) set to 400\r\n- grpc custom message (grpc-message) set to \"custom message for 400 http status code\"\r\nThis is ok and very useful for me.\r\n\r\nBut I miss the grpc-status in metadata. Related to this: \u00bfIs it the standard behaviour of the filter (and only sets the grpc-status trailer when response content is not empty)? Or I'm doing or understanding something wrong.\r\n\r\nRelated to:\r\nsnowp: \"For grpc-status we'd have to make some code changes. Making the filter just pass through the status if the upstream provided one sounds reasonable, would that work for you?\"\r\nI guess this is related to what I said above. The standard mapping would work for me (in that case 11 (400 Bad Request)). If I would want a custom error I could add it into grpc-message using a protocolbuffer serialized as string or checking http2 generic \":status\".\r\n\r\nMany thanks in advance and apologize for the inconveniences.\r\n\n\n---\n\nAlso, I've checked to add grpc-status as response header to do mapping manually and I get the next grpc status:\r\n\r\n```\r\n(\r\n    [metadata] => Array\r\n        (\r\n            [server] => Array\r\n                (\r\n                    [0] => envoy\r\n                )\r\n\r\n            [content-length] => Array\r\n                (\r\n                    [0] => 5\r\n                )\r\n\r\n            [x-powered-by] => Array\r\n                (\r\n                    [0] => PHP/7.2.15\r\n                )\r\n\r\n            [cache-control] => Array\r\n                (\r\n                    [0] => no-cache, private\r\n                )\r\n\r\n            [date] => Array\r\n                (\r\n                    [0] => Thu, 04 Apr 2019 10:46:08 GMT\r\n                )\r\n\r\n            [access-control-allow-origin] => Array\r\n                (\r\n                    [0] =>\r\n                )\r\n\r\n            [x-envoy-upstream-service-time] => Array\r\n                (\r\n                    [0] => 1293\r\n                )\r\n\r\n        )\r\n\r\n    [code] => 11\r\n    [details] => custom message for 400 http status code\r\n)\r\n```\r\n\r\nNow is not added in metadata but is added in code and details.\r\nIs this behaviour correct?\r\nOr I'm doing or understanding something wrong.\r\n\r\nI attach the headers example too (HTTP/1.1 request/response):\r\n\r\n```\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 10000 (#0)\r\n> POST /TestAPI/TestMethod HTTP/1.1\r\n> Host: localhost:10000\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 400 Bad Request\r\n< server: envoy\r\n< content-type: application/grpc+proto\r\n< content-length: 0\r\n< x-powered-by: PHP/7.2.15\r\n< grpc-message: custom message for 400 http status code\r\n< grpc-status: 11\r\n< cache-control: no-cache, private\r\n< date: Thu, 04 Apr 2019 10:52:22 GMT\r\n< access-control-allow-origin:\r\n< x-envoy-upstream-service-time: 1411\r\n<\r\n* Connection #0 to host localhost left intact\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2019-04-04T13:31:23Z",
        "body": "Ah yeah, looking over the code again we don't do anything fancy if it's a header only response, so it makes sense that grpc-status is propagated in that case. \r\n\r\nI would expect them to not show up in metadata since they are headers with special meaning in gRPC."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-11T14:14:47Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ],
    "satisfaction_conditions": [
      "Ability to set custom error messages in grpc-message header from upstream responses",
      "Clear mapping between HTTP status codes and gRPC status codes",
      "Proper propagation of grpc-status headers from HTTP/1.1 responses to gRPC clients",
      "Support for gRPC error handling standards without requiring gRPC semantics in upstream services",
      "Header-only response handling for error cases"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:33:36"
    }
  },
  {
    "number": 5359,
    "title": "[Question] Meaning of [C~] character included in connection log and stream log etc",
    "created_at": "2018-12-20T02:41:56Z",
    "closed_at": "2018-12-20T03:37:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5359",
    "body": "*Title*: *[Question] Meaning of [C~] character included in connection log and stream log etc*\r\n\r\n*Description*:\r\nYou could see a character string [C (number)] as follows in Envoy's log. Would you tell this meaning?\r\nI think logs with the same id are logs in the same connection since I recognize it's like a connection id, but is it right?\r\nThanks.\r\n\r\n```\r\n[2018-12-20 01:52:58.930][000011][debug][router] [source/common/router/router.cc:322] [C0][S539228372188944921] router decoding headers:\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5359/comments",
    "author": "nakabonne",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-12-20T03:30:54Z",
        "body": "Yes that's right, it's an internal connection ID."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that [C~] represents a connection identifier in Envoy's logs",
      "Clarification about log correlation within the same connection",
      "Authoritative validation of the identifier's purpose"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:34:32"
    }
  },
  {
    "number": 4672,
    "title": "[Question] /envoy_shared_memory_110 check user permissions. Error: File exists",
    "created_at": "2018-10-10T14:28:45Z",
    "closed_at": "2018-10-11T01:10:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4672",
    "body": "**Issue Template**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\nI started envoy with root user by mistake. I killed it.\r\nThen when i start envoy again with my own user, it says:\r\n[!184 06:42:56  ~]$ [2018-10-10 06:42:56.702][23046][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n\r\nWhich file should i remove before i start envoy with my own users? Thanks\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4672/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-10T14:30:15Z",
        "body": "Full screen output\r\n===============\r\n[!185 07:29:28  ~]$ /home/zapp/apps/envoy -c /home/zapp/apps/11/envoy/config.yaml --base-id 11 --v2-config-only\r\n[2018-10-10 07:29:31.869][26733][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n[2018-10-10 07:29:31.871][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:125] Caught Aborted, suspect faulting address 0x1f40000686d\r\n[2018-10-10 07:29:31.873][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:94] Backtrace thr<0> obj</lib64/libc.so.6> (If unsymbolized, use tools/stack_decode.py):\r\n[2018-10-10 07:29:31.887][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #0 0x7f4a4c829277 raise\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #1 0x7f4a4c82a967 abort\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:31.984][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #2 0x6cf1c4 Envoy::Server::SharedMemory::initialize()\r\n[2018-10-10 07:29:32.068][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #3 0x6cf8b2 Envoy::Server::HotRestartImpl::HotRestartImpl()\r\n[2018-10-10 07:29:32.153][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #4 0x48e3d7 Envoy::MainCommonBase::MainCommonBase()\r\n[2018-10-10 07:29:32.238][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #5 0x48e732 Envoy::MainCommon::MainCommon()\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #6 0x419905 main\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</lib64/libc.so.6>\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #7 0x7f4a4c815444 __libc_start_main\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<0> #8 0x484834 (unknown)\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:121] end backtrace thread 0\r\nAborted (core dumped)\r\n[!186 07:29:32 zapp@5.fet.stg.slv.zuora ~]$\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-10T16:30:01Z",
        "body": "`/dev/shm/envoy_shared_memory_110` on Linux."
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-11T01:10:40Z",
        "body": "@mattklein123   Thank you very much!!!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the exact shared memory file location created by the previous root-owned Envoy process",
      "Specifies the directory where shared memory files are stored in Linux systems"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:34:39"
    }
  },
  {
    "number": 4651,
    "title": "jwt-authn exception",
    "created_at": "2018-10-09T13:13:53Z",
    "closed_at": "2018-10-09T16:29:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4651",
    "body": "**Issue Template**\r\n\r\n*Title*: *jwt-authn exception*\r\n\r\n*Description*:\r\n>Im using the jwt-authn http-filter for validating JWT. Im using the version 2 API reference of envoy and the following envoy image version tag : fdfa5bde3343372ad662a830da0bdc3aea806f4d\r\n\r\n>Im getting following exception : \r\n[critical][main] source/server/server.cc:80] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.jwt_authn'\r\n\r\n*config*:\r\n```\r\n- name: envoy.jwt_authn\r\n            config:\r\n```\r\n      \r\nAm i using the wrong name ?\r\n\r\nthanks     \r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4651/comments",
    "author": "githubYasser",
    "comments": [
      {
        "user": "qiwzhang",
        "created_at": "2018-10-09T16:04:30Z",
        "body": "It should be:  envoy.filters.http.jwt_authn"
      },
      {
        "user": "githubYasser",
        "created_at": "2018-10-09T16:29:38Z",
        "body": "thank you , working now."
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct filter name format required by the Envoy version being used",
      "Addresses API version compatibility requirements for Envoy filters"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:34:46"
    }
  },
  {
    "number": 4640,
    "title": "How to run multi envoy process on one linux server",
    "created_at": "2018-10-08T16:30:21Z",
    "closed_at": "2018-10-10T14:19:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4640",
    "body": "*Title*: *One line description*\r\nHow to run multi envoy process on one linux server\r\n\r\n*Description*:\r\nI am not using docker container.\r\nI have the need to want to start two envoy processes on one linux server.\r\n\r\nSteps:\r\n1) create two folder on a centos server\r\n/root/envoy1 and /root/envoy2\r\n2) scp envoy binary (built on centos7) to the server under /root/envoy1 and /root/envoy2\r\n3) Prepare config.yaml files with diff ports \r\n4) Start envoy from folder 1. it works well\r\n/root/envoy1/envoy -c /root/envoy1/config.yaml --service-cluster myapp1 --service-node myapp1 \r\n5) Start envoy from folder 2. Nothing happens. And there is no screen output and log for further information\r\n/root/envoy2/envoy -c /root/envoy2/config.yaml --service-cluster myapp2 --service-node myapp2\r\n\r\nAny suggestion?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4640/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "moderation",
        "created_at": "2018-10-08T16:36:34Z",
        "body": "@huxiaobabaer from `envoy --help`. Works well and I run multiple Envoy's on a single host.\r\n\r\n```\r\n   --base-id <uint32_t>\r\n     base ID so that multiple envoys can run on the same host if needed\r\n```"
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-08T16:43:25Z",
        "body": "@moderation What is the default value of base id if not specified? TKS"
      },
      {
        "user": "moderation",
        "created_at": "2018-10-08T16:52:15Z",
        "body": "I don't believe there is a default. I start with 0 and count up from there. 0 to 4294967295 works."
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-08T16:59:25Z",
        "body": "Thank you very much for your help!!! @moderation "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to avoid port/resource conflicts between multiple Envoy processes",
      "Clarification of required unique identifiers for concurrent Envoy instances",
      "Non-containerized solution approach"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:34:58"
    }
  },
  {
    "number": 2638,
    "title": "mTLS and traffic split",
    "created_at": "2018-02-17T20:08:53Z",
    "closed_at": "2018-02-21T02:41:18Z",
    "labels": [
      "question",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2638",
    "body": "# Background\r\n\r\nI am preparing a traffic shift from a web API in a Swarm cluster (let's call it `swarm_api`) to the same API in a Kubernetes cluster (`k8s_api`). The two clusters are on distinct private networks. The API's clients are on the same private network as the Swarm cluster, making requests over HTTP. However, the Kubernetes cluster is isolated (and fully managed by a cloud provider; the nodes' network is abstracted); the connection must therefore be secured.\r\n\r\nMy plan is to add an edge envoy in the Swarm cluster (`swarm_proxy`) to split traffic between `swarm_api` and a second edge envoy in the Kubernetes cluster (`k8s_proxy`), which would route traffic to `k8s_api`. The connection between `swarm_proxy` and `k8s_proxy` must be secured with mutual TLS as it goes through the Internet.\r\n\r\n```\r\nPrivate network 1\r\n                      (swarm_proxy)---HTTP--->(swarm_api)\r\n____________________________|__________________________\r\nInternet                    |\r\n                          HTTPS\r\n____________________________|__________________________\r\nPrivate network 2           |\r\n                            \u2304\r\n                       (k8s_proxy)----HTTP---->(k8s_api)\r\n```\r\n\r\n# What I have so far\r\n\r\n1) One the one hand, I have successfully prototyped a traffic split over HTTP, *without* mTLS:\r\n    ```\r\n    Private network 1\r\n                          (swarm_proxy)---HTTP--->(swarm_api)\r\n    ____________________________|__________________________\r\n    Internet                    |\r\n                              HTTP\r\n    ____________________________|__________________________\r\n    Private network 2           |\r\n                                \u2304\r\n                           (k8s_proxy)----HTTP---->(k8s_api)\r\n    ```\r\n    `swarm_proxy` uses an `http_connection_manager` to split traffic between `swarm_api` and `k8s_proxy`; `k8s_proxy` simply routes traffic to `k8s_api` (see configurations below).\r\n2) On the other hand, I have successfully prototyped simple routing with mTLS:\r\n    ```\r\n    Private network 1\r\n                          (swarm_proxy)\r\n    ____________________________|__________________________\r\n    Internet                    |\r\n                              HTTPS\r\n    ____________________________|__________________________\r\n    Private network 2           |\r\n                                \u2304\r\n                           (k8s_proxy)----HTTP---->(k8s_api)\r\n    ```\r\n    `swarm_proxy` uses a `tcp_proxy` to route traffic to `k8s_proxy`; the envoy _cluster_ adds a `tls_context` including a client certificate; `k8s_proxy` requires TLS with client certificate, then routes traffic to `k8s_api` (see configurations below).\r\n\r\n# Issue\r\n\r\nNow I'm struggling to make both traffic split and mTLS work at the same time: if I try to combine the two approaches in a third prototype, `swarm_proxy` returns 301 Moved Permanently when it routes traffic to `k8s_proxy` (see configurations below).\r\n\r\nIs there something I'm missing / don't understand, or is this a bug?\r\n\r\n# Envoy Configurations\r\n\r\n## Prototype 1: traffic split\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  weighted_clusters:\r\n                    runtime_key_prefix: routing.traffic_split.api\r\n                    clusters:\r\n                    - name: swarm_api\r\n                      weight: 50\r\n                    - name: k8s_proxy\r\n                      weight: 50\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: swarm_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: swarm_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin:\r\n  access_log_path: \"/dev/stdout\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\n```\r\n\r\n```yaml\r\n# k8s_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: k8s_api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  cluster: k8s_api\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: k8s_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin: {...}\r\n```\r\n\r\n## Prototype 2: mutual TLS\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.tcp_proxy\r\n        config:\r\n          stat_prefix: ingress\r\n          cluster: k8s_proxy\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 443\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /etc/certs/ca.crt.pem\r\n        tls_certificates:\r\n        - certificate_chain:\r\n            filename: /etc/certs/swarm_proxy.crt.pem\r\n          private_key:\r\n            filename: /etc/certs/swarm_proxy.key.pem\r\nadmin: {...}\r\n```\r\n\r\n```yaml\r\n# k8s_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: k8s_api\r\n              domains:\r\n              - \"*\"\r\n              require_tls: ALL\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  cluster: k8s_api\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n      tls_context:\r\n        require_client_certificate: true\r\n        common_tls_context:\r\n          validation_context:\r\n            trusted_ca:\r\n              filename: /etc/certs/ca.crt.pem\r\n          tls_certificates:\r\n          - certificate_chain:\r\n              filename: /etc/certs/k8s_proxy.crt.pem\r\n            private_key:\r\n              filename: /etc/certs/k8s_proxy.key.pem\r\n  clusters:\r\n  - name: k8s_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin: {...}\r\n```\r\n\r\n## Prototype 3: traffic split and mutual TLS (NOT WORKING)\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  weighted_clusters:\r\n                    runtime_key_prefix: routing.traffic_split.api\r\n                    clusters:\r\n                    - name: swarm_api\r\n                      weight: 50\r\n                    - name: k8s_proxy\r\n                      weight: 50\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: swarm_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: swarm_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 443\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /etc/certs/ca.crt.pem\r\n        tls_certificates:\r\n        - certificate_chain:\r\n            filename: /etc/certs/swarm_proxy.crt.pem\r\n          private_key:\r\n            filename: /etc/certs/swarm_proxy.key.pem\r\nadmin: {...}\r\n```\r\n\r\n```yaml\r\n# k8s_proxy : same as prototype 2\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2638/comments",
    "author": "adrienjt",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2018-02-21T01:44:44Z",
        "body": "You need to either add `use_remote_address: true` to `k8s_proxy` or remove `require_tls: ALL` from it.\r\n\r\nIf you're using `use_remote_address: false` (default), then `k8s_proxy` is going to receive and accept `X-Forwarded-Proto: http` from `swarm_proxy` and reject client's HTTP request, since it doesn't fulfill the `require_tls` restriction, which applies to client's HTTP request and not to the connection to `k8s_proxy`, `require_client_certificate: true` is enough to enforce mTLS between the proxies.\r\n\r\nAlso, you should add `use_remote_address: true` to `swarm_proxy`, since it's acting as an edge proxy."
      },
      {
        "user": "adrienjt",
        "created_at": "2018-02-21T02:41:00Z",
        "body": "Thanks for the solution *and explanations* @PiotrSikora! That worked."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to reconcile TLS requirements between proxies with traffic splitting logic",
      "Clarification of Envoy's header handling between proxies in mTLS scenarios",
      "Guidance on proper proxy role configuration (edge vs internal)",
      "Clear separation of TLS termination concerns from traffic management logic",
      "Validation that mTLS between proxies doesn't interfere with upstream HTTP routing"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:35:25"
    }
  },
  {
    "number": 2462,
    "title": "Question on license of envoy 1.3.0 dependencies",
    "created_at": "2018-01-26T16:24:25Z",
    "closed_at": "2018-01-30T14:53:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2462",
    "body": "I've a question on one of the envoy 1.3.0 dependencies: rapidjson \u00a01.1.0 \r\n\r\nIt's license states: \r\n\r\n**_If you have downloaded a copy of the RapidJSON source code from Tencent, please note that RapidJSON source code is licensed under the MIT License, except for the third-party components listed below which are subject to different license terms.\u00a0 Your integration of RapidJSON into your own projects may require compliance with the MIT License, as well as the other licenses applicable to the third-party components included within RapidJSON. To avoid the problematic JSON license in your own projects, it's sufficient to exclude the bin/jsonchecker/ directory, as it's the only code under the JSON license.\r\nA copy of the MIT License is included in this file._** \r\n\r\nSo my question is:  is the bin/jsonchecker/ directory included in the envoy 1.3.0 binary referenced as:\r\n\r\n**_proxy/envoy-1.3.0.tg: \r\nsize:2266298 \r\nobject_id:c10f7dcc-4010-4dfe-460a-250a0e1cde1 \r\nsha: 45d667aa64a876ab857853b112f065a8800d3161_**     ?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2462/comments",
    "author": "luisapace",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-01-26T16:29:54Z",
        "body": "@rshriram can you or someone else from IBM answer this? Thank you."
      },
      {
        "user": "rshriram",
        "created_at": "2018-01-27T03:30:44Z",
        "body": "Why ibm? \r\nEither way, Envoy was approved internally a year ago. "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-01-27T18:07:23Z",
        "body": "@rshriram because @luisapace works at IBM and has been emailing me. :)"
      },
      {
        "user": "luisapace",
        "created_at": "2018-01-29T09:15:23Z",
        "body": "I'm sorry, but really I do not understand your point... if I've a question on the build of Envoy (not produced by IBM), why do I have to write to IBM instead of the developers of that package that has produced that binary?  Could you please clarify? I've done this several times for other packages and always their developers have answered me. Please let me know if you know the answer to my question or not. Thanks a lot for your time and help. \r\n\r\n\r\n"
      },
      {
        "user": "rshriram",
        "created_at": "2018-01-29T20:25:52Z",
        "body": "Short answer: no.\r\nContents of bazel-envoy/external/com_github_tencent_rapidjson/bin/jsonchecker/ are\r\n```\r\nfail1.json   fail13.json  fail17.json  fail20.json  fail24.json  fail28.json  fail31.json  fail5.json   fail9.json   readme.txt   \r\nfail10.json  fail14.json  fail18.json  fail21.json  fail25.json  fail29.json  fail32.json  fail6.json   pass1.json   \r\nfail11.json  fail15.json  fail19.json  fail22.json  fail26.json  fail3.json   fail33.json  fail7.json   pass2.json   \r\nfail12.json  fail16.json  fail2.json   fail23.json  fail27.json  fail30.json  fail4.json   fail8.json   pass3.json   \r\n```\r\n\r\nwhich are not part of envoy binary."
      },
      {
        "user": "luisapace",
        "created_at": "2018-01-30T13:58:24Z",
        "body": "Great, thanks a lot for your help!"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-01-30T14:53:52Z",
        "body": "I think this answers your question so closing this off, but please reopen if I'm wrong!"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether the bin/jsonchecker directory is included in the final Envoy 1.3.0 binary",
      "Clarification about license compliance implications for the Envoy binary",
      "Evidence of binary composition analysis"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:35:35"
    }
  },
  {
    "number": 3819,
    "title": "improve syscall API wrappers",
    "created_at": "2018-07-09T20:24:01Z",
    "closed_at": "2020-01-23T17:10:39Z",
    "labels": [
      "tech debt",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3819",
    "body": "Per discussion in #3813 it'd be good if the doRead call didn't latch errno several levels away from where the actual syscall was performed.  We should change the APIs to return {rc, errnno} in some form to better futureproof in case the intermediate function calls add their own errno-corrupting debug logging.\r\n\r\nWe could also get really fancy and insist that errno is latched via fix_format fixes, as also discussed there.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3819/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "venilnoronha",
        "created_at": "2018-07-10T16:25:23Z",
        "body": "Hi @alyssawilk, I'd like to give it a try."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-10T17:11:50Z",
        "body": "@venilnoronha go for it!"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2018-07-25T00:25:50Z",
        "body": "Erm, looking at the already merged and outstanding PRs, the errno seems to be latched one level too high (inside functions calling syscall wrappers and not in the syscall wrappers themselves).\r\n\r\nIs there any reason not to do it in the syscall wrappers and return tuple from them?"
      },
      {
        "user": "venilnoronha",
        "created_at": "2018-07-25T00:50:43Z",
        "body": "@PiotrSikora that's next in the plan of action. Briefly, I wanted to start off one level higher to keep the PRs smaller."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-01-23T17:10:39Z",
        "body": "I think the final fix was solid, and we forgot to close this one out.  Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Errno must be captured at the syscall wrapper level",
      "API must return both return code and errno in a unified structure",
      "Solution must prevent future errno corruption from intermediate debug logging"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 00:36:35"
    }
  }
]