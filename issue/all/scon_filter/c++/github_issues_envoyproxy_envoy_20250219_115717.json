[
  {
    "number": 37478,
    "title": "Fine-tuning of happy eyeballs dns resolution",
    "created_at": "2024-12-03T09:22:31Z",
    "closed_at": "2024-12-18T14:33:53Z",
    "labels": [
      "question",
      "area/load balancing",
      "area/dns"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/37478",
    "body": "*Title*: *Fine-tuning of happy eyeballs dns resolution*\r\n\r\n*Description*:\r\n\r\nWith that configuration and clusterType \"LOGICAL_DNS\"\r\n\r\n```\r\n        caresDnsResolverConfig, err := anypb.New(&cares.CaresDnsResolverConfig{\r\n\t\tDnsResolverOptions: &configCoreV3.DnsResolverOptions{\r\n\t\t\tUseTcpForDnsLookups: true,\r\n\t\t},\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Println(\"error converting to Any: %v\\n\", err)\r\n\t}\r\n\tdnsResolverConfig := &configCoreV3.TypedExtensionConfig{\r\n\t\tName:        \"envoy.network.dns_resolver.cares\",\r\n\t\tTypedConfig: caresDnsResolverConfig,\r\n\t}\r\n\r\n\treturn &clusterV3.Cluster{\r\n\t\tName:                   clusterName,\r\n\t\tConnectTimeout:         durationpb.New(time.Duration(connectTimeout) * time.Second),\r\n\t\tClusterDiscoveryType:   &clusterV3.Cluster_Type{Type: clusterType},\r\n\t\tLoadAssignment:         BuildEndpoint(clusterName, host, port),\r\n\t\tDnsLookupFamily:        clusterV3.Cluster_ALL,\r\n\t\tCircuitBreakers:        circuitBreakers,\r\n\t\tTypedDnsResolverConfig: dnsResolverConfig,\r\n\t}\r\n}\r\n```\r\n\r\nWe are having issues to access a system that has both valid IPv6 and IPv4 addresses like www.google.com. We are trying to access it from a proxy that relies on Envoy and when source IP is V4 we are getting \r\n`upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: immediate connect error: Network is unreachable|remote address:[2404:6800:4004:818::2004]:443`\r\nand it it did not pick the IPv4 address. We have tried adding \"UseTcpForDnsLookups\" with no success. Could you help on that, how can we fine-tune happy eyeballs to pick IPv4 when source is IPv4 and to pick IPv6 when source is IPv6 in order to work for such dual-stack target systems? \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/37478/comments",
    "author": "VladislavAtanasov95",
    "comments": [
      {
        "user": "ggreenway",
        "created_at": "2024-12-03T17:15:06Z",
        "body": "cc @RyanTheOptimist"
      },
      {
        "user": "RyanTheOptimist",
        "created_at": "2024-12-05T02:43:01Z",
        "body": "Hm. I'm surprised. I would have expected that with `DnsLookupFamily` set to `ALL` that DNS would have returned both IPv4 and IPv6 addresses. Then the Happy Eyeballs code would make connection attempts for each address in the list until the a connection succeeded. Is that not what you're seeing? Do you have a log? "
      },
      {
        "user": "VladislavAtanasov95",
        "created_at": "2024-12-05T13:16:53Z",
        "body": "We sometimes receive\r\n\r\n< HTTP/1.1 503 Service Unavailable\r\n< content-length: 231\r\n< content-type: text/plain\r\n< date: Thu, 21 Nov 2024 13:43:13 GMT\r\n< upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: immediate connect error: Network is unreachable|remote address:[2404:6800:4004:818::2004]:443\r\n\r\nWhen the client service only has IPv4 and we are trying to reach google.com through our envoy-based proxy, sometimes it picks ipv6, sometime ipv4 of the target url.\n\n---\n\nThe Cluster type is actually \"STRICT_DNS\", not \"LOGICAL_DNS\"(my mistake) when we sporadically receive the error. I ran an example with static envoy locally \r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 2000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager \r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: http\r\n          route_config:\r\n            name: search_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/abv\"\r\n                route:\r\n                  cluster: abv\r\n                  host_rewrite_literal: www.abv.bg\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  cluster: google\r\n                  host_rewrite_literal: www.google.com\r\n          http_filters:\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n  clusters:\r\n  - name: google\r\n    connect_timeout: 10s\r\n    type: strict_dns\r\n    dns_lookup_family: ALL\r\n    load_assignment:\r\n      cluster_name: google\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.google.com\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.google.com\r\n  - name: abv\r\n    connect_timeout: 10s\r\n    type: logical_dns\r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: google\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.abv.bg\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.abv.bg\r\nadmin:\r\n  access_log_path: \"/dev/stdout\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 15000\r\n```\r\n\r\nWhen I change it from STRICT_DNS to LOGICAL_DNS, requests to google are working stable every time, but when it is STRICT_DNS sometimes I receive \r\n`upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: delayed connect error: 61`\r\n Does that make sense?\r\n"
      },
      {
        "user": "akhilsingh-git",
        "created_at": "2024-12-18T14:08:19Z",
        "body": "When Envoy is configured with dns_lookup_family: ALL and receives both IPv4 and IPv6 addresses for a host, it uses a \u201chappy eyeballs\u201d approach to try establishing a connection. This approach attempts to connect to IPv6 first, and if that doesn\u2019t succeed within a certain timeframe, it will try IPv4. However, depending on network conditions, default settings might lead to occasional failures if IPv6 is not truly reachable or if there\u2019s an immediate IPv6 route issue.\r\n\r\nKey Points to Consider:\r\n\t1.\tHappy Eyeballs Mechanism:\r\nBy default, Envoy implements a Happy Eyeballs algorithm (RFC 6555-like behavior) when dns_lookup_family: ALL is set. Envoy will attempt a connection to an IPv6 address first and then, after a delay (happy_eyeballs_connection_delay), attempt IPv4 if IPv6 hasn\u2019t connected. If IPv6 is immediately unreachable, you might see intermittent failures before Envoy attempts IPv4.\r\n\t2.\tStrict DNS vs. Logical DNS:\r\n\t\u2022\tSTRICT_DNS: Envoy continuously re-resolves DNS at runtime and uses all returned IPs (both v4 and v6) as load-balancing endpoints. This can lead to Envoy attempting IPv6 endpoints that aren\u2019t actually reachable, causing occasional errors.\r\n\t\u2022\tLOGICAL_DNS: Envoy resolves DNS once at startup and treats the resulting IP addresses as a logical group, typically sticking to a single address family more consistently. This often appears more stable for dual-stack hosts because Envoy is less aggressive in rotating through all endpoints, but it may not provide the same dynamic behavior as STRICT_DNS.\r\n\t3.\tFine-Tuning Happy Eyeballs Behavior:\r\nEnvoy\u2019s cluster configuration allows you to adjust the Happy Eyeballs timing. By default, the IPv4 connection attempt is delayed by a certain amount of time if IPv6 is available. If IPv6 repeatedly fails immediately, reducing this delay can help Envoy fall back to IPv4 faster, preventing those intermittent \u201cnetwork unreachable\u201d errors.\r\nIn the cluster configuration for your STRICT_DNS cluster, try setting:\r\n\r\nhappy_eyeballs_connection_delay: 50ms\r\n\r\nor another suitably small value. The default is 300ms, so lowering it can speed up IPv4 fallback.\r\nExample:\r\n\r\nclusters:\r\n- name: google\r\n  connect_timeout: 10s\r\n  type: STRICT_DNS\r\n  dns_lookup_family: ALL\r\n  happy_eyeballs_connection_delay: 50ms\r\n  load_assignment:\r\n    cluster_name: google\r\n    endpoints:\r\n      - lb_endpoints:\r\n          - endpoint:\r\n              address:\r\n                socket_address:\r\n                  address: www.google.com\r\n                  port_value: 443\r\n  transport_socket:\r\n    name: envoy.transport_sockets.tls\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n      sni: www.google.com\r\n\r\nWith this change, if IPv6 is unreachable, Envoy should fall back to IPv4 more quickly, reducing the frequency of those \u201cnetwork unreachable\u201d errors.\r\n\r\n\t4.\tIf You Don\u2019t Need IPv6:\r\nIf IPv6 connectivity is not required or not guaranteed from your environment, consider simplifying by using:\r\n\r\ndns_lookup_family: V4_ONLY\r\n\r\nThis will ensure Envoy only attempts IPv4 addresses, eliminating IPv6-related connect errors altogether.\r\n\r\n\t5.\tEnvironment Considerations:\r\nIf your source network is IPv4-only, and the destination returns both IPv6 and IPv4 addresses, Envoy will try IPv6 first (due to Happy Eyeballs). Ensuring proper IPv6 routing, or just disabling it if not needed, is often the simplest solution. If dual-stack support is truly required, fine-tuning happy_eyeballs_connection_delay is your best bet.\r\n\r\nSummary:\r\n\t\u2022\tUse dns_lookup_family: ALL for dual-stack, but tune happy_eyeballs_connection_delay to shorten the fallback time to IPv4.\r\n\t\u2022\tConsider switching from STRICT_DNS to LOGICAL_DNS if that yields more stable behavior in your environment.\r\n\t\u2022\tIf IPv6 isn\u2019t actually needed, use V4_ONLY to avoid complexity.\r\n\t\u2022\tAdjusting the cluster configuration, particularly happy_eyeballs_connection_delay, should help Envoy more reliably fall back to IPv4 when IPv6 fails, reducing the intermittent \u201cnetwork unreachable\u201d errors."
      },
      {
        "user": "VladislavAtanasov95",
        "created_at": "2024-12-18T14:33:53Z",
        "body": "Thank you for the detailed explanation, appreciate it, closing"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how Envoy's Happy Eyeballs implementation prioritizes IPv4/IPv6 addresses based on source IP capabilities",
      "Configuration guidance for adjusting connection attempt timing between address families",
      "Clarification of STRICT_DNS vs LOGICAL_DNS behavior in dual-stack environments",
      "Recommendations for handling immediate network unreachable errors during connection attempts",
      "Guidance on when to use V4_ONLY vs ALL DNS lookup families"
    ]
  },
  {
    "number": 32004,
    "title": "FQDN Filter",
    "created_at": "2024-01-24T12:01:04Z",
    "closed_at": "2024-04-15T12:02:35Z",
    "labels": [
      "question",
      "stale",
      "area/tcp_proxy"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/32004",
    "body": "*FQDN Filter*:\r\n\r\n*Description*:\r\nHello everyone,\r\nThe following question: Is it possible to implement a listener with an FQDN? I want Envoy to only forward traffic when Test.test.io is called. Currently, Envoy Proxy forwards all traffic that comes to port 443.\r\n\r\n```\r\n    -   connect_timeout: 5s\r\n        load_assignment:\r\n            cluster_name: ingress_https\r\n            endpoints:\r\n            -   lb_endpoints:\r\n                -   endpoint:\r\n                        address:\r\n                            socket_address:\r\n                                address: bla.bla.bla.io\r\n                                port_value: 443\r\n                                \r\n         name: ingress_https\r\n        per_connection_buffer_limit_bytes: 32768\r\n        type: strict_dns\r\n```\r\n\r\n```\r\n    -   address:\r\n            socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 443\r\n        filter_chains:\r\n        -   filters:\r\n            -   name: envoy.filters.network.tcp_proxy\r\n                typed_config:\r\n                    '@type': type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n                    access_log:\r\n                    -   name: envoy.access_loggers.file\r\n                        typed_config:\r\n                            '@type': type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n                            path: /var/log/envoy/ingress_https_access.log\r\n                    cluster: ingress_https\r\n                    stat_prefix: ingress_https\r\n        name: listener_ingress_https\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/32004/comments",
    "author": "eliassteiner",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2024-01-25T18:11:19Z",
        "body": "I'm not sure I understand your question.\r\n\r\nYour config has the listener accept connections on port 443 for any IPv4 assigned to the host (that's what 0.0.0.0 implies). A specific IP can be configured (e.g. 10.0.0.1), which would cause Envoy to only listen on that IP address (not, for example, 127.0.0.1). But most hosts only have 1 IP and localhost, and your DNS name presumably points at the IP address.\r\n"
      },
      {
        "user": "jewertow",
        "created_at": "2024-01-25T21:02:59Z",
        "body": "@eliassteiner you need tls_inspector in you listener:\r\n```\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    listener_filters:\r\n    - name: \"envoy.filters.listener.tls_inspector\"\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\r\n    filter_chains:\r\n    - filter_chain_match:\r\n        server_names: [\"test.test.io\"]\r\n      filters:\r\n      - name: envoy.filters.network.tcp_proxy\r\n        typed_config:\r\n          '@type': type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\r\n          ...\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-02-25T00:03:45Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "eliassteiner",
        "created_at": "2024-02-25T06:36:18Z",
        "body": "perfect thank you. but do this need a ssl certificate? i will check this option thank you"
      },
      {
        "user": "jewertow",
        "created_at": "2024-03-09T09:59:08Z",
        "body": "> but do this need a ssl certificate?\r\n\r\nNo"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2024-04-15T12:02:35Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Mechanism to filter traffic based on FQDN (Test.test.io) at the listener level",
      "Configuration that works with TLS traffic without requiring SSL certificates",
      "Use of TLS inspection to extract SNI (Server Name Indication) information",
      "Filter chain matching based on server name"
    ]
  },
  {
    "number": 29814,
    "title": "`RESPONSE_CODE` is always zero when added as a response header",
    "created_at": "2023-09-26T17:33:40Z",
    "closed_at": "2023-09-28T14:49:07Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29814",
    "body": "*Title*: `RESPONSE_CODE` is always zero when added as a response header\r\n\r\n*Description*:\r\nI am trying to add `RESPONSE_CODE` to the header of calls going through envoy, but have not been successful. This may sound like an odd request because the call already returns status, but we have a few microservices that are responsible for communicating with third-parties and proxying the response, and we want to be 100% sure the issue is not inside the microservice.\r\n\r\n*Repro steps*:\r\nI have crafted what I think is the simplest possible example where envoy is doing a direct response, and the response code header is still 0. I have tons of other examples but this is the smallest.\r\n```\r\nstatic_resources:\r\n  listeners:\r\n    - name: reverse_proxy\r\n      address:\r\n        socket_address:\r\n          protocol: TCP\r\n          address: 0.0.0.0\r\n          port_value: 10005\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.filters.network.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                route_config:\r\n                  name: local_route\r\n                  response_headers_to_add:\r\n                    - header:\r\n                        key: \"response-code\"\r\n                        value: \"%RESPONSE_CODE%\"\r\n                  virtual_hosts:\r\n                    - name: local_service\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match:\r\n                            prefix: \"/\"\r\n                          direct_response:\r\n                            status: 200\r\n                            body:\r\n                              inline_string: \"{true}\"\r\n                http_filters:\r\n                  - name: envoy.filters.http.router\r\n                    typed_config:\r\n                      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n\r\nAt this point I am thinking its just an edge-case problem, or I am missing something small but critical.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29814/comments",
    "author": "inssein",
    "comments": [
      {
        "user": "inssein",
        "created_at": "2023-09-26T21:52:32Z",
        "body": "I ended up getting it working via lua, but would be nice to keep it simpler :)\r\n\r\n```\r\nhttp_filters:\r\n  - name: lua_response_code\r\n    typed_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\r\n      default_source_code:\r\n        inline_string: |\r\n          function envoy_on_response(response_handle)\r\n            response_handle:headers():add(\"response-code\", response_handle:headers():get(\":status\"))\r\n          end\r\n```"
      },
      {
        "user": "StarryVae",
        "created_at": "2023-09-27T08:21:32Z",
        "body": "i think `RESPONSE_CODE` in `response_headers_to_add` is only supported after this patch #29028 , maybe you can update your Envoy version and try again."
      },
      {
        "user": "alyssawilk",
        "created_at": "2023-09-27T12:26:39Z",
        "body": "If this works with modern versions of Envoy and you think it merits backports, let us know!"
      },
      {
        "user": "inssein",
        "created_at": "2023-09-27T16:09:31Z",
        "body": "I just tried the latest dev build and it works! Not a huge rush for us and looks like this will make it out on the next release (2023/10/16)."
      }
    ],
    "satisfaction_conditions": [
      "Identifies why RESPONSE_CODE placeholder returns zero in response headers",
      "Provides a solution compatible with Envoy's header manipulation capabilities",
      "Confirms version compatibility for RESPONSE_CODE header functionality",
      "Addresses direct_response route configuration limitations",
      "Maintains response code integrity for third-party proxy verification"
    ]
  },
  {
    "number": 29315,
    "title": "Is it necessary for go extension route config to store the configuration of different plugins",
    "created_at": "2023-08-29T09:59:35Z",
    "closed_at": "2023-10-07T12:01:41Z",
    "labels": [
      "question",
      "stale",
      "area/golang"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29315",
    "body": "*Title*: *Is it necessary for go extension route config to store the configuration of different plugins?*\r\n\r\n*Description*:\r\n```\r\nmessage ConfigsPerRoute {\r\n  // Configuration of the Go plugin at the per-router or per-virtualhost level,\r\n  // keyed on the :ref:`plugin_name <envoy_v3_api_field_extensions.filters.http.golang.v3alpha.Config.plugin_name>`\r\n  // of the Go plugin.\r\n  //\r\n  map<string, RouterPlugin> plugins_config = 1;\r\n}\r\n```\r\nSince now getting per filter config by the http filter config name is supported, is it necessary for go extension route config to  store the configuration of different go filters? For example, if we have two go filters `go-basic-auth` and `go-rate-limit`, for `http_filters` and `typed_per_filter_config`, the config name can be the same, so the go filter can get its own route config.\r\n\r\n```\r\n          http_filters:\r\n          - name: go-basic-auth\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              ......\r\n          - name: go-rate-limit\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n              ......\r\n\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n              - name: host-one\r\n                domains:\r\n                  - \"*\"\r\n                routes:\r\n                  - match:\r\n                      prefix: \"/\"\r\n                    route:\r\n                      cluster: httpbin\r\n                    typed_per_filter_config:\r\n                      go-basic-auth:\r\n                        \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.ConfigsPerRoute\r\n                        config:\r\n                        ......\r\n                      go-rate-limit:\r\n                        \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.ConfigsPerRoute\r\n                        config:\r\n                        ......\r\n```\r\n\r\nIn this way, the go filter will just be like a c++ filter, every go filter has its own route config, which maybe simple for control plane.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29315/comments",
    "author": "StarryVae",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2023-08-29T10:00:00Z",
        "body": "cc @doujiang24 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T00:50:34Z",
        "body": "@StarryVae Sorry, I may not understand you correctly. For now, every go filter has its own per route config is already supported. Now, as you said `ConfigsPerRoute` store per route configs for each golang filters, indexed by their plugin name, and each golang filter could get this own per route config by the plugin name.\r\n\r\nCould you please describe the changes that you sugguested? I haven't see what do we need to change. maybe a little design doc? Thanks."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T02:09:43Z",
        "body": "Ah, what i mean is that golang filter route config may not need store per route configs for each golang filters, it could only be its own route config just like c++ filter:\r\n\r\n```\r\nmessage ConfigPerRoute {\r\n  oneof override {\r\n    option (validate.required) = true;\r\n\r\n    // [#not-implemented-hide:]\r\n    // Disable the filter for this particular vhost or route.\r\n    // If disabled is specified in multiple per-filter-configs, the most specific one will be used.\r\n    bool disabled = 1 [(validate.rules).bool = {const: true}];\r\n\r\n    // The config field is used for setting per-route and per-virtualhost plugin config.\r\n    google.protobuf.Any config = 2;\r\n  }\r\n}\r\n```"
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T03:59:07Z",
        "body": "Oh, I see, your suggestion is not about lack of capability, but about implementation of capability.\r\n\r\nWe have also considered the way that you suggested, which is also a attractive solution, it's simper.\r\nHowever, we finally chose the current implementation way, since we do think it is more controllable for the golang plugin ecosystem, for example, the plugin name does not need to care about the conflicting about the existing name."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T06:26:41Z",
        "body": "`for example, the plugin name does not need to care about the conflicting about the existing name.`\r\n\r\ndoes the plugin name refer to http filter name `go-basic-auth` above? if so, there may be conflicts on route config, but it can be avoided by control plane. Anyway, the design now is also fine, just a littile bit complex. \ud83d\ude04 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T11:00:02Z",
        "body": "nope, each golang filter/plugin has its own plugin name, which is defined in `Golang.Config` proto:\r\n```\r\n  string plugin_name = 3 [(validate.rules).string = {min_len: 1}];\r\n```\r\nalso golang source code register golang plugin with that name, see: `RegisterHttpFilterConfigFactoryAndParser`.\r\nit's another name compare to the http filter name, so that we do not need to care about the confliction.\r\n\r\nyep, they both should works, we choose to introduce the new plugin name, since we think it might be more controllable, as said above."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-30T11:27:15Z",
        "body": "emm,,, if the plugin name refers to `plugin_name` in `Golang.Config`, then i am confused about what confliction it can solve? \ud83d\ude15 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-30T13:59:17Z",
        "body": "after discussed in wechat, summary a bit here:\r\nin the current implementation, introduce a new `plugin_name`, so that the per route config won't rely on the http filter name.\r\neven two golang plugins/filters in the http filter chain, has the same http filter name, the two plugins still have their own per route plugin config, since the keys in the `ConfigPerRoute. plugins_config ` are different.\r\n\r\non the other hand, rely on the http filter name, is also a workable solution, and it is simpler."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-31T07:13:34Z",
        "body": "yes, but how can we ensure the keys in the `ConfigPerRoute. plugins_config`  are different? \ud83d\ude15 \n\n---\n\nas we discussed in wechat, the `plugin name` should be better suited for resolving potential routing configuration conflicts, thanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2023-10-07T12:01:40Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why a separate plugin_name-based configuration storage is necessary despite potential complexity",
      "Clarification on how configuration conflicts are prevented between multiple Go filters",
      "Comparison of tradeoffs between http filter name-based vs plugin_name-based configuration approaches",
      "Demonstration that each Go filter can maintain isolated route configurations without interference"
    ]
  },
  {
    "number": 29260,
    "title": "should go extension support header operation in body phase?",
    "created_at": "2023-08-25T09:40:15Z",
    "closed_at": "2023-08-28T06:52:15Z",
    "labels": [
      "enhancement",
      "question",
      "area/go"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/29260",
    "body": "*Title*: *should go extension support header operation in body phase?*\r\n\r\n*Description*:\r\ninspired by #28587 , should go extension also support header operation in body phase? since some filters may need to change headers according to some content from body.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/29260/comments",
    "author": "StarryVae",
    "comments": [
      {
        "user": "StarryVae",
        "created_at": "2023-08-25T09:40:37Z",
        "body": "cc @doujiang24 "
      },
      {
        "user": "doujiang24",
        "created_at": "2023-08-26T01:18:28Z",
        "body": "yep, I do think the golang extension already support it.\r\nGo could return `StopAndBufferWatermark` in `DecodeHeaders`, then Go will receive data in `DecodeData` and could still write headers before invoke `Continue`.\r\nGolang extension is designed to be able do the same thing(almostly) align to C++."
      },
      {
        "user": "StarryVae",
        "created_at": "2023-08-28T06:51:46Z",
        "body": "oh, i see, we can store the headers in the filter object just like what C++ do \ud83e\udd23 ."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that header operations in the body phase are supported in the Go extension",
      "Demonstration of parity with C++ filter capabilities for header-body interaction",
      "Mechanism to modify headers after accessing body data"
    ]
  },
  {
    "number": 28386,
    "title": "Connection draining on SDS update",
    "created_at": "2023-07-13T15:57:15Z",
    "closed_at": "2023-07-17T10:09:08Z",
    "labels": [
      "question",
      "api"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28386",
    "body": "In case I'm using LDS, where some of the filter chains in the listener config have SDS config (downstream transport TLS that uses SDS to fetch secrets). After a while, the certificate is refreshed from the SDS. Does it cause connections that currently use the filter chain to drain? Will only new connections get the new certificate?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28386/comments",
    "author": "ohadvano",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2023-07-14T17:04:59Z",
        "body": "cc @adisuissa "
      },
      {
        "user": "soulxu",
        "created_at": "2023-07-17T09:51:52Z",
        "body": "there is no draining, only the new connection will use the new certificate"
      },
      {
        "user": "ohadvano",
        "created_at": "2023-07-17T10:09:08Z",
        "body": "Thanks"
      }
    ],
    "satisfaction_conditions": [
      "Clarify whether existing connections are terminated or allowed to complete when SDS-updated certificates are applied",
      "Confirm whether certificate updates are applied only to new connections",
      "Address the relationship between SDS updates and connection lifecycle management"
    ]
  },
  {
    "number": 28293,
    "title": "Is dispatcher thread-local?",
    "created_at": "2023-07-08T03:26:32Z",
    "closed_at": "2023-08-18T04:01:27Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/28293",
    "body": "That is, could I call `dispatcher.post()` from another thread? I need to put a task into the worker thread from another thread.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/28293/comments",
    "author": "kingluo",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2023-07-08T20:51:39Z",
        "body": "Yes, that's a normal way how thread local storage works, it posts from main thread to worker threads with an updated value."
      },
      {
        "user": "kingluo",
        "created_at": "2023-07-09T04:14:14Z",
        "body": "> Yes, that's a normal way how thread local storage works, it posts from main thread to worker threads with an updated value.\r\n\r\nThanks for your reply. And I just confirmed this fact in my coding. BTW, you mean the main thread posts xds update to the worker thread, right?\r\n\r\nAnother side question is whether the post is done by a locked queue and eventfd in Linux, and the worker thread will handle the post callbacks in batch, right?"
      },
      {
        "user": "kyessenov",
        "created_at": "2023-07-11T21:31:24Z",
        "body": "I think it's edge triggered. It's using libevent for event management, but I don't recall the details."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that dispatcher.post() can be safely called across threads",
      "Explanation of the synchronization mechanism between threads",
      "Description of event management architecture"
    ]
  },
  {
    "number": 26885,
    "title": "invalid value Invalid type URL, unknown type",
    "created_at": "2023-04-24T03:52:37Z",
    "closed_at": "2023-04-25T00:54:17Z",
    "labels": [
      "question",
      "area/golang"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/26885",
    "body": "*Title*: *Configuration file running error*\r\n\r\n*Description*:\r\nI plan to use Golang to develop an HTTP filter plugin for Envoy, but after building it locally, there was an error running the golang example in Repo\r\n\r\nCompilation and operation process\r\n>\r\n>- Building the environment\r\n>   - Apple M1\r\n>   - macos 13.3.1 (22E261)\r\n>   - envoy (v1.26.0)\r\n>\r\n>- Build Command\r\n>\r\n>   ```sh\r\n>    bazel build -c dbg --spawn_strategy=local //source/exe:envoy-static \r\n>    ```\r\n>\r\n>   ```sh\r\n>    go build -o simple.so -buildmode=c-shared .\r\n>    ```\r\n>\r\n>- Execute Command\r\n>\r\n>   ```sh\r\n>    ../../bazel-bin/source/exe/envoy-static -c ./envoy.yaml\r\n>    ```\r\n>\r\nError message output by the terminal\r\n>\r\n>- Command execution error\r\n>\r\n>  ```shell\r\n>  [2023-04-24 11:42:08.658][161426][info][main] [source/server/server.cc:408]   envoy.transport_sockets.downstream: envoy.transport_sockets.alts, envoy.transport_sockets.quic, envoy.transport_sockets.raw_buffer, envoy.transport_sockets.starttls, envoy.transport_sockets.tap, envoy.transport_sockets.tcp_stats, envoy.transport_sockets.tls, raw_buffer, starttls, tls\r\n>  [2023-04-24 11:42:08.658][161426][info][main] [source/server/server.cc:408]   envoy.path.rewrite: envoy.path.rewrite.uri_template.uri_template_rewriter\r\n>  [2023-04-24 11:42:08.658][161426][info][main] [source/server/server.cc:408]   envoy.matching.network.input: envoy.matching.inputs.application_protocol, envoy.matching.inputs.destination_ip, envoy.matching.inputs.destination_port, envoy.matching.inputs.direct_source_ip, envoy.matching.inputs.dns_san, envoy.matching.inputs.filter_state, envoy.matching.inputs.server_name, envoy.matching.inputs.source_ip, envoy.matching.inputs.source_port, envoy.matching.inputs.source_type, envoy.matching.inputs.subject, envoy.matching.inputs.transport_protocol, envoy.matching.inputs.uri_san\r\n>  [2023-04-24 11:42:08.682][161426][critical][main] [source/server/server.cc:131] error initializing configuration './envoy.yaml': Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[0].typed_config): invalid value Invalid type URL, unknown type: envoy.extensions.filters.http.golang.v3alpha.Config for type Any): {\"static_resources\":{\"clusters\":[{\"type\":\"STRICT_DNS\",\"name\":\"helloworld_service_cluster\",\"load_assignment\":{\"cluster_name\":\"helloworld_service_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"port_value\":8080,\"address\":\"helloworld_service\"}}}}]}]},\"lb_policy\":\"ROUND_ROBIN\"}],\"listeners\":[{\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typed_config\":{\"http_filters\":[{\"name\":\"envoy.filters.http.golang\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"library_id\":\"simple\",\"library_path\":\"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\",\"plugin_name\":\"simple\",\"plugin_config\":{\"value\":{\"prefix_localreply_body\":\"Configured local reply from go\"},\"@type\":\"type.googleapis.com/xds.type.v3.TypedStruct\"}}},{\"name\":\"envoy.filters.http.router\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"route_config\":{\"virtual_hosts\":[{\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"helloworld_service_cluster\"}}],\"domains\":[\"*\"],\"name\":\"local_service\"}],\"name\":\"local_route\"},\"stat_prefix\":\"ingress_http\"}}]}],\"address\":{\"socket_address\":{\"port_value\":10000,\"address\":\"0.0.0.0\"}},\"name\":\"listener_0\"}]}}\r\n>  [2023-04-24 11:42:08.684][161426][info][main] [source/server/server.cc:980] exiting\r\n>  Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[0].typed_config): invalid value Invalid type URL, unknown type: envoy.extensions.filters.http.golang.v3alpha.Config for type Any): {\"static_resources\":{\"clusters\":[{\"type\":\"STRICT_DNS\",\"name\":\"helloworld_service_cluster\",\"load_assignment\":{\"cluster_name\":\"helloworld_service_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"port_value\":8080,\"address\":\"helloworld_service\"}}}}]}]},\"lb_policy\":\"ROUND_ROBIN\"}],\"listeners\":[{\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typed_config\":{\"http_filters\":[{\"name\":\"envoy.filters.http.golang\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"library_id\":\"simple\",\"library_path\":\"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\",\"plugin_name\":\"simple\",\"plugin_config\":{\"value\":{\"prefix_localreply_body\":\"Configured local reply from go\"},\"@type\":\"type.googleapis.com/xds.type.v3.TypedStruct\"}}},{\"name\":\"envoy.filters.http.router\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"}}],\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"route_config\":{\"virtual_hosts\":[{\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"helloworld_service_cluster\"}}],\"domains\":[\"*\"],\"name\":\"local_service\"}],\"name\":\"local_route\"},\"stat_prefix\":\"ingress_http\"}}]}],\"address\":{\"socket_address\":{\"port_value\":10000,\"address\":\"0.0.0.0\"}},\"name\":\"listener_0\"}]}}\r\n>  ```\r\n>\r\n>- Error in verifying configuration command\r\n>\r\n>  ```shell\r\n>  \u279c  golang git:(v1.26.0) \u2717 ../../bazel-bin/source/exe/envoy-static --mode validate -c ./envoy.yaml\r\n>  [2023-04-24 11:42:37.440][161852][critical][main] [source/server/config_validation/server.cc:66] error initializing configuration './envoy.yaml': Unable to parse JSON as proto (INVALID_ARGUMENT:(http_filters[0].typed_config): invalid value Invalid type URL, unknown type: envoy.extensions.filters.http.golang.v3alpha.Config for type Any): {\"static_resources\":{\"clusters\":[{\"load_assignment\":{\"cluster_name\":\"helloworld_service_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"port_value\":8080,\"address\":\"helloworld_service\"}}}}]}]},\"lb_policy\":\"ROUND_ROBIN\",\"name\":\"helloworld_service_cluster\",\"type\":\"STRICT_DNS\"}],\"listeners\":[{\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"stat_prefix\":\"ingress_http\",\"http_filters\":[{\"typed_config\":{\"library_id\":\"simple\",\"plugin_config\":{\"value\":{\"prefix_localreply_body\":\"Configured local reply from go\"},\"@type\":\"type.googleapis.com/xds.type.v3.TypedStruct\"},\"library_path\":\"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\",\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\",\"plugin_name\":\"simple\"},\"name\":\"envoy.filters.http.golang\"},{\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"},\"name\":\"envoy.filters.http.router\"}],\"route_config\":{\"name\":\"local_route\",\"virtual_hosts\":[{\"domains\":[\"*\"],\"name\":\"local_service\",\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"helloworld_service_cluster\"}}]}]}}}]}],\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":10000}},\"name\":\"listener_0\"}]}}\r\n>  ```\r\n>\r\nconfiguration file\r\n>\r\n>```yaml\r\n># envoy demo with golang extension enabled\r\n>static_resources:\r\n>  listeners:\r\n>  - name: listener_0\r\n>    address:\r\n>      socket_address:\r\n>        address: 0.0.0.0\r\n>        port_value: 10000\r\n>    filter_chains:\r\n>    - filters:\r\n>      - name: envoy.filters.network.http_connection_manager\r\n>        typed_config:\r\n>          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n>          stat_prefix: ingress_http\r\n>          http_filters:\r\n>          - name: envoy.filters.http.golang\r\n>            typed_config:\r\n>              \"@type\": type.googleapis.com/envoy.extensions.filters.http.golang.v3alpha.Config\r\n>              library_id: simple\r\n>              library_path: \"/Users/mu/Downloads/attains/be/envoy/examples/golang/simple/simple.so\"\r\n>              plugin_name: simple\r\n>              plugin_config:\r\n>                \"@type\": type.googleapis.com/xds.type.v3.TypedStruct\r\n>                value:\r\n>                  prefix_localreply_body: \"Configured local reply from go\"\r\n>          - name: envoy.filters.http.router\r\n>            typed_config:\r\n>              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n>          route_config:\r\n>            name: local_route\r\n>            virtual_hosts:\r\n>            - name: local_service\r\n>              domains: [\"*\"]\r\n>              routes:\r\n>              - match:\r\n>                  prefix: \"/\"\r\n>                route:\r\n>                  cluster: helloworld_service_cluster\r\n>  clusters:\r\n>  - name: helloworld_service_cluster\r\n>    type: STRICT_DNS\r\n>    lb_policy: ROUND_ROBIN\r\n>    load_assignment:\r\n>      cluster_name: helloworld_service_cluster\r\n>      endpoints:\r\n>      - lb_endpoints:\r\n>        - endpoint:\r\n>            address:\r\n>              socket_address:\r\n>                address: helloworld_service\r\n>                port_value: 8080\r\n>```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/26885/comments",
    "author": "lyonmu",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-04-24T07:00:32Z",
        "body": "golang filter is only provided by the binary with contrib extensions. You may need to build envoy binary with target `//contrib/exe:envoy-static` ."
      },
      {
        "user": "lyonmu",
        "created_at": "2023-04-24T07:49:59Z",
        "body": "Hi @wbpcode \r\nI attempted to use the following command for the build operation, but a new error occurred.\r\n>\r\n>\r\n>```shell\r\n>\u279c  envoy git:(v1.26.0) \u2717 bazel build -c dbg --spawn_strategy=local //contrib/exe:envoy-static                                                     \r\n>ERROR: Target //contrib/exe:envoy-static is incompatible and cannot be built, but was explicitly requested.\r\n>Dependency chain:\r\n>    //contrib/exe:envoy-static (e0803d)\r\n>    //contrib/vcl/source:config_envoy_extension (e0803d)\r\n>    //contrib/vcl/source:config (e0803d)\r\n>    //contrib/vcl/source:vcl_interface_lib (e0803d)\r\n>    //contrib/vcl/source:vpp_vcl (e0803d)\r\n>    //contrib/vcl/source:external/libsvm.a (e0803d)\r\n>    //contrib/vcl/source:build (e0803d)   <-- target platform (@envoy//bazel:macos_arm64) didn't satisfy constraint @platforms//os:linux\r\n>INFO: Elapsed time: 0.128s\r\n>INFO: 0 processes.\r\n>FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n>```"
      },
      {
        "user": "wbpcode",
        "created_at": "2023-04-24T08:27:46Z",
        "body": "Lots of contrib extensions have constraints to the os/platform. You may need comment these extenstions in the `contrib_build_config.bzl`."
      },
      {
        "user": "lyonmu",
        "created_at": "2023-04-24T09:18:24Z",
        "body": "Thank you, I have successfully built the envoy locally."
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable building Envoy with Golang HTTP filter extension support on macOS ARM architecture",
      "Build configuration must exclude Linux-only contrib extensions when targeting macOS",
      "Final Envoy binary must recognize envoy.extensions.filters.http.golang.v3alpha.Config type",
      "Solution must maintain functionality to load Golang filter plugins via .so files"
    ]
  },
  {
    "number": 25938,
    "title": "Safe Regex not working for External Authorization Filter..",
    "created_at": "2023-03-06T17:39:16Z",
    "closed_at": "2023-03-07T05:35:45Z",
    "labels": [
      "question",
      "area/matching"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/25938",
    "body": "I do not want to apply external authorization filter for routes starting with /css, /img, /assets. While it is working fine if I put 3 entries using prefix but its not working with safe_regex.\r\n```\r\nstatic_resources:\r\n\r\n  listeners:\r\n  - name: listener_0\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 10000\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.filters.network.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n          - name: envoy.access_loggers.stdout\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: local_service\r\n              domains: [\"*\"]\r\n              typed_per_filter_config:\r\n                envoy.filters.http.ext_authz:\r\n                  \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute\r\n                  check_settings:\r\n                    context_extensions:\r\n                      virtual_host: local_service\r\n              routes:\r\n              - match:\r\n                  safe_regex:\r\n                    google_re2: {}\r\n                    regex: \"^/(css|img|assets)/\"\r\n                route:\r\n                  host_rewrite_literal: www.envoyproxy.io\r\n                  cluster: service_envoyproxy_io\r\n                typed_per_filter_config:\r\n                  envoy.filters.http.ext_authz:\r\n                    \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute\r\n                    disabled: true\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  host_rewrite_literal: www.envoyproxy.io\r\n                  cluster: service_envoyproxy_io    \r\n          http_filters:\r\n          - name: envoy.filters.http.ext_authz\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\r\n              grpc_service:\r\n                envoy_grpc:\r\n                  cluster_name: ext_authz-grpc-service\r\n                timeout: 0.250s\r\n              transport_api_version: V3\r\n          - name: envoy.filters.http.router\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n\r\n  clusters:\r\n  - name: service_envoyproxy_io\r\n    type: LOGICAL_DNS\r\n    dns_lookup_family: V4_ONLY\r\n    load_assignment:\r\n      cluster_name: service_envoyproxy_io\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: www.envoyproxy.io\r\n                port_value: 443\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\r\n        sni: www.envoyproxy.io\r\n\r\n  - name: ext_authz-grpc-service\r\n    type: STRICT_DNS\r\n    lb_policy: ROUND_ROBIN\r\n    typed_extension_protocol_options:\r\n      envoy.extensions.upstreams.http.v3.HttpProtocolOptions:\r\n        \"@type\": type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\r\n        explicit_http_config:\r\n          http2_protocol_options: {}\r\n    load_assignment:\r\n      cluster_name: ext_authz-grpc-service\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: 0.0.0.0\r\n                port_value: 7058\r\n```    ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/25938/comments",
    "author": "rakesh-eltropy",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2023-03-07T03:09:57Z",
        "body": "cc @rakesh-eltropy full match is used by the safe_regex matching here. So, may be `\"^/(css|img|assets)/.*\"` should be used here?"
      },
      {
        "user": "rakesh-eltropy",
        "created_at": "2023-03-07T05:35:45Z",
        "body": "Thanks @wbpcode. I was not aware that full match is used by safe_regex."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how Envoy's safe_regex matching requires full path matching",
      "Regex pattern that matches entire paths starting with /css, /img, or /assets",
      "Clarification on path matching semantics for route exclusions in Envoy filters"
    ]
  },
  {
    "number": 23487,
    "title": "Question about release: 1.22.3 2aca584: envoy/VERSION.txt => 1.22.3-dev",
    "created_at": "2022-10-13T19:53:54Z",
    "closed_at": "2022-10-14T12:22:56Z",
    "labels": [
      "question",
      "area/release"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23487",
    "body": "We use git submodule like this: git submodule status\r\n2aca584b3bca81622d3b009612f0c7be93eeea34 envoy (v1.22.3)\r\n\r\nBut envoy/VERSION.txt contains:\r\n1.22.3-dev\r\n\r\nThus, the RELEASE full string looks like this:\r\n\"version\": \"b87295fae172bc56584b65fdb05a05c31acd2ffa/1.22.3-dev/Clean/RELEASE/BoringSSL\",\r\n\r\n\"xxx-dev\" can be an issue for us if running in the PRODUCTION even though it's official release.  Anyway to overwrite this version txt?\r\n\r\nWe do have c++ filters internal to us, requests us to a full envoy build.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23487/comments",
    "author": "newwuhan5",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2022-10-14T08:49:42Z",
        "body": "this was a mistake - release 1.22.5 should have the correct versioning"
      },
      {
        "user": "newwuhan5",
        "created_at": "2022-10-14T12:23:39Z",
        "body": "We will upgrade to use release 1.22.25 then.  Thank you @phlax  for your quick response!"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to obtain an official Envoy release without '-dev' suffix in the version string",
      "Confirms availability of production-ready builds with stable version identifiers",
      "Identifies supported upgrade path for version consistency"
    ]
  },
  {
    "number": 23016,
    "title": "ECDS config source from path - discovery response format for resource ",
    "created_at": "2022-09-07T12:30:04Z",
    "closed_at": "2022-10-15T16:01:53Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/23016",
    "body": "Hi folks,\r\nI'm also trying to implement ECDS but the config should come from a file.\r\nI'm struggling to make it work... please help me with how to define the contents of the yaml file.\r\n\r\nthis config:\r\n```\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\nfails with:\r\n`Filesystem config update rejected: Unable to unpack as envoy.config.core.v3.TypedExtensionConfig: [type.googleapis.com/envoy.extensions.filters.http.router.v3.Router] `\r\n\r\n```\r\n          http_filters:\r\n          - name: router\r\n            config_discovery:\r\n              type_urls: [\"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"]\r\n              config_source:\r\n                path: /usr/local/bin/test-ecds-v1.yml\r\n              default_config:\r\n                \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/23016/comments",
    "author": "pxpnetworks",
    "comments": [
      {
        "user": "kyessenov",
        "created_at": "2022-09-07T16:18:05Z",
        "body": "The issue is that you need to wrap `Router` message into `TypedExtensionConfig` message. That means something like this:\r\n```yaml\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.core.v3.TypedExtensionConfig\r\n  name: router\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```"
      },
      {
        "user": "pxpnetworks",
        "created_at": "2022-09-08T08:27:09Z",
        "body": "Thank you @kyessenov , it is accepted now however i tried to add a second http filter (rbac) before router and still get errors loading both rbac and router.\r\nCan you help me with this once again? Thanks!\r\n\r\ncode:\r\n```\r\n- name: envoy.filters.http.rbac\r\n  typed_config:\r\n    \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\"\r\n    shadow_rules:\r\n      action: LOG\r\n      policies:\r\n        \"log\":\r\n          permissions: {any: true}\r\n          principals: {any: true}\r\n```\r\n```\r\nversion_info: \"100\"\r\nresources:\r\n- \"@type\": type.googleapis.com/envoy.config.core.v3.TypedExtensionConfig\r\n  name: router\r\n  typed_config:\r\n    \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n```\r\nhttp_filters:\r\n- name: router\r\n  config_discovery:\r\n    type_urls:\r\n    - \"type.googleapis.com/envoy.extensions.filters.http.rbac.v3.RBAC\"\r\n    - \"type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\"\r\n    config_source:\r\n      path: /usr/local/bin/test-ecds-v1.yml\r\n    default_config:\r\n      \"@type\": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router\r\n```\r\n\r\nBR,\r\nStoyan\n\n---\n\nFigured out I need a separate config_source file for each HTTP filter in the chain, hope that is how it is supposed to work.\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Clear explanation of how to properly structure ECDS configuration resources in YAML format",
      "Guidance on handling multiple HTTP filters in discovery configuration",
      "Explanation of resource separation requirements for different filter types"
    ]
  },
  {
    "number": 22372,
    "title": "How to get SSL socket file descriptor from eBPF uprobes on SSL_{read,write}",
    "created_at": "2022-07-22T21:13:46Z",
    "closed_at": "2022-09-02T04:31:01Z",
    "labels": [
      "question",
      "area/tls",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/22372",
    "body": "*Title*: *How to get SSL socket file descriptor from eBPF uprobes on SSL_{read,write}*\r\n\r\n*Description*:\r\n\r\nPixie uses eBPF to capture network traffic. When tracing Envoy, Pixie uses eBPF uprobes on SSL_{read,write} functions to trace the clear-text network traffic. Envoy uses SSL_{read,write} in a way that does not set file descriptor of the socket connection in the SSL struct. Where is the file descriptor get set and how to access them?\r\n\r\n[optional *Relevant Links*:]\r\npx.dev is the Pixie project website.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/22372/comments",
    "author": "yzhao1012",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2022-07-26T22:25:23Z",
        "body": "Take this with a grain of salt, since I've not puzzled all this out before, but as I understand the code:\r\n\r\nEnvoy is passing a boringssl `SSL*` to SSL_read/write and that struct is actually a `struct ssl_st*`. That `SSL*` contains `rbio` and `wbio` of type `bssl::UniquePtr<BIO>` which are used for reading and writing.\r\n\r\nEnvoy constructs the `BIO` struct (a `bio_st`) with a `ptr` initialized to an `Envoy::Newtork::IoHandle`, and `method` configured to use Envoy-provided functions for reading and writing (see `source/extensions/transport_sockets/tls/io_handle_bio.cc`).\r\n\r\nNormally, the `IoHandle` is an `Envoy::Network::IoHandleSocketHandeImpl` (`source/common/network/io_socket_handle_impl.h`) which holds the socket's file descriptor and provides methods to operate on it.\r\n"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2022-09-02T04:31:00Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      },
      {
        "user": "yzhao1012",
        "created_at": "2022-09-16T22:18:14Z",
        "body": "> Take this with a grain of salt, since I've not puzzled all this out before, but as I understand the code:\r\n> \r\n> Envoy is passing a boringssl `SSL*` to SSL_read/write and that struct is actually a `struct ssl_st*`. That `SSL*` contains `rbio` and `wbio` of type `bssl::UniquePtr<BIO>` which are used for reading and writing.\r\n> \r\n> Envoy constructs the `BIO` struct (a `bio_st`) with a `ptr` initialized to an `Envoy::Newtork::IoHandle`, and `method` configured to use Envoy-provided functions for reading and writing (see `source/extensions/transport_sockets/tls/io_handle_bio.cc`).\r\n> \r\n> Normally, the `IoHandle` is an `Envoy::Network::IoHandleSocketHandeImpl` (`source/common/network/io_socket_handle_impl.h`) which holds the socket's file descriptor and provides methods to operate on it.\r\n\r\nThis has been what we found as well. Thanks! "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to trace from SSL struct to Envoy's IoHandle implementation",
      "Identification of the connection path between SSL operations and underlying socket descriptors",
      "Method to access socket descriptor through Envoy's BIO abstraction layer",
      "Clarification of Envoy-specific SSL implementation details"
    ]
  },
  {
    "number": 21121,
    "title": "How envoy queues requests?",
    "created_at": "2022-05-03T06:06:46Z",
    "closed_at": "2024-06-26T13:38:23Z",
    "labels": [
      "question",
      "area/grpc"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/21121",
    "body": "In my case, there are multiple grpc endpoints, each of which can only process one request at a time. It may take several seconds to several minutes to process one request.\r\n\r\nWhat I need:\r\n\r\nEnvoy takes a bunch of requests, assigns one to each endpoint, and enqueues the rest. Once an endpoint finishes, envoy assigns the next in queue to this endpoint.\r\n\r\nI have looked into \"Circuit Breakers\", but it just fails the requests beyond max_requests. \r\n\r\n```\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 5\r\n          max_pending_requests: 20\r\n          max_requests: 5\r\n```\r\nUsing the config above, I send 10 requests, only first 5 are successful.\r\n\r\nI have also checked \"connection pool\" and tested max_concurrent_streams. It seems not relevant.\r\n\r\nI am new to envoy. Thanks if anyone could give a hint.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/21121/comments",
    "author": "exhau",
    "comments": [
      {
        "user": "wbpcode",
        "created_at": "2022-05-03T14:26:57Z",
        "body": "cc @alyssawilk "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-05-04T16:16:41Z",
        "body": "so the circuit breakers will hard fail if you go over the request limit.\r\nI _believe_ if you go over the connection limit it'll queue.\r\nso I think if you configure your connect limit at 1, and max concurrent streams at 1 you'd get the behavior you wanted.  Please give it a shot and let me know if it doesn't work."
      },
      {
        "user": "exhau",
        "created_at": "2022-05-04T17:10:48Z",
        "body": "thanks very much @alyssawilk. Your config works.\r\n\r\nmax_connections: 1\r\nmax_concurrent_streams: 1\r\n\r\nBut it seems, envoy assigns requests when they come, not assign when one endpoint becomes available. "
      },
      {
        "user": "alyssawilk",
        "created_at": "2022-05-04T20:46:47Z",
        "body": "Yeah, the queuing is done in the connection pool, not at the cluster level, which is sub-optimal for your use case.  I think your use case was one Envoy wasn't really designed for, but I think we'd welcome changes if you're game for queuing at a higher level."
      },
      {
        "user": "exhau",
        "created_at": "2024-06-23T12:02:36Z",
        "body": "got it. thanks!\n\n---\n\n``` \r\n    http2_protocol_options: \r\n      max_concurrent_streams: 1\r\n    circuit_breakers:\r\n      thresholds:\r\n        - max_connections: 1\r\n```\r\n \r\nIf \"one\" client sends a lot of requests simultaneously, this config works as expected. Each backend processes one request at a time. \r\n \r\nBut when a second client starts sending requests, the backend may process two requests at the same time.\r\n \r\nI wonder if it is by designed. Is there a way to achieve processing request one by one for each endpoint, when multiple clients are sending multiple request?\r\n\r\nThanks~"
      },
      {
        "user": "alyssawilk",
        "created_at": "2024-06-24T13:07:06Z",
        "body": "Do you perhaps have multiple worker threads?  These limits apply to each worker thread so my suspicion is Envoy is working as intended but you need to limit worker threads if you want to rate limit so much"
      },
      {
        "user": "exhau",
        "created_at": "2024-06-26T13:38:23Z",
        "body": "--concurrency 1\r\nsolved it. thank you again!"
      }
    ],
    "satisfaction_conditions": [
      "Solution must enable request queuing without hard failures when endpoints are busy",
      "Configuration must enforce strict 1-request-at-a-time processing per endpoint",
      "Mechanism must work across multiple clients/connections",
      "Solution must account for Envoy's threading model in resource allocation"
    ]
  },
  {
    "number": 20607,
    "title": "generate compile_commands.json fail",
    "created_at": "2022-03-31T06:02:12Z",
    "closed_at": "2022-03-31T11:29:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/20607",
    "body": "[Wed Mar 30 22:59:31][#120# ] (main)$./tools/gen_compilation_database.py \r\nERROR: /mnt/cache/_bazel_stack/5a617312af51e0dbefb3398d3212136c/external/base_pip3_sphinx/BUILD.bazel:22:11: no such package '@base_pip3_importlib_metadata//': The repository '@base_pip3_importlib_metadata' could not be resolved: Repository '@base_pip3_importlib_metadata' is not defined and referenced by '@base_pip3_sphinx//:pkg'\r\nERROR: Analysis of target '//tools/docs:sphinx_runner' failed; build aborted: \r\nINFO: Elapsed time: 1.008s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (3 packages loaded, 0 targets configured)\r\n    Fetching @emscripten_bin_linux; fetching\r\n    Fetching @base_pip3_pyparsing; fetching\r\n    Fetching @base_pip3_yarl; fetching\r\n    Fetching @base_pip3_python_gnupg; fetching\r\n    Fetching @base_pip3_multidict; fetching\r\n    Fetching @rust_linux_x86_64; fetching\r\n    Fetching @base_pip3_idna; fetching\r\n    Fetching @base_pip3_cffi; fetching\r\nTraceback (most recent call last):\r\n  File \"./tools/gen_compilation_database.py\", line 124, in <module>\r\n    fix_compilation_database(args, generate_compilation_database(args))\r\n  File \"./tools/gen_compilation_database.py\", line 20, in generate_compilation_database\r\n    subprocess.check_call([\"bazel\", \"build\"] + bazel_options + [\r\n  File \"/usr/lib/python3.8/subprocess.py\", line 364, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['bazel', 'build', '--config=compdb', '--remote_download_outputs=all', '--aspects=@bazel_compdb//:aspects.bzl%compilation_database_aspect', '--output_groups=compdb_files,header_files', '//source/...', '//test/...', '//tools/...', '//contrib/...']' returned non-zero exit status 1.\r\n\r\n\r\n[Wed Mar 30 22:59:46][#121# ] (main)$pip3 install importlib_metadata\r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.8/dist-packages (4.10.1)\r\nRequirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib_metadata) (1.0.0)\r\n\r\nBut I have already installed the pkg.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/20607/comments",
    "author": "zhangbo1882",
    "comments": [
      {
        "user": "zhxie",
        "created_at": "2022-03-31T06:10:10Z",
        "body": "You may require Python 3.10 to refresh compilation database."
      },
      {
        "user": "wbpcode",
        "created_at": "2022-03-31T08:06:23Z",
        "body": "cc @phlax "
      },
      {
        "user": "phlax",
        "created_at": "2022-03-31T09:10:16Z",
        "body": "> You may require Python 3.10 to refresh compilation database.\r\n\r\nyes, this is the issue - python3.10 _doesnt_ require `importlib-metadata` (and its deps) so it is not listed/pinned in the requirements file"
      },
      {
        "user": "zhangbo1882",
        "created_at": "2022-03-31T11:29:05Z",
        "body": "Thanks @zhxie.  It works now after I upgrade the python to 3.10. "
      }
    ],
    "satisfaction_conditions": [
      "Resolve Python version incompatibility causing missing dependencies",
      "Ensure environment meets Python version requirements for compilation database generation",
      "Address missing package errors caused by underlying environment configuration issues"
    ]
  },
  {
    "number": 18891,
    "title": "Can xds control plane be dynamically discovered after bootstrap?",
    "created_at": "2021-11-04T02:42:39Z",
    "closed_at": "2021-11-19T06:45:50Z",
    "labels": [
      "question",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18891",
    "body": "*Title*: *Can xds control plane be dynamically discovered after initial bootstrap?*\r\n\r\n*Description*:\r\nAll the Envoy config today define xds control plane cluster as static resource. Usually it is accessed by resolving a DNS name. Is it possible after the initial bootstrapping process, the xds control plane just push down its own endpoints to proxies? That is to dynamically override xds control plane's static cluster configuration. \r\n\r\nThanks.  ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18891/comments",
    "author": "manlinl",
    "comments": [
      {
        "user": "rojkov",
        "created_at": "2021-11-15T13:59:29Z",
        "body": "/cc @markdroth @htuch "
      },
      {
        "user": "htuch",
        "created_at": "2021-11-16T05:50:54Z",
        "body": "I have deja vu on this topic of dynamic clusters overriding static ones. I think we don't allow it due to complexity and confusion potential, @mattklein123 or @snowp  might remember better (it's come up a few times)."
      },
      {
        "user": "mattklein123",
        "created_at": "2021-11-16T15:28:02Z",
        "body": "> I have deja vu on this topic of dynamic clusters overriding static ones. I think we don't allow it due to complexity and confusion potential, @mattklein123 or @snowp might remember better (it's come up a few times).\r\n\r\nYeah agreed this has been discussed before, and I also agree that the complexity seems dubiously worth it. Is there some reason DNS is not a workable solution for you?"
      },
      {
        "user": "manlinl",
        "created_at": "2021-11-19T06:45:50Z",
        "body": "Thanks for the insight. DNS works for us. I was curious if current setup allows making XDS control plane part of the mesh itself.  "
      }
    ],
    "satisfaction_conditions": [
      "Clarification of whether Envoy allows dynamic updates to xDS control plane cluster configuration after bootstrap",
      "Explanation of architectural constraints preventing dynamic control plane discovery",
      "Identification of supported alternatives to achieve control plane availability goals",
      "Addressing the underlying need for control plane to be part of the mesh"
    ]
  },
  {
    "number": 18172,
    "title": "Question: Can envoy mirror tcp traffic when acting as a tcp proxy?",
    "created_at": "2021-09-18T12:01:51Z",
    "closed_at": "2021-09-22T04:49:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/18172",
    "body": "I'm running envoy in tcp proxy mode. I know envoy can mirror traffic when acting in http mode, but not sure if that is possible in tcp proxy mode. I don't see any documentation which suggests  mirroring is supported in tcp mode, but I still wanted to ask.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/18172/comments",
    "author": "samitpal",
    "comments": [
      {
        "user": "alyssawilk",
        "created_at": "2021-09-20T14:04:24Z",
        "body": "As far as I know that's not possible today."
      },
      {
        "user": "samitpal",
        "created_at": "2021-09-22T04:49:20Z",
        "body": "Ok, thanks! Closing the issue."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether Envoy supports traffic mirroring in TCP proxy mode"
    ]
  },
  {
    "number": 17540,
    "title": "Context management in request-response cycle",
    "created_at": "2021-07-29T18:42:11Z",
    "closed_at": "2021-08-09T17:21:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17540",
    "body": "*Title*: *Context management in request-response cycle*\r\n\r\n*Description*:\r\n\r\n> We are currently testing out Envoy's new ExternalProcessor filter type.  We are using this filter to mutate request headers and request bodies before they reach our downstream applications.  We are removing some of this data from the request and then hoping to re-access it on response.\r\n> \r\n> In development, our filter containers (we have tested with 3 containers running a Go application as our cluster) seems to be maintaining a single context object across the entire request-response lifecycle, which is not what we expected to have happen.  Though this makes our lives easier in some ways (we don't need an external cache to hold onto this data if it can be held in memory of a container), we are concerned that this is not expected behavior and could change in the future.  Since Go's Context library is not doing anything fancy behind the scenes, we have a sneaking suspicion that this peculiar behavior is Envoy-related.\r\n> \r\n> Is maintaining this context expected behavior for Envoy?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17540/comments",
    "author": "mdettelson",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-08-06T22:01:36Z",
        "body": "cc @gbrail "
      },
      {
        "user": "gbrail",
        "created_at": "2021-08-07T00:49:44Z",
        "body": "ext_proc starts a bidirectional gRPC stream for each HTTP request/response. (It's basically a long-running gRPC with data going back in forth in two ways.) \r\n\r\nIn go, you'd receive a single call to \"Process,\" and in there you read and write to the stream. Envoy doesn't know anything about Go contexts, but I assume that the Go gRPC code creates a single context and you'll use it as you interact with the stream.\r\n\r\nThis is on purpose, and it is indeed supposed to make things easier for implementers of external processors, since you can maintain state with the gRPC stream. (In Go you handle the whole stream from a single function, running in a single goroutine, so it's particularly easy.) In most cases you shouldn't need a separate state table or anything like that."
      },
      {
        "user": "mdettelson",
        "created_at": "2021-08-09T17:21:27Z",
        "body": "Awesome!  Thank you for clarifying that this behavior is expected :)"
      },
      {
        "user": "liu-cong",
        "created_at": "2024-09-26T19:17:20Z",
        "body": "Had the same question and found this super useful. Thanks!\r\n\r\nBTW is this documented anywhere? It will help future developers :)"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether Envoy's ExternalProcessor filter is designed to maintain a single context across the entire request-response lifecycle",
      "Explanation of the relationship between gRPC stream handling and context persistence in Envoy",
      "Assurance about the stability of this behavior in future Envoy versions",
      "Clarification of state management requirements for external processors"
    ]
  },
  {
    "number": 17476,
    "title": "Is there a command to view the configuration of eds?",
    "created_at": "2021-07-24T02:30:22Z",
    "closed_at": "2021-07-29T07:06:53Z",
    "labels": [
      "question",
      "area/xds",
      "area/admin"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17476",
    "body": "The configuration of endpoints cannot be found using `127.0.0.1:15000/config_dump`. Is there a command to see the configuration issued by eds?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17476/comments",
    "author": "zhangzerui20",
    "comments": [
      {
        "user": "ramaraochavali",
        "created_at": "2021-07-25T09:59:51Z",
        "body": "config_dump?include_eds will give eds details"
      },
      {
        "user": "htuch",
        "created_at": "2021-07-25T18:10:28Z",
        "body": "Also some of this information is available on the `/clusters` admin endpoint."
      },
      {
        "user": "zhangzerui20",
        "created_at": "2021-07-29T07:06:53Z",
        "body": " I got what I want through `config_dump?include_eds`"
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to retrieve EDS-specific configuration details",
      "Works with existing admin interface endpoints",
      "Includes cluster-level endpoint discovery information",
      "Allows programmatic access to configuration data"
    ]
  },
  {
    "number": 17353,
    "title": "Relationship between grpc service definition timeout and cluster definition connect_timeout",
    "created_at": "2021-07-14T22:12:20Z",
    "closed_at": "2021-07-16T15:36:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17353",
    "body": "In the configuration shown below there is timeout: 1s of grpc_service which is defined to call cluster: ext_authz which also has connect_timeout: 5s. Is there a relationship between those two timeouts? The grpc_service timeout is defined as \"the timeout for a specific request\", vs connect_timeout as \"The timeout for new network connections to hosts in the cluster\", so in case the grpc_service timeout is 'started' first, should it be larger or at least, equal to that of the cluster connect_timeout? \r\n\r\nIs there a best practice advice I can follow? For now, I have set both to the same value of 5s for testing, to be set via an environment variable to 3s in prod.\r\n\r\nthanks in advance,\r\nswav\r\n\r\n\"http_filters\": [\r\n             {\r\n              \"name\": \"envoy.filters.http.ext_authz\",\r\n              \"typed_config\": {\r\n               \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz\",\r\n               \"grpc_service\": {\r\n                \"envoy_grpc\": {\r\n                 \"cluster_name\": \"ext_authz\"\r\n                },\r\n                **\"timeout\": \"1s\"**\r\n               },\r\n               \"transport_api_version\": \"V3\"\r\n              }\r\n             },\r\n...\r\n\"dynamic_active_clusters\": [\r\n{\r\n     \"version_info\": \"1626296116754330624\",\r\n     \"cluster\": {\r\n      \"@type\": \"type.googleapis.com/envoy.config.cluster.v3.Cluster\",\r\n      \"name\": \"ext_authz\",\r\n      \"type\": \"LOGICAL_DNS\",\r\n      **\"connect_timeout\": \"5s\",**\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17353/comments",
    "author": "swav",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-07-16T13:56:02Z",
        "body": "The connect timeout is how long to wait for the connection to be established, while the timeout is how long to wait for the response for a given request. It probably makes sense to have timeout > connect_timeout"
      },
      {
        "user": "swav",
        "created_at": "2021-07-16T15:36:01Z",
        "body": "@snowp  Thank you very much for clarifying this. :)"
      }
    ],
    "satisfaction_conditions": [
      "Clarify the relationship between gRPC service request timeout and cluster connection timeout",
      "Provide guidance on relative timeout duration configuration",
      "Offer best practices for timeout configuration in this context"
    ]
  },
  {
    "number": 17012,
    "title": "Question: How to embed XDS client in Envoy",
    "created_at": "2021-06-16T16:25:07Z",
    "closed_at": "2021-07-24T00:01:42Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/17012",
    "body": "*Title*: *How to embed XDS client in Envoy*\r\n\r\n*Description*:\r\nWe are currently exploring possibilities of a file system based configuration reader which does the following:\r\n- Reads any updates to configuration from a  file/directory \r\n- Translates the configuration changes (whole or delta) to XDS\r\n- Speaks XDS to Envoy's XDS listener.\r\n\r\nHow can we achieve this within Envoy? One possibility is to launch a new thread for this which can setup a listener for file system. But this would be intrusive and change the threading model.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/17012/comments",
    "author": "conqerAtapple",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2021-06-16T20:02:06Z",
        "body": "Couldn't you just use file based xDS and have some other process write the config out?"
      },
      {
        "user": "conqerAtapple",
        "created_at": "2021-06-16T20:23:55Z",
        "body": "> Couldn't you just use file based xDS and have some other process write the config out?\r\n\r\nAbsolutely makes sense to have another process write the xDS config.  We were exploring if the client/user could be saved from writing the error prone configurations and let the translator do that. But i think this idea might bring in other issues. \r\nThanks for the answer. "
      }
    ],
    "satisfaction_conditions": [
      "Avoids modifying Envoy's internal threading model or core architecture",
      "Provides a way to automatically translate configuration changes to xDS format",
      "Maintains separation between configuration translation and Envoy's runtime"
    ]
  },
  {
    "number": 15869,
    "title": "Connecting with IP when the listener is configured with SNI",
    "created_at": "2021-04-07T13:10:16Z",
    "closed_at": "2021-04-20T01:59:49Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15869",
    "body": "We are PoC-ing envoy to use it as our load balancer, we are trying to utilize the SNI feature and it works perfectky fine.\r\n\r\nBut when we try to connect to IP of the listener that is configured with SNI, we get `no matching filter chain found` and the request fails.\r\n\r\nWe use curl with option `-k` to do this.\r\n\r\nBelow is the minimal configuration to do this\r\n\r\n```\r\n- \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener\r\n  name: demo-https\r\n  address:\r\n    socket_address:\r\n      address: 100.x.x.x\r\n      port_value: 443\r\n  listener_filters:\r\n  - name: \"envoy.filters.listener.tls_inspector\"\r\n    typed_config: {}\r\n  filter_chains:\r\n          #- use_proxy_proto: true\r\n  - filter_chain_match:\r\n      server_names: [\"*.example.net\", \"example.net\"]\r\n    transport_socket:\r\n      name: envoy.transport_sockets.tls\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\r\n        cluster: demo-https-cluster\r\n        common_tls_context:\r\n          tls_certificates:\r\n          - certificate_chain: { filename: \"/etc/certs/asterisk.example.net.chain\" }\r\n            private_key: { filename: \"/etc/certs/asterisk.example.net.key\" }\r\n    filters:\r\n    - name: envoy.filters.network.http_connection_manager\r\n      typed_config:\r\n        \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\r\n        stat_prefix: http\r\n        cluster: demo-https-cluster\r\n        rds:\r\n          route_config_name: demo-rds\r\n          config_source:\r\n            path: \"/etc/envoy/rds/demo-rds.yaml\"\r\n        access_log:\r\n        - name: log\r\n          typed_config:\r\n            \"@type\": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\r\n            path: \"/var/log/envoy.log\"\r\n            typed_json_format: *json_Format\r\n        http_filters:\r\n        - name: envoy.router\r\n          config: {}\r\n```\r\nMy questions are : \r\n\r\n1. Will connecting with IP work if SNI is enabled and specified?\r\n2. If it does not support, any suggestions to use multiple certificates like haproxy supports?\r\n\r\nThank you !",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15869/comments",
    "author": "VigneshSP94",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2021-04-08T17:11:29Z",
        "body": "I believe you'd either need to include the ip as one of the SNIs (assuming the client sets the IP as the SNI) or use a second filter chain that doesn't try to match on SNI."
      },
      {
        "user": "lambdai",
        "created_at": "2021-04-08T21:12:41Z",
        "body": "Or you can create another filter chain with empty `server_names` to expclity accept either no-SNI or `unknown-SNI`"
      },
      {
        "user": "VigneshSP94",
        "created_at": "2021-04-13T15:54:22Z",
        "body": "> Or you can create another filter chain with empty `server_names` to expclity accept either no-SNI or `unknown-SNI`\n\nThis worked, thanks for your humble help !"
      }
    ],
    "satisfaction_conditions": [
      "Explains how to handle client connections that don't provide SNI or provide an unrecognized SNI",
      "Provides a method to support multiple certificates without strict SNI matching requirements",
      "Clarifies the relationship between IP-based connections and SNI configuration in Envoy"
    ]
  },
  {
    "number": 15071,
    "title": "round robin load balancing issue on TCP_Proxy with envoy.filters.network.sni_cluster ",
    "created_at": "2021-02-17T08:54:27Z",
    "closed_at": "2021-04-29T08:01:18Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/15071",
    "body": "Currently I am using Istio to form a service mesh on 2 k8s clusters, say, clusterA and clusterB.  15443 port is used for cross cluster communication. i.e. in clusterA, we can access the service in clusterB through the mtls port 15443 on the istio ingressgateway of clusterB.   The problem is the traffic is not evenly distributed to the work load of the service in clusterB. \r\n\r\ne.g. \r\n\r\n kubectl logs test-deploy-6df899c68d-fm7h6  -c istio-proxy  --context dev-svc-cluster | grep \"GET\" | wc -l\r\n11659\r\n kubectl logs test-deploy-6df899c68d-sbswr  -c istio-proxy  --context dev-svc-cluster | grep \"GET\" | wc -l\r\n19837\r\n\r\nMay I know there is anything I can do to make  round robin load balancing work in my case?  Thanks.\r\n\r\n\r\nhere is the listener configuration for port 15443 of istio ingressgateway of clusterB:\r\n\r\n    {\r\n        \"name\": \"0.0.0.0_15443\",\r\n        \"address\": {\r\n            \"socketAddress\": {\r\n                \"address\": \"0.0.0.0\",\r\n                \"portValue\": 15443\r\n            }\r\n        },\r\n        \"filterChains\": [\r\n            {\r\n                \"filterChainMatch\": {\r\n                    \"serverNames\": [\r\n                        \"*.local\"\r\n                    ]\r\n                },\r\n                \"filters\": [\r\n                    {\r\n                        \"name\": \"envoy.filters.network.sni_cluster\"\r\n                    },\r\n                    {\r\n                        \"name\": \"envoy.filters.network.rbac\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.rbac.v3.RBAC\",\r\n                            \"rules\": {\r\n                                \"policies\": {\r\n                                    \"ns[istio-system]-policy[allow-ingress-gateway]-rule[0]\": {\r\n                                        \"permissions\": [\r\n                                            {\r\n                                                \"andRules\": {\r\n                                                    \"rules\": [\r\n                                                        {\r\n                                                            \"any\": true\r\n                                                        }\r\n                                                    ]\r\n                                                }\r\n                                            }\r\n                                        ],\r\n                                        \"principals\": [\r\n                                            {\r\n                                                \"andIds\": {\r\n                                                    \"ids\": [\r\n                                                        {\r\n                                                            \"any\": true\r\n                                                        }\r\n                                                    ]\r\n                                                }\r\n                                            }\r\n                                        ]\r\n                                    }\r\n                                }\r\n                            },\r\n                            \"statPrefix\": \"tcp.\"\r\n                        }\r\n                    },\r\n                    {\r\n                        \"name\": \"istio.stats\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                            \"typeUrl\": \"type.googleapis.com/envoy.extensions.filters.network.wasm.v3.Wasm\",\r\n                            \"value\": {\r\n                                \"config\": {\r\n                                    \"configuration\": {\r\n                                        \"@type\": \"type.googleapis.com/google.protobuf.StringValue\",\r\n                                        \"value\": \"{\\n  \\\"metrics\\\": [\\n    {\\n      \\\"dimensions\\\": {\\n        \\\"source_cluster\\\": \\\"node.metadata['CLUSTER_ID']\\\",\\n        \\\"destination_cluster\\\": \\\"upstream_peer.cluster_id\\\"\\n      }\\n    }\\n  ]\\n}\\n\"\r\n                                    },\r\n                                    \"root_id\": \"stats_outbound\",\r\n                                    \"vm_config\": {\r\n                                        \"code\": {\r\n                                            \"local\": {\r\n                                                \"inline_string\": \"envoy.wasm.stats\"\r\n                                            }\r\n                                        },\r\n                                        \"runtime\": \"envoy.wasm.runtime.null\",\r\n                                        \"vm_id\": \"tcp_stats_outbound\"\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                    },\r\n                    {\r\n                        \"name\": \"envoy.filters.network.tcp_proxy\",\r\n                        \"typedConfig\": {\r\n                            \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\r\n                            \"statPrefix\": \"BlackHoleCluster\",\r\n                            \"cluster\": \"BlackHoleCluster\",\r\n                            \"accessLog\": [\r\n                                {\r\n                                    \"name\": \"envoy.access_loggers.file\",\r\n                                    \"typedConfig\": {\r\n                                        \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n                                        \"path\": \"/dev/stdout\",\r\n                                        \"logFormat\": {\r\n                                            \"textFormat\": \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\"\r\n                                        }\r\n                                    }\r\n                                }\r\n                            ]\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\n        ],\r\n        \"listenerFilters\": [\r\n            {\r\n                \"name\": \"envoy.filters.listener.tls_inspector\",\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector\"\r\n                }\r\n            }\r\n        ],\r\n        \"trafficDirection\": \"OUTBOUND\",\r\n        \"accessLog\": [\r\n            {\r\n                \"name\": \"envoy.access_loggers.file\",\r\n                \"filter\": {\r\n                    \"responseFlagFilter\": {\r\n                        \"flags\": [\r\n                            \"NR\"\r\n                        ]\r\n                    }\r\n                },\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\",\r\n                    \"path\": \"/dev/stdout\",\r\n                    \"logFormat\": {\r\n                        \"textFormat\": \"[%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% \\\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\\\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\\n\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    },\r\n\r\nhere is cluster config\r\n\r\n    {\r\n        \"name\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\",\r\n        \"type\": \"EDS\",\r\n        \"edsClusterConfig\": {\r\n            \"edsConfig\": {\r\n                \"ads\": {},\r\n                \"resourceApiVersion\": \"V3\"\r\n            },\r\n            \"serviceName\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\"\r\n        },\r\n        \"connectTimeout\": \"10s\",\r\n        \"circuitBreakers\": {\r\n            \"thresholds\": [\r\n                {\r\n                    \"maxConnections\": 4294967295,\r\n                    \"maxPendingRequests\": 4294967295,\r\n                    \"maxRequests\": 4294967295,\r\n                    \"maxRetries\": 4294967295\r\n                }\r\n            ]\r\n        },\r\n        \"metadata\": {\r\n            \"filterMetadata\": {\r\n                \"istio\": {\r\n                    \"default_original_port\": 8000,\r\n                    \"services\": [\r\n                        {\r\n                            \"host\": \"test-svc.default.svc.cluster.local\",\r\n                            \"name\": \" test-svc\",\r\n                            \"namespace\": \"default\"\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        },\r\n        \"filters\": [\r\n            {\r\n                \"name\": \"istio.metadata_exchange\",\r\n                \"typedConfig\": {\r\n                    \"@type\": \"type.googleapis.com/udpa.type.v1.TypedStruct\",\r\n                    \"typeUrl\": \"type.googleapis.com/envoy.tcp.metadataexchange.config.MetadataExchange\",\r\n                    \"value\": {\r\n                        \"protocol\": \"istio-peer-exchange\"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    },\r\n\r\nhere is the end points configuration:\r\n\r\n    {\r\n        \"name\": \"outbound_.8000_._.test-svc.default.svc.cluster.local\",\r\n        \"addedViaApi\": true,\r\n        \"hostStatuses\": [\r\n            {\r\n                \"address\": {\r\n                    \"socketAddress\": {\r\n                        \"address\": \"192.168.241.194\",\r\n                        \"portValue\": 8000\r\n                    }\r\n                },\r\n                \"stats\": [\r\n                    {\r\n                        \"name\": \"cx_connect_fail\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"cx_total\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_error\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_success\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_timeout\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"rq_total\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"31\",\r\n                        \"name\": \"cx_active\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"31\",\r\n                        \"name\": \"rq_active\"\r\n                    }\r\n                ],\r\n                \"healthStatus\": {\r\n                    \"edsHealthStatus\": \"HEALTHY\"\r\n                },\r\n                \"weight\": 1,\r\n                \"locality\": {}\r\n            },\r\n            {\r\n                \"address\": {\r\n                    \"socketAddress\": {\r\n                        \"address\": \"192.168.249.65\",\r\n                        \"portValue\": 8000\r\n                    }\r\n                },\r\n                \"stats\": [\r\n                    {\r\n                        \"name\": \"cx_connect_fail\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"cx_total\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_error\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_success\"\r\n                    },\r\n                    {\r\n                        \"name\": \"rq_timeout\"\r\n                    },\r\n                    {\r\n                        \"value\": \"38\",\r\n                        \"name\": \"rq_total\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"30\",\r\n                        \"name\": \"cx_active\"\r\n                    },\r\n                    {\r\n                        \"type\": \"GAUGE\",\r\n                        \"value\": \"30\",\r\n                        \"name\": \"rq_active\"\r\n                    }\r\n                ],\r\n                \"healthStatus\": {\r\n                    \"edsHealthStatus\": \"HEALTHY\"\r\n                },\r\n                \"weight\": 1,\r\n                \"locality\": {}\r\n            }\r\n        ],\r\n        \"circuitBreakers\": {\r\n            \"thresholds\": [\r\n                {\r\n                    \"maxConnections\": 4294967295,\r\n                    \"maxPendingRequests\": 4294967295,\r\n                     \"maxRequests\": 4294967295,\r\n                    \"maxRetries\": 4294967295\r\n                },\r\n                {\r\n                    \"priority\": \"HIGH\",\r\n                    \"maxConnections\": 1024,\r\n                    \"maxPendingRequests\": 1024,\r\n                    \"maxRequests\": 1024,\r\n                    \"maxRetries\": 3\r\n                }\r\n            ]\r\n        }\r\n    },\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/15071/comments",
    "author": "debbyku",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2021-02-18T05:41:45Z",
        "body": "This config looks good.\r\ngrep GET at log file is vague. Is there any metric, graph or access log that can drill down to \"gateway - 192.168.241.194\" and \"gateway - 192.168.249.65\" ? "
      },
      {
        "user": "debbyku",
        "created_at": "2021-02-18T06:27:25Z",
        "body": "grep GET is to return the access log like this \r\n\r\n[2021-02-18T06:16:33.248Z] \"GET /rt0/v1/api/racingtouch/ping HTTP/1.1\" 200 - \"-\" 0 18 1 1 \"192.168.177.192\" \"Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.7\" \"82bf70f0-7b2c-9b69-aae0-94e3e963c989\" \"istio-ingressgateway.istio-system\" \"127.0.0.1:8000\" inbound|8000|| 127.0.0.1:53962 192.168.53.7:8000 192.168.177.192:0 outbound_.8000_._.test-svc.default.svc.cluster.local default\r\n[2021-02-18T06:16:33.249Z] \"GET /rt0/v1/api/racingtouch/ping HTTP/1.1\" 200 - \"-\" 0 18 1 0 \"192.168.98.64\" \"Mozilla/5.0 (pc-x86_64-linux-gnu) Siege/4.0.7\" \"b28ff319-1dc2-994e-aca3-26f88881f40b\" \"istio-ingressgateway.istio-system\" \"127.0.0.1:8000\" inbound|8000|| 127.0.0.1:53922 192.168.53.7:8000 192.168.98.64:0 outbound_.8000_._.test-svc.default.svc.cluster.local default\r\n\r\nit is to count how many requests going to the pod\r\nsorry, the ip changed as I restarted the pod many times.\n\n---\n\nThe stats in the endpoints do not count correctly.  The rq_total should be much larger than it and I rarely find the access log for 15443 in the istio ingressgateway."
      },
      {
        "user": "lambdai",
        "created_at": "2021-02-18T07:46:58Z",
        "body": "> The stats in the endpoints do not count correctly. The rq_total should be much larger than it and I rarely find the access log for 15443 in the istio ingressgateway.\r\n\r\nThe request in istio-gateway is loadbalanced per `tcp connection` since you use sni_cluster with tcp_proxy filter at istio-ingressgateway. This config doesn't guarantee http request is balanced.\r\n\r\nAt an extreme case, if your siege client use only 1 tcp connection during the your load test, you will see only 1 endpoint handle all the http request. You are right at the beginning: SNI cluster + tcp_proxy doesn't well load balancing http request. \r\n\r\nYou can either switch to another http benchmark tool with max-request-per-connection to give istio-ingressgateway more chances to load balance."
      },
      {
        "user": "debbyku",
        "created_at": "2021-02-18T13:32:46Z",
        "body": "Hi @lambdai,  thanks for your advice.\r\nAs the request is from siege -> clusterA isto-ingressgateway -> clusterB istio-ingressgateway(15443) -> service, in clusterA istio-ingressgateway, we set the max-request-per-connection to 10 in order to max. the no. of connections to clusterB 15443, it seems the load balancing performance is much better.  For 4xxxx requests, the difference of number of requests to the service pods is reduced within 100.  \r\n\r\nMay I ask, if max-request-per-connection is set to 10, is there any adverse effect to the overall performance, i.e. it takes more time to create connections. etc?  Originally there is no setting for it.   Thanks."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-03-20T16:05:28Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "lambdai",
        "created_at": "2021-03-23T02:07:10Z",
        "body": "Sorry I missed this one.\r\nFor light weight request the major cost will be tls handshake. I usually use 5ms cpu time to estimate. YMMV"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-04-29T08:01:17Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how TCP connection reuse affects HTTP request distribution in SNI-based load balancing",
      "Guidance on balancing HTTP request-level distribution over TCP connections",
      "Analysis of performance implications for connection cycling settings",
      "Verification methodology for load balancing effectiveness"
    ]
  },
  {
    "number": 14440,
    "title": "question: how to fetch the remote IP address in WASM",
    "created_at": "2020-12-16T14:03:30Z",
    "closed_at": "2020-12-23T11:20:12Z",
    "labels": [
      "question",
      "area/wasm"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14440",
    "body": "I do not find any doc about how to fetch the remote IP address in WASM.\r\n\r\nMany thx for your help.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14440/comments",
    "author": "membphis",
    "comments": [
      {
        "user": "alandiegosantos",
        "created_at": "2020-12-18T23:15:56Z",
        "body": "It is possible to get the upstream IP address by getting the property _upstream.address_.\r\nI am working with WASM filters written in Rust, so the code looks like: \r\n```\r\nuse log::error;\r\nuse proxy_wasm::traits::*;\r\nuse proxy_wasm::types::*;\r\nuse std::str;\r\n\r\n#[no_mangle]\r\npub fn _start() {\r\n    proxy_wasm::set_log_level(LogLevel::Info);\r\n    proxy_wasm::set_http_context(|_, _| -> Box<dyn HttpContext> { Box::new(HttpFilter) });\r\n}\r\n\r\nstruct HttpFilter;\r\n\r\nimpl Context for HttpFilter{}\r\n\r\nimpl HttpContext for HttpFilter {\r\n    fn on_http_response_headers(&mut self, _: usize) -> Action {\r\n        // Add a header on the response.\r\n        let prop = self.get_property([\"upstream\", \"address\"].to_vec()).unwrap();\r\n        let addr = match str::from_utf8(&prop) {\r\n            Ok(v) => v,\r\n            Err(_e) => \"\",\r\n        };\r\n        error!(\"upstream address {}\",addr);\r\n        Action::Continue\r\n    }\r\n}\r\n```\r\nPS: Do not use that as production code. It is only an example.\r\n\r\nI would like to create the docs about these properties, if possible."
      },
      {
        "user": "membphis",
        "created_at": "2020-12-23T11:20:12Z",
        "body": "ok, got it. many thx"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct property path to retrieve upstream network information in WASM context",
      "Explains how to access network context properties through WASM host environment",
      "Works within proxy-WASM filter architecture constraints"
    ]
  },
  {
    "number": 14075,
    "title": "Questions re OAuth2 plugin behaviour",
    "created_at": "2020-11-18T11:07:50Z",
    "closed_at": "2021-01-30T04:06:58Z",
    "labels": [
      "question",
      "stale",
      "area/oauth"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14075",
    "body": "In the docs for the OAuth2 plugin it contains the following bullet point:-\r\n\r\n> - Upon receiving an access token, the filter sets cookies so that subseqeuent requests can skip the full flow. These cookies are calculated using the hmac_secret to assist in encoding.\r\n\r\nI'm wondering what this means in practical terms, e.g. is the access token sent back to the client as part of the cookie content? Or, is it an 'opaque' cookie that is sent and when subsequent requests are received from the client the access token is retrieved from some internal cache and added to the upstream requests?\r\n\r\nThis actually leads to another question, and, again quoting from the docs:\r\n\r\n> When the authn server validates the client and returns an authorization token back to the OAuth filter, no matter what format that token is, if forward_bearer_token is set to true the filter will send over a cookie named BearerToken to the upstream. Additionally, the Authorization header will be populated with the same value.\r\n\r\nIf the access token is returned as part of the cookie content, the client will not then populate the `Authorization` header value, it will simply send the cookie in all subsequent requests. In this instance what is the plugins behaviour? Does it extract the value from the BearerToken cookie and insert it as the `Authorization` header, or does this header value remain unset?\r\n\r\nMy final questions relate to cookie expiry, is it a simple session cookie that expires when the browser is closed? Also, if there is any kind of session cache in the plugin, is there a built in expiration based on inactivity, and if so what is the inactive period?\r\n\r\nApologies for asking these questions rather than simply testing the scenarios for myself but I'm not currently able to test things out.\r\n\r\nThanks in advance!\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14075/comments",
    "author": "andye2004",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2020-11-19T01:35:35Z",
        "body": "@williamsfu99 perhaps you can address these questions?"
      },
      {
        "user": "williamsfu99",
        "created_at": "2020-11-19T02:40:39Z",
        "body": "@junr03 Thanks for the tag.\r\n\r\n@andye2004 \r\n> e.g. is the access token sent back to the client as part of the cookie content? Or, is it an 'opaque' cookie that is sent and when subsequent requests are received from the client the access token is retrieved from some internal cache and added to the upstream requests?\r\n\r\nAfter the OAuth flow completely finishes, the client receives a response with SetCookie headers for an HMAC value and an Expiry epoch. The HMAC value is calculated by concatenating the domain, expiration, and token together before hashing it using the hmac-sha256(hmac_secret, data) function. Consequently, when that same user visits the domain again, the filter will first inspect these Cookie values, independently perform the same hash function, and compare these two values to validate the session; if this succeeds, the filter verifies that the current epoch is not larger than the Expiry cookie. As you may realize, the filter is stateless and the ability to auto-skip authentication is powered entirely by the exact contents of these explicit cookies. No internal cache.\r\n\r\n> If the access token is returned as part of the cookie content, the client will not then populate the Authorization header value, it will simply send the cookie in all subsequent requests. In this instance what is the plugins behaviour? Does it extract the value from the BearerToken cookie and insert it as the Authorization header, or does this header value remain unset?\r\n\r\nThe first behavior - it will extract the value from the BearerToken cookie and insert it into the Authorization header. This is an unconventional pattern but its purpose is to help the upstream expect consistently populated Authorization headers (when the boolean `forward_bearer_token` is flipped to true).\r\n\r\n> My final questions relate to cookie expiry, is it a simple session cookie that expires when the browser is closed? Also, if there is any kind of session cache in the plugin, is there a built in expiration based on inactivity, and if so what is the inactive period?\r\n\r\nThe cookies do not expire when the browser is necessarily closed - they are your standard HTTP Cookie headers with the `secure` and `httpOnly` flags enabled. The expiration epoch is determined from the `expires_in` field within the JSON response body received from the token_endpoint. Unless you clear these cookies in your browser, they will continue to be sent up until expiration.\r\n\r\nHope this helps - I do recognize that the filter needs some work before it can be widely adopted. We had to prune the filter substantially to omit proprietary interactions and the outcome is an oversimplified authentication plugin. Some improvements we need:\r\n* The filter should perform relevant data fetches - such as username - using the extracted token and pass them upstream for you, instead of expecting the upstream to do so.\r\n* Token signing from the token server should be supported, so that clients cannot arbitrary send invalid bearer tokens."
      },
      {
        "user": "juanvasquezreyes",
        "created_at": "2020-11-23T14:35:46Z",
        "body": "> Hope this helps - I do recognize that the filter needs some work before it can be widely adopted. We had to prune the filter substantially to omit proprietary interactions and the outcome is an oversimplified authentication plugin. Some improvements we need:\r\n> \r\n> * The filter should perform relevant data fetches - such as username - using the extracted token and pass them upstream for you, instead of expecting the upstream to do so.\r\n> * Token signing from the token server should be supported, so that clients cannot arbitrary send invalid bearer tokens.\r\n\r\nAnother enhancement is to add authorization mechanism such as group validations or roles validation and based on that deny or approved the request and forward to the cluster"
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2020-12-23T16:14:57Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions."
      },
      {
        "user": "andye2004",
        "created_at": "2020-12-23T23:12:16Z",
        "body": "I meant to respond to this before now but other things got in the way and I forgot, apologies. I just wanted to thank everyone, especially @williamsfu99, for commenting and clearing up some things. I'm sure in the longer term this will prove to be a very valuable plugin, but, for the moment at least, we will look at alternative methods of implementing oauth/oidc at the proxy level."
      },
      {
        "user": "github-actions[bot]",
        "created_at": "2021-01-30T04:06:58Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or \"no stalebot\" or other activity occurs. Thank you for your contributions.\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\" or \"no stalebot\". Thank you for your contributions."
      }
    ],
    "satisfaction_conditions": [
      "Clarify whether the access token is stored in client-side cookies or handled via HMAC validation without exposing the token",
      "Explain how Authorization headers are populated when cookies are present",
      "Detail cookie expiration behavior relative to token validity periods",
      "Confirm stateless session validation mechanism"
    ]
  },
  {
    "number": 14060,
    "title": "NACK RDS rejections for a cluster that seems available in CDS",
    "created_at": "2020-11-17T04:29:17Z",
    "closed_at": "2020-11-18T00:07:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/14060",
    "body": "*Title*: Seeing NACK RDS rejections for a cluster that seems available in CDS\r\n\r\n*Description*:\r\nUsing `envoyproxy/envoy-alpine:v1.15.0`\r\nWe are seeing NACK rejections from RDS in certain circumstances when populating both CDS and RDS to reach a remote cluster, with error looking like `route: unknown weighted cluster '<server/server>'`, from the client cluster, uwing `validate_clusters` in RDS config. \r\nIt seems, from envoy logging, that envoy does properly recognize the added/updated cluster, however the following RDS update using that cluster as a weighted cluster gives us back an error.\r\n\r\nAdditionally, when retried with the exact (diff-compared) same config, the RDS config is accepted. We are wondering if this can be a timing issue of sorts.\r\n\r\n```\r\n[2020-11-16 22:40:40.247][1][info][upstream] [source/common/upstream/cds_api_impl.cc:80] cds: add/update cluster 'server/server'\r\n[2020-11-16 22:40:40.249][1][info][upstream] [source/common/upstream/cds_api_impl.cc:80] cds: add/update cluster 'client0/client0-local'\r\n[2020-11-16 22:40:40.255][1][info][upstream] [source/server/lds_api.cc:74] lds: add/update listener 'outbound_listener'\r\n[2020-11-16 22:40:40.256][1][warning][config] [source/common/config/grpc_subscription_impl.cc:100] gRPC config for type.googleapis.com/envoy.config.route.v3.RouteConfiguration rejected: route: unknown weighted cluster 'server/server'\r\n```\r\n\r\n*Config*:\r\nAdding only relevant CDS and RDS to avoid clutter. \r\nCDS\r\n```\r\nresources:{[type.googleapis.com/envoy.config.cluster.v3.Cluster]:{name:\\\"server/server\\\" type:EDS eds_cluster_config:{eds_config:{ads:{} resource_api_version:V3}} connect_timeout:{seconds:1} http2_protocol_options:{} transport_socket:{name:\\\"envoy.transport_sockets.tls\\\" typed_config:{[type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext]:{common_tls_context:{tls_params:{tls_minimum_protocol_version:TLSv1_2 tls_maximum_protocol_version:TLSv1_3} tls_certificate_sds_secret_configs:{name:\\\"service-cert:client0/client0\\\" sds_config:{ads:{} resource_api_version:V3}} validation_context_sds_secret_config:{name:\\\"root-cert-for-mtls-outbound:server/server\\\" sds_config:{ads:{} resource_api_version:V3}} alpn_protocols:\\\"osm\\\"} sni:\\\"server.server.svc.cluster.local\\\"}}} protocol_selection:USE_DOWNSTREAM_PROTOCOL}}\r\n```\r\nRDS\r\n```\r\nresources:{[type.googleapis.com/envoy.config.route.v3.RouteConfiguration]:{name:\\\"RDS_Outbound\\\" virtual_hosts:{name:\\\"outbound_virtualHost|server\\\" domains:\\\"server.server.svc.cluster\\\" domains:\\\"server.server.svc.cluster.local\\\" domains:\\\"server.server:80\\\" domains:\\\"server.server.svc:80\\\" domains:\\\"server.server.svc.cluster:80\\\" domains:\\\"server.server.svc.cluster.local:80\\\" domains:\\\"server.server\\\" domains:\\\"server.server.svc\\\" routes:{match:{safe_regex:{google_re2:{} regex:\\\".*\\\"} headers:{name:\\\":method\\\" safe_regex_match:{google_re2:{} regex:\\\".*\\\"}}} route:{weighted_clusters:{clusters:{name:\\\"server/server\\\" weight:{value:100}} total_weight:{value:100}}}}} validate_clusters:{value:true}}}\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/14060/comments",
    "author": "eduser25",
    "comments": [
      {
        "user": "lambdai",
        "created_at": "2020-11-17T20:08:12Z",
        "body": "> Additionally, when retried with the exact (diff-compared) same config, the RDS config is accepted. We are wondering if this can be a timing issue of sorts.\r\n\r\nThe target cluster 'server/server' can be found by RDS only if the cluster is warmed up.\r\nIf you can enable debug log, you can probably see the validation passed after \"warming cluster server/server complete\" shows up."
      },
      {
        "user": "eduser25",
        "created_at": "2020-11-18T00:07:03Z",
        "body": "Confirmed that's the case. Thanks lambdai."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of cluster warm-up dependency for RDS validation",
      "Clarification of timing requirements between CDS and RDS updates",
      "Diagnostic guidance for cluster availability verification",
      "Explanation of validate_clusters behavior in dynamic configurations"
    ]
  },
  {
    "number": 13992,
    "title": "[Question] prefix_rewrite based on selected host in the upstream cluster",
    "created_at": "2020-11-12T07:30:38Z",
    "closed_at": "2020-11-14T16:44:10Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/13992",
    "body": "*Title*: prefix_rewrite based on selected upstream cluster\r\n\r\n*Description*:\r\nIs there anyway possible to rewrite the prefix based on the selected host in a cluster?\r\nI have a static cluster of 4 hosts - 1,2,3,4 and when I receive a request with prefix `/msg.host_x/`, I want the `x` to be replaced by 1,2,3 or 4 based on whichever host is selected in the cluster.\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/13992/comments",
    "author": "rkbalgi",
    "comments": [
      {
        "user": "phlax",
        "created_at": "2020-11-14T09:36:06Z",
        "body": "quoting @antoniovicente on slack channel:\r\n\r\n> No, that's not possible.  One of the things that makes this kind of rewrite difficult is that the proxy would need to rewrite the request again if a different host is selected on retry.\r\n"
      },
      {
        "user": "rkbalgi",
        "created_at": "2020-11-14T16:44:10Z",
        "body": "Closing as this is not possible at the moment. Thanks guys."
      }
    ],
    "satisfaction_conditions": [
      "Dynamic path rewriting based on upstream host selection",
      "Handling retries with different hosts",
      "Cluster-aware request processing"
    ]
  },
  {
    "number": 12675,
    "title": "Stats filtering inclusion list is not working for server level metrics",
    "created_at": "2020-08-17T05:16:20Z",
    "closed_at": "2020-08-19T04:34:35Z",
    "labels": [
      "question",
      "area/stats"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12675",
    "body": "We have the following configuration in envoy\r\n\r\n```yml\r\n    stats_config:\r\n      stats_matcher:\r\n        inclusion_list:\r\n          patterns:\r\n          - suffix: upstream_cx_total\r\n          - exact: envoy_server_state\r\n```\r\nIn this case, the metrics with suffix upstream_cx_total is emitted properly, but the metric envoy_server_state is never emitted. Please check if this a bug, we have a large config and metrics is slowing envoy down, hence wanted to include only the required metrics which is a combination of cluster and server level metrics.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12675/comments",
    "author": "shyamradhakrishnan",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2020-08-18T11:03:00Z",
        "body": "@shyamradhakrishnan I believe what you want is `exact: server.state`, the stats matcher is based on their canonical name, names in Prometheus exporter are normalized to Prometheus naming convention (with envoy prefix)."
      },
      {
        "user": "shyamradhakrishnan",
        "created_at": "2020-08-19T04:34:35Z",
        "body": "Thanks @lizan , that was the issue."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of Envoy's metric naming conventions for stats matcher inclusion lists",
      "Clarification on how server-level metrics differ from cluster-level metrics in naming patterns",
      "Guidance on proper pattern matching syntax for Envoy's stats configuration"
    ]
  },
  {
    "number": 12665,
    "title": "Timeout sometimes gives 408 instead of 504",
    "created_at": "2020-08-15T06:56:18Z",
    "closed_at": "2020-09-01T08:15:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/12665",
    "body": "Description:\r\n\r\n> While testing timeout in envoy v1.12.2 most of the request gives 504 which is as expected, but once in a while it throws 408 status code which is unexpected.\r\n\r\nRepro steps:\r\n\r\n> In the testing environment 15s delay was tried for a timeout with below routing config.\r\n```\r\n\"route\": {\r\n           \"cluster\": \"test\",\r\n           \"timeout\": \"15s\",\r\n           \"idle_timeout\": \"315s\"\r\n          }\r\n```\r\nAdmin and Stats Output:\r\n\r\n<details>\r\n<summary>Stats</summary>\r\ncluster.testt1.assignment_stale: 0\r\ncluster.testt1.assignment_timeout_received: 0\r\ncluster.testt1.bind_errors: 0\r\ncluster.testt1.circuit_breakers.default.cx_open: 0\r\ncluster.testt1.circuit_breakers.default.cx_pool_open: 0\r\ncluster.testt1.circuit_breakers.default.rq_open: 0\r\ncluster.testt1.circuit_breakers.default.rq_pending_open: 0\r\ncluster.testt1.circuit_breakers.default.rq_retry_open: 0\r\ncluster.testt1.circuit_breakers.high.cx_open: 0\r\ncluster.testt1.circuit_breakers.high.cx_pool_open: 0\r\ncluster.testt1.circuit_breakers.high.rq_open: 0\r\ncluster.testt1.circuit_breakers.high.rq_pending_open: 0\r\ncluster.testt1.circuit_breakers.high.rq_retry_open: 0\r\ncluster.testt1.client_ssl_socket_factory.downstream_context_secrets_not_ready: 0\r\ncluster.testt1.client_ssl_socket_factory.ssl_context_update_by_sds: 0\r\ncluster.testt1.client_ssl_socket_factory.upstream_context_secrets_not_ready: 0\r\ncluster.testt1.default.total_match_count: 240\r\ncluster.testt1.external.upstream_rq_200: 4\r\ncluster.testt1.external.upstream_rq_2xx: 4\r\ncluster.testt1.external.upstream_rq_408: 3\r\ncluster.testt1.external.upstream_rq_4xx: 3\r\ncluster.testt1.external.upstream_rq_504: 236\r\ncluster.testt1.external.upstream_rq_5xx: 236\r\ncluster.testt1.external.upstream_rq_completed: 243\r\ncluster.testt1.http1.metadata_not_supported_error: 0\r\ncluster.testt1.lb_healthy_panic: 0\r\ncluster.testt1.lb_local_cluster_not_ok: 0\r\ncluster.testt1.lb_recalculate_zone_structures: 0\r\ncluster.testt1.lb_subsets_active: 0\r\ncluster.testt1.lb_subsets_created: 0\r\ncluster.testt1.lb_subsets_fallback: 0\r\ncluster.testt1.lb_subsets_fallback_panic: 0\r\ncluster.testt1.lb_subsets_removed: 0\r\ncluster.testt1.lb_subsets_selected: 0\r\ncluster.testt1.lb_zone_cluster_too_small: 0\r\ncluster.testt1.lb_zone_no_capacity_left: 0\r\ncluster.testt1.lb_zone_number_differs: 0\r\ncluster.testt1.lb_zone_routing_all_directly: 0\r\ncluster.testt1.lb_zone_routing_cross_zone: 0\r\ncluster.testt1.lb_zone_routing_sampled: 0\r\ncluster.testt1.max_host_weight: 1\r\ncluster.testt1.membership_change: 1\r\ncluster.testt1.membership_degraded: 0\r\ncluster.testt1.membership_excluded: 0\r\ncluster.testt1.membership_healthy: 1\r\ncluster.testt1.membership_total: 1\r\ncluster.testt1.original_dst_host_invalid: 0\r\ncluster.testt1.retry_or_shadow_abandoned: 0\r\ncluster.testt1.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 222\r\ncluster.testt1.ssl.connection_error: 0\r\ncluster.testt1.ssl.curves.X25519: 222\r\ncluster.testt1.ssl.fail_verify_cert_hash: 0\r\ncluster.testt1.ssl.fail_verify_error: 0\r\ncluster.testt1.ssl.fail_verify_no_cert: 0\r\ncluster.testt1.ssl.fail_verify_san: 0\r\ncluster.testt1.ssl.handshake: 222\r\ncluster.testt1.ssl.no_certificate: 0\r\ncluster.testt1.ssl.session_reused: 221\r\ncluster.testt1.ssl.sigalgs.unknown_ssl_algorithm: 222\r\ncluster.testt1.ssl.versions.TLSv1.2: 222\r\ncluster.testt1.update_attempt: 240\r\ncluster.testt1.update_empty: 0\r\ncluster.testt1.update_failure: 0\r\ncluster.testt1.update_no_rebuild: 239\r\ncluster.testt1.update_success: 240\r\ncluster.testt1.upstream_cx_active: 1\r\ncluster.testt1.upstream_cx_close_notify: 0\r\ncluster.testt1.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.testt1.upstream_cx_connect_fail: 0\r\ncluster.testt1.upstream_cx_connect_timeout: 0\r\ncluster.testt1.upstream_cx_destroy: 221\r\ncluster.testt1.upstream_cx_destroy_local: 221\r\ncluster.testt1.upstream_cx_destroy_local_with_active_rq: 221\r\ncluster.testt1.upstream_cx_destroy_remote: 0\r\ncluster.testt1.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.testt1.upstream_cx_destroy_with_active_rq: 221\r\ncluster.testt1.upstream_cx_http1_total: 222\r\ncluster.testt1.upstream_cx_http2_total: 0\r\ncluster.testt1.upstream_cx_idle_timeout: 0\r\ncluster.testt1.upstream_cx_max_requests: 0\r\ncluster.testt1.upstream_cx_none_healthy: 0\r\ncluster.testt1.upstream_cx_overflow: 0\r\ncluster.testt1.upstream_cx_pool_overflow: 0\r\ncluster.testt1.upstream_cx_protocol_error: 0\r\ncluster.testt1.upstream_cx_rx_bytes_buffered: 154\r\ncluster.testt1.upstream_cx_rx_bytes_total: 7516\r\ncluster.testt1.upstream_cx_total: 222\r\ncluster.testt1.upstream_cx_tx_bytes_buffered: 0\r\ncluster.testt1.upstream_cx_tx_bytes_total: 108854\r\ncluster.testt1.upstream_flow_control_backed_up_total: 0\r\ncluster.testt1.upstream_flow_control_drained_total: 0\r\ncluster.testt1.upstream_flow_control_paused_reading_total: 0\r\ncluster.testt1.upstream_flow_control_resumed_reading_total: 0\r\ncluster.testt1.upstream_internal_redirect_failed_total: 0\r\ncluster.testt1.upstream_internal_redirect_succeeded_total: 0\r\ncluster.testt1.upstream_rq_200: 4\r\ncluster.testt1.upstream_rq_2xx: 4\r\ncluster.testt1.upstream_rq_408: 3\r\ncluster.testt1.upstream_rq_4xx: 3\r\ncluster.testt1.upstream_rq_504: 236\r\ncluster.testt1.upstream_rq_5xx: 236\r\ncluster.testt1.upstream_rq_active: 0\r\ncluster.testt1.upstream_rq_cancelled: 0\r\ncluster.testt1.upstream_rq_completed: 243\r\ncluster.testt1.upstream_rq_maintenance_mode: 0\r\ncluster.testt1.upstream_rq_pending_active: 0\r\ncluster.testt1.upstream_rq_pending_failure_eject: 0\r\ncluster.testt1.upstream_rq_pending_overflow: 0\r\ncluster.testt1.upstream_rq_pending_total: 222\r\ncluster.testt1.upstream_rq_per_try_timeout: 0\r\ncluster.testt1.upstream_rq_retry: 0\r\ncluster.testt1.upstream_rq_retry_overflow: 0\r\ncluster.testt1.upstream_rq_retry_success: 0\r\ncluster.testt1.upstream_rq_rx_reset: 0\r\ncluster.testt1.upstream_rq_timeout: 221\r\ncluster.testt1.upstream_rq_total: 243\r\ncluster.testt1.upstream_rq_tx_reset: 0\r\ncluster.testt1.version: 0\r\ncluster.cont-ads.assignment_stale: 0\r\ncluster.cont-ads.assignment_timeout_received: 0\r\ncluster.cont-ads.bind_errors: 0\r\ncluster.cont-ads.circuit_breakers.default.cx_open: 0\r\ncluster.cont-ads.circuit_breakers.default.cx_pool_open: 0\r\ncluster.cont-ads.circuit_breakers.default.rq_open: 0\r\ncluster.cont-ads.circuit_breakers.default.rq_pending_open: 0\r\ncluster.cont-ads.circuit_breakers.default.rq_retry_open: 0\r\ncluster.cont-ads.circuit_breakers.high.cx_open: 0\r\ncluster.cont-ads.circuit_breakers.high.cx_pool_open: 0\r\ncluster.cont-ads.circuit_breakers.high.rq_open: 0\r\ncluster.cont-ads.circuit_breakers.high.rq_pending_open: 0\r\ncluster.cont-ads.circuit_breakers.high.rq_retry_open: 0\r\ncluster.cont-ads.client_ssl_socket_factory\r\n.downstream_context_secrets_not_ready: 0\r\ncluster.cont-ads.client_ssl_socket_factory.ssl_context_update_by_sds: 0\r\ncluster.cont-ads.client_ssl_socket_factory.upstream_context_secrets_not_ready: 0\r\ncluster.cont-ads.default.total_match_count: 269\r\ncluster.cont-ads.http2.header_overflow: 0\r\ncluster.cont-ads.http2.headers_cb_no_stream: 0\r\ncluster.cont-ads.http2.inbound_empty_frames_flood: 0\r\ncluster.cont-ads.http2.inbound_priority_frames_flood: 0\r\ncluster.cont-ads.http2.inbound_window_update_frames_flood: 0\r\ncluster.cont-ads.http2.outbound_control_flood: 0\r\ncluster.cont-ads.http2.outbound_flood: 0\r\ncluster.cont-ads.http2.rx_messaging_error: 0\r\ncluster.cont-ads.http2.rx_reset: 0\r\ncluster.cont-ads.http2.too_many_header_frames: 0\r\ncluster.cont-ads.http2.trailers: 0\r\ncluster.cont-ads.http2.tx_reset: 0\r\ncluster.cont-ads.internal.upstream_rq_200: 1\r\ncluster.cont-ads.internal.upstream_rq_2xx: 1\r\ncluster.cont-ads.internal.upstream_rq_503: 2\r\ncluster.cont-ads.internal.upstream_rq_5xx: 2\r\ncluster.cont-ads.internal.upstream_rq_completed: 3\r\ncluster.cont-ads.lb_healthy_panic: 1\r\ncluster.cont-ads.lb_local_cluster_not_ok: 0\r\ncluster.cont-ads.lb_recalculate_zone_structures: 0\r\ncluster.cont-ads.lb_subsets_active: 0\r\ncluster.cont-ads.lb_subsets_created: 0\r\ncluster.cont-ads.lb_subsets_fallback: 0\r\ncluster.cont-ads.lb_subsets_fallback_panic: 0\r\ncluster.cont-ads.lb_subsets_removed: 0\r\ncluster.cont-ads.lb_subsets_selected: 0\r\ncluster.cont-ads.lb_zone_cluster_too_small: 0\r\ncluster.cont-ads.lb_zone_no_capacity_left: 0\r\ncluster.cont-ads.lb_zone_number_differs: 0\r\ncluster.cont-ads.lb_zone_routing_all_directly: 0\r\ncluster.cont-ads.lb_zone_routing_cross_zone: 0\r\ncluster.cont-ads.lb_zone_routing_sampled: 0\r\ncluster.cont-ads.max_host_weight: 1\r\ncluster.cont-ads.membership_change: 1\r\ncluster.cont-ads.membership_degraded: 0\r\ncluster.cont-ads.membership_excluded: 0\r\ncluster.cont-ads.membership_healthy: 1\r\ncluster.cont-ads.membership_total: 1\r\ncluster.cont-ads.original_dst_host_invalid: 0\r\ncluster.cont-ads.retry_or_shadow_abandoned: 0\r\ncluster.cont-ads.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 1\r\ncluster.cont-ads.ssl.connection_error: 0\r\ncluster.cont-ads.ssl.curves.X25519: 1\r\ncluster.cont-ads.ssl.fail_verify_cert_hash: 0\r\ncluster.cont-ads.ssl.fail_verify_error: 0\r\ncluster.cont-ads.ssl.fail_verify_no_cert: 0\r\ncluster.cont-ads.ssl.fail_verify_san: 0\r\ncluster.cont-ads.ssl.handshake: 1\r\ncluster.cont-ads.ssl.no_certificate: 0\r\ncluster.cont-ads.ssl.session_reused: 0\r\ncluster.cont-ads.ssl.sigalgs.unknown_ssl_algorithm: 1\r\ncluster.cont-ads.ssl.versions.TLSv1.2: 1\r\ncluster.cont-ads.update_attempt: 269\r\ncluster.cont-ads.update_empty: 0\r\ncluster.cont-ads.update_failure: 0\r\ncluster.cont-ads.update_no_rebuild: 268\r\ncluster.cont-ads.update_success: 269\r\ncluster.cont-ads.upstream_cx_active: 1\r\ncluster.cont-ads.upstream_cx_close_notify: 0\r\ncluster.cont-ads.upstream_cx_connect_attempts_exceeded: 0\r\ncluster.cont-ads.upstream_cx_connect_fail: 1\r\ncluster.cont-ads.upstream_cx_connect_timeout: 1\r\ncluster.cont-ads.upstream_cx_destroy: 1\r\ncluster.cont-ads.upstream_cx_destroy_local: 1\r\ncluster.cont-ads.upstream_cx_destroy_local_with_active_rq: 0\r\ncluster.cont-ads.upstream_cx_destroy_remote: 0\r\ncluster.cont-ads.upstream_cx_destroy_remote_with_active_rq: 0\r\ncluster.cont-ads.upstream_cx_destroy_with_active_rq: 0\r\ncluster.cont-ads.upstream_cx_http1_total: 0\r\ncluster.cont-ads.upstream_cx_http2_total: 2\r\ncluster.cont-ads.upstream_cx_idle_timeout: 0\r\ncluster.cont-ads.upstream_cx_max_requests: 0\r\ncluster.cont-ads.upstream_cx_none_healthy: 1\r\ncluster.cont-ads.upstream_cx_overflow: 0\r\ncluster.cont-ads.upstream_cx_pool_overflow: 0\r\ncluster.cont-ads.upstream_cx_protocol_error: 0\r\ncluster.cont-ads.upstream_cx_rx_bytes_buffered: 30\r\ncluster.cont-ads.upstream_cx_rx_bytes_total: 10578457\r\ncluster.cont-ads.upstream_cx_total: 2\r\ncluster.cont-ads.upstream_cx_tx_bytes_buffered: 0\r\ncluster.cont-ads.upstream_cx_tx_bytes_total: 1286626\r\ncluster.cont-ads.upstream_flow_control_backed_up_total: 0\r\ncluster.cont-ads.upstream_flow_control_drained_total: 0\r\ncluster.cont-ads.upstream_flow_control_paused_reading_total: 0\r\ncluster.cont-ads.upstream_flow_control_resumed_reading_total: 0\r\ncluster.cont-ads.upstream_internal_redirect_failed_total: 0\r\ncluster.cont-ads.upstream_internal_redirect_succeeded_total: 0\r\ncluster.cont-ads.upstream_rq_200: 1\r\ncluster.cont-ads.upstream_rq_2xx: 1\r\ncluster.cont-ads.upstream_rq_503: 2\r\ncluster.cont-ads.upstream_rq_5xx: 2\r\ncluster.cont-ads.upstream_rq_active: 1\r\ncluster.cont-ads.upstream_rq_cancelled: 0\r\ncluster.cont-ads.upstream_rq_completed: 3\r\ncluster.cont-ads.upstream_rq_maintenance_mode: 0\r\ncluster.cont-ads.upstream_rq_pending_active: 0\r\ncluster.cont-ads.upstream_rq_pending_failure_eject: 1\r\ncluster.cont-ads.upstream_rq_pending_overflow: 0\r\ncluster.cont-ads.upstream_rq_pending_total: 2\r\ncluster.cont-ads.upstream_rq_per_try_timeout: 0\r\ncluster.cont-ads.upstream_rq_retry: 0\r\ncluster.cont-ads.upstream_rq_retry_overflow: 0\r\ncluster.cont-ads.upstream_rq_retry_success: 0\r\ncluster.cont-ads.upstream_rq_rx_reset: 0\r\ncluster.cont-ads.upstream_rq_timeout: 0\r\ncluster.cont-ads.upstream_rq_total: 1\r\ncluster.cont-ads.upstream_rq_tx_reset: 0\r\ncluster.cont-ads.version: 0\r\ncluster_manager.active_clusters: 2\r\ncluster_manager.cds.init_fetch_timeout: 0\r\ncluster_manager.cds.update_attempt: 1440\r\ncluster_manager.cds.update_failure: 1\r\ncluster_manager.cds.update_rejected: 0\r\ncluster_manager.cds.update_success: 1438\r\ncluster_manager.cds.version: 310770956959226155\r\ncluster_manager.cluster_added: 2\r\ncluster_manager.cluster_modified: 0\r\ncluster_manager.cluster_removed: 0\r\ncluster_manager.cluster_updated: 0\r\ncluster_manager.cluster_updated_via_merge: 0\r\ncluster_manager.update_merge_cancelled: 0\r\ncluster_manager.update_out_of_merge_window: 0\r\ncluster_manager.warming_clusters: 0\r\ncontrol_plane.connected_state: 1\r\ncontrol_plane.pending_requests: 0\r\ncontrol_plane.rate_limit_enforced: 19\r\nfilesystem.flushed_by_timer: 721\r\nfilesystem.reopen_failed: 0\r\nfilesystem.write_buffered: 107386\r\nfilesystem.write_completed: 4504\r\nfilesystem.write_failed: 0\r\nfilesystem.write_total_buffered: 122\r\nhttp.admin.downstream_cx_active: 1\r\nhttp.admin.downstream_cx_delayed_close_timeout: 0\r\nhttp.admin.downstream_cx_destroy: 2651\r\nhttp.admin.downstream_cx_destroy_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_local: 2640\r\nhttp.admin.downstream_cx_destroy_local_active_rq: 0\r\nhttp.admin.downstream_cx_destroy_remote: 11\r\nhttp.admin.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.admin.downstream_cx_drain_close: 0\r\nhttp.admin.downstream_cx_http1_active: 1\r\nhttp.admin.downstream_cx_http1_total: 2652\r\nhttp.admin.downstream_cx_http2_active: 0\r\nhttp.admin.downstream_cx_http2_total: 0\r\nhttp.admin.downstream_cx_http3_active: 0\r\nhttp.admin.downstream_cx_http3_total: 0\r\nhttp.admin.downstream_cx_idle_timeout: 0\r\nhttp.admin.downstream_cx_max_duration_reached: 0\r\nhttp.admin.downstream_cx_overload_disable_keepalive: 0\r\nhttp.admin.downstream_cx_protocol_error: 0\r\nhttp.admin.downstream_cx_rx_bytes_buffered: 86\r\nhttp.admin.downstream_cx_rx_bytes_total: 312741\r\nhttp.admin.downstream_cx_ssl_active: 0\r\nhttp.admin.downstream_cx_ssl_total: 0\r\nhttp.admin.downstream_cx_total: 2652\r\nhttp.admin.downstream_cx_tx_bytes_buffered: 0\r\nhttp.admin.downstream_cx_tx_bytes_total: 664935\r\nhttp.admin.downstream_cx_upgrades_active: 0\r\nhttp.admin.downstream_cx_upgrades_total: 0\r\nhttp.admin.downstream_flow_control_paused_reading_total: 0\r\nhttp.admin.downstream_flow_control_resumed_reading_total: 0\r\nhttp.admin.downstream_rq_1xx: 0\r\nhttp.admin.downstream_rq_2xx: 2640\r\nhttp.admin.downstream_rq_3xx: 0\r\nhttp.admin.downstream_rq_4xx: 8\r\nhttp.admin.downstream_rq_5xx: 4\r\nhttp.admin.downstream_rq_active: 1\r\nhttp.admin.downstream_rq_completed: 2652\r\nhttp.admin.downstream_rq_http1_total: 2653\r\nhttp.admin.downstream_rq_http2_total: 0\r\nhttp.admin.downstream_rq_http3_total: 0\r\nhttp.admin.downstream_rq_idle_timeout: 0\r\nhttp.admin.downstream_rq_non_relative_path: 0\r\nhttp.admin.downstream_rq_overload_close: 0\r\nhttp.admin.downstream_rq_response_before_rq_complete: 0\r\nhttp.admin.downstream_rq_rx_reset: 0\r\nhttp.admin.downstream_rq_timeout: 0\r\nhttp.admin.downstream_rq_too_large: 0\r\nhttp.admin.downstream_rq_total: 2653\r\nhttp.admin.downstream_rq_tx_reset: 0\r\nhttp.admin.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.admin.rs_too_large: 0\r\nhttp.async-client.no_cluster: 0\r\nhttp.async-client.no_route: 0\r\nhttp.async-client.rq_direct_response: 0\r\nhttp.async-client.rq_redirect: 0\r\nhttp.async-client.rq_reset_after_downstream_response_started: 0\r\nhttp.async-client.rq_retry_skipped_request_not_complete: 0\r\nhttp.async-client.rq_total: 3\r\nhttp.ingress_http.downstream_cx_active: 0\r\nhttp.ingress_http.downstream_cx_delayed_close_timeout: 0\r\nhttp.ingress_http.downstream_cx_destroy: 10\r\nhttp.ingress_http.downstream_cx_destroy_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_local: 0\r\nhttp.ingress_http.downstream_cx_destroy_local_active_rq: 0\r\nhttp.ingress_http.downstream_cx_destroy_remote: 10\r\nhttp.ingress_http.downstream_cx_destroy_remote_active_rq: 0\r\nhttp.ingress_http.downstream_cx_drain_close: 0\r\nhttp.ingress_http.downstream_cx_http1_active: 0\r\nhttp.ingress_http.downstream_cx_http1_total: 10\r\nhttp.ingress_http.downstream_cx_http2_active: 0\r\nhttp.ingress_http.downstream_cx_http2_total: 0\r\nhttp.ingress_http.downstream_cx_http3_active: 0\r\nhttp.ingress_http.downstream_cx_http3_total: 0\r\nhttp.ingress_http.downstream_cx_idle_timeout: 0\r\nhttp.ingress_http.downstream_cx_max_duration_reached: 0\r\nhttp.ingress_http.downstream_cx_overload_disable_keepalive: 0\r\nhttp.ingress_http.downstream_cx_protocol_error: 0\r\nhttp.ingress_http.downstream_cx_rx_bytes_buffered: 0\r\nhttp.ingress_http.downstream_cx_rx_bytes_total: 59282\r\nhttp.ingress_http.downstream_cx_ssl_active: 0\r\nhttp.ingress_http.downstream_cx_ssl_total: 10\r\nhttp.ingress_http.downstream_cx_total: 10\r\nhttp.ingress_http.downstream_cx_tx_bytes_buffered: 0\r\nhttp.ingress_http.downstream_cx_tx_bytes_total: 42234\r\nhttp.ingress_http.downstream_cx_upgrades_active: 0\r\nhttp.ingress_http.downstream_cx_upgrades_total: 0\r\nhttp.ingress_http.downstream_flow_control_paused_reading_total: 0\r\nhttp.ingress_http.downstream_flow_control_resumed_reading_total: 0\r\nhttp.ingress_http.downstream_rq_1xx: 0\r\nhttp.ingress_http.downstream_rq_2xx: 4\r\nhttp.ingress_http.downstream_rq_3xx: 0\r\nhttp.ingress_http.downstream_rq_4xx: 3\r\nhttp.ingress_http.downstream_rq_5xx: 236\r\nhttp.ingress_http.downstream_rq_active: 0\r\nhttp.ingress_http.downstream_rq_completed: 243\r\nhttp.ingress_http.downstream_rq_http1_total: 243\r\nhttp.ingress_http.downstream_rq_http2_total: 0\r\nhttp.ingress_http.downstream_rq_http3_total: 0\r\nhttp.ingress_http.downstream_rq_idle_timeout: 0\r\nhttp.ingress_http.downstream_rq_non_relative_path: 0\r\nhttp.ingress_http.downstream_rq_overload_close: 0\r\nhttp.ingress_http.downstream_rq_response_before_rq_complete: 0\r\nhttp.ingress_http.downstream_rq_rx_reset: 0\r\nhttp.ingress_http.downstream_rq_timeout: 0\r\nhttp.ingress_http.downstream_rq_too_large: 0\r\nhttp.ingress_http.downstream_rq_total: 243\r\nhttp.ingress_http.downstream_rq_tx_reset: 0\r\nhttp.ingress_http.downstream_rq_ws_on_non_ws_route: 0\r\nhttp.ingress_http.no_cluster: 0\r\nhttp.ingress_http.no_route: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.config_reload: 1\r\nhttp.ingress_http.rds.ingress-http-route-config.init_fetch_timeout: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_attempt: 1439\r\nhttp.ingress_http.rds.ingress-http-route-config.update_empty: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_failure: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_rejected: 0\r\nhttp.ingress_http.rds.ingress-http-route-config.update_success: 1438\r\nhttp.ingress_http.rds.ingress-http-route-config.version: 310770956959226155\r\nhttp.ingress_http.rds.route-t1-t1-vh1.config_reload: 1\r\nhttp.ingress_http.rds.route-t1-t1-vh1.init_fetch_timeout: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_attempt: 1439\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_empty: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_failure: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_rejected: 0\r\nhttp.ingress_http.rds.route-t1-t1-vh1.update_success: 1438\r\nhttp.ingress_http.rds.route-t1-t1-vh1.version: 310770956959226155\r\nhttp.ingress_http.rq_direct_response: 0\r\nhttp.ingress_http.rq_redirect: 0\r\nhttp.ingress_http.rq_reset_after_downstream_response_started: 0\r\nhttp.ingress_http.rq_retry_skipped_request_not_complete: 0\r\nhttp.ingress_http.rq_total: 243\r\nhttp.ingress_http.rs_too_large: 0\r\nhttp.ingress_http.tracing.client_enabled: 0\r\nhttp.ingress_http.tracing.health_check: 0\r\nhttp.ingress_http.tracing.not_traceable: 0\r\nhttp.ingress_http.tracing.random_sampling: 0\r\nhttp.ingress_http.tracing.service_forced: 0\r\nhttp1.metadata_not_supported_error: 0\r\ninit_fetch_timeout: 0\r\nlistener.0.0.0.0_10000.downstream_cx_active: 0\r\nlistener.0.0.0.0_10000.downstream_cx_destroy: 0\r\nlistener.0.0.0.0_10000.downstream_cx_total: 0\r\nlistener.0.0.0.0_10000.downstream_pre_cx_active: 0\r\nlistener.0.0.0.0_10000.downstream_pre_cx_timeout: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_1xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_2xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_3xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_4xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_5xx: 0\r\nlistener.0.0.0.0_10000.http.ingress_http.downstream_rq_completed: 0\r\nlistener.0.0.0.0_10000.no_filter_chain_match: 0\r\nlistener.0.0.0.0_10000.worker_0.downstream_cx_active: 0\r\nlistener.0.0.0.0_10000.worker_0.downstream_cx_total: 0\r\nlistener.0.0.0.0_10000.worker_1.downstream_cx_active: 0\r\nlistener.0.0.0.0_10000.worker_1.downstream_cx_total: 0\r\nlistener.0.0.0.0_10443.downstream_cx_active: 0\r\nlistener.0.0.0.0_10443.downstream_cx_destroy: 10\r\nlistener.0.0.0.0_10443.downstream_cx_total: 10\r\nlistener.0.0.0.0_10443.downstream_pre_cx_active: 0\r\nlistener.0.0.0.0_10443.downstream_pre_cx_timeout: 0\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_1xx: 0\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_2xx: 4\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_3xx: 0\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_4xx: 3\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_5xx: 236\r\nlistener.0.0.0.0_10443.http.ingress_http.downstream_rq_completed: 243\r\nlistener.0.0.0.0_10443.no_filter_chain_match: 100\r\nlistener.0.0.0.0_10443.server_ssl_socket_factory.downstream_context_secrets_not_ready: 0\r\nlistener.0.0.0.0_10443.server_ssl_socket_factory.ssl_context_update_by_sds: 2\r\nlistener.0.0.0.0_10443.server_ssl_socket_factory.upstream_context_secrets_not_ready: 0\r\nlistener.0.0.0.0_10443.ssl.ciphers.ECDHE-RSA-AES128-GCM-SHA256: 3\r\nlistener.0.0.0.0_10443.ssl.ciphers.TLS_AES_128_GCM_SHA256: 7\r\nlistener.0.0.0.0_10443.ssl.connection_error: 0\r\nlistener.0.0.0.0_10443.ssl.curves.X25519: 10\r\nlistener.0.0.0.0_10443.ssl.fail_verify_cert_hash: 0\r\nlistener.0.0.0.0_10443.ssl.fail_verify_error: 0\r\nlistener.0.0.0.0_10443.ssl.fail_verify_no_cert: 0\r\nlistener.0.0.0.0_10443.ssl.fail_verify_san: 0\r\nlistener.0.0.0.0_10443.ssl.handshake: 10\r\nlistener.0.0.0.0_10443.ssl.no_certificate: 10\r\nlistener.0.0.0.0_10443.ssl.session_reused: 7\r\nlistener.0.0.0.0_10443.ssl.versions.TLSv1.2: 3\r\nlistener.0.0.0.0_10443.ssl.versions.TLSv1.3: 7\r\nlistener.0.0.0.0_10443.worker_0.downstream_cx_active: 0\r\nlistener.0.0.0.0_10443.worker_0.downstream_cx_total: 2\r\nlistener.0.0.0.0_10443.worker_1.downstream_cx_active: 0\r\nlistener.0.0.0.0_10443.worker_1.downstream_cx_total: 8\r\nlistener.admin.downstream_cx_active: 1\r\nlistener.admin.downstream_cx_destroy: 2651\r\nlistener.admin.downstream_cx_total: 2652\r\nlistener.admin.downstream_pre_cx_active: 0\r\nlistener.admin.downstream_pre_cx_timeout: 0\r\nlistener.admin.http.admin.downstream_rq_1xx: 0\r\nlistener.admin.http.admin.downstream_rq_2xx: 2640\r\nlistener.admin.http.admin.downstream_rq_3xx: 0\r\nlistener.admin.http.admin.downstream_rq_4xx: 8\r\nlistener.admin.http.admin.downstream_rq_5xx: 4\r\nlistener.admin.http.admin.downstream_rq_completed: 2652\r\nlistener.admin.main_thread.downstream_cx_active: 1\r\nlistener.admin.main_thread.downstream_cx_total: 2652\r\nlistener.admin.no_filter_chain_match: 0\r\nlistener_manager.lds.init_fetch_timeout: 0\r\nlistener_manager.lds.update_attempt: 1440\r\nlistener_manager.lds.update_failure: 1\r\nlistener_manager.lds.update_rejected: 0\r\nlistener_manager.lds.update_success: 1438\r\nlistener_manager.lds.version: 310770956959226155\r\nlistener_manager.listener_added: 2\r\nlistener_manager.listener_create_failure: 0\r\nlistener_manager.listener_create_success: 4\r\nlistener_manager.listener_modified: 0\r\nlistener_manager.listener_removed: 0\r\nlistener_manager.listener_stopped: 0\r\nlistener_manager.total_listeners_active: 2\r\nlistener_manager.total_listeners_draining: 0\r\nlistener_manager.total_listeners_warming: 0\r\nruntime.admin_overrides_active: 0\r\nruntime.deprecated_feature_use: 0\r\nruntime.load_error: 0\r\nruntime.load_success: 1\r\nruntime.num_keys: 0\r\nruntime.num_layers: 2\r\nruntime.override_dir_exists: 0\r\nruntime.override_dir_not_exists: 1\r\nserver.concurrency: 2\r\nserver.days_until_first_cert_expiring: 0\r\nserver.debug_assertion_failures: 0\r\nserver.dynamic_unknown_fields: 0\r\nserver.hot_restart_epoch: 0\r\nserver.live: 1\r\nserver.main_thread.watchdog_mega_miss: 0\r\nserver.main_thread.watchdog_miss: 0\r\nserver.memory_allocated: 4368128\r\nserver.memory_heap_size: 6291456\r\nserver.parent_connections: 0\r\nserver.state: 0\r\nserver.static_unknown_fields: 0\r\nserver.stats_recent_lookups: 0\r\nserver.total_connections: 0\r\nserver.uptime: 7211\r\nserver.version: 9381141\r\nserver.watchdog_mega_miss: 0\r\nserver.watchdog_miss: 0\r\nserver.worker_0.watchdog_mega_miss: 0\r\nserver.worker_0.watchdog_miss: 0\r\nserver.worker_1.watchdog_mega_miss: 0\r\nserver.worker_1.watchdog_miss: 0\r\ntls_inspector.alpn_found: 90\r\ntls_inspector.alpn_not_found: 20\r\ntls_inspector.client_hello_too_large: 0\r\ntls_inspector.connection_closed: 2841\r\ntls_inspector.read_error: 0\r\ntls_inspector.sni_found: 109\r\ntls_inspector.sni_not_found: 1\r\ntls_inspector.tls_found: 110\r\ntls_inspector.tls_not_found: 0\r\nupdate_attempt: 2878\r\nupdate_failure: 0\r\nupdate_rejected: 0\r\nupdate_success: 2876\r\nversion: 310770956959226155\r\n</details>\r\n\r\nConfig:\r\n\r\n<details><summary>config</summary>\r\n<p>\r\n```json\r\n{\r\n \"configs\": [\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.BootstrapConfigDump\",\r\n   \"bootstrap\": {\r\n    \"node\": {\r\n     \"id\": \"ingressnode\",\r\n     \"cluster\": \"cluster1\",\r\n     \"build_version\": \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n    },\r\n    \"static_resources\": {\r\n     \"clusters\": [\r\n      {\r\n       \"name\": \"rtcontroller-ads\",\r\n       \"type\": \"STRICT_DNS\",\r\n       \"connect_timeout\": \"25s\",\r\n       \"tls_context\": {\r\n        \"common_tls_context\": {\r\n         \"tls_certificates\": [\r\n          {\r\n           \"certificate_chain\": {\r\n            \"filename\": \"/home/cert/tls.crt\"\r\n           },\r\n           \"private_key\": {\r\n            \"filename\": \"/home/cert/tls.key\"\r\n           }\r\n          }\r\n         ],\r\n         \"validation_context\": {\r\n          \"trusted_ca\": {\r\n           \"filename\": \"home/rootca/tls.crt\"\r\n          }\r\n         }\r\n        }\r\n       },\r\n       \"http2_protocol_options\": {},\r\n       \"upstream_connection_options\": {\r\n        \"tcp_keepalive\": {\r\n         \"keepalive_probes\": 3,\r\n         \"keepalive_time\": 30,\r\n         \"keepalive_interval\": 5\r\n        }\r\n       },\r\n       \"load_assignment\": {\r\n        \"cluster_name\": \"---ads\",\r\n        \"endpoints\": [\r\n         {\r\n          \"lb_endpoints\": [\r\n           {\r\n            \"endpoint\": {\r\n             \"address\": {\r\n              \"socket_address\": {\r\n               \"address\": \"...svc.cluster.local\",\r\n               \"port_value\": 443\r\n              }\r\n             }\r\n            }\r\n           }\r\n          ]\r\n         }\r\n        ]\r\n       },\r\n       \"respect_dns_ttl\": true\r\n      }\r\n     ]\r\n    },\r\n    \"dynamic_resources\": {\r\n     \"lds_config\": {\r\n      \"ads\": {}\r\n     },\r\n     \"cds_config\": {\r\n      \"ads\": {}\r\n     },\r\n     \"ads_config\": {\r\n      \"api_type\": \"GRPC\",\r\n      \"grpc_services\": [\r\n       {\r\n        \"envoy_grpc\": {\r\n         \"cluster_name\": \"..-ads\"\r\n        }\r\n       }\r\n      ],\r\n      \"rate_limit_settings\": {\r\n       \"max_tokens\": 10,\r\n       \"fill_rate\": 1\r\n      }\r\n     }\r\n    },\r\n    \"admin\": {\r\n     \"access_log_path\": \"/dev/stdout\",\r\n     \"address\": {\r\n      \"socket_address\": {\r\n       \"address\": \"0.0.0.0\",\r\n       \"port_value\": 9901\r\n      }\r\n     }\r\n    }\r\n   },\r\n   \"last_updated\": \"2020-08-14T09:13:04.710Z\"\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.ClustersConfigDump\",\r\n   \"version_info\": \"---\",\r\n   \"static_clusters\": [\r\n    {\r\n     \"cluster\": {\r\n      \"name\": \"rtcontroller-ads\",\r\n      \"type\": \"STRICT_DNS\",\r\n      \"connect_timeout\": \"25s\",\r\n      \"tls_context\": {\r\n       \"common_tls_context\": {\r\n        \"tls_certificates\": [\r\n         {\r\n          \"certificate_chain\": {\r\n           \"filename\": \"/home/cert/tls.crt\"\r\n          },\r\n          \"private_key\": {\r\n           \"filename\": \"/home/cert/tls.key\"\r\n          }\r\n         }\r\n        ],\r\n        \"validation_context\": {\r\n         \"trusted_ca\": {\r\n          \"filename\": \"home/rootca/tls.crt\"\r\n         }\r\n        }\r\n       }\r\n      },\r\n      \"http2_protocol_options\": {},\r\n      \"upstream_connection_options\": {\r\n       \"tcp_keepalive\": {\r\n        \"keepalive_probes\": 3,\r\n        \"keepalive_time\": 30,\r\n        \"keepalive_interval\": 5\r\n       }\r\n      },\r\n      \"load_assignment\": {\r\n       \"cluster_name\": \"..-ads\",\r\n       \"endpoints\": [\r\n        {\r\n         \"lb_endpoints\": [\r\n          {\r\n           \"endpoint\": {\r\n            \"address\": {\r\n             \"socket_address\": {\r\n              \"address\": \"..svc.cluster.local\",\r\n              \"port_value\": 443\r\n             }\r\n            }\r\n           }\r\n          }\r\n         ]\r\n        }\r\n       ]\r\n      },\r\n      \"respect_dns_ttl\": true\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:04.715Z\"\r\n    }\r\n   ],\r\n   \"dynamic_active_clusters\": [\r\n    {\r\n     \"version_info\": \"----\",\r\n     \"cluster\": {\r\n      \"name\": \"----\",\r\n      \"type\": \"STRICT_DNS\",\r\n      \"connect_timeout\": \"5s\",\r\n      \"circuit_breakers\": {\r\n       \"thresholds\": [\r\n        {\r\n         \"max_connections\": 8000\r\n        },\r\n        {\r\n         \"priority\": \"HIGH\",\r\n         \"max_connections\": 10000\r\n        }\r\n       ]\r\n      },\r\n      \"transport_socket\": {\r\n       \"name\": \"envoy.transport_sockets.tls\",\r\n       \"typed_config\": {\r\n        \"@type\": \"type.googleapis.com/envoy.api.v2.auth.UpstreamTlsContext\",\r\n        \"common_tls_context\": {\r\n         \"validation_context\": {\r\n          \"trusted_ca\": {\r\n           \"filename\": \"/home/rootca/tls.crt\"\r\n          }\r\n         }\r\n        },\r\n        \"sni\": \"..svc.cluster.local\"\r\n       }\r\n      },\r\n      \"load_assignment\": {\r\n       \"cluster_name\": \"----\",\r\n       \"endpoints\": [\r\n        {\r\n         \"lb_endpoints\": [\r\n          {\r\n           \"endpoint\": {\r\n            \"address\": {\r\n             \"socket_address\": {\r\n              \"address\": \"...svc.cluster.local\",\r\n              \"port_value\": 8443\r\n             }\r\n            }\r\n           }\r\n          }\r\n         ]\r\n        }\r\n       ]\r\n      },\r\n      \"respect_dns_ttl\": true\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.576Z\"\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.ListenersConfigDump\",\r\n   \"version_info\": \"----\",\r\n   \"dynamic_active_listeners\": [\r\n    {\r\n     \"version_info\": \"----\",\r\n     \"listener\": {\r\n      \"name\": \"listener-ingress-http\",\r\n      \"address\": {\r\n       \"socket_address\": {\r\n        \"address\": \"0.0.0.0\",\r\n        \"port_value\": 10000\r\n       }\r\n      },\r\n      \"filter_chains\": [\r\n       {\r\n        \"filters\": [\r\n         {\r\n          \"name\": \"envoy.http_connection_manager\",\r\n          \"typed_config\": {\r\n           \"@type\": \"type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\",\r\n           \"stat_prefix\": \"ingress_http\",\r\n           \"http_filters\": [\r\n            {\r\n             \"name\": \"envoy.router\"\r\n            }\r\n           ],\r\n           \"add_user_agent\": true,\r\n           \"access_log\": [\r\n            {\r\n             \"name\": \"envoy.file_access_log\",\r\n             \"typed_config\": {\r\n              \"@type\": \"type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\",\r\n              \"path\": \"/dev/stdout\",\r\n              \"format\": \"%START_TIME% [INFO] [INGRESS]:[Request_Id: %REQ(X-REQUEST-ID)%], Protocol: %PROTOCOL%, Method : %REQ(:METHOD)%, Path: %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%, Response Code: %RESPONSE_CODE%, Host: %REQ(:AUTHORITY)%, Duration: %DURATION%, Upstream Service Time: %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%, Bytes Received: %BYTES_RECEIVED%, Bytes Sent: %BYTES_SENT%\\n\"\r\n             }\r\n            }\r\n           ],\r\n           \"rds\": {\r\n            \"config_source\": {\r\n             \"ads\": {}\r\n            },\r\n            \"route_config_name\": \"ingress-http-route-config\"\r\n           }\r\n          }\r\n         }\r\n        ]\r\n       }\r\n      ],\r\n      \"listener_filters\": [\r\n       {\r\n        \"name\": \"envoy.listener.original_dst\"\r\n       }\r\n      ]\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.578Z\"\r\n    },\r\n    {\r\n     \"version_info\": \"---\",\r\n     \"listener\": {\r\n      \"name\": \"listener-ingress-tls\",\r\n      \"address\": {\r\n       \"socket_address\": {\r\n        \"address\": \"0.0.0.0\",\r\n        \"port_value\": 10443\r\n       }\r\n      },\r\n      \"filter_chains\": [\r\n       {\r\n        \"filter_chain_match\": {\r\n         \"server_names\": [\r\n          \"t1-vh1.apim.sap.com\"\r\n         ]\r\n        },\r\n        \"filters\": [\r\n         {\r\n          \"name\": \"envoy.http_connection_manager\",\r\n          \"typed_config\": {\r\n           \"@type\": \"type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\",\r\n           \"stat_prefix\": \"ingress_http\",\r\n           \"http_filters\": [\r\n            {\r\n             \"name\": \"envoy.router\"\r\n            }\r\n           ],\r\n           \"add_user_agent\": true,\r\n           \"access_log\": [\r\n            {\r\n             \"name\": \"envoy.file_access_log\",\r\n             \"typed_config\": {\r\n              \"@type\": \"type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\",\r\n              \"path\": \"/dev/stdout\",\r\n              \"format\": \"%START_TIME% [INFO] [INGRESS]:[Request_Id: %REQ(X-REQUEST-ID)%], Protocol: %PROTOCOL%, Method : %REQ(:METHOD)%, Path: %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%, Response Code: %RESPONSE_CODE%, Host: %REQ(:AUTHORITY)%, Duration: %DURATION%, Upstream Service Time: %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)%, Bytes Received: %BYTES_RECEIVED%, Bytes Sent: %BYTES_SENT%\\n\"\r\n             }\r\n            }\r\n           ],\r\n           \"rds\": {\r\n            \"config_source\": {\r\n             \"ads\": {}\r\n            },\r\n            \"route_config_name\": \"route----\"\r\n           }\r\n          }\r\n         }\r\n        ],\r\n        \"transport_socket\": {\r\n         \"name\": \"envoy.transport_sockets.tls\",\r\n         \"typed_config\": {\r\n          \"@type\": \"type.googleapis.com/envoy.api.v2.auth.DownstreamTlsContext\",\r\n          \"common_tls_context\": {\r\n           \"tls_certificate_sds_secret_configs\": [\r\n            {\r\n             \"name\": \"---\",\r\n             \"sds_config\": {\r\n              \"ads\": {}\r\n             }\r\n            }\r\n           ],\r\n           \"validation_context_sds_secret_config\": {\r\n            \"name\": \"---\",\r\n            \"sds_config\": {\r\n             \"ads\": {}\r\n            }\r\n           }\r\n          }\r\n         }\r\n        },\r\n        \"name\": \"filterchain-t1-t1-vh1\"\r\n       }\r\n      ],\r\n      \"listener_filters\": [\r\n       {\r\n        \"name\": \"envoy.listener.tls_inspector\"\r\n       }\r\n      ]\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.579Z\"\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.ScopedRoutesConfigDump\"\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.RoutesConfigDump\",\r\n   \"dynamic_route_configs\": [\r\n    {\r\n     \"version_info\": \"---\",\r\n     \"route_config\": {\r\n      \"name\": \"route-1--\",\r\n      \"virtual_hosts\": [\r\n       {\r\n        \"name\": \"route-1\",\r\n        \"domains\": [\r\n         \"---\"\r\n        ],\r\n        \"routes\": [\r\n         {\r\n          \"match\": {\r\n           \"prefix\": \"/\"\r\n          },\r\n          \"route\": {\r\n           \"cluster\": \"test\",\r\n           \"timeout\": \"15s\",\r\n           \"idle_timeout\": \"315s\"\r\n          }\r\n         }\r\n        ]\r\n       }\r\n      ]\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.581Z\"\r\n    },\r\n    {\r\n     \"version_info\": \"---\",\r\n     \"route_config\": {\r\n      \"name\": \"ingress-http-route-config\"\r\n     },\r\n     \"last_updated\": \"2020-08-14T09:13:31.581Z\"\r\n    }\r\n   ]\r\n  },\r\n  {\r\n   \"@type\": \"type.googleapis.com/envoy.admin.v2alpha.SecretsConfigDump\",\r\n   \"dynamic_active_secrets\": [\r\n    {\r\n     \"name\": \"---\",\r\n     \"version_info\": \"---\",\r\n     \"last_updated\": \"2020-08-14T09:15:54.416Z\",\r\n     \"secret\": {\r\n      \"name\": \"----\",\r\n      \"tls_certificate\": {\r\n       \"certificate_chain\": {\r\n        \"inline_bytes\": \"---\"\r\n       },\r\n       \"private_key\": {\r\n        \"inline_string\": \"[redacted]\"\r\n       }\r\n      }\r\n     }\r\n    },\r\n    {\r\n     \"name\": \"----\",\r\n     \"version_info\": \"----\",\r\n     \"last_updated\": \"2020-08-14T09:15:54.416Z\",\r\n     \"secret\": {\r\n      \"name\": \"----\",\r\n      \"validation_context\": {\r\n       \"trusted_ca\": {\r\n        \"inline_bytes\": \"-------\"\r\n       }\r\n      }\r\n     }\r\n    }\r\n   ]\r\n  }\r\n ]\r\n}\r\n```\r\n</p>\r\n</details>\r\n\r\nLogs:\r\n\r\n<details><summary>Log</summary>\r\n[2020-08-14 10:35:44.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:35:44.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:35:44.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:44.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:44.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:44.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:44.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1953] new connection\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1953] socket event: 2\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1953] write ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1953] socket event: 3\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1953] write ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1953] read ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1953] read returns: 118\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1953] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1953] parsing 118 bytes\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1953] message begin\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1953] new stream\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1953] headers complete\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1953] completed header: key=Connection value=close\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1953] message complete\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1953][S5255068674068296761] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1953][S5255068674068296761] request end stream\r\n[2020-08-14 10:35:44.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1953][S5255068674068296761] request complete: path: /ready\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1953][S5255068674068296761] closing connection due to connection close header\r\n[2020-08-14 10:35:44.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1953][S5255068674068296761] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:44 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1953] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1953][S5255068674068296761] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1953] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1953] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1953][S5255068674068296761] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:44.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1953] parsed 118 bytes\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1953] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:44.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1953] socket event: 2\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1953] write ready\r\n[2020-08-14 10:35:44.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1953] write returns: 243\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1953] write flush complete\r\n[2020-08-14 10:35:44.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1953] closing socket: 1\r\n[2020-08-14 10:35:44.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1953] adding to cleanup list\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:44.636][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:44.636][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:45.041][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:35:47.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1954] new connection\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1954] socket event: 2\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1954] write ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1954] socket event: 3\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1954] write ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1954] read ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1954] read returns: 118\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1954] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1954] parsing 118 bytes\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1954] message begin\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1954] new stream\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1954] headers complete\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1954] completed header: key=Connection value=close\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1954] message complete\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1954][S14082497283590410047] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1954][S14082497283590410047] request end stream\r\n[2020-08-14 10:35:47.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1954][S14082497283590410047] request complete: path: /ready\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1954][S14082497283590410047] closing connection due to connection close header\r\n[2020-08-14 10:35:47.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1954][S14082497283590410047] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:47 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1954] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1954][S14082497283590410047] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1954] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1954] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1954][S14082497283590410047] decode headers called: filter=0x564e43373f80 status=1\r\n[2020-08-14 10:35:47.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1954] parsed 118 bytes\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1954] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:47.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1954] socket event: 2\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1954] write ready\r\n[2020-08-14 10:35:47.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1954] write returns: 243\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1954] write flush complete\r\n[2020-08-14 10:35:47.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1954] closing socket: 1\r\n[2020-08-14 10:35:47.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1954] adding to cleanup list\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:47.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:49.523][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:49.523][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.523][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 4644\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 2674\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7318 bytes into 1 slices\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7318 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4635 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3953\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 231 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=430 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:49.524][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:35:49.524][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3954\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 191 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=339 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3955\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 246 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1878 end_stream=false)\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:35:49.524][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:35:49.524][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:49.524][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:35:49.52335533 +0000 UTC\r\n[2020-08-14 10:35:49.524][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:35:49.52335533 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3956\"\r\n\r\n[2020-08-14 10:35:49.524][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7318 bytes\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 869 bytes, end_stream false\r\n[2020-08-14 10:35:49.524][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:49.524][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 869\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:49.525][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:49.525][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:49.777][14][debug][router] [source/common/router/router.cc:724] [C1916][S6524082276721005270] upstream timeout\r\n[2020-08-14 10:35:49.777][14][debug][router] [source/common/router/router.cc:1564] [C1916][S6524082276721005270] resetting pool request\r\n[2020-08-14 10:35:49.777][14][debug][client] [source/common/http/codec_client.cc:111] [C1948] request reset\r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/common/network/connection_impl.cc:104] [C1948] closing data_to_write=0 type=1\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/common/network/connection_impl.cc:193] [C1948] closing socket: 1\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C1948] SSL shutdown: rc=0\r\n[2020-08-14 10:35:49.777][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C1948] \r\n[2020-08-14 10:35:49.777][14][debug][client] [source/common/http/codec_client.cc:88] [C1948] disconnect. resetting 0 pending requests\r\n[2020-08-14 10:35:49.777][14][debug][pool] [source/common/http/http1/conn_pool.cc:136] [C1948] client disconnected, failure reason: \r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=2)\r\n[2020-08-14 10:35:49.777][14][debug][http] [source/common/http/conn_manager_impl.cc:1354] [C1916][S6524082276721005270] Sending local reply with details upstream_response_timeout\r\n[2020-08-14 10:35:49.777][14][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1916][S6524082276721005270] encoding headers via codec (end_stream=false):\r\n':status', '504'\r\n'content-length', '24'\r\n'content-type', 'text/plain'\r\n'date', 'Fri, 14 Aug 2020 10:35:49 GMT'\r\n'server', 'envoy'\r\n\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 130 bytes, end_stream false\r\n[2020-08-14 10:35:49.777][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1916][S6524082276721005270] encoding data via codec (size=24 end_stream=true)\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 24 bytes, end_stream false\r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=3)\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=false disable=false\r\n[2020-08-14 10:35:49.777][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=3)\r\n2020-08-14T10:35:34.778Z [INFO] [ACCESS_LOG_INGRESS]:[Request_Id: 5dde75fe-11c3-446e-8e21-478fb13c82cc], Protocol: HTTP/1.1, Method : GET, Path: /pthru?d=20000, Response Code: 504, Host: test.host, Duration: 14999, Upstream Service Time: -, Bytes Received: 0, Bytes Sent: 24\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:35:49.777][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1916] ssl write returns: 154\r\n[2020-08-14 10:35:50.042][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 3\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1916] read ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: 244\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: -1\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1916] ssl read 244 bytes into 1 slices\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1916] parsing 244 bytes\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1916] message begin\r\n[2020-08-14 10:35:50.204][14][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1916] new stream\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Host value=test.host\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=User-Agent value=PostmanRuntime/7.26.3\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept value=*/*\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Cache-Control value=no-cache\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Postman-Token value=89dd6ee9-2ec4-405b-8303-5743fc405d59\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept-Encoding value=gzip, deflate, br\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1916] headers complete\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Connection value=keep-alive\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1916] message complete\r\n[2020-08-14 10:35:50.204][14][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1916][S14993064456358697936] request headers complete (end_stream=true):\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '89dd6ee9-2ec4-405b-8303-5743fc405d59'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'connection', 'keep-alive'\r\n\r\n[2020-08-14 10:35:50.204][14][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1916][S14993064456358697936] request end stream\r\n[2020-08-14 10:35:50.204][14][debug][router] [source/common/router/router.cc:434] [C1916][S14993064456358697936] cluster 'policyengine-t1' match for URL '/pthru?d=20000'\r\n[2020-08-14 10:35:50.204][14][debug][router] [source/common/router/router.cc:549] [C1916][S14993064456358697936] router decoding headers:\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n':scheme', 'https'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '89dd6ee9-2ec4-405b-8303-5743fc405d59'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'x-forwarded-proto', 'https'\r\n'x-envoy-downstream-service-cluster', 'cluster1'\r\n'x-envoy-downstream-service-node', 'ingressnode'\r\n'x-request-id', '7c1115df-3b51-458c-ab2e-ce744b40c6b0'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2020-08-14 10:35:50.204][14][debug][pool] [source/common/http/http1/conn_pool.cc:95] creating a new connection\r\n[2020-08-14 10:35:50.204][14][debug][client] [source/common/http/codec_client.cc:31] [C1955] connecting\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/common/network/connection_impl.cc:711] [C1955] connecting to 100.69.188.67:8443\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/common/network/connection_impl.cc:720] [C1955] connection in progress\r\n[2020-08-14 10:35:50.204][14][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1916][S14993064456358697936] decode headers called: filter=0x564e4337e060 status=1\r\n[2020-08-14 10:35:50.204][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1916] parsed 244 bytes\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=true disable=true\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 2\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/common/network/connection_impl.cc:559] [C1955] connected\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C1955] handshake expecting read\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 3\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.204][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:176] [C1955] handshake complete\r\n[2020-08-14 10:35:50.204][14][debug][client] [source/common/http/codec_client.cc:69] [C1955] connected\r\n[2020-08-14 10:35:50.204][14][debug][pool] [source/common/http/http1/conn_pool.cc:249] [C1955] attaching to next request\r\n[2020-08-14 10:35:50.204][14][debug][router] [source/common/router/router.cc:1618] [C1916][S14993064456358697936] pool ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1955] writing 448 bytes, end_stream false\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1955] ssl write returns: 448\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1955] read ready\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1955] ssl read returns: -1\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1955] ssl read 0 bytes into 0 slices\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 2\r\n[2020-08-14 10:35:50.204][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:35:50.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1956] new connection\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1956] socket event: 2\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1956] write ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1956] socket event: 3\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1956] write ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1956] read ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1956] read returns: 118\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1956] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1956] parsing 118 bytes\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1956] message begin\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1956] new stream\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1956] headers complete\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1956] completed header: key=Connection value=close\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1956] message complete\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1956][S6915782699599929615] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1956][S6915782699599929615] request end stream\r\n[2020-08-14 10:35:50.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1956][S6915782699599929615] request complete: path: /ready\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1956][S6915782699599929615] closing connection due to connection close header\r\n[2020-08-14 10:35:50.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1956][S6915782699599929615] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:50 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1956] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1956][S6915782699599929615] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1956] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1956] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1956][S6915782699599929615] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:50.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1956] parsed 118 bytes\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1956] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:50.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1956] socket event: 2\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1956] write ready\r\n[2020-08-14 10:35:50.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1956] write returns: 243\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1956] write flush complete\r\n[2020-08-14 10:35:50.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1956] closing socket: 1\r\n[2020-08-14 10:35:50.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1956] adding to cleanup list\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:50.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:53.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1957] new connection\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1957] socket event: 2\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1957] write ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1957] socket event: 3\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1957] write ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1957] read ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1957] read returns: 118\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1957] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1957] parsing 118 bytes\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1957] message begin\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1957] new stream\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1957] headers complete\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1957] completed header: key=Connection value=close\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1957] message complete\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1957][S1757894607638928359] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1957][S1757894607638928359] request end stream\r\n[2020-08-14 10:35:53.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1957][S1757894607638928359] request complete: path: /ready\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1957][S1757894607638928359] closing connection due to connection close header\r\n[2020-08-14 10:35:53.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1957][S1757894607638928359] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:53 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1957] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1957][S1757894607638928359] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1957] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1957] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1957][S1757894607638928359] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:53.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1957] parsed 118 bytes\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1957] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:53.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1957] socket event: 2\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1957] write ready\r\n[2020-08-14 10:35:53.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1957] write returns: 243\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1957] write flush complete\r\n[2020-08-14 10:35:53.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1957] closing socket: 1\r\n[2020-08-14 10:35:53.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1957] adding to cleanup list\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:53.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3957\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:35:54.514][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:35:54.514][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3958\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:54.514][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:35:54.514][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3959\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:54.513647013 +0000 UTC\r\n[2020-08-14 10:35:54.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:35:54.513647013 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3960\"\r\n\r\n[2020-08-14 10:35:54.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:35:54.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:54.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:54.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:54.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:35:54.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:54.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:55.039][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:35:56.121][14][debug][router] [source/common/router/router.cc:724] [C1633][S10994230258690132099] upstream timeout\r\n[2020-08-14 10:35:56.121][14][debug][router] [source/common/router/router.cc:1564] [C1633][S10994230258690132099] resetting pool request\r\n[2020-08-14 10:35:56.121][14][debug][client] [source/common/http/codec_client.cc:111] [C1951] request reset\r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/common/network/connection_impl.cc:104] [C1951] closing data_to_write=0 type=1\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/common/network/connection_impl.cc:193] [C1951] closing socket: 1\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:298] [C1951] SSL shutdown: rc=0\r\n[2020-08-14 10:35:56.121][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:226] [C1951] \r\n[2020-08-14 10:35:56.121][14][debug][client] [source/common/http/codec_client.cc:88] [C1951] disconnect. resetting 0 pending requests\r\n[2020-08-14 10:35:56.121][14][debug][pool] [source/common/http/http1/conn_pool.cc:136] [C1951] client disconnected, failure reason: \r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=2)\r\n[2020-08-14 10:35:56.121][14][debug][http] [source/common/http/conn_manager_impl.cc:1354] [C1633][S10994230258690132099] Sending local reply with details upstream_response_timeout\r\n[2020-08-14 10:35:56.121][14][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1633][S10994230258690132099] encoding headers via codec (end_stream=false):\r\n':status', '504'\r\n'content-length', '24'\r\n'content-type', 'text/plain'\r\n'date', 'Fri, 14 Aug 2020 10:35:56 GMT'\r\n'server', 'envoy'\r\n\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1633] writing 130 bytes, end_stream false\r\n[2020-08-14 10:35:56.121][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1633][S10994230258690132099] encoding data via codec (size=24 end_stream=true)\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1633] writing 24 bytes, end_stream false\r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=3)\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1633] readDisable: enabled=false disable=false\r\n[2020-08-14 10:35:56.121][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=3)\r\n2020-08-14T10:35:41.121Z [INFO] [ACCESS_LOG_INGRESS]:[Request_Id: a68e0545-1402-4aa9-b3c9-bd527a89b31d], Protocol: HTTP/1.1, Method : GET, Path: /pthru?d=20000, Response Code: 504, Host: test.host, Duration: 15000, Upstream Service Time: -, Bytes Received: 0, Bytes Sent: 24\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1633] socket event: 2\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1633] write ready\r\n[2020-08-14 10:35:56.121][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1633] ssl write returns: 154\r\n[2020-08-14 10:35:56.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1958] new connection\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1958] socket event: 2\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1958] write ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1958] socket event: 3\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1958] write ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1958] read ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1958] read returns: 118\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1958] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1958] parsing 118 bytes\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1958] message begin\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1958] new stream\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1958] headers complete\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1958] completed header: key=Connection value=close\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1958] message complete\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1958][S9710212564483937592] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1958][S9710212564483937592] request end stream\r\n[2020-08-14 10:35:56.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1958][S9710212564483937592] request complete: path: /ready\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1958][S9710212564483937592] closing connection due to connection close header\r\n[2020-08-14 10:35:56.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1958][S9710212564483937592] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:56 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1958] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1958][S9710212564483937592] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1958] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1958] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1958][S9710212564483937592] decode headers called: filter=0x564e4337eba0 status=1\r\n[2020-08-14 10:35:56.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1958] parsed 118 bytes\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1958] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:56.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1958] socket event: 2\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1958] write ready\r\n[2020-08-14 10:35:56.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1958] write returns: 243\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1958] write flush complete\r\n[2020-08-14 10:35:56.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1958] closing socket: 1\r\n[2020-08-14 10:35:56.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1958] adding to cleanup list\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:56.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3961\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3962\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:35:59.511][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:35:59.511][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3963\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:59.511][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:35:59.511][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:35:59.511][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:35:59.510821916 +0000 UTC\r\n[2020-08-14 10:35:59.511][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:35:59.510821916 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3964\"\r\n\r\n[2020-08-14 10:35:59.511][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:35:59.511][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.511][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 43\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 43 bytes into 1 slices\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 43 bytes\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 43 bytes\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:35:59.512][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:35:59.512][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:35:59.528][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:92] starting async DNS resolution for sapapim-rtcontroller.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:35:59.528][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.529][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.529][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.529][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.530][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.531][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.532][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:35:59.532][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:99] async DNS resolution complete for sapapim-rtcontroller.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:35:59.532][6][debug][upstream] [source/common/upstream/upstream_impl.cc:250] transport socket match, socket default selected for host with address 100.68.89.88:443\r\n[2020-08-14 10:35:59.532][6][debug][upstream] [source/common/upstream/strict_dns_cluster.cc:156] DNS refresh rate reset for sapapim-rtcontroller.apim-lrt.svc.cluster.local, refresh rate 30000 ms\r\n[2020-08-14 10:35:59.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1959] new connection\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1959] socket event: 2\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1959] write ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1959] socket event: 3\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1959] write ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1959] read ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1959] read returns: 118\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1959] read error: Resource temporarily unavailable\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1959] parsing 118 bytes\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1959] message begin\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1959] new stream\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1959] headers complete\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1959] completed header: key=Connection value=close\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1959] message complete\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1959][S9808164756817688405] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1959][S9808164756817688405] request end stream\r\n[2020-08-14 10:35:59.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1959][S9808164756817688405] request complete: path: /ready\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1959][S9808164756817688405] closing connection due to connection close header\r\n[2020-08-14 10:35:59.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1959][S9808164756817688405] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:35:59 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1959] writing 228 bytes, end_stream false\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1959][S9808164756817688405] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1959] writing 15 bytes, end_stream false\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1959] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1959][S9808164756817688405] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:35:59.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1959] parsed 118 bytes\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1959] closing data_to_write=243 type=2\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:35:59.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1959] socket event: 2\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1959] write ready\r\n[2020-08-14 10:35:59.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1959] write returns: 243\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1959] write flush complete\r\n[2020-08-14 10:35:59.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1959] closing socket: 1\r\n[2020-08-14 10:35:59.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1959] adding to cleanup list\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:35:59.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:00.041][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:36:02.308][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:92] starting async DNS resolution for policyengine-t1.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:36:02.308][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.308][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.309][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.309][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.310][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.311][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.311][6][trace][upstream] [source/common/network/dns_impl.cc:160] Setting DNS resolution timer for 5000 milliseconds\r\n[2020-08-14 10:36:02.312][6][trace][upstream] [source/common/upstream/strict_dns_cluster.cc:99] async DNS resolution complete for policyengine-t1.apim-lrt.svc.cluster.local\r\n[2020-08-14 10:36:02.312][6][debug][upstream] [source/common/upstream/upstream_impl.cc:250] transport socket match, socket default selected for host with address 100.69.188.67:8443\r\n[2020-08-14 10:36:02.312][6][debug][upstream] [source/common/upstream/strict_dns_cluster.cc:156] DNS refresh rate reset for policyengine-t1.apim-lrt.svc.cluster.local, refresh rate 30000 ms\r\n[2020-08-14 10:36:02.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1960] new connection\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1960] socket event: 2\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1960] write ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1960] socket event: 3\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1960] write ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1960] read ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1960] read returns: 118\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1960] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1960] parsing 118 bytes\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1960] message begin\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1960] new stream\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1960] headers complete\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1960] completed header: key=Connection value=close\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1960] message complete\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1960][S5133849145229437806] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1960][S5133849145229437806] request end stream\r\n[2020-08-14 10:36:02.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1960][S5133849145229437806] request complete: path: /ready\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1960][S5133849145229437806] closing connection due to connection close header\r\n[2020-08-14 10:36:02.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1960][S5133849145229437806] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:02 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1960] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1960][S5133849145229437806] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1960] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1960] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1960][S5133849145229437806] decode headers called: filter=0x564e4337e6c0 status=1\r\n[2020-08-14 10:36:02.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1960] parsed 118 bytes\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1960] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:02.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1960] socket event: 2\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1960] write ready\r\n[2020-08-14 10:36:02.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1960] write returns: 243\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1960] write flush complete\r\n[2020-08-14 10:36:02.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1960] closing socket: 1\r\n[2020-08-14 10:36:02.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1960] adding to cleanup list\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:02.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1341] socket event: 3\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1341] write ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1341] read ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1341] ssl read returns: 244\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1341] ssl read returns: -1\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1341] ssl read 244 bytes into 1 slices\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1341] parsing 244 bytes\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1341] message begin\r\n[2020-08-14 10:36:02.954][14][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1341] new stream\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Host value=test.host\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=User-Agent value=PostmanRuntime/7.22.0\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Accept value=*/*\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Cache-Control value=no-cache\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Postman-Token value=3e3bae22-3458-489f-b8df-c909387119d7\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Accept-Encoding value=gzip, deflate, br\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1341] headers complete\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1341] completed header: key=Connection value=keep-alive\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1341] message complete\r\n[2020-08-14 10:36:02.954][14][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1341][S5885534533524513809] request headers complete (end_stream=true):\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n'user-agent', 'PostmanRuntime/7.22.0'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '3e3bae22-3458-489f-b8df-c909387119d7'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'connection', 'keep-alive'\r\n\r\n[2020-08-14 10:36:02.954][14][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1341][S5885534533524513809] request end stream\r\n[2020-08-14 10:36:02.954][14][debug][router] [source/common/router/router.cc:434] [C1341][S5885534533524513809] cluster 'policyengine-t1' match for URL '/pthru?d=20000'\r\n[2020-08-14 10:36:02.954][14][debug][router] [source/common/router/router.cc:549] [C1341][S5885534533524513809] router decoding headers:\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n':scheme', 'https'\r\n'user-agent', 'PostmanRuntime/7.22.0'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '3e3bae22-3458-489f-b8df-c909387119d7'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'x-forwarded-proto', 'https'\r\n'x-envoy-downstream-service-cluster', 'cluster1'\r\n'x-envoy-downstream-service-node', 'ingressnode'\r\n'x-request-id', 'f5f4b35c-0e90-4197-b2c3-6186e4bb0fe5'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2020-08-14 10:36:02.954][14][debug][pool] [source/common/http/http1/conn_pool.cc:95] creating a new connection\r\n[2020-08-14 10:36:02.954][14][debug][client] [source/common/http/codec_client.cc:31] [C1961] connecting\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/common/network/connection_impl.cc:711] [C1961] connecting to 100.69.188.67:8443\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/common/network/connection_impl.cc:720] [C1961] connection in progress\r\n[2020-08-14 10:36:02.954][14][debug][pool] [source/common/http/conn_pool_base.cc:20] queueing request due to no available connections\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1341][S5885534533524513809] decode headers called: filter=0x564e4337e120 status=1\r\n[2020-08-14 10:36:02.954][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1341] parsed 244 bytes\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1341] readDisable: enabled=true disable=true\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1341] socket event: 2\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1341] write ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1961] socket event: 2\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/common/network/connection_impl.cc:559] [C1961] connected\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:191] [C1961] handshake expecting read\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1961] socket event: 3\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:02.954][14][debug][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:176] [C1961] handshake complete\r\n[2020-08-14 10:36:02.954][14][debug][client] [source/common/http/codec_client.cc:69] [C1961] connected\r\n[2020-08-14 10:36:02.954][14][debug][pool] [source/common/http/http1/conn_pool.cc:249] [C1961] attaching to next request\r\n[2020-08-14 10:36:02.954][14][debug][router] [source/common/router/router.cc:1618] [C1341][S5885534533524513809] pool ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1961] writing 448 bytes, end_stream false\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1961] ssl write returns: 448\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1961] read ready\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1961] ssl read returns: -1\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1961] ssl read 0 bytes into 0 slices\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1961] socket event: 2\r\n[2020-08-14 10:36:02.954][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1961] write ready\r\n[2020-08-14 10:36:04.326][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: 0\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.326][13][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:04.326][13][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.326][13][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.449][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1962] new connection\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1962] socket event: 2\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1962] write ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1962] socket event: 3\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1962] write ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1962] read ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1962] read returns: 118\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1962] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1962] parsing 118 bytes\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1962] message begin\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1962] new stream\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1962] headers complete\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1962] completed header: key=Connection value=close\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1962] message complete\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1962][S2492011494506520494] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1962][S2492011494506520494] request end stream\r\n[2020-08-14 10:36:04.449][6][debug][admin] [source/server/http/admin.cc:1200] [C1962][S2492011494506520494] request complete: path: /ready\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1962][S2492011494506520494] closing connection due to connection close header\r\n[2020-08-14 10:36:04.449][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1962][S2492011494506520494] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:04 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1962] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1962][S2492011494506520494] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1962] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1962] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1962][S2492011494506520494] decode headers called: filter=0x564e4337e6c0 status=1\r\n[2020-08-14 10:36:04.449][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1962] parsed 118 bytes\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1962] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:04.449Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1962] socket event: 2\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1962] write ready\r\n[2020-08-14 10:36:04.449][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1962] write returns: 243\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1962] write flush complete\r\n[2020-08-14 10:36:04.449][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1962] closing socket: 1\r\n[2020-08-14 10:36:04.449][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1962] adding to cleanup list\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.449][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:04.515][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.515][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.515][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3965\"\r\n\r\n[2020-08-14 10:36:04.515][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.515][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.515][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:36:04.515][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:04.516][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:36:04.516][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3966\"\r\n\r\n[2020-08-14 10:36:04.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3967\"\r\n\r\n[2020-08-14 10:36:04.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:04.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:36:04.516][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:36:04.516][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:04.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:36:04.515233532 +0000 UTC\r\n[2020-08-14 10:36:04.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:36:04.515233532 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3968\"\r\n\r\n[2020-08-14 10:36:04.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:36:04.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:04.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:36:04.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.636][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:04.636][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:04.636][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.041][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 3\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1955] read ready\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1955] ssl read returns: 144\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1955] ssl read returns: -1\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1955] ssl read 144 bytes into 1 slices\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1955] parsing 144 bytes\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1955] message begin\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Content-Length value=14\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Content-Type value=text/plain\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Date value=Fri, 14 Aug 2020 10:36:05 GMT\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1955] headers complete\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1955] completed header: key=Server value=envoy\r\n[2020-08-14 10:36:05.206][14][debug][router] [source/common/router/router.cc:1036] [C1916][S14993064456358697936] upstream headers complete: end_stream=false\r\n[2020-08-14 10:36:05.206][14][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1916][S14993064456358697936] encoding headers via codec (end_stream=false):\r\n':status', '408'\r\n'content-length', '14'\r\n'content-type', 'text/plain'\r\n'date', 'Fri, 14 Aug 2020 10:36:05 GMT'\r\n'server', 'envoy'\r\n'x-envoy-upstream-service-time', '15001'\r\n\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 168 bytes, end_stream false\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1916][S14993064456358697936] encoding data via codec (size=14 end_stream=false)\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1916] writing 14 bytes, end_stream false\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1955] message complete\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:827] [C1955] message complete\r\n[2020-08-14 10:36:05.206][14][debug][client] [source/common/http/codec_client.cc:101] [C1955] response complete\r\n[2020-08-14 10:36:05.206][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1916][S14993064456358697936] encoding data via codec (size=0 end_stream=true)\r\n[2020-08-14 10:36:05.206][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=2)\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=false disable=false\r\n[2020-08-14 10:36:05.206][14][debug][pool] [source/common/http/http1/conn_pool.cc:206] [C1955] response complete\r\n[2020-08-14 10:36:05.206][14][debug][pool] [source/common/http/http1/conn_pool.cc:244] [C1955] moving to ready\r\n[2020-08-14 10:36:05.206][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1955] parsed 144 bytes\r\n[2020-08-14 10:36:05.206][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=2)\r\n2020-08-14T10:35:50.204Z [INFO] [ACCESS_LOG_INGRESS]:[Request_Id: 7c1115df-3b51-458c-ab2e-ce744b40c6b0], Protocol: HTTP/1.1, Method : GET, Path: /pthru?d=20000, Response Code: 408, Host: test.host, Duration: 15001, Upstream Service Time: 15001, Bytes Received: 0, Bytes Sent: 14\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:36:05.206][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1916] ssl write returns: 182\r\n[2020-08-14 10:36:05.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1963] new connection\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1963] socket event: 2\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1963] write ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1963] socket event: 3\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1963] write ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1963] read ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1963] read returns: 118\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1963] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1963] parsing 118 bytes\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1963] message begin\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1963] new stream\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1963] headers complete\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1963] completed header: key=Connection value=close\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1963] message complete\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1963][S17788979837976716579] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1963][S17788979837976716579] request end stream\r\n[2020-08-14 10:36:05.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1963][S17788979837976716579] request complete: path: /ready\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1963][S17788979837976716579] closing connection due to connection close header\r\n[2020-08-14 10:36:05.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1963][S17788979837976716579] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:05 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1963] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1963][S17788979837976716579] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1963] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1963] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1963][S17788979837976716579] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:36:05.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1963] parsed 118 bytes\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1963] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:05.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1963] socket event: 2\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1963] write ready\r\n[2020-08-14 10:36:05.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1963] write returns: 243\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1963] write flush complete\r\n[2020-08-14 10:36:05.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1963] closing socket: 1\r\n[2020-08-14 10:36:05.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1963] adding to cleanup list\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 3\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/common/network/connection_impl.cc:500] [C1916] read ready\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: 244\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C1916] ssl read returns: -1\r\n[2020-08-14 10:36:05.726][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C1916] ssl read 244 bytes into 1 slices\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1916] parsing 244 bytes\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1916] message begin\r\n[2020-08-14 10:36:05.726][14][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1916] new stream\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Host value=test.host\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=User-Agent value=PostmanRuntime/7.26.3\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept value=*/*\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Cache-Control value=no-cache\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Postman-Token value=353fdf1b-00a7-48f1-a93d-bb552c479330\r\n[2020-08-14 10:36:05.726][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Accept-Encoding value=gzip, deflate, br\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1916] headers complete\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1916] completed header: key=Connection value=keep-alive\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1916] message complete\r\n[2020-08-14 10:36:05.727][14][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1916][S332189432177281115] request headers complete (end_stream=true):\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '353fdf1b-00a7-48f1-a93d-bb552c479330'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'connection', 'keep-alive'\r\n\r\n[2020-08-14 10:36:05.727][14][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1916][S332189432177281115] request end stream\r\n[2020-08-14 10:36:05.727][14][debug][router] [source/common/router/router.cc:434] [C1916][S332189432177281115] cluster 'policyengine-t1' match for URL '/pthru?d=20000'\r\n[2020-08-14 10:36:05.727][14][debug][router] [source/common/router/router.cc:549] [C1916][S332189432177281115] router decoding headers:\r\n':authority', 'test.host'\r\n':path', '/pthru?d=20000'\r\n':method', 'GET'\r\n':scheme', 'https'\r\n'user-agent', 'PostmanRuntime/7.26.3'\r\n'accept', '*/*'\r\n'cache-control', 'no-cache'\r\n'postman-token', '353fdf1b-00a7-48f1-a93d-bb552c479330'\r\n'accept-encoding', 'gzip, deflate, br'\r\n'x-forwarded-proto', 'https'\r\n'x-envoy-downstream-service-cluster', 'cluster1'\r\n'x-envoy-downstream-service-node', 'ingressnode'\r\n'x-request-id', '247beb82-5d01-43da-8e7a-c49d6901d664'\r\n'x-envoy-expected-rq-timeout-ms', '15000'\r\n\r\n[2020-08-14 10:36:05.727][14][debug][pool] [source/common/http/http1/conn_pool.cc:104] [C1955] using existing connection\r\n[2020-08-14 10:36:05.727][14][debug][router] [source/common/router/router.cc:1618] [C1916][S332189432177281115] pool ready\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:398] [C1955] writing 448 bytes, end_stream false\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1916][S332189432177281115] decode headers called: filter=0x564e4337e060 status=1\r\n[2020-08-14 10:36:05.727][14][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1916] parsed 244 bytes\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:293] [C1916] readDisable: enabled=true disable=true\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1955] socket event: 2\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1955] write ready\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C1955] ssl write returns: 448\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:462] [C1916] socket event: 2\r\n[2020-08-14 10:36:05.727][14][trace][connection] [source/common/network/connection_impl.cc:550] [C1916] write ready\r\n[2020-08-14 10:36:08.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1964] new connection\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1964] socket event: 2\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1964] write ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1964] socket event: 3\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1964] write ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1964] read ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1964] read returns: 118\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1964] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1964] parsing 118 bytes\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1964] message begin\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1964] new stream\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1964] headers complete\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1964] completed header: key=Connection value=close\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1964] message complete\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1964][S6323848685696604929] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1964][S6323848685696604929] request end stream\r\n[2020-08-14 10:36:08.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1964][S6323848685696604929] request complete: path: /ready\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1964][S6323848685696604929] closing connection due to connection close header\r\n[2020-08-14 10:36:08.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1964][S6323848685696604929] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:08 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1964] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1964][S6323848685696604929] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1964] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1964] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1964][S6323848685696604929] decode headers called: filter=0x564e4337e7e0 status=1\r\n[2020-08-14 10:36:08.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1964] parsed 118 bytes\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1964] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:08.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1964] socket event: 2\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1964] write ready\r\n[2020-08-14 10:36:08.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1964] write returns: 243\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1964] write flush complete\r\n[2020-08-14 10:36:08.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1964] closing socket: 1\r\n[2020-08-14 10:36:08.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1964] adding to cleanup list\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:08.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:09.513][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3969\"\r\n\r\n[2020-08-14 10:36:09.513][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 232 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:36:09.513][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:36:09.513][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3970\"\r\n\r\n[2020-08-14 10:36:09.513][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:09.513][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:36:09.513][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.513][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3971\"\r\n\r\n[2020-08-14 10:36:09.513][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:09.513][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:09.513][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=340 end_stream=false)\r\n[2020-08-14 10:36:09.513][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.RouteConfiguration at version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.514][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.RouteConfiguration accepted with 1 resources with version 2020-08-14 10:36:09.512752108 +0000 UTC\r\n[2020-08-14 10:36:09.514][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.RouteConfiguration: version_info: \"2020-08-14 10:36:09.512752108 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"route-t1-t1-vh1\"\r\nresource_names: \"ingress-http-route-config\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.RouteConfiguration\"\r\nresponse_nonce: \"3972\"\r\n\r\n[2020-08-14 10:36:09.514][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 247 bytes\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 7322 bytes\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 873 bytes, end_stream false\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=0\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 873\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 30\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 30 bytes into 1 slices\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 30 bytes\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=8, flags=0\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=8\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=6, flags=0\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=6\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:424] [C5] dispatched 30 bytes\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:666] [C5] about to send frame type=6, flags=1\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:720] [C5] send data: bytes=17\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:398] [C5] writing 17 bytes, end_stream false\r\n[2020-08-14 10:36:09.514][6][trace][http2] [source/common/http/http2/codec_impl.cc:608] [C5] sent frame type=6\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 2\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:09.514][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:259] [C5] ssl write returns: 17\r\n[2020-08-14 10:36:10.039][6][debug][main] [source/server/server.cc:175] flushing stats\r\n[2020-08-14 10:36:11.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:335] [C1965] new connection\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1965] socket event: 2\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1965] write ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1965] socket event: 3\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1965] write ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:500] [C1965] read ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:24] [C1965] read returns: 118\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:38] [C1965] read error: Resource temporarily unavailable\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:409] [C1965] parsing 118 bytes\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:561] [C1965] message begin\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:259] [C1965] new stream\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=Host value=100.96.1.47:9901\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=User-Agent value=kube-probe/1.18\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=Accept-Encoding value=gzip\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:503] [C1965] headers complete\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:371] [C1965] completed header: key=Connection value=close\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:548] [C1965] message complete\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:708] [C1965][S5175704883643059425] request headers complete (end_stream=true):\r\n':authority', '100.96.1.47:9901'\r\n':path', '/ready'\r\n':method', 'GET'\r\n'user-agent', 'kube-probe/1.18'\r\n'accept-encoding', 'gzip'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1257] [C1965][S5175704883643059425] request end stream\r\n[2020-08-14 10:36:11.612][6][debug][admin] [source/server/http/admin.cc:1200] [C1965][S5175704883643059425] request complete: path: /ready\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1490] [C1965][S5175704883643059425] closing connection due to connection close header\r\n[2020-08-14 10:36:11.612][6][debug][http] [source/common/http/conn_manager_impl.cc:1552] [C1965][S5175704883643059425] encoding headers via codec (end_stream=false):\r\n':status', '200'\r\n'content-type', 'text/plain; charset=UTF-8'\r\n'cache-control', 'no-cache, max-age=0'\r\n'x-content-type-options', 'nosniff'\r\n'date', 'Fri, 14 Aug 2020 10:36:11 GMT'\r\n'server', 'envoy'\r\n'connection', 'close'\r\n\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1965] writing 228 bytes, end_stream false\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/conn_manager_impl.cc:1700] [C1965][S5175704883643059425] encoding data via codec (size=5 end_stream=true)\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:398] [C1965] writing 15 bytes, end_stream false\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1965] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/conn_manager_impl.cc:966] [C1965][S5175704883643059425] decode headers called: filter=0x564e4337f080 status=1\r\n[2020-08-14 10:36:11.612][6][trace][http] [source/common/http/http1/codec_impl.cc:430] [C1965] parsed 118 bytes\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:104] [C1965] closing data_to_write=243 type=2\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14T10:36:11.612Z] \"GET /ready HTTP/1.1\" 200 - 0 5 0 - \"10.250.6.76\" \"kube-probe/1.18\" \"-\" \"100.96.1.47:9901\" \"-\"\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:462] [C1965] socket event: 2\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/connection_impl.cc:550] [C1965] write ready\r\n[2020-08-14 10:36:11.612][6][trace][connection] [source/common/network/raw_buffer_socket.cc:67] [C1965] write returns: 243\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:589] [C1965] write flush complete\r\n[2020-08-14 10:36:11.612][6][debug][connection] [source/common/network/connection_impl.cc:193] [C1965] closing socket: 1\r\n[2020-08-14 10:36:11.612][6][debug][conn_handler] [source/server/connection_handler_impl.cc:88] [C1965] adding to cleanup list\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:11.612][6][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.326][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:14.326][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:14.327][14][debug][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:72] tls inspector: new connection accepted\r\n[2020-08-14 10:36:14.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:165] tls inspector: recv: -1\r\n[2020-08-14 10:36:14.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.327][14][trace][filter] [source/extensions/filters/listener/tls_inspector/tls_inspector.cc:186] tls inspector: done: false\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:160] item added to deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.327][14][trace][main] [source/common/event/dispatcher_impl.cc:79] clearing deferred deletion list (size=1)\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/common/network/connection_impl.cc:462] [C5] socket event: 3\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/common/network/connection_impl.cc:550] [C5] write ready\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/common/network/connection_impl.cc:500] [C5] read ready\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: 7322\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:80] [C5] ssl read returns: -1\r\n[2020-08-14 10:36:14.516][6][trace][connection] [source/extensions/transport_sockets/tls/ssl_socket.cc:155] [C5] ssl read 7322 bytes into 1 slices\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:405] [C5] dispatching 7322 bytes\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:14.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=431 end_stream=false)\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Cluster at version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:14.516][6][info][upstream] [source/common/upstream/cds_api_impl.cc:67] cds: add 1 cluster(s), remove 1 cluster(s)\r\n[2020-08-14 10:36:14.516][6][debug][upstream] [source/common/upstream/cds_api_impl.cc:85] cds: add/update cluster 'policyengine-t1' skipped\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.ClusterLoadAssignment\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Cluster accepted with 1 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.516][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Cluster: version_info: \"2020-08-14 10:36:14.515810246 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Cluster\"\r\nresponse_nonce: \"3973\"\r\n\r\n[2020-08-14 10:36:14.516][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 192 bytes\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:14.516][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:14.516][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=1879 end_stream=false)\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.Listener at version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/common/config/grpc_mux_impl.cc:101] Pausing discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-http hash=15192452732192267292\r\n[2020-08-14 10:36:14.516][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-http'. no add/update\r\n[2020-08-14 10:36:14.516][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-http' skipped\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/server/listener_manager_impl.cc:269] begin add/update listener: name=listener-ingress-tls hash=9325604105818206458\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/server/listener_manager_impl.cc:280] duplicate/locked listener 'listener-ingress-tls'. no add/update\r\n[2020-08-14 10:36:14.517][6][debug][upstream] [source/server/lds_api.cc:66] lds: add/update listener 'listener-ingress-tls' skipped\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_impl.cc:109] Resuming discovery requests for type.googleapis.com/envoy.api.v2.RouteConfiguration\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.Listener accepted with 2 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.Listener: version_info: \"2020-08-14 10:36:14.515810246 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\ntype_url: \"type.googleapis.com/envoy.api.v2.Listener\"\r\nresponse_nonce: \"3974\"\r\n\r\n[2020-08-14 10:36:14.517][6][trace][router] [source/common/router/router.cc:1487] [C0][S6236246387049523829] proxying 193 bytes\r\n[2020-08-14 10:36:14.517][6][trace][http2] [source/common/http/http2/codec_impl.cc:468] [C5] about to recv frame type=0, flags=0\r\n[2020-08-14 10:36:14.517][6][trace][http2] [source/common/http/http2/codec_impl.cc:483] [C5] recv frame type=0\r\n[2020-08-14 10:36:14.517][6][trace][http] [source/common/http/async_client_impl.cc:109] async http request response data (length=4636 end_stream=false)\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_impl.cc:132] Received gRPC message for type.googleapis.com/envoy.api.v2.auth.Secret at version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][debug][config] [source/common/config/grpc_mux_subscription_impl.cc:62] gRPC config for type.googleapis.com/envoy.api.v2.auth.Secret accepted with 1 resources with version 2020-08-14 10:36:14.515810246 +0000 UTC\r\n[2020-08-14 10:36:14.517][6][trace][config] [source/common/config/grpc_mux_impl.cc:63] Sending DiscoveryRequest for type.googleapis.com/envoy.api.v2.auth.Secret: version_info: \"2020-08-14 10:36:14.515810246 +0000 UTC\"\r\nnode {\r\n  id: \"ingressnode\"\r\n  cluster: \"cluster1\"\r\n  build_version: \"8f2515a19bdcc75bea0bfd7016231a7661d0be6e/1.12.2/Clean/RELEASE/BoringSSL\"\r\n}\r\nresource_names: \"testmst\"\r\nresource_names: \"t1-mockkeystore\"\r\ntype_url: \"type.googleapis.com/envoy.api.v2.auth.Secret\"\r\nresponse_nonce: \"3975\"\r\n\r\n</details>",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/12665/comments",
    "author": "ankur-anand",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-08-16T21:07:08Z",
        "body": "There have been various changes in this area over the months. Please try with a more recent build and report back. In general though a 408 can happen if idle timeouts, buffer timeouts, etc. are hit."
      },
      {
        "user": "ankur-anand",
        "created_at": "2020-09-01T08:15:51Z",
        "body": "@mattklein123 Thank you for the help. Didn't received such inconsistency in the newer version. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of the conditions under which Envoy would return 408 vs 504 for timeout scenarios",
      "Identification of potential idle timeout interactions with request timeout settings",
      "Differentiation between network-level timeouts and application-level timeouts"
    ]
  },
  {
    "number": 11012,
    "title": "Question: Envoy configured to use V3 connects to /v2/discovery:clusters",
    "created_at": "2020-04-30T12:53:22Z",
    "closed_at": "2020-05-01T10:20:21Z",
    "labels": [
      "question",
      "area/xds"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/11012",
    "body": "Envoy 1.14.1 is configured to use transport_api_version: V3 for REST xDS. However it still sends requests to \"/v2/discovery:routes\" and \"/v2/discovery:clusters\". Am I missing something obvious here ?\r\n\r\n\r\nConfig:\r\n```\r\ndynamic_resources:\r\n  cds_config:\r\n    api_config_source:\r\n      api_type: REST\r\n      cluster_names: [xds_cluster]\r\n      refresh_delay: 5s\r\n      transport_api_version: V3\r\n\r\n...\r\n          rds:\r\n            route_config_name: Route_configuration\r\n            config_source:\r\n              api_config_source:\r\n                api_type: REST\r\n                cluster_names: [xds_cluster]\r\n                refresh_delay: 5s\r\n                transport_api_version: V3\r\n\r\n```\r\n\r\nLog:\r\n\r\nRoutes:\r\n```\r\n[2020-04-30 12:41:44.722][6][debug][router] [source/common/router/router.cc:477] [C0][S2429303885158800883] cluster 'xds_cluster' match for URL '/v2/discovery:routes'\r\n[2020-04-30 12:41:44.722][6][debug][router] [source/common/router/router.cc:634] [C0][S2429303885158800883] router decoding headers:\r\n':method', 'POST'\r\n':path', '/v2/discovery:routes'\r\n':authority', 'xds_cluster'\r\n':scheme', 'http'\r\n'content-type', 'application/json'\r\n'content-length', '11287'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.2'\r\n'x-envoy-expected-rq-timeout-ms', '1000'\r\n\r\n```\r\nClusters:\r\n```\r\n[2020-04-30 12:41:37.982][6][debug][config] [source/common/config/http_subscription_impl.cc:68] Sending REST request for /v2/discovery:clusters\r\n[2020-04-30 12:41:37.983][6][debug][router] [source/common/router/router.cc:477] [C0][S7534484415178178826] cluster 'xds_cluster' match for URL '/v2/discovery:clusters'\r\n[2020-04-30 12:41:37.983][6][debug][router] [source/common/router/router.cc:634] [C0][S7534484415178178826] router decoding headers:\r\n':method', 'POST'\r\n':path', '/v2/discovery:clusters'\r\n':authority', 'xds_cluster'\r\n':scheme', 'http'\r\n'content-type', 'application/json'\r\n'content-length', '11246'\r\n'x-envoy-internal', 'true'\r\n'x-forwarded-for', '172.17.0.2'\r\n'x-envoy-expected-rq-timeout-ms', '1000'\r\n\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/11012/comments",
    "author": "andrewtikhonov",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-04-30T16:30:44Z",
        "body": "cc @htuch @Shikugawa "
      },
      {
        "user": "htuch",
        "created_at": "2020-05-01T01:06:22Z",
        "body": "I think this is definitely not the correct behavior Envoy side, but curious what happens if you set `resource_api_version` to v3 as well?"
      },
      {
        "user": "andrewtikhonov",
        "created_at": "2020-05-01T10:02:50Z",
        "body": "Thanks. `resource_api_version` enabled it.\r\n\r\n```\r\ndynamic_resources:\r\n  cds_config:\r\n    resource_api_version: V3\r\n    api_config_source:\r\n      api_type: REST\r\n      cluster_names: [xds_cluster]\r\n      refresh_delay: 5s\r\n\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of the relationship between transport_api_version and resource_api_version in Envoy configuration",
      "Clarification of required configuration parameters for V3 xDS API usage",
      "Identification of all version-related configuration settings affecting REST endpoint paths",
      "Documentation of version compatibility between different Envoy configuration components"
    ]
  },
  {
    "number": 10967,
    "title": "Clarifications on upstream_rq_time and downstream_rq_time",
    "created_at": "2020-04-27T18:48:00Z",
    "closed_at": "2020-06-06T09:10:40Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10967",
    "body": "Hi, I am trying to understand better what `upstream_rq_time` and `downstream_rq_time` exactly measure. I referred to the documentation but it wasn't clear to me. For ex. consider  `request: service A -> Envoy A -> Envoy B -> Service B; response: Service B -> Envoy B -> Envoy A -> Service A`, what do `upstream_rq_time` and `downstream_rq_time` mean here? How is the total RTT calculated?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10967/comments",
    "author": "shashankram",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-04-27T19:27:39Z",
        "body": "Envoy A's downstream_rq_time measures the time elapsed from when Envoy A starts handling Service A's request until the entire response has sent to Service A.\r\n\r\nEnvoy A's upstream_rq_time measures the time elapsed from the point where Service A's entire request has been received by the HTTP router filter until the entire upstream response from Envoy B has been received.\r\n\r\nThe same is true for Envoy B, except the downstream is Envoy A's request/response and the upstream is Service B.\r\n\r\nSo Envoy A's downstream_rq_time > Envoy A's upstream_rq_time > Envoy B's downtream_rq_time > Envoy B's upstream_rq_time.\r\n\r\nAssuming there are no blocking filters in use (e.g. ext_auth) I expect the times to be reasonably close together."
      },
      {
        "user": "shashankram",
        "created_at": "2020-04-28T00:30:45Z",
        "body": "Thanks for clarifying!"
      },
      {
        "user": "stale[bot]",
        "created_at": "2020-06-06T09:10:39Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ],
    "satisfaction_conditions": [
      "Clear definition of the start and end points for upstream_rq_time and downstream_rq_time in a multi-Envoy service chain",
      "Explanation of how these metrics relate hierarchically between Envoy instances",
      "Description of how total RTT can be derived from the combination of these metrics",
      "Clarification of the directionality (request vs response flow) captured by each metric"
    ]
  },
  {
    "number": 10952,
    "title": "examples/jaeger-tracing and examples/zipkin-tracing protobuf Bootstrap has unknown fields",
    "created_at": "2020-04-25T23:05:16Z",
    "closed_at": "2020-04-28T16:01:00Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10952",
    "body": "```\r\n$ cd examples/jaeger-tracing\r\n\r\n$ docker-compose pull\r\n\r\n$ docker-compose build -d\r\n...\r\nfront-envoy_1  | [2020-04-25 23:00:48.737][6][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:00:48.737][6][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10952/comments",
    "author": "chadm-sq",
    "comments": [
      {
        "user": "chadm-sq",
        "created_at": "2020-04-25T23:18:12Z",
        "body": "Same for \"jaeger-native-tracing\" example.\r\n\r\n```\r\nfront-envoy_1  | [2020-04-25 23:17:06.614][10][critical][main] [source/server/server.cc:94] error initializing configuration '/etc/front-envoy.yaml': Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\nfront-envoy_1  | [2020-04-25 23:17:06.614][10][info][main] [source/server/server.cc:602] exiting\r\nfront-envoy_1  | Protobuf message (type envoy.config.bootstrap.v3.Bootstrap reason INVALID_ARGUMENT:(tracing) provider: Cannot find field.) has unknown fields\r\njaeger-native-tracing_front-envoy_1 exited with code 1\r\n```"
      },
      {
        "user": "mk46",
        "created_at": "2020-04-27T13:29:33Z",
        "body": "@cmiller-sq I have tested Jaeger-tracing and Zipkin-tracing. it's working for me. Here,  you need to remove all previous pulled images regarding envoy. "
      },
      {
        "user": "chadm-sq",
        "created_at": "2020-04-28T16:00:59Z",
        "body": "Well, I can' reproduce it now. Sorry for noise."
      }
    ],
    "satisfaction_conditions": [
      "Resolve Protobuf schema compatibility issues in Envoy configuration",
      "Ensure version compatibility between Envoy and tracing configuration",
      "Validate configuration against target Envoy API version"
    ]
  },
  {
    "number": 9937,
    "title": "Too many Host headers caused via lua headers():add(\":a\",\"value\")",
    "created_at": "2020-02-05T13:53:45Z",
    "closed_at": "2020-02-06T07:39:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9937",
    "body": "*Description*:\r\n\r\nThe lua filters makes it possible to add headers which are available\r\nonly for access logs by adding ':' character to them. However if this\r\nkind of \"hidden\" header starts with \":a\" it is used as \":authority\"\r\nand this causes multiple \"host\" headers to appear with HTTP/1.X\r\nrequests. This in turns causes error with at least golang based http server:\r\n\"HTTP/1.1 400 Bad Request: too many Host headers\".\r\n\r\nWith the envoy virtual hosts routes config it is not possible to\r\nmodify the \":\" prefixed headers but it's nicely possible via lua api.\r\n\r\nThe use of \":\" started headers via lua api may be an unplanned feature\r\nbut it makes it possible to have these hidden/log-only headers like\r\nthe Nginx supports via variables. These variables/hidden headers are\r\nvery convenient especially when using external authorizers or other\r\nexternal subrequests where one needs to log the subrequest result\r\nheaders as part of access logs but not pass all of them to the\r\nupstream.\r\n\r\n*Proposed fix*:\r\n\r\nEnvoy should copy only the \":authority\" header as HTTP/1.X \"host\" header and not all of the \":a*\" headers.\r\n\r\n*Proposed new feature*:\r\n\r\nAdd documented support for using hidden/access-log only headers.\r\n\r\n*Repro steps*:\r\n- Use the configuration below\r\n- Have upstream server ready (written in go), or some other server but verify headers with tcpdump.\r\n- make a request to the envoy\r\n- notice that the server returns 400 error (you can verify the issue by using tcpdump to capture the traffic).\r\n\r\n*Config*:\r\n\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8081\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        typed_config:\r\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n          codec_type: auto\r\n          stat_prefix: proxy_http\r\n          http_protocol_options:\r\n            accept_http_10: true\r\n          #Disabled delayed close for the http/1.0 (and all other...)\r\n          delayed_close_timeout: 0.0s\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\r\n              path: /var/log/envoy/envoy-access.log\r\n              format: \"[%START_TIME%] \\\r\n                       host=\\\"%REQ(:AUTHORITY)%\\\" \\\r\n                       m=\\\"%REQ(:METHOD)%\\\" \\\r\n                       st=\\\"%RESPONSE_CODE%\\\" \\\r\n                       ahidden=\\\"%REQ(:AHIDDEN)%\\\" \\\r\n                       bhidden=\\\"%REQ(:BHIDDEN)%\\\" \\\r\n                       \\\"\\n\"\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  path: /envoy-status\r\n                direct_response:\r\n                  status: 200\r\n                  body:\r\n                    inline_string: OK\r\n              - match:\r\n                  prefix: /\r\n                route:\r\n                  cluster: upstream\r\n          http_filters:\r\n          - name: envoy.lua\r\n            typed_config:\r\n              \"@type\": type.googleapis.com/envoy.config.filter.http.lua.v2.Lua\r\n              #################################################################\r\n              inline_code: |\r\n                function envoy_on_request(handle)\r\n                     -- testing request header modifications,\r\n                     -- adding a \"hidden\" log only header starting with \":a\" will be used\r\n                     -- as \":authority\" causing \"HTTP/1.1 400 Bad Request: too many Host headers\" error\r\n                     -- Example headers caused by this example:\r\n                     --     host: localhost:8081\r\n                     --     host: a-value\r\n                     handle:headers():replace(\":ahidden\", \"a-value\")\r\n                     handle:headers():replace(\":bhidden\", \"b-value\")\r\n                end \r\n              ##########################################################\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: upstream\r\n    connect_timeout: 0.25s\r\n    type: strict_dns\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: upstream\r\n      endpoints:\r\n      - lb_endpoints:\r\n        - endpoint:\r\n            address:\r\n              socket_address:\r\n                address: upstream\r\n                port_value: 9090\r\n### LISTENER FOR ADMIN API\r\nadmin:\r\n  access_log_path: \"/var/log/envoy/envoy-admin.log\"\r\n  address:\r\n    socket_address:\r\n      address: 127.0.0.1\r\n      port_value: 8001\r\n\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9937/comments",
    "author": "jusmaki",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-02-05T17:25:58Z",
        "body": "You are relying on undocumented behavior which I don't think we are going to change. Can you use dynamic metadata for this? IIRC it can also be logged. cc @dio "
      },
      {
        "user": "jusmaki",
        "created_at": "2020-02-06T07:39:57Z",
        "body": "Yes, the dynamic metadata works perfectly for this case. Thanks for the the tip. "
      }
    ],
    "satisfaction_conditions": [
      "Provides a method to add log-only headers without modifying upstream request headers",
      "Uses officially supported Envoy features rather than header manipulation hacks",
      "Prevents unintended side effects on HTTP/1.X request processing"
    ]
  },
  {
    "number": 9904,
    "title": "help(build): ./ci/do_ci.sh: Permission denied",
    "created_at": "2020-02-01T13:34:44Z",
    "closed_at": "2020-02-05T05:28:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9904",
    "body": "when I try to build a dev version, I got this error:\r\n\r\n```shell\r\n[root@instance-1 envoy-1.13.0]# pwd\r\n/root/envoy-1.13.0\r\n[root@instance-1 envoy-1.13.0]# ./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.dev'\r\nbash: ./ci/do_ci.sh: Permission denied\r\n[root@instance-1 envoy-1.13.0]# ll ./ci/do_ci.sh\r\n-rwxrwxrwx. 1 root root 14138 Jan 20 22:06 ./ci/do_ci.sh\r\n\r\n[root@instance-1 envoy-1.13.0]# uname -r\r\n3.10.0-1062.9.1.el7.x86_64\r\n[root@instance-1 envoy-1.13.0]# cat /etc/redhat-release\r\nCentOS Linux release 7.7.1908 (Core)\r\n\r\n[root@instance-1 envoy-1.13.0]# docker version\r\nClient:\r\n Version:         1.13.1\r\n API version:     1.26\r\n Package version: docker-1.13.1-108.git4ef4b30.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      4ef4b30/1.13.1\r\n Built:           Tue Jan 21 17:16:25 2020\r\n OS/Arch:         linux/amd64\r\n\r\nServer:\r\n Version:         1.13.1\r\n API version:     1.26 (minimum version 1.12)\r\n Package version: docker-1.13.1-108.git4ef4b30.el7.centos.x86_64\r\n Go version:      go1.10.3\r\n Git commit:      4ef4b30/1.13.1\r\n Built:           Tue Jan 21 17:16:25 2020\r\n OS/Arch:         linux/amd64\r\n Experimental:    false\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9904/comments",
    "author": "membphis",
    "comments": [
      {
        "user": "kylebevans",
        "created_at": "2020-02-01T17:46:47Z",
        "body": "Try this and then build again:\r\n\r\n`mount -o remount,exec /tmp`\r\n\r\nThe run_envoy_docker.sh creates a build dir in /tmp and I bet CentOS mounts tmp with the noexec flag to disallow executing from /tmp for security reasons."
      },
      {
        "user": "membphis",
        "created_at": "2020-02-03T01:27:52Z",
        "body": "It works fine under Ubuntu OS  18.04 :(\r\n\r\nIt seems I have to change my working OS. "
      },
      {
        "user": "kylebevans",
        "created_at": "2020-02-04T15:14:11Z",
        "body": "@membphis glad you got it working. Do you want to close the issue?"
      },
      {
        "user": "membphis",
        "created_at": "2020-02-05T05:28:49Z",
        "body": "many thx"
      }
    ],
    "satisfaction_conditions": [
      "Resolve execution permission issues related to the environment where scripts are run",
      "Ensure compatibility with security restrictions of the host operating system",
      "Provide a method to execute build scripts from temporary directories"
    ]
  },
  {
    "number": 9794,
    "title": "Multiple validation_context's",
    "created_at": "2020-01-23T08:18:12Z",
    "closed_at": "2020-01-29T02:11:08Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9794",
    "body": "Is there a way to setup multiple validation_context's with Envoy, particularly for TCP traffic not HTTP?\r\n\r\nWe'd like to provide a single URL, and use mTLS w/ ~3-10 other external systems. We can't influence the other systems to set unique headers, domains, SNI, or anything else.\r\n\r\nWe could associate their CA certs with their hostname/IP to match on based on that, or we could eat the cost of trying every cert till one matched.\r\n\r\nOr Is this possible to do at the moment leveraging filter chains?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9794/comments",
    "author": "steeling",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2020-01-26T19:03:15Z",
        "body": "As long as you can match on hostname/IP I think you can do this with filter chains? cc @lambdai @PiotrSikora "
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-26T23:16:39Z",
        "body": "Yes, you could use multiple filter chains to do that, but the cost of matching to a particular certificate is negligible (it's just a quick comparison of hashes), so unless you need/want to segregate traffic from those external systems for some reason (it doesn't sound like you do), the best and easiest solution would be to simply provide `trusted_ca` that contains all the trusted CA certificates or provide multiple valid values in `match_subject_alt_names`."
      },
      {
        "user": "steeling",
        "created_at": "2020-01-28T03:03:49Z",
        "body": "can we match on client IP or hostname?\r\n\r\nAll of the traffic will come to a single hostname. At the moment we don't have any influence on enforcing a specific CA to use, so we first request their CA cert with us, and we add it to a list we will verify against"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-28T03:17:07Z",
        "body": "Yes, you can match on the client (source) IP address or subnet.\r\n\r\nYou cannot match on the client's hostname."
      },
      {
        "user": "mattklein123",
        "created_at": "2020-01-28T16:49:19Z",
        "body": "> You cannot match on the client's hostname.\r\n\r\nJust to clarify, \"hostname\" is an L7/HTTP concept. You can match on SNI."
      },
      {
        "user": "PiotrSikora",
        "created_at": "2020-01-28T17:00:29Z",
        "body": "> Just to clarify, \"hostname\" is an L7/HTTP concept. You can match on SNI.\r\n\r\nHe means client's hostname (reverse DNS of client's IP address), not SNI / `Host` / `:authority`."
      },
      {
        "user": "steeling",
        "created_at": "2020-01-29T02:11:08Z",
        "body": "Thanks all, that helps!"
      }
    ],
    "satisfaction_conditions": [
      "Support validation of multiple CA certificates for mTLS without requiring client-side headers/SNI/domain customization",
      "Enable traffic segregation based on client IP addresses or certificate attributes",
      "Handle TCP traffic without relying on HTTP-layer features",
      "Provide certificate validation efficiency when handling multiple CAs"
    ]
  },
  {
    "number": 9689,
    "title": "Leak of information about cluster member",
    "created_at": "2020-01-15T15:44:24Z",
    "closed_at": "2020-01-17T14:47:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9689",
    "body": "Please, how do I retrieve the `envoy_cluster_internal_upstream_rq_xx` by cluster members?\r\n\r\nI have a cluster with 2 members... (load_assignment/lb_endpoints)... but I can't find any information to get errors from each cluster nodes.\r\n\r\nOne of these cluster members is buggy, but I can't find which node. No tip found.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9689/comments",
    "author": "hakuno",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2020-01-15T17:16:15Z",
        "body": "@hakuno does the `/clusters` endpoint provide you the information you need?"
      },
      {
        "user": "hakuno",
        "created_at": "2020-01-15T18:19:22Z",
        "body": "So I can read `rq_error` as bad HTTP status code. Nice!\r\n\r\n```\r\ncluster::node1::rq_active::0\r\ncluster::node1::rq_error::0\r\ncluster::node1::rq_success::94\r\ncluster::node1::rq_timeout::0\r\ncluster::node1::rq_total::94\r\n```\r\n\r\nHow do I read that with the Prometheus/Grafana?"
      },
      {
        "user": "htuch",
        "created_at": "2020-01-15T19:57:44Z",
        "body": "@hakuno I think that question is outside my domain knowledge, so I'll leave this open for others to comment on for the normal question period of time (2 weeks). "
      },
      {
        "user": "mattklein123",
        "created_at": "2020-01-16T01:19:33Z",
        "body": "Per-host stats are not currently exported to the stats sinks, due to cardinality issues."
      },
      {
        "user": "hakuno",
        "created_at": "2020-01-17T14:47:35Z",
        "body": "All right. Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Ability to identify error metrics per individual cluster member",
      "Integration with monitoring tools (Prometheus/Grafana)",
      "Access to per-host upstream request statistics",
      "Solution addresses cardinality concerns in metrics collection"
    ]
  },
  {
    "number": 9423,
    "title": "Ability to force restart of envoy upon certain trigger?",
    "created_at": "2019-12-19T18:05:52Z",
    "closed_at": "2019-12-20T20:53:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9423",
    "body": "*Title*: *Force server restart upon triggered condition?*\r\n\r\n*Description*:\r\nWhen setting up Envoy with a central control plane, the static cluster that serves up the xDS interface is vital to the functioning of the server.  If that cluster isn't up/reachable when Envoy is trying to connect, it goes through it's retries and eventually gets into a state where it's never gonna get updates, but also doesn't shut down.\r\n\r\nThis makes sense from the perspective that the static cluster for xDS is just another cluster, and envoy should stay up.  But now the proxy is in an unrecoverable state and since the server didn't shut down the orchestration tool in use (k8s for instance) won't restart the pod automatically.  \r\n\r\nIs there anything like this in Envoy, or do I need to relegate this to readiness probes in the orchestration tool?  I didn't see anything that could force a shutdown in the docs, but wanted to ask here before I gave up.  \r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9423/comments",
    "author": "justincely",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-12-20T16:15:41Z",
        "body": "> Is there anything like this in Envoy, or do I need to relegate this to readiness probes in the orchestration tool? I didn't see anything that could force a shutdown in the docs, but wanted to ask here before I gave up.\r\n\r\nCorrect, there is nothing like this today and we suggest defining readiness probes that do what you want. We have quite a few different admin endpoints that help with writing such probes."
      },
      {
        "user": "justincely",
        "created_at": "2019-12-20T20:43:34Z",
        "body": "thanks @mattklein123, i suspected as much, but appreciate the quick reply.  Happy to have this closed, as my question is answered.  "
      }
    ],
    "satisfaction_conditions": [
      "A mechanism to detect Envoy's unrecoverable state when xDS control plane connectivity is lost",
      "Integration with orchestration systems to trigger automated restarts",
      "Clear guidance on monitoring xDS cluster health status",
      "Solution that works without requiring Envoy self-termination capability"
    ]
  },
  {
    "number": 9417,
    "title": "any reason on \"no healthy host for HTTP connection pool\"",
    "created_at": "2019-12-19T13:49:47Z",
    "closed_at": "2019-12-23T01:58:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9417",
    "body": "Hello,\r\n\r\nWe have seen many logs like \"no healthy host for HTTP connection pool\". Envoy failed to establish gRPC to peer side. Could someone shed some light on it?\r\n\r\nThanks a lot!\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9417/comments",
    "author": "mailzyok",
    "comments": [
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T06:27:27Z",
        "body": "@mailzyok\r\n\r\n\"no healthy\" is generally because the health check is configured and the upstream machine's health status is unhealth, so when there is a request, there is no way to find a healthy host."
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-20T06:38:36Z",
        "body": "@zyfjeff  Thanks for you comment.\r\n\r\nHere is additional log which indicated the gRPC cannot be established. It was a response from upstream. It is strange the status code is 200. \r\nI am still not clear on what the reason for unhealth, because the upstream service is up and running in K8S. Is there any other log i can check?\r\n\r\n> [2019-12-20 02:39:18.786][18][debug][http] [external/envoy/source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=true):\r\n> ':status', '200'\r\n> 'content-type', 'application/grpc'\r\n> 'grpc-status', '14'\r\n> 'grpc-message', 'no healthy upstream'\r\n> \r\n\r\nThanks."
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T06:45:11Z",
        "body": "@mailzyok can you upload the complete log and config_dump files?"
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-20T06:55:29Z",
        "body": "@zyfjeff Thanks. I cannot upload a complete config_dump log here, could you let me know which part you want to see for config_dump, i will copy it here.\r\n\r\nHere is the log for complete log when the problem happens\r\n\r\n> [2019-12-20 02:39:18.784][18][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:74] cm init: adding: cluster=xds-grpc primary=1 secondary=0\r\n> [2019-12-20 02:39:18.784][18][debug][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:42] Establishing new gRPC bidi stream for rpc StreamAggregatedResources(stream .envoy.api.v2.DiscoveryRequest) returns (stream .envoy.api.v2.DiscoveryResponse);\r\n> \r\n> [2019-12-20 02:39:18.784][18][debug][router] [external/envoy/source/common/router/router.cc:332] [C0][S5272412103284310368] cluster 'xds-grpc' match for URL '/envoy.service.discovery.v2.AggregatedDiscoveryService/StreamAggregatedResources'\r\n> [2019-12-20 02:39:18.784][18][debug][upstream] [external/envoy/source/common/upstream/cluster_manager_impl.cc:1124] no healthy host for HTTP connection pool\r\n> [2019-12-20 02:39:18.786][18][debug][http] [external/envoy/source/common/http/async_client_impl.cc:94] async http request response headers (end_stream=true):\r\n> ':status', '200'\r\n> 'content-type', 'application/grpc'\r\n> 'grpc-status', '14'\r\n> 'grpc-message', 'no healthy upstream'\r\n> \r\n> [2019-12-20 02:39:18.786][18][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:86] gRPC config stream closed: 14, no healthy upstream\r\n> [2019-12-20 02:39:18.786][18][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:49] Unable to establish new stream\r\n> \r\n\r\nHere is the cluster info in config_dump\r\n\r\n>      \"clusters\": [\r\n>       {\r\n>        \"name\": \"prometheus_stats\",\r\n>        \"type\": \"STATIC\",\r\n>        \"connect_timeout\": \"0.250s\",\r\n>        \"hosts\": [\r\n>         {\r\n>          \"socket_address\": {\r\n>           \"address\": \"::1\",\r\n>           \"port_value\": 15000\r\n>          }\r\n>         }\r\n>        ]\r\n>       },\r\n>       {\r\n>        \"name\": \"xds-grpc\",\r\n>        \"type\": \"STRICT_DNS\",\r\n>        \"connect_timeout\": \"10s\",\r\n>        \"hosts\": [\r\n>         {\r\n>          \"socket_address\": {\r\n>           \"address\": \"istio-pilot\",\r\n>           \"port_value\": 15010\r\n>          }\r\n>         }\r\n>        ],\r\n>        \"circuit_breakers\": {\r\n>         \"thresholds\": [\r\n>          {\r\n>           \"max_connections\": 100000,\r\n>           \"max_pending_requests\": 100000,\r\n>           \"max_requests\": 100000\r\n>          },\r\n>          {\r\n>           \"priority\": \"HIGH\",\r\n>           \"max_connections\": 100000,\r\n>           \"max_pending_requests\": 100000,\r\n>           \"max_requests\": 100000\r\n>          }\r\n>         ]\r\n>        },\r\n>        \"http2_protocol_options\": {},\r\n>        \"dns_refresh_rate\": \"300s\",\r\n>        \"upstream_connection_options\": {\r\n>         \"tcp_keepalive\": {\r\n>          \"keepalive_time\": 300\r\n>         }\r\n>        }\r\n>       }\r\n>      ]\r\n> "
      },
      {
        "user": "zyfjeff",
        "created_at": "2019-12-20T07:44:28Z",
        "body": "@mailzyok \r\nAccording to your logs, it should be that you cannot connect to the` istio-pilot` to cause \" no health upstream\", you can confirm whether `istio-pilot.istio-system` can resolve the IP"
      },
      {
        "user": "mailzyok",
        "created_at": "2019-12-23T01:58:13Z",
        "body": "@zyfjeff  Thanks a lot, seems DNS resolution problem, see logs below:\r\n\r\nWhen envoy starting resolution for istio-pilot, it takes 150s no response, i think it is a timeout, but i didn't find the default timeout values,\r\n\r\nThen after 300s, envoy start a new resolution, i think 300s is configured as dns_refresh_rate in static cluster configuration. This time the resolution succeeded.\r\n\r\nI will ask the admin to check the issue of DNS resolution. Thanks again and i would take this opportunity to wish you a Merry Xmas and Happy new Year.\r\n\r\n> [2019-12-20 08:32:22.245][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:88] starting async DNS resolution for istio-pilot\r\n> [2019-12-20 08:34:52.249][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:95] async DNS resolution complete for istio-pilot \r\n> [2019-12-20 08:39:52.252][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:88] starting async DNS resolution for istio-pilot\r\n> [2019-12-20 08:39:52.253][20][trace][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:95] async DNS resolution complete for istio-pilot\r\n> [2019-12-20 08:39:52.253][20][debug][upstream] [external/envoy/source/common/upstream/strict_dns_cluster.cc:117] DNS hosts have changed for istio-pilot \n\n---\n\nThank you all for the support. The trace log is useful, we figured out the timeout values finally."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Envoy cannot establish a connection to the upstream service despite it being reported as running in Kubernetes",
      "Identification of specific log types or diagnostic methods to investigate connection failures between Envoy and istio-pilot",
      "Clarification of how DNS resolution timeouts interact with Envoy's cluster configuration",
      "Guidance on validating network connectivity between Envoy and upstream services in Kubernetes environments"
    ]
  },
  {
    "number": 9138,
    "title": "Browser getting static files from rewritten URLs instead of matched route",
    "created_at": "2019-11-26T05:17:37Z",
    "closed_at": "2019-11-27T06:54:27Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9138",
    "body": "*Title*\r\nQuestion: Browser getting static files from rewritten URLs instead of matched route\r\n\r\n*Description*:\r\nCurrently I want Envoy to route to a backend cluster using this endpoint: <some-domain>.com/cluster1.\r\n\r\nThe virtual host config contains the following:\r\n`\r\n              - match:\r\n                  prefix: \"/cluster1/\"\r\n                route:\r\n                  cluster: cluster1\r\n                  prefix_rewrite: \"/\"\r\n`\r\n\r\nThis routes browser's requests to /cluster1 as expected. However, all the static files (js, css...) from this backend is now resolved to root URL. Ex: <some-domain>.com/index.js instead of <some-domain>.com/cluster1/index.js, resulting in 404 error.\r\n\r\nI want to ask if there is a way to configure Envoy to resolve this issue?\r\n\r\nMany thanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9138/comments",
    "author": "RisingSun777",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2019-11-26T21:37:07Z",
        "body": "Envoy doesn't support rewriting responses.\r\n\r\nIt sounds like the responses from cluster1 contains URLs based on the path of requests sent to cluster1. Perhaps that application could use relative URLs?"
      },
      {
        "user": "RisingSun777",
        "created_at": "2019-11-27T06:54:27Z",
        "body": "Yeah changing to using relative URLs from the application side works.\r\n\r\nThanks for your support."
      }
    ],
    "satisfaction_conditions": [
      "Ensures static resource URLs maintain the original path prefix (/cluster1/)",
      "Works within Envoy's limitations of not supporting response rewriting",
      "Preserves routing functionality to backend cluster while maintaining resource accessibility",
      "Does not require absolute URL generation from backend application"
    ]
  },
  {
    "number": 9084,
    "title": "Simple Ratelimit doesnt work with envoy frontproxy",
    "created_at": "2019-11-20T15:46:42Z",
    "closed_at": "2019-11-20T16:19:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/9084",
    "body": "Simple Ratelimit doesnt work with envoy frontproxy\r\n\r\nenvoy front proxy config:\r\n```\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          codec_type: auto\r\n          use_remote_address: true\r\n          stat_prefix: ingress_http\r\n          access_log:\r\n            - name: envoy.file_access_log\r\n              config:\r\n                path: /dev/stdout\r\n          route_config:\r\n            name: local_route\r\n            virtual_hosts:\r\n            - name: backend\r\n              domains: [\"*\"]\r\n              routes:\r\n              - match:\r\n                  prefix: \"/\"\r\n                route:\r\n                  # Send request to an endpoint in the Google cluster\r\n                  cluster: google\r\n                  host_rewrite: www.google.com\r\n                rate_limits:\r\n                  - actions:\r\n                      - remote_address: {}\r\n          http_filters:\r\n          - name: envoy.rate_limit\r\n            config:\r\n              domain: rate_per_ip\r\n              rate_limit_service:\r\n                grpc_service:\r\n                  envoy_grpc:\r\n                    cluster_name: rate_limit_cluster\r\n                  timeout: 0.25s\r\n          - name: envoy.router\r\n            typed_config: {}\r\n  clusters:\r\n  - name: google\r\n    connect_timeout: 1s\r\n    type: logical_dns \r\n    dns_lookup_family: V4_ONLY\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n    hosts: [{ socket_address: { address: google.com, port_value: 443 }}]\r\n    tls_context: { sni: www.google.com }\r\n\r\n  - name: rate_limit_cluster\r\n    type: STATIC \r\n    connect_timeout: 1s\r\n    lb_policy: ROUND_ROBIN\r\n    http2_protocol_options: {}\r\n    hosts: [{ socket_address: { address: 127.0.0.1 , port_value: 8081 }}]\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 8001\r\n```\r\n\r\n\r\n\r\nratelimit config:\r\n```\r\ndomain: rate_per_ip\r\ndescriptors:\r\n  - key: database\r\n    value: users\r\n    rate_limit:\r\n      unit: minute\r\n      requests_per_unit: 5\r\n  - key: remote_address\r\n    rate_limit:\r\n      unit: minute\r\n      requests_per_unit: 5\r\n```\r\n\r\nI can see both ratelimit and envoy working, and can access them, but envoy doesnt hit ratelimit service at all. Please help me.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/9084/comments",
    "author": "nagireddygatla",
    "comments": [
      {
        "user": "junr03",
        "created_at": "2019-11-20T16:10:18Z",
        "body": "in the configuration above the rate_limits field is not indented correctly. The field should be inside of the route field."
      },
      {
        "user": "nagireddygatla",
        "created_at": "2019-11-20T16:19:30Z",
        "body": "Sir, you are such a savior that worked, been struggling with for a week now. very thankful for your help."
      }
    ],
    "satisfaction_conditions": [
      "Correct structural placement of rate_limits configuration in Envoy route definition",
      "Ensures Envoy proxy communicates with rate limit service"
    ]
  },
  {
    "number": 8774,
    "title": "Propagate Cluster.alt_stat_name to RouteEntry",
    "created_at": "2019-10-25T22:44:30Z",
    "closed_at": "2019-10-26T04:23:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8774",
    "body": "We use custom `PassThroughEncoderFilter` which emits some cluster-specific stats.\r\nCurrently we use `streamInfo.routeEntry()->clusterName()` for this purpose, but since Cluster.alt_stat_name is not propagated to RouteEntry we can't achieve consistent logging across different code paths.\r\nWould it make sense to provide cluster's alt_stat_name via RouteEntry?\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8774/comments",
    "author": "veshij",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-10-25T23:08:22Z",
        "body": "You can look up the cluster and then get the alt stat name from cluster info right?"
      },
      {
        "user": "veshij",
        "created_at": "2019-10-26T04:23:32Z",
        "body": "Yep, that should work, sorry for bothering."
      }
    ],
    "satisfaction_conditions": [
      "Maintains compatibility with existing stat emission patterns",
      "Ensures cluster configuration data is available where routing decisions are implemented"
    ]
  },
  {
    "number": 8123,
    "title": "Question : Custom filter to split PUT in several multipart requests",
    "created_at": "2019-09-03T08:02:02Z",
    "closed_at": "2019-09-03T17:38:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8123",
    "body": "Let's say I want to upload a large file to Amazon S3 using their multipart API in which the file is sent in several pieces, but I want to do that in a single upload.\r\n\r\nWould it be possible to use Envoy for that ? i.e. the client makes a single PUT request, Envoy splits the request in several pieces (and thus several requests to the upstream), all that in full streaming (no buffering whatsoever).\r\n\r\nI'm guessing it could be feasible if I code a custom filter, but before investing a lot of time in this I want to make sure I'm not missing some constraints that would make this impossible.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8123/comments",
    "author": "s-vivien",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-09-03T15:08:54Z",
        "body": "Yeah you could do this with a custom filter."
      },
      {
        "user": "htuch",
        "created_at": "2019-09-03T15:23:21Z",
        "body": "@s-vivien does this answer your question? If so, can we close out this issue?"
      },
      {
        "user": "s-vivien",
        "created_at": "2019-09-03T15:37:41Z",
        "body": "It does answer my question. Thanks."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that Envoy's architecture supports splitting a single PUT request into multiple upstream multipart requests",
      "Capability to handle full streaming without buffering during request splitting",
      "Validation that custom filters can manipulate request bodies in a chunked/streaming manner",
      "Absence of architectural limitations preventing request transformation during streaming"
    ]
  },
  {
    "number": 8054,
    "title": "envoy.ext_authz with HTTP with POST method doesn't return the exact status.",
    "created_at": "2019-08-27T15:04:53Z",
    "closed_at": "2019-08-27T16:16:37Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/8054",
    "body": "envoy.ext_authz with HTTP with the POST method doesn't return the exact status.\r\nWe set \r\n\r\n- `with_request_body:\r\n                max_request_bytes: 2048\r\n                allow_partial_message: false`\r\n\r\n- And do a JSON POST, from the server 200 is returned, but the envoy gets \r\n`':status', '504'\r\n'content-length', '24'\r\n'content-type', 'text/plain'`\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/8054/comments",
    "author": "ghost",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-08-27T15:17:27Z",
        "body": "It sounds like the call is timing out, have you tried to increase the timeout to the ext authz server?"
      },
      {
        "user": "ghost",
        "created_at": "2019-08-27T16:16:33Z",
        "body": "it works by increasing the timeout. Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Addresses timeout configuration for the ext_authz service",
      "Ensures the authorization service has sufficient time to process POST requests with bodies"
    ]
  },
  {
    "number": 7441,
    "title": "Trying to add linkopts, Bazel getting in the way...",
    "created_at": "2019-07-02T00:38:22Z",
    "closed_at": "2019-07-02T03:04:57Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7441",
    "body": "Looks like similar opinions were raised before about Bazel but I am also in the opinion that it is overall a hard ecosystem to navigate.\r\n\r\nSo much so that I am unable to have my project (derived from the `filter-example`) link with an additional system library.\r\n\r\nFor simplicity, let's say it is `curl`. I simply want to link with `libcurl.so`. I don't need to build curl from source (although I tried that and failed to do that with Bazel and there aren't many examples except for `tensorflow` and `googlecartographer/async_grpc` projects), I have it installed on my system.\r\n\r\nI am trying to simply add a `-lcurl` to the `linkopts` but since `envoy_cc_binary` wraps the `cc_binary`, I am unable to specify `linkopts`.\r\n\r\nAny help is appreciated. I feel like this should be rather simple but Bazel is getting in the way. :) ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7441/comments",
    "author": "canselcik",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-07-02T02:44:52Z",
        "body": "Presumably since you are trying to link against a system library, you have private code. You can define your own binary target like so and pass whatever linkops you want. E.g.,\r\n\r\n```\r\nenvoy_cc_binary(\r\n    name = \"envoy\",\r\n    repository = \"@envoy\",\r\n    stamped = True,\r\n    deps = LYFT_CUSTOM_FILTER_CONFIGS + [\r\n        \"@envoy//source/exe:envoy_main_entry_lib\",\r\n    ],\r\n)\r\n```"
      },
      {
        "user": "canselcik",
        "created_at": "2019-07-02T03:04:57Z",
        "body": "Thanks @mattklein123, that was actually the first thing I had tried but that had resulted in an obscure Bazel error so I had figured the `envoy_cc_binary` wrapper didn't accept a list of `linkopts`. Turns out it was something else. Perhaps my `bazel clean --expunge` from earlier addressed something.\r\n\r\nThanks for the quick response."
      }
    ],
    "satisfaction_conditions": [
      "Allows linking against system libraries without requiring source-based builds",
      "Works within Bazel's envoy_cc_binary wrapper constraints",
      "Provides a clear pattern for custom binary target configuration",
      "Avoids dependency on complex external build integrations"
    ]
  },
  {
    "number": 7309,
    "title": "Compilation stalls when building Envoy in a container",
    "created_at": "2019-06-18T00:59:00Z",
    "closed_at": "2019-06-19T14:37:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/7309",
    "body": "*Title*: Build progress stalls in the *envoyproxy/envoy-build-ubuntu* container.\r\n\r\n*Description*:\r\n\r\nI checked out the latest master and followed the instructions in `ci` directory - `./ci/run_envoy_docker.sh './ci/do_ci.sh bazel.dev` - to build envoy from source inside the ubuntu container. My host OS is a macOS 10.14.5 with docker desktop installed.\r\n\r\nAfter triggering the above command the container is successfully launched but the compilation stalled at:\r\n\r\n```\r\n[2,182 / 2,334] 151 actions, 2 running\r\n    Compiling source/extensions/filters/http/jwt_authn/extractor.cc; 22s local\r\n    Compiling source/common/router/header_formatter.cc; 17s local\r\n    [Analy] Compiling external/io_opencensus_cpp/opencensus/exporters/trace/stackdriver/internal/stackdriver_exporter.cc; 1828s\r\n    [Analy] Compiling source/common/http/context_impl.cc; 1778s\r\n    [Analy] Compiling source/extensions/tracers/common/ot/opentracing_driver_impl.cc; 1778s\r\n    [Analy] Compiling source/common/router/header_parser.cc; 1778s\r\n    [Analy] Compiling source/extensions/filters/http/buffer/buffer_filter.cc; 1778s\r\n    [Analy] Compiling source/extensions/filters/http/lua/wrappers.cc; 1778s ...\r\n``` \r\n\r\nIf I send Ctrl-C on the terminal, it seems Bazel complained as:\r\n\r\n```\r\n^C\r\nSession terminated, terminating shell...\r\nBazel caught interrupt signal; shutting down.\r\n\r\n\r\nCould not interrupt server (Deadline Exceeded)\r\n```\r\nAnd if I use `docker stop [envoy-build-container-id]`, it doesn't work as well.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/7309/comments",
    "author": "InfoHunter",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-06-18T15:06:33Z",
        "body": "I suspect your docker VM doesn't have enough memory, causing compilation to grind to a halt. Maybe try increasing the resources given to the VM?"
      },
      {
        "user": "InfoHunter",
        "created_at": "2019-06-19T14:37:04Z",
        "body": "Yes, you are right. After I increased the memory from 2GB to 8GB, the problem was gone. Thanks for the help!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies resource allocation constraints as a potential root cause for build stalls",
      "Provides guidance on configuring adequate compute resources for containerized builds",
      "Offers methods to diagnose resource starvation during compilation",
      "Includes verification mechanism for resource-related fixes"
    ]
  },
  {
    "number": 6950,
    "title": "Envoy is crashed when try to log if there is no space",
    "created_at": "2019-05-15T08:30:01Z",
    "closed_at": "2019-05-16T06:09:45Z",
    "labels": [
      "bug",
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6950",
    "body": "**Envoy is crashed when try to log if there is no space**\r\n\r\n*Envoy is crashed when try to log if there is no space*: *Enabled access log and if no space left, envoy is crashed immediately*\r\n\r\n*Description*:\r\nI enabled access logs and on load traffic, the log partition can be full in sometimes. In that time, envoy is crashed and the log is like following. The expected behaviour is not crashed and not try to log if no space left.\r\n\r\nThanks in advance\r\nSincerely\r\n\r\n*Logs*:\r\n[2019-05-14 16:41:00.934][6193][critical][assert] [external/envoy/source/common/access_log/access_log_manager_impl.cc:95] assert failure: result.rc_ == static_cast<ssize_t>(slice.len_).\r\n[2019-05-14 16:41:00.934][6193][critical][backtrace] [bazel-out/k8-dbg/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Aborted, suspect faulting address 0x181e\r\n[2019-05-14 16:41:00.934][6193][critical][backtrace] [bazel-out/k8-dbg/bin/external/envoy/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6950/comments",
    "author": "cihankom",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-05-15T14:37:38Z",
        "body": "@cihankom that's a debug assert. It will be compiled out on release builds. IMO it's reasonable to keep that assert in this case. In release builds the data will be dropped."
      },
      {
        "user": "cihankom",
        "created_at": "2019-05-16T05:58:52Z",
        "body": "Ok, I see. thanks for your help"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Envoy doesn't crash in production environments when disk space is exhausted",
      "Clarification of debug vs release build behavior regarding critical asserts",
      "Confirmation that Envoy handles full disk scenarios gracefully in normal operation"
    ]
  },
  {
    "number": 6930,
    "title": "Envoy doesn't execute automatic retries using 5xx Envoy retry policy ",
    "created_at": "2019-05-14T10:40:44Z",
    "closed_at": "2019-06-20T20:45:10Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6930",
    "body": "**Envoy doesn't execute automatic retries using 5xx Envoy retry policy**\r\n\r\n*Description*:\r\n\r\nI'm interested in some envoy behavior. I implemented email sending service that is used as cluster in envoy configuration.  I want to retry requests using envoy, if each of 500, 502, 503, 504 status codes return back from service to envoy. \r\n\r\n**Bug Template**\r\n\r\n*Description*:\r\n\r\nIn the first case I'm sending several request in parallel with binary file to service through envoy and get 500 status code in response from service or 503 service code, if the service is shut down. I except that envoy automatically retry requests, but retries are not occur.\r\n\r\nIn the second case I'am implement POST request with empty data and in this case I get 500 status code and retries are successfully happen.\r\n\r\n*Repro steps*:\r\n> There are 20 parallel requests in the first case with data binary and in the second case with empty data\r\n\r\n```\r\nfile=$1\r\ncurl --data-binary @${file}.post  -L --post301 --connect-timeout 300 $envoy-url \r\n```\r\n*Admin and Stats Output*:\r\n\r\n/stats/prometheus | grep email-common\r\n>The first case: sending data binary file\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 4183272\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 8380\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 40\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 36\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 0\r\n\r\n```\r\n>The second case: sending request with empty data\r\n```\r\nenvoy_cluster_upstream_cx_tx_bytes_total{envoy_cluster_name=\"email-common\"} 26950\r\nenvoy_cluster_upstream_rq_pending_failure_eject{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_removed{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_attempt{envoy_cluster_name=\"email-common\"} 30\r\nenvoy_cluster_internal_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_subsets_fallback{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_cx_destroy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_change{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_tx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_local_cluster_not_ok{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_fail{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http2_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_total{envoy_cluster_name=\"email-common\"} 23045\r\nenvoy_cluster_bind_errors{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_no_rebuild{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_max_requests{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_drained_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_no_capacity_left{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_connect_attempts_exceeded{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_per_try_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_http1_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_rx_reset{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_internal_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_lb_zone_number_differs{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_close_notify{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_zone_routing_sampled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_original_dst_host_invalid{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_idle_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_cluster_too_small{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_resumed_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_internal_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_upstream_rq_retry_overflow{envoy_cluster_name=\"email-common\"} 13\r\nenvoy_cluster_upstream_rq_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_retry_upstream_rq{envoy_response_code=\"500\",envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_destroy_remote{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_remote_with_active_rq{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_upstream_rq_xx{envoy_response_code_class=\"5\",envoy_cluster_name=\"email-common\"} 20\r\nenvoy_cluster_retry_upstream_rq_completed{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_rq_total{envoy_cluster_name=\"email-common\"} 55\r\nenvoy_cluster_upstream_cx_none_healthy{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_failure{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_local_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_upstream_cx_connect_timeout{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_total{envoy_cluster_name=\"email-common\"} 44\r\nenvoy_cluster_lb_zone_routing_all_directly{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_protocol_error{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_cancelled{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_empty{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_retry_success{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_backed_up_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_recalculate_zone_structures{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_update_success{envoy_cluster_name=\"email-common\"} 31\r\nenvoy_cluster_retry_or_shadow_abandoned{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_created{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_destroy_with_active_rq{envoy_cluster_name=\"email-common\"} 35\r\nenvoy_cluster_lb_zone_routing_cross_zone{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_maintenance_mode{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_flow_control_paused_reading_total{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_healthy_panic{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_selected{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_membership_total{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_circuit_breakers_default_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_tx_bytes_buffered{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_rx_bytes_buffered{envoy_cluster_name=\"email-common\"} 3771\r\nenvoy_cluster_membership_healthy{envoy_cluster_name=\"email-common\"} 1\r\nenvoy_cluster_upstream_rq_pending_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_max_host_weight{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_version{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_pending_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_rq_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_default_rq_retry_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_circuit_breakers_high_cx_open{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_lb_subsets_active{envoy_cluster_name=\"email-common\"} 0\r\nenvoy_cluster_upstream_cx_active{envoy_cluster_name=\"email-common\"} 9\r\n```\r\n*Config*:\r\nEnvoy version is v1.9.0, pulled from docker hub\r\n\r\n```\r\nadmin:\r\n  access_log_path: /app/admin_access.log\r\n  address:\r\n    socket_address: { address: 0.0.0.0, port_value: 8081 }\r\nstatic_resources:\r\n  listeners:\r\n  - name: https_listener\r\n    address:\r\n      socket_address: { address: 0.0.0.0, port_value: 8080 }\r\n    filter_chains:\r\n      - filters:\r\n        - name: envoy.http_connection_manager\r\n          config:\r\n            codec_type: AUTO\r\n            stat_prefix: ingress\r\n            route_config:\r\n              name: router\r\n              virtual_hosts:\r\n              - name: common\r\n                domains: [\"*\"]\r\n                routes:\r\n                - match: { prefix: \"/\" }\r\n                  route:\r\n                    cluster: email-common\r\n                    auto_host_rewrite: true\r\n                    timeout: 50s\r\n                    retry_policy:\r\n                      retry_on: \"5xx\"\r\n                      num_retries: 5\r\n                      per_try_timeout: 10s\r\n            http_filters:\r\n            - name: envoy.router\r\n              config: { deprecated_v1: true }\r\n  clusters:\r\n    name: email-common\r\n    type: LOGICAL_DNS\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /app/data/fullchain.pem\r\n    hosts:\r\n      - socket_address: { address: email-service, port_value: 443 }\r\n```\r\n\r\n\r\n*Logs*:\r\n>Logs of requests for requests with data binary:\r\n\r\n```\r\n[2019-05-14 10:25:28.054][000036][debug][router] [source/common/router/router.cc:1023] [C782][S9375733934444584087] pool ready\r\n--\r\n\u00a0 | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:615] [C782][S9375733934444584087] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:25:28.056][000036][debug][router] [source/common/router/router.cc:968] [C782][S9375733934444584087] resetting pool request\r\n\u00a0 | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:270] [C784][S6563403434288646526] cluster 'email-common' match for URL '/email/'\r\n\u00a0 | [2019-05-14 10:25:28.074][000036][debug][router] [source/common/router/router.cc:328] [C784][S6563403434288646526] router decoding headers:\r\n\u00a0 | ':authority', 'envoy-proxy'\r\n\u00a0 | ':path', '/email/'\r\n\u00a0 | ':method', 'POST'\r\n\u00a0 | ':scheme', 'http'\r\n\u00a0 | 'content-length', '2266446'\r\n\u00a0 | 'user-agent', 'curl/7.64.0'\r\n\u00a0 | 'accept', '*/*'\r\n\u00a0 | 'authorization', 'none'\r\n\u00a0 | 'content-type', 'application/json'\r\n\u00a0 | 'x-request-id', '693db9a5-46ee-4bd7-9bb3-0766fbe90ed4'\r\n\u00a0 | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n\u00a0 | 'x-forwarded-host', 'envoy-proxy.host.ru;'\r\n\u00a0 | 'x-forwarded-port', '80'\r\n\u00a0 | 'x-forwarded-proto', 'http'\r\n\u00a0 | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;;proto=http'\r\n\u00a0 | 'x-forwarded-for', '10.233.53.119'\r\n\u00a0 | 'x-envoy-internal', 'true'\r\n```\r\n>Logs of requests for requests with empty data:\r\n```\r\n[2019-05-14 10:10:08.241][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n--\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.243][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.248][000040][debug][router] [source/common/router/router.cc:1023] [C254][S6944810802321297265] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:615] [C254][S6944810802321297265] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:779] [C254][S6944810802321297265] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.250][000040][debug][router] [source/common/router/router.cc:968] [C254][S6944810802321297265] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.260][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.263][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.271][000045][debug][router] [source/common/router/router.cc:1023] [C258][S7255259366709180105] pool ready\r\n\u00a0 | [2019-05-14 10:10:08.276][000045][debug][router] [source/common/router/router.cc:615] [C258][S7255259366709180105] upstream headers complete: end_stream=false\r\n\u00a0 | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:779] [C258][S7255259366709180105] performing retry\r\n\u00a0 | [2019-05-14 10:10:08.284][000045][debug][router] [source/common/router/router.cc:968] [C258][S7255259366709180105] resetting pool request\r\n\u00a0 | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:270] [C263][S2736688878384099272] cluster 'email-common' match for URL '/email/'\r\n\u00a0 | [2019-05-14 10:10:08.290][000039][debug][router] [source/common/router/router.cc:328] [C263][S2736688878384099272] router decoding headers:\r\n\u00a0 | ':authority', 'envoy-proxy'\r\n\u00a0 | ':path', '/email/'\r\n\u00a0 | ':method', 'POST'\r\n\u00a0 | ':scheme', 'http'\r\n\u00a0 | 'content-length', '0'\r\n\u00a0 | 'user-agent', 'curl/7.64.0'\r\n\u00a0 | 'accept', '*/*'\r\n\u00a0 | 'authorization', 'none'\r\n\u00a0 | 'content-type', 'application/json'\r\n\u00a0 | 'x-request-id', 'f627a2c5-a91d-4c71-aa56-61c04163eb88'\r\n\u00a0 | 'x-envoy-expected-rq-timeout-ms', '10000'\r\n\u00a0 | 'x-forwarded-host', 'envoy-proxy.host.ru'\r\n\u00a0 | 'x-forwarded-port', '80'\r\n\u00a0 | 'x-forwarded-proto', 'http'\r\n\u00a0 | 'forwarded', 'for=10.233.53.119;host=envoy-proxy.host.ru;proto=http'\r\n\u00a0 | 'x-forwarded-for', '10.233.53.119'\r\n\u00a0 | 'x-envoy-internal', 'true'\r\n```\r\n\r\nQuestions:\r\n\r\n1. Why envoy don't execute retries, if I send to it some data binary in requests? The target service return 500 error code, and the envoy can executing retries according '5xx', for example for requests with empty data.\r\n\r\n1. What it can be changed to retry occurring for requests with binary data? Can it is due with some retry policy parameters, such as 'per_try_timeout' or some clusters parameters like type or connect timeout? .\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6930/comments",
    "author": "PetrovMikhail",
    "comments": [
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T13:21:34Z",
        "body": "Size of sending file in request is in the first case ~2 MB."
      },
      {
        "user": "snowp",
        "created_at": "2019-05-14T13:27:21Z",
        "body": "I suspect you're running into the limitation that Envoy is unable to retry requests that are too large to fit into its buffer, evident by the fact that `envoy_cluster_retry_or_shadow_abandoned` is non-zero. Can you try setting `per_connection_buffer_limit_bytes` on the cluster to something higher? The default is 1MiB."
      },
      {
        "user": "PetrovMikhail",
        "created_at": "2019-05-14T20:29:23Z",
        "body": "Yes, I think that your advise in right way, thank you! but I increased this value, and it did not work yet, I think that i have to increase limit file value in another parameters of Envoy too. Do you where it is may be? \n\n---\n\n@snowp , thank you very match, this was a right solution. Besides envoy we have nginx in series, and after increasing `client_max_body_size`  it works fine.\n\n---\n\nThis is my folder, I didn't understood correct results of tests, which I describe in two previous comments. I already change `per_connection_buffer_limit_bytes` to 30 000 000 value, which just about equal 30 MiB, but when I was testing envoy behavior next, I discovered that envoy still doesn't retry request with files > 1 MiB. Maybe another parameter is occurs which changes buffer size of cluster and so on? "
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-06-20T20:45:09Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      },
      {
        "user": "feature-id",
        "created_at": "2021-03-15T12:01:49Z",
        "body": "For all, who got the same problem: there's also per_connection_buffer_limit_bytes option on a listener level, which is 1MB by default."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Envoy cannot buffer large request bodies for retries",
      "Identification of buffer-related configuration parameters affecting retry behavior",
      "Clarification of Envoy's retry requirements for different request types",
      "Guidance on configuring buffer limits at multiple levels (listener, cluster, upstream)",
      "Explanation of relationship between request body size and retry capability"
    ]
  },
  {
    "number": 6633,
    "title": "Using request_headers_to_ad to set Host header, the port value is removed!?",
    "created_at": "2019-04-18T09:13:31Z",
    "closed_at": "2019-04-19T16:00:33Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6633",
    "body": "I'm trying to perform this configuration:\r\n`        request_headers_to_add:\r\n            - header:\r\n                key: \"Host\"\r\n                value: \"127.0.0.1:10000\"\r\n              append: false\r\n`\r\nBut on the http request the \"Host\" header lost the value port:\r\n\"Host\": \"127.0.0.1\"\r\nIs this an intentional Envoy behavior? Or I'm using a wrong configuration?\r\nThanks in advance.\r\nAlessandro",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6633/comments",
    "author": "alessandro-vincelli",
    "comments": [
      {
        "user": "htuch",
        "created_at": "2019-04-18T12:19:48Z",
        "body": "I think you probably want to use `host_rewrite` instead of `request_headers_to_add`. The `host` header is special and has a bunch of interactions with HTTP connection manager and the codecs."
      },
      {
        "user": "alessandro-vincelli",
        "created_at": "2019-04-19T16:00:33Z",
        "body": "Thanks hutch, Evoy it works correctly, my fault, the header was override by another system."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Host header manipulation through request_headers_to_add might not work as expected",
      "Identification of the correct configuration approach for Host header modifications",
      "Clarification of header override mechanisms in Envoy",
      "Differentiation between standard header manipulation and Host header special cases"
    ]
  },
  {
    "number": 6471,
    "title": "Help with gRPC HTTP / 1.1 reverse bridge: grpc-message mapping.",
    "created_at": "2019-04-03T13:45:12Z",
    "closed_at": "2019-05-11T14:14:48Z",
    "labels": [
      "question",
      "stale"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6471",
    "body": "**Help with gRPC HTTP / 1.1 reverse bridge.**\r\n\r\n*Title*: *Help with gRPC HTTP / 1.1 reverse bridge: grpc-message mapping.*\r\n\r\n*Description*:\r\nI'm using the gRPC HTTP / 1.1 reverse bridge and I have the next doubt:\r\n\r\nIn case of error, (besides the **grpc-status** \"auto\" map) is there any way to map an error message to the trailer **grpc-message** when my upstream does not understand any gRPC semantics? \r\n\r\nI would like to avoid passing errors in the body and use the standard way.\r\n\r\nMany thanks in advance and excuse my ignorance.\r\n\r\n*Config*:\r\nThis is the config I'm using it and working correctly:\r\n```\r\nadmin:\r\n  access_log_path: /tmp/admin_access.log\r\n  address:\r\n    socket_address:\r\n      protocol: TCP\r\n      address: 127.0.0.1\r\n      port_value: 9901\r\nstatic_resources:\r\n  listeners:\r\n    - name: listener_0\r\n      address:\r\n        socket_address:\r\n          protocol: TCP\r\n          address: 0.0.0.0\r\n          port_value: 10000\r\n      filter_chains:\r\n        - filters:\r\n            - name: envoy.http_connection_manager\r\n              typed_config:\r\n                \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\r\n                stat_prefix: ingress_http\r\n                codec_type: AUTO\r\n                route_config:\r\n                  name: backend\r\n                  virtual_hosts:\r\n                    - name: backend\r\n                      domains: [\"*\"]\r\n                      routes:\r\n                        - match: { prefix: \"/\" }\r\n                          route: { host_rewrite: nginx, cluster: backend, timeout: 59.99s }\r\n                http_filters:\r\n                  - name: envoy.filters.http.grpc_http1_reverse_bridge\r\n                    config:\r\n                      content_type: application/grpc+proto\r\n                      withhold_grpc_frames: true\r\n                  - name: envoy.router\r\n                    typed_config: {}\r\n  clusters:\r\n  - name: backend\r\n    connect_timeout: 59.99s\r\n    type: logical_dns\r\n    dns_lookup_family: v4_only\r\n    lb_policy: round_robin\r\n    load_assignment:\r\n      cluster_name: backend\r\n      endpoints:\r\n        - lb_endpoints:\r\n            - endpoint:\r\n                address:\r\n                  socket_address:\r\n                    address: nginx\r\n                    port_value: 80\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6471/comments",
    "author": "sp-manuel-jurado",
    "comments": [
      {
        "user": "snowp",
        "created_at": "2019-04-03T14:30:13Z",
        "body": "The filter already injects error messages into grpc-message when it receives an unsupported response. Are you talking about being able to respond with a custom message?"
      },
      {
        "user": "sp-manuel-jurado",
        "created_at": "2019-04-03T14:45:23Z",
        "body": "@snowp yes, I was referring to custom message error (even the possibility to customize grpc-status too).\r\n\r\nNote: my upstream does not understand any gRPC semantics, I'm using PHP."
      },
      {
        "user": "snowp",
        "created_at": "2019-04-03T15:21:03Z",
        "body": "To send a custom message you can just send a header only response (no body) with a header named `grpc-message` with your message. As log as the content-type matches what the filter expects it should just pass through the `grpc-message`.\r\n\r\nFor `grpc-status` we'd have to make some code changes. Making the filter just pass through the status if the upstream provided one sounds reasonable, would that work for you?"
      },
      {
        "user": "sp-manuel-jurado",
        "created_at": "2019-04-04T10:54:33Z",
        "body": "Hi @snowp \r\nI've checked grpc-message pass through using your directions and works as expected.\r\n\r\nFor example:\r\n\r\nHTTP/1.1 request/response (within content-type, content-length: 0, grpc-message headers set)\r\n```\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 10000 (#0)\r\n> POST /TestAPI/TestMethod HTTP/1.1\r\n> Host: localhost:10000\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 400 Bad Request\r\n< server: envoy\r\n< content-type: application/grpc+proto\r\n< content-length: 0\r\n< x-powered-by: PHP/7.2.15\r\n< grpc-message: custom message for 400 http status code\r\n< cache-control: no-cache, private\r\n< date: Thu, 04 Apr 2019 10:19:12 GMT\r\n< access-control-allow-origin:\r\n< x-envoy-upstream-service-time: 966\r\n<\r\n* Connection #0 to host localhost left intact\r\n```\r\n\r\ngRPC response status (as PHP array, sorry for that. I'm testing with PHP client too)\r\n```\r\n(\r\n    [metadata] => Array\r\n        (\r\n            [:status] => Array\r\n                (\r\n                    [0] => 400\r\n                )\r\n\r\n            [server] => Array\r\n                (\r\n                    [0] => envoy\r\n                )\r\n\r\n            [content-type] => Array\r\n                (\r\n                    [0] => application/grpc\r\n                )\r\n\r\n            [content-length] => Array\r\n                (\r\n                    [0] => 5\r\n                )\r\n\r\n            [x-powered-by] => Array\r\n                (\r\n                    [0] => PHP/7.2.15\r\n                )\r\n\r\n            [grpc-message] => Array\r\n                (\r\n                    [0] => custom message for 400 http status code\r\n                )\r\n\r\n            [cache-control] => Array\r\n                (\r\n                    [0] => no-cache, private\r\n                )\r\n\r\n            [date] => Array\r\n                (\r\n                    [0] => Thu, 04 Apr 2019 10:22:16 GMT\r\n                )\r\n\r\n            [access-control-allow-origin] => Array\r\n                (\r\n                    [0] =>\r\n                )\r\n\r\n            [x-envoy-upstream-service-time] => Array\r\n                (\r\n                    [0] => 1255\r\n                )\r\n\r\n        )\r\n\r\n    [code] => 1\r\n    [details] => Received http2 header with status: 400\r\n)\r\n```\r\n\r\nI can see:\r\n- http2 status (:status) set to 400\r\n- grpc custom message (grpc-message) set to \"custom message for 400 http status code\"\r\nThis is ok and very useful for me.\r\n\r\nBut I miss the grpc-status in metadata. Related to this: \u00bfIs it the standard behaviour of the filter (and only sets the grpc-status trailer when response content is not empty)? Or I'm doing or understanding something wrong.\r\n\r\nRelated to:\r\nsnowp: \"For grpc-status we'd have to make some code changes. Making the filter just pass through the status if the upstream provided one sounds reasonable, would that work for you?\"\r\nI guess this is related to what I said above. The standard mapping would work for me (in that case 11 (400 Bad Request)). If I would want a custom error I could add it into grpc-message using a protocolbuffer serialized as string or checking http2 generic \":status\".\r\n\r\nMany thanks in advance and apologize for the inconveniences.\r\n\n\n---\n\nAlso, I've checked to add grpc-status as response header to do mapping manually and I get the next grpc status:\r\n\r\n```\r\n(\r\n    [metadata] => Array\r\n        (\r\n            [server] => Array\r\n                (\r\n                    [0] => envoy\r\n                )\r\n\r\n            [content-length] => Array\r\n                (\r\n                    [0] => 5\r\n                )\r\n\r\n            [x-powered-by] => Array\r\n                (\r\n                    [0] => PHP/7.2.15\r\n                )\r\n\r\n            [cache-control] => Array\r\n                (\r\n                    [0] => no-cache, private\r\n                )\r\n\r\n            [date] => Array\r\n                (\r\n                    [0] => Thu, 04 Apr 2019 10:46:08 GMT\r\n                )\r\n\r\n            [access-control-allow-origin] => Array\r\n                (\r\n                    [0] =>\r\n                )\r\n\r\n            [x-envoy-upstream-service-time] => Array\r\n                (\r\n                    [0] => 1293\r\n                )\r\n\r\n        )\r\n\r\n    [code] => 11\r\n    [details] => custom message for 400 http status code\r\n)\r\n```\r\n\r\nNow is not added in metadata but is added in code and details.\r\nIs this behaviour correct?\r\nOr I'm doing or understanding something wrong.\r\n\r\nI attach the headers example too (HTTP/1.1 request/response):\r\n\r\n```\r\n*   Trying ::1...\r\n* TCP_NODELAY set\r\n* Connected to localhost (::1) port 10000 (#0)\r\n> POST /TestAPI/TestMethod HTTP/1.1\r\n> Host: localhost:10000\r\n> User-Agent: curl/7.54.0\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 400 Bad Request\r\n< server: envoy\r\n< content-type: application/grpc+proto\r\n< content-length: 0\r\n< x-powered-by: PHP/7.2.15\r\n< grpc-message: custom message for 400 http status code\r\n< grpc-status: 11\r\n< cache-control: no-cache, private\r\n< date: Thu, 04 Apr 2019 10:52:22 GMT\r\n< access-control-allow-origin:\r\n< x-envoy-upstream-service-time: 1411\r\n<\r\n* Connection #0 to host localhost left intact\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2019-04-04T13:31:23Z",
        "body": "Ah yeah, looking over the code again we don't do anything fancy if it's a header only response, so it makes sense that grpc-status is propagated in that case. \r\n\r\nI would expect them to not show up in metadata since they are headers with special meaning in gRPC."
      },
      {
        "user": "stale[bot]",
        "created_at": "2019-05-11T14:14:47Z",
        "body": "This issue has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in the next 7 days unless it is tagged \"help wanted\" or other activity occurs. Thank you for your contributions.\n\n\n---\n\nThis issue has been automatically closed because it has not had activity in the last 37 days. If this issue is still valid, please ping a maintainer and ask them to label it as \"help wanted\". Thank you for your contributions.\n"
      }
    ],
    "satisfaction_conditions": [
      "Ability to set custom error messages in grpc-message header from upstream responses",
      "Clear mapping between HTTP status codes and gRPC status codes",
      "Proper propagation of grpc-status headers from HTTP/1.1 responses to gRPC clients",
      "Support for gRPC error handling standards without requiring gRPC semantics in upstream services",
      "Header-only response handling for error cases"
    ]
  },
  {
    "number": 6412,
    "title": "envoy  proxy Segmentation fault",
    "created_at": "2019-03-28T08:42:50Z",
    "closed_at": "2019-03-29T02:18:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6412",
    "body": "*Title*:  envoy proxy Segmentation fault when I learn examples/redis\r\n\r\n*Description*:\r\n>this is  log\r\n\r\nproxy_1  | [2019-03-28 08:30:11.919][12][debug][redis] [source/extensions/filters/network/redis_proxy/command_splitter_impl.cc:403] redis: splitting '[\"set\", \"name\", \"envoy\"]'\r\nproxy_1  | [2019-03-28 08:30:11.919][12][debug][connection] [source/common/network/connection_impl.cc:644] [C4] connecting to 172.20.0.2:6379\r\nproxy_1  | [2019-03-28 08:30:11.921][12][debug][connection] [source/common/network/connection_impl.cc:653] [C4] connection in progress\r\nproxy_1  | [2019-03-28 08:30:11.921][12][debug][connection] [source/common/network/connection_impl.cc:517] [C4] connected\r\nproxy_1  | [2019-03-28 08:30:11.923][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:81] Caught Segmentation fault, suspect faulting address 0xa\r\nproxy_1  | [2019-03-28 08:30:11.923][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:69] Backtrace (use tools/stack_decode.py to get line numbers):\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #0: __restore_rt [0x7f78d87f7390]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #1: [0x88058f]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #2: [0x889b3b]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #3: [0x88b8ee]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #4: [0x88be9f]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #5: [0x8897a9]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #6: [0x88985d]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #7: [0xaa5b4c]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #8: [0xaa23ba]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #9: [0xaa2aca]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #10: [0xa9c78a]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #11: [0xde74c9]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #12: [0xde79ff]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #13: [0xde9608]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #14: [0xa9bdfd]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #15: [0xa96c9e]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:75] #16: [0xfc7575]\r\nproxy_1  | [2019-03-28 08:30:11.924][12][critical][backtrace] [bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:73] #17: start_thread [0x7f78d87ed6ba]\r\nproxy_1  | Segmentation fault (core dumped)\r\nredis_proxy_1 exited with code 139\r\n\r\nthis is my test steps:\r\n[root@localhost ~]# redis-cli -p 1999\r\n127.0.0.1:1999> set name envoy\r\n(error) no upstream host\r\n127.0.0.1:1999>\r\n\r\nwhat's trouble, I  not charge config  and  follow the examples/redis steps to operate \u3002\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6412/comments",
    "author": "skywli",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2019-03-28T16:42:34Z",
        "body": "Please try current master the bug here was reverted."
      },
      {
        "user": "skywli",
        "created_at": "2019-03-29T02:17:26Z",
        "body": "@mattklein123 thank your help,the new image is ok."
      }
    ],
    "satisfaction_conditions": [
      "Identifies the root cause of the segmentation fault in the Redis proxy example",
      "Provides a working version/build of Envoy that resolves the crash",
      "Confirms compatibility between the Redis example configuration and the Envoy version used"
    ]
  },
  {
    "number": 6159,
    "title": "Bazel build failed in foreign_cc",
    "created_at": "2019-03-04T18:31:44Z",
    "closed_at": "2019-03-04T22:14:04Z",
    "labels": [
      "question",
      "area/build"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/6159",
    "body": "**Bug Template**\r\n\r\n*Title*: Bazel build failed in foreign_cc\r\n\r\n*Description*:\r\nWith latest tree, and with bazel 0.21.0,   when \"bazel build //souce/...\", I got\r\n\r\nbazel build //source/exe:envoy-static\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/home/qiwzhang/github/envoyproxy/envoy/tools/bazel.rc\r\nERROR: /home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/envoy/bazel/foreign_cc/BUILD:86:1: in cmake_external rule @envoy//bazel/foreign_cc:yaml: \r\nTraceback (most recent call last):\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/envoy/bazel/foreign_cc/BUILD\", line 86\r\n                cmake_external(name = 'yaml')\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/rules_foreign_cc/tools/build_defs/cmake.bzl\", line 47, in _cmake_external\r\n                cc_external_rule_impl(ctx, attrs)\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/foreign_cc_impl/framework.bzl\", line 209, in cc_external_rule_impl\r\n                _define_out_cc_info(ctx, attrs, inputs, outputs)\r\n        File \"/home/qiwzhang/.cache/bazel/_bazel_qiwzhang/85114b85e3290f3d9d9a93c62b41d7d0/external/foreign_cc_impl/framework.bzl\", line 627, in _define_out_cc_info\r\n                cc_common.create_compilation_context(ctx = ctx, headers = depset([outpu...]), <2 more arguments>)\r\nunexpected keyword 'ctx', for call to method create_compilation_context(headers = unbound, system_includes = unbound, includes = unbound, quote_includes = unbound, defines = unbound) of 'cc_common'\r\nERROR: Analysis of target '//source/exe:envoy-static' failed; build aborted: Analysis of target '@envoy//bazel/foreign_cc:yaml' failed; build aborted\r\nINFO: Elapsed time: 1.483s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (213 packages loaded, 4783 targets configured)\r\n    Fetching @com_github_gperftools_gperftools; Patching repository\r\n    Fetching @com_github_nghttp2_nghttp2; fetching\r\n    Fetching @com_github_c_ares_c_ares; fetching\r\n    Fetching @com_github_circonus_labs_libcircllhist; fetching\r\n    Fetching @boringssl; fetching\r\n    Fetching @com_github_madler_zlib; fetching\r\n\r\n\r\n\r\nBTW,  I also tried with bazel 0.23.0. Same issue\r\n\r\nAny clues?\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/6159/comments",
    "author": "qiwzhang",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-03-04T20:28:55Z",
        "body": "Did you try `bazel clean --expunge`? 0.21 doesn't work with latest, but 0.22 or 0.23 should work and they are tested in CI."
      },
      {
        "user": "qiwzhang",
        "created_at": "2019-03-04T22:13:58Z",
        "body": "Thanks.   It works now after \"bazel clean --expunge\"\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Resolves cached build artifact incompatibility with newer Bazel versions",
      "Ensures environment state matches required Bazel version expectations"
    ]
  },
  {
    "number": 5788,
    "title": "Envoy \"Connection: close\" causes 1s rq_time overhead",
    "created_at": "2019-01-31T12:05:04Z",
    "closed_at": "2019-02-01T12:13:44Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5788",
    "body": "Given the below configuration:\r\n\r\nRequest with __Connection: close__, causes envoy to delays the sending the tcp (FIN) / closing the socket by 1s.\r\n\r\nEnvoy sends the response data, rightaway (no delay visible in tcpdump / wireshark), the FIN / closing of the connection is the issue.\r\n```bash\r\ntime fortio curl -loglevel=debug -keepalive=false localhost:8080/\r\n```\r\n\r\nRequest without 'Connection: close', works as expected\r\n```bash\r\ntime fortio curl -loglevel=debug localhost:8080/\r\n```\r\n\r\n```yaml\r\n---\r\nnode:\r\n  locality:\r\n    zone: default-zone\r\n\r\nstatic_resources:\r\n  listeners:\r\n  - name: default_listener\r\n    address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 8080\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress_proxy\r\n          access_log:\r\n            name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n              format: >\r\n                [%START_TIME%] %PROTOCOL% %REQ(:METHOD)% %REQ(:authority)% %REQ(:PATH)% %RESPONSE_CODE% %RESPONSE_FLAGS%\r\n                %BYTES_RECEIVED%b %BYTES_SENT%b %DURATION%ms \"%DOWNSTREAM_REMOTE_ADDRESS%\" -> \"%UPSTREAM_HOST%\"\r\n          route_config:\r\n            name: \"ingress_routes\"\r\n            virtual_hosts:\r\n              - name: \"local_service\"\r\n                domains:\r\n                  - \"*\"\r\n                routes:\r\n                  - match:\r\n                      prefix: \"/\"\r\n                    route:\r\n                      cluster: \"example\"\r\n          http_filters:\r\n            - name: envoy.router\r\n          http_protocol_options:\r\n            allow_absolute_url: true\r\n\r\n  clusters:\r\n  - name: example\r\n    type: STRICT_DNS\r\n    hosts:\r\n    - socket_address:\r\n        address: 127.0.0.1\r\n        port_value: 9091\r\n    connect_timeout:\r\n      seconds: 1\r\n\r\nadmin:\r\n  access_log_path: \"/dev/null\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9091\r\n```\r\n\r\nThis looks similar to #234, we've noticed unhealthy instances in varnish, when probes had .timeout < 1s.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5788/comments",
    "author": "fkowal",
    "comments": [
      {
        "user": "fkowal",
        "created_at": "2019-01-31T12:37:02Z",
        "body": "```\r\ntcpdump -nni lo0 -s 0 -A tcp portrange 8080\r\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\r\nlistening on lo0, link-type NULL (BSD loopback), capture size 262144 bytes\r\n13:15:10.238945 IP6 ::1.65046 > ::1.8080: Flags [S], seq 1696331147, win 65535, options [mss 16324,nop,wscale 6,nop,nop,TS val 803548502 ecr 0,sackOK,eol], length 0\r\n`..0.,.@....................................e............4....?........\r\n/.-V........\r\n13:15:10.239025 IP6 ::1.8080 > ::1.65046: Flags [S.], seq 2050231525, ack 1696331148, win 65535, options [mss 16324,nop,wscale 6,nop,nop,TS val 803548502 ecr 803548502,sackOK,eol], length 0\r\n`....,.@....................................z4..e........4....?........\r\n/.-V/.-V....\r\n13:15:10.239037 IP6 ::1.65046 > ::1.8080: Flags [.], ack 1, win 6371, options [nop,nop,TS val 803548502 ecr 803548502], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.-V/.-V\r\n13:15:10.239045 IP6 ::1.8080 > ::1.65046: Flags [.], ack 1, win 6371, options [nop,nop,TS val 803548502 ecr 803548502], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.-V/.-V\r\n13:15:10.239089 IP6 ::1.65046 > ::1.8080: Flags [P.], seq 1:97, ack 1, win 6371, options [nop,nop,TS val 803548502 ecr 803548502], length 96: HTTP: GET / HTTP/1.1\r\n`..0...@....................................e...z4.............\r\n/.-V/.-VGET / HTTP/1.1\r\nHost: localhost:8080\r\nConnection: close\r\nUser-Agent: fortio.org/fortio-1.3.0\r\n\r\n\r\n13:15:10.239101 IP6 ::1.8080 > ::1.65046: Flags [.], ack 97, win 6370, options [nop,nop,TS val 803548502 ecr 803548502], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.-V/.-V\r\n13:15:10.247298 IP6 ::1.8080 > ::1.65046: Flags [P.], seq 1:4807, ack 97, win 6370, options [nop,nop,TS val 803548510 ecr 803548502], length 4806: HTTP: HTTP/1.1 200 OK\r\n`......@....................................z4..e..............\r\n/.-^/.-VHTTP/1.1 200 OK\r\ncontent-type: text/html; charset=UTF-8\r\ncache-control: no-cache, max-age=0\r\nx-content-type-options: nosniff\r\ndate: Thu, 31 Jan 2019 12:15:10 GMT\r\nserver: envoy\r\nx-envoy-upstream-service-time: 0\r\nconnection: close\r\ntransfer-encoding: chunked\r\n\r\n11b4\r\n<head>\r\n  <title>Envoy Admin</title> etc\r\n</body>\r\n\r\n0\r\n\r\n\r\n13:15:10.247329 IP6 ::1.65046 > ::1.8080: Flags [.], ack 4807, win 6296, options [nop,nop,TS val 803548510 ecr 803548510], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.-^/.-^\r\n13:15:11.250726 IP6 ::1.8080 > ::1.65046: Flags [.], ack 97, win 6370, length 0\r\n`......@....................................z4..e...P.......\r\n13:15:11.250767 IP6 ::1.65046 > ::1.8080: Flags [.], ack 4807, win 6296, options [nop,nop,TS val 803549510 ecr 803548510], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.1F/.-^\r\n13:15:11.252739 IP6 ::1.8080 > ::1.65046: Flags [F.], seq 4807, ack 97, win 6370, options [nop,nop,TS val 803549512 ecr 803549510], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.1H/.1F\r\n13:15:11.252769 IP6 ::1.65046 > ::1.8080: Flags [.], ack 4808, win 6296, options [nop,nop,TS val 803549512 ecr 803549512], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.1H/.1H\r\n13:15:11.252815 IP6 ::1.65046 > ::1.8080: Flags [F.], seq 97, ack 4808, win 6296, options [nop,nop,TS val 803549512 ecr 803549512], length 0\r\n`..0. .@....................................e...z4.......(.....\r\n/.1H/.1H\r\n13:15:11.252866 IP6 ::1.8080 > ::1.65046: Flags [.], ack 98, win 6370, options [nop,nop,TS val 803549512 ecr 803549512], length 0\r\n`.... .@....................................z4..e........(.....\r\n/.1H/.1H\r\n```\r\n\r\nHere is the tcpdump"
      },
      {
        "user": "alyssawilk",
        "created_at": "2019-01-31T15:56:19Z",
        "body": "If it's a close due to early response, it's likely ConnectionCloseType::FlushWriteAndDelay which was introduced to address the race described in #2929  \r\n\r\nIf it's not a close due to early response there might be some other corner case triggering the delay.\r\n\r\nIf that's the problem you could fix by adjusting  delayed_close_timeout in your HCM config but depending on how your client handles resets you might reintroduce the race described in that issue\r\n\r\ncc @AndresGuedez "
      },
      {
        "user": "fkowal",
        "created_at": "2019-02-01T12:13:44Z",
        "body": "Changing the __delayed_close_timeout__ did helped lowering the delay with the responses.\r\n\r\nThese metrics keep increasing, which I belive is an indication that varnish is not issuing a FIN after receiving the response.\r\n*http.ingress_http.downstream_cx_delayed_close_timeout* and *http.ingress_http.downstream_cx_destroy_local*\r\n\r\nThank you @alyssawilk "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why Envoy delays FIN packets when 'Connection: close' is present",
      "Configuration mechanism to control connection closure timing without reintroducing race conditions",
      "Clarification of metrics indicating improper connection termination"
    ]
  },
  {
    "number": 5410,
    "title": "Building envoy on OS X",
    "created_at": "2018-12-24T15:16:55Z",
    "closed_at": "2018-12-25T09:52:47Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5410",
    "body": "I'm trying to build envoy on OS X mojave `10.14.2 (18C54)` without success.\r\n\r\nHere is the executed command :\r\n`bazel build --incompatible_package_name_is_a_function=false --action_env=PATH=/usr/local/bin:/opt/local/bin:/usr/bin:/bin //source/exe:envoy-static`\r\n\r\nHere is the result: \r\n```shell\r\nINFO: Invocation ID: 9d5adc00-d36e-4957-b2c0-3f984d6cdc44\r\n/private/var/tmp/_bazel_nicolassterchele/19b5387e66de067346e6f0614dfa8b23/external/envoy_deps/./repositories.sh: line 8: md5: command not found\r\nExternal dependency cache directory /private/var/tmp/_bazel_nicolassterchele/19b5387e66de067346e6f0614dfa8b23/external/envoy_deps_cache_\r\n./build_and_install_deps.sh: line 12: sysctl: command not found\r\nmake: the `-j' option requires a positive integral argument\r\n```\r\nI guess that there is specifics commands for linux that is not available on darwin... Am I right ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5410/comments",
    "author": "sterchelen",
    "comments": [
      {
        "user": "moderation",
        "created_at": "2018-12-24T16:34:14Z",
        "body": "@sterchelen What version of Bazel are your running? You should upgrade to 0.21.0 if you are on something earlier. There were some recent changes for MacOS using that build that work. I build with `bazel build -c opt //source/exe:envoy-static.stripped` and `.bazelrc` is:\r\n```\r\nimport %workspace%/tools/bazel.rc\r\nbuild --announce_rc\r\nbuild --define google_grpc=disabled\r\nbuild --define signal_trace=disabled\r\nbuild --action_env=PATH=/bin:/opt/local/bin:/usr/bin:/usr/local/bin\r\n```"
      },
      {
        "user": "snowp",
        "created_at": "2018-12-24T18:24:48Z",
        "body": "This: \r\n\r\n```\r\n./build_and_install_deps.sh: line 12: sysctl: command not found\r\n```\r\n\r\nshould have been fixed on latest master to use the full path, can you try to pull latest?"
      },
      {
        "user": "sterchelen",
        "created_at": "2018-12-25T09:52:47Z",
        "body": "In fact, getting last version of master resolved it. Thanks."
      }
    ],
    "satisfaction_conditions": [
      "Resolve macOS-specific command compatibility issues in the build process",
      "Ensure build scripts handle macOS path/environment differences"
    ]
  },
  {
    "number": 5359,
    "title": "[Question] Meaning of [C~] character included in connection log and stream log etc",
    "created_at": "2018-12-20T02:41:56Z",
    "closed_at": "2018-12-20T03:37:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5359",
    "body": "*Title*: *[Question] Meaning of [C~] character included in connection log and stream log etc*\r\n\r\n*Description*:\r\nYou could see a character string [C (number)] as follows in Envoy's log. Would you tell this meaning?\r\nI think logs with the same id are logs in the same connection since I recognize it's like a connection id, but is it right?\r\nThanks.\r\n\r\n```\r\n[2018-12-20 01:52:58.930][000011][debug][router] [source/common/router/router.cc:322] [C0][S539228372188944921] router decoding headers:\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5359/comments",
    "author": "nakabonne",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-12-20T03:30:54Z",
        "body": "Yes that's right, it's an internal connection ID."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that [C~] represents a connection identifier in Envoy's logs",
      "Clarification about log correlation within the same connection",
      "Authoritative validation of the identifier's purpose"
    ]
  },
  {
    "number": 4672,
    "title": "[Question] /envoy_shared_memory_110 check user permissions. Error: File exists",
    "created_at": "2018-10-10T14:28:45Z",
    "closed_at": "2018-10-11T01:10:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4672",
    "body": "**Issue Template**\r\n\r\n*Title*: *One line description*\r\n\r\n*Description*:\r\nI started envoy with root user by mistake. I killed it.\r\nThen when i start envoy again with my own user, it says:\r\n[!184 06:42:56  ~]$ [2018-10-10 06:42:56.702][23046][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n\r\nWhich file should i remove before i start envoy with my own users? Thanks\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4672/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-10T14:30:15Z",
        "body": "Full screen output\r\n===============\r\n[!185 07:29:28  ~]$ /home/zapp/apps/envoy -c /home/zapp/apps/11/envoy/config.yaml --base-id 11 --v2-config-only\r\n[2018-10-10 07:29:31.869][26733][critical][assert] source/server/hot_restart_impl.cc:62] panic: cannot open shared memory region /envoy_shared_memory_110 check user permissions. Error: File exists\r\n[2018-10-10 07:29:31.871][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:125] Caught Aborted, suspect faulting address 0x1f40000686d\r\n[2018-10-10 07:29:31.873][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:94] Backtrace thr<0> obj</lib64/libc.so.6> (If unsymbolized, use tools/stack_decode.py):\r\n[2018-10-10 07:29:31.887][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #0 0x7f4a4c829277 raise\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #1 0x7f4a4c82a967 abort\r\n[2018-10-10 07:29:31.899][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:31.984][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #2 0x6cf1c4 Envoy::Server::SharedMemory::initialize()\r\n[2018-10-10 07:29:32.068][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #3 0x6cf8b2 Envoy::Server::HotRestartImpl::HotRestartImpl()\r\n[2018-10-10 07:29:32.153][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #4 0x48e3d7 Envoy::MainCommonBase::MainCommonBase()\r\n[2018-10-10 07:29:32.238][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #5 0x48e732 Envoy::MainCommon::MainCommon()\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #6 0x419905 main\r\n[2018-10-10 07:29:32.322][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</lib64/libc.so.6>\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:114] thr<0> #7 0x7f4a4c815444 __libc_start_main\r\n[2018-10-10 07:29:32.335][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:104] thr<0> obj</home/zapp/apps/envoy>\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:117] thr<0> #8 0x484834 (unknown)\r\n[2018-10-10 07:29:32.420][26733][critical][backtrace] bazel-out/k8-opt/bin/source/server/_virtual_includes/backtrace_lib/server/backtrace.h:121] end backtrace thread 0\r\nAborted (core dumped)\r\n[!186 07:29:32 zapp@5.fet.stg.slv.zuora ~]$\r\n"
      },
      {
        "user": "mattklein123",
        "created_at": "2018-10-10T16:30:01Z",
        "body": "`/dev/shm/envoy_shared_memory_110` on Linux."
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-11T01:10:40Z",
        "body": "@mattklein123   Thank you very much!!!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the exact shared memory file location created by the previous root-owned Envoy process",
      "Specifies the directory where shared memory files are stored in Linux systems"
    ]
  },
  {
    "number": 4651,
    "title": "jwt-authn exception",
    "created_at": "2018-10-09T13:13:53Z",
    "closed_at": "2018-10-09T16:29:41Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4651",
    "body": "**Issue Template**\r\n\r\n*Title*: *jwt-authn exception*\r\n\r\n*Description*:\r\n>Im using the jwt-authn http-filter for validating JWT. Im using the version 2 API reference of envoy and the following envoy image version tag : fdfa5bde3343372ad662a830da0bdc3aea806f4d\r\n\r\n>Im getting following exception : \r\n[critical][main] source/server/server.cc:80] error initializing configuration '/etc/front-envoy.yaml': Didn't find a registered implementation for name: 'envoy.jwt_authn'\r\n\r\n*config*:\r\n```\r\n- name: envoy.jwt_authn\r\n            config:\r\n```\r\n      \r\nAm i using the wrong name ?\r\n\r\nthanks     \r\n\r\n\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4651/comments",
    "author": "githubYasser",
    "comments": [
      {
        "user": "qiwzhang",
        "created_at": "2018-10-09T16:04:30Z",
        "body": "It should be:  envoy.filters.http.jwt_authn"
      },
      {
        "user": "githubYasser",
        "created_at": "2018-10-09T16:29:38Z",
        "body": "thank you , working now."
      }
    ],
    "satisfaction_conditions": [
      "Identifies the correct filter name format required by the Envoy version being used",
      "Addresses API version compatibility requirements for Envoy filters"
    ]
  },
  {
    "number": 4640,
    "title": "How to run multi envoy process on one linux server",
    "created_at": "2018-10-08T16:30:21Z",
    "closed_at": "2018-10-10T14:19:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/4640",
    "body": "*Title*: *One line description*\r\nHow to run multi envoy process on one linux server\r\n\r\n*Description*:\r\nI am not using docker container.\r\nI have the need to want to start two envoy processes on one linux server.\r\n\r\nSteps:\r\n1) create two folder on a centos server\r\n/root/envoy1 and /root/envoy2\r\n2) scp envoy binary (built on centos7) to the server under /root/envoy1 and /root/envoy2\r\n3) Prepare config.yaml files with diff ports \r\n4) Start envoy from folder 1. it works well\r\n/root/envoy1/envoy -c /root/envoy1/config.yaml --service-cluster myapp1 --service-node myapp1 \r\n5) Start envoy from folder 2. Nothing happens. And there is no screen output and log for further information\r\n/root/envoy2/envoy -c /root/envoy2/config.yaml --service-cluster myapp2 --service-node myapp2\r\n\r\nAny suggestion?",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/4640/comments",
    "author": "huxiaobabaer",
    "comments": [
      {
        "user": "moderation",
        "created_at": "2018-10-08T16:36:34Z",
        "body": "@huxiaobabaer from `envoy --help`. Works well and I run multiple Envoy's on a single host.\r\n\r\n```\r\n   --base-id <uint32_t>\r\n     base ID so that multiple envoys can run on the same host if needed\r\n```"
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-08T16:43:25Z",
        "body": "@moderation What is the default value of base id if not specified? TKS"
      },
      {
        "user": "moderation",
        "created_at": "2018-10-08T16:52:15Z",
        "body": "I don't believe there is a default. I start with 0 and count up from there. 0 to 4294967295 works."
      },
      {
        "user": "huxiaobabaer",
        "created_at": "2018-10-08T16:59:25Z",
        "body": "Thank you very much for your help!!! @moderation "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to avoid port/resource conflicts between multiple Envoy processes",
      "Clarification of required unique identifiers for concurrent Envoy instances",
      "Non-containerized solution approach"
    ]
  },
  {
    "number": 2638,
    "title": "mTLS and traffic split",
    "created_at": "2018-02-17T20:08:53Z",
    "closed_at": "2018-02-21T02:41:18Z",
    "labels": [
      "question",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2638",
    "body": "# Background\r\n\r\nI am preparing a traffic shift from a web API in a Swarm cluster (let's call it `swarm_api`) to the same API in a Kubernetes cluster (`k8s_api`). The two clusters are on distinct private networks. The API's clients are on the same private network as the Swarm cluster, making requests over HTTP. However, the Kubernetes cluster is isolated (and fully managed by a cloud provider; the nodes' network is abstracted); the connection must therefore be secured.\r\n\r\nMy plan is to add an edge envoy in the Swarm cluster (`swarm_proxy`) to split traffic between `swarm_api` and a second edge envoy in the Kubernetes cluster (`k8s_proxy`), which would route traffic to `k8s_api`. The connection between `swarm_proxy` and `k8s_proxy` must be secured with mutual TLS as it goes through the Internet.\r\n\r\n```\r\nPrivate network 1\r\n                      (swarm_proxy)---HTTP--->(swarm_api)\r\n____________________________|__________________________\r\nInternet                    |\r\n                          HTTPS\r\n____________________________|__________________________\r\nPrivate network 2           |\r\n                            \u2304\r\n                       (k8s_proxy)----HTTP---->(k8s_api)\r\n```\r\n\r\n# What I have so far\r\n\r\n1) One the one hand, I have successfully prototyped a traffic split over HTTP, *without* mTLS:\r\n    ```\r\n    Private network 1\r\n                          (swarm_proxy)---HTTP--->(swarm_api)\r\n    ____________________________|__________________________\r\n    Internet                    |\r\n                              HTTP\r\n    ____________________________|__________________________\r\n    Private network 2           |\r\n                                \u2304\r\n                           (k8s_proxy)----HTTP---->(k8s_api)\r\n    ```\r\n    `swarm_proxy` uses an `http_connection_manager` to split traffic between `swarm_api` and `k8s_proxy`; `k8s_proxy` simply routes traffic to `k8s_api` (see configurations below).\r\n2) On the other hand, I have successfully prototyped simple routing with mTLS:\r\n    ```\r\n    Private network 1\r\n                          (swarm_proxy)\r\n    ____________________________|__________________________\r\n    Internet                    |\r\n                              HTTPS\r\n    ____________________________|__________________________\r\n    Private network 2           |\r\n                                \u2304\r\n                           (k8s_proxy)----HTTP---->(k8s_api)\r\n    ```\r\n    `swarm_proxy` uses a `tcp_proxy` to route traffic to `k8s_proxy`; the envoy _cluster_ adds a `tls_context` including a client certificate; `k8s_proxy` requires TLS with client certificate, then routes traffic to `k8s_api` (see configurations below).\r\n\r\n# Issue\r\n\r\nNow I'm struggling to make both traffic split and mTLS work at the same time: if I try to combine the two approaches in a third prototype, `swarm_proxy` returns 301 Moved Permanently when it routes traffic to `k8s_proxy` (see configurations below).\r\n\r\nIs there something I'm missing / don't understand, or is this a bug?\r\n\r\n# Envoy Configurations\r\n\r\n## Prototype 1: traffic split\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  weighted_clusters:\r\n                    runtime_key_prefix: routing.traffic_split.api\r\n                    clusters:\r\n                    - name: swarm_api\r\n                      weight: 50\r\n                    - name: k8s_proxy\r\n                      weight: 50\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: swarm_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: swarm_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin:\r\n  access_log_path: \"/dev/stdout\"\r\n  address:\r\n    socket_address:\r\n      address: 0.0.0.0\r\n      port_value: 9901\r\n```\r\n\r\n```yaml\r\n# k8s_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: k8s_api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  cluster: k8s_api\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: k8s_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin: {...}\r\n```\r\n\r\n## Prototype 2: mutual TLS\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.tcp_proxy\r\n        config:\r\n          stat_prefix: ingress\r\n          cluster: k8s_proxy\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 443\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /etc/certs/ca.crt.pem\r\n        tls_certificates:\r\n        - certificate_chain:\r\n            filename: /etc/certs/swarm_proxy.crt.pem\r\n          private_key:\r\n            filename: /etc/certs/swarm_proxy.key.pem\r\nadmin: {...}\r\n```\r\n\r\n```yaml\r\n# k8s_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 443\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: k8s_api\r\n              domains:\r\n              - \"*\"\r\n              require_tls: ALL\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  cluster: k8s_api\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n      tls_context:\r\n        require_client_certificate: true\r\n        common_tls_context:\r\n          validation_context:\r\n            trusted_ca:\r\n              filename: /etc/certs/ca.crt.pem\r\n          tls_certificates:\r\n          - certificate_chain:\r\n              filename: /etc/certs/k8s_proxy.crt.pem\r\n            private_key:\r\n              filename: /etc/certs/k8s_proxy.key.pem\r\n  clusters:\r\n  - name: k8s_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\nadmin: {...}\r\n```\r\n\r\n## Prototype 3: traffic split and mutual TLS (NOT WORKING)\r\n\r\n```yaml\r\n# swarm_proxy\r\nstatic_resources:\r\n  listeners:\r\n  - address:\r\n      socket_address:\r\n        address: 0.0.0.0\r\n        port_value: 80\r\n    filter_chains:\r\n    - filters:\r\n      - name: envoy.http_connection_manager\r\n        config:\r\n          stat_prefix: ingress\r\n          route_config:\r\n            virtual_hosts:\r\n            - name: api\r\n              domains:\r\n              - \"*\"\r\n              routes:\r\n              - match:\r\n                  prefix: \"/myapi/v1\"\r\n                route:\r\n                  weighted_clusters:\r\n                    runtime_key_prefix: routing.traffic_split.api\r\n                    clusters:\r\n                    - name: swarm_api\r\n                      weight: 50\r\n                    - name: k8s_proxy\r\n                      weight: 50\r\n          http_filters:\r\n          - name: envoy.router\r\n          access_log:\r\n          - name: envoy.file_access_log\r\n            config:\r\n              path: \"/dev/stdout\"\r\n  clusters:\r\n  - name: swarm_api\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: swarm_api\r\n        port_value: 80\r\n    connect_timeout: 1s\r\n  - name: k8s_proxy\r\n    type: strict_dns\r\n    hosts:\r\n    - socket_address:\r\n        address: k8s_proxy # TODO replace with public DNS when implemented\r\n        port_value: 443\r\n    connect_timeout: 1s\r\n    tls_context:\r\n      common_tls_context:\r\n        validation_context:\r\n          trusted_ca:\r\n            filename: /etc/certs/ca.crt.pem\r\n        tls_certificates:\r\n        - certificate_chain:\r\n            filename: /etc/certs/swarm_proxy.crt.pem\r\n          private_key:\r\n            filename: /etc/certs/swarm_proxy.key.pem\r\nadmin: {...}\r\n```\r\n\r\n```yaml\r\n# k8s_proxy : same as prototype 2\r\n```",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2638/comments",
    "author": "adrienjt",
    "comments": [
      {
        "user": "PiotrSikora",
        "created_at": "2018-02-21T01:44:44Z",
        "body": "You need to either add `use_remote_address: true` to `k8s_proxy` or remove `require_tls: ALL` from it.\r\n\r\nIf you're using `use_remote_address: false` (default), then `k8s_proxy` is going to receive and accept `X-Forwarded-Proto: http` from `swarm_proxy` and reject client's HTTP request, since it doesn't fulfill the `require_tls` restriction, which applies to client's HTTP request and not to the connection to `k8s_proxy`, `require_client_certificate: true` is enough to enforce mTLS between the proxies.\r\n\r\nAlso, you should add `use_remote_address: true` to `swarm_proxy`, since it's acting as an edge proxy."
      },
      {
        "user": "adrienjt",
        "created_at": "2018-02-21T02:41:00Z",
        "body": "Thanks for the solution *and explanations* @PiotrSikora! That worked."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how to reconcile TLS requirements between proxies with traffic splitting logic",
      "Clarification of Envoy's header handling between proxies in mTLS scenarios",
      "Guidance on proper proxy role configuration (edge vs internal)",
      "Clear separation of TLS termination concerns from traffic management logic",
      "Validation that mTLS between proxies doesn't interfere with upstream HTTP routing"
    ]
  },
  {
    "number": 2462,
    "title": "Question on license of envoy 1.3.0 dependencies",
    "created_at": "2018-01-26T16:24:25Z",
    "closed_at": "2018-01-30T14:53:52Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/2462",
    "body": "I've a question on one of the envoy 1.3.0 dependencies: rapidjson \u00a01.1.0 \r\n\r\nIt's license states: \r\n\r\n**_If you have downloaded a copy of the RapidJSON source code from Tencent, please note that RapidJSON source code is licensed under the MIT License, except for the third-party components listed below which are subject to different license terms.\u00a0 Your integration of RapidJSON into your own projects may require compliance with the MIT License, as well as the other licenses applicable to the third-party components included within RapidJSON. To avoid the problematic JSON license in your own projects, it's sufficient to exclude the bin/jsonchecker/ directory, as it's the only code under the JSON license.\r\nA copy of the MIT License is included in this file._** \r\n\r\nSo my question is:  is the bin/jsonchecker/ directory included in the envoy 1.3.0 binary referenced as:\r\n\r\n**_proxy/envoy-1.3.0.tg: \r\nsize:2266298 \r\nobject_id:c10f7dcc-4010-4dfe-460a-250a0e1cde1 \r\nsha: 45d667aa64a876ab857853b112f065a8800d3161_**     ?\r\n\r\nThanks.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/2462/comments",
    "author": "luisapace",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2018-01-26T16:29:54Z",
        "body": "@rshriram can you or someone else from IBM answer this? Thank you."
      },
      {
        "user": "rshriram",
        "created_at": "2018-01-27T03:30:44Z",
        "body": "Why ibm? \r\nEither way, Envoy was approved internally a year ago. "
      },
      {
        "user": "mattklein123",
        "created_at": "2018-01-27T18:07:23Z",
        "body": "@rshriram because @luisapace works at IBM and has been emailing me. :)"
      },
      {
        "user": "luisapace",
        "created_at": "2018-01-29T09:15:23Z",
        "body": "I'm sorry, but really I do not understand your point... if I've a question on the build of Envoy (not produced by IBM), why do I have to write to IBM instead of the developers of that package that has produced that binary?  Could you please clarify? I've done this several times for other packages and always their developers have answered me. Please let me know if you know the answer to my question or not. Thanks a lot for your time and help. \r\n\r\n\r\n"
      },
      {
        "user": "rshriram",
        "created_at": "2018-01-29T20:25:52Z",
        "body": "Short answer: no.\r\nContents of bazel-envoy/external/com_github_tencent_rapidjson/bin/jsonchecker/ are\r\n```\r\nfail1.json   fail13.json  fail17.json  fail20.json  fail24.json  fail28.json  fail31.json  fail5.json   fail9.json   readme.txt   \r\nfail10.json  fail14.json  fail18.json  fail21.json  fail25.json  fail29.json  fail32.json  fail6.json   pass1.json   \r\nfail11.json  fail15.json  fail19.json  fail22.json  fail26.json  fail3.json   fail33.json  fail7.json   pass2.json   \r\nfail12.json  fail16.json  fail2.json   fail23.json  fail27.json  fail30.json  fail4.json   fail8.json   pass3.json   \r\n```\r\n\r\nwhich are not part of envoy binary."
      },
      {
        "user": "luisapace",
        "created_at": "2018-01-30T13:58:24Z",
        "body": "Great, thanks a lot for your help!"
      },
      {
        "user": "alyssawilk",
        "created_at": "2018-01-30T14:53:52Z",
        "body": "I think this answers your question so closing this off, but please reopen if I'm wrong!"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether the bin/jsonchecker directory is included in the final Envoy 1.3.0 binary",
      "Clarification about license compliance implications for the Envoy binary",
      "Evidence of binary composition analysis"
    ]
  },
  {
    "number": 1189,
    "title": "TLS between Envoys is not working",
    "created_at": "2017-06-29T00:34:06Z",
    "closed_at": "2017-06-29T16:27:58Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/1189",
    "body": "Hi,\r\n\r\nWe are trying to initiate a TCP connection through two Envoys, like so:\r\n```\r\nClient (nc) --> \"Client\" Envoy ==> \"Server\" Envoy --> Server (nc -l)\r\n```\r\nWhere the `==>` arrow is TLS (server certs, no client certs).\r\n\r\nThe client connection looks like\r\n```\r\ndate | nc -v 127.0.0.1 10000\r\n```\r\n\r\nAnd the server runs as\r\n```\r\nnc -l -k 8080\r\n```\r\n\r\nWhen trying to connect, we get inconsistent behavior.   Sometimes the date appears on the server side `nc`, but often it does not.\r\n\r\nHowever, the client side `nc` always reports \r\n```\r\nConnection to 127.0.0.1 10000 port [tcp/webmin] succeeded!\r\n```\r\n\r\n\r\nOn the server side, the Envoy debug logs are:\r\n```\r\n[2017-06-29 00:32:55.981][116][info][main] source/server/connection_handler_impl.cc:109] [C1] new connection\r\n[2017-06-29 00:32:55.981][116][info][filter] source/common/filter/tcp_proxy.cc:117] [C1] new tcp proxy session\r\n[2017-06-29 00:32:55.981][116][debug][filter] source/common/filter/tcp_proxy.cc:133] [C1] Creating connection to cluster service_local\r\n[2017-06-29 00:32:55.981][116][debug][connection] source/common/network/connection_impl.cc:418] [C2] connecting to 127.0.0.1:8080\r\n[2017-06-29 00:32:55.981][116][debug][connection] source/common/network/connection_impl.cc:427] [C2] connection in progress\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/ssl/connection_impl.cc:128] [C1] handshake error: 5\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:341] [C1] remote close\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:135] [C1] closing socket: 1\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:102] [C2] closing data_to_write=0 type=1\r\n[2017-06-29 00:32:55.983][116][debug][connection] source/common/network/connection_impl.cc:135] [C2] closing socket: 2\r\n[2017-06-29 00:32:55.983][116][info][main] source/server/connection_handler_impl.cc:51] [C1] adding to cleanup list\r\n```\r\n\r\nAnd the client side, the Envoy debug logs are:\r\n\r\n```\r\n[2017-06-29 00:32:55.980][894][info][main] source/server/connection_handler_impl.cc:109] [C1] new connection\r\n[2017-06-29 00:32:55.980][894][info][filter] source/common/filter/tcp_proxy.cc:117] [C1] new tcp proxy session\r\n[2017-06-29 00:32:55.980][894][debug][filter] source/common/filter/tcp_proxy.cc:133] [C1] Creating connection to cluster service_remote\r\n[2017-06-29 00:32:55.980][894][debug][connection] source/common/network/connection_impl.cc:418] [C2] connecting to 172.17.0.2:10001\r\n[2017-06-29 00:32:55.980][894][debug][connection] source/common/network/connection_impl.cc:427] [C2] connection in progress\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:341] [C1] remote close\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:135] [C1] closing socket: 1\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:102] [C2] closing data_to_write=29 type=1\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/ssl/connection_impl.cc:128] [C2] handshake error: 2\r\n[2017-06-29 00:32:55.981][894][debug][connection] source/common/network/connection_impl.cc:135] [C2] closing socket: 2\r\n[2017-06-29 00:32:55.981][894][info][main] source/server/connection_handler_impl.cc:51] [C1] adding to cleanup list\r\n```\r\n\r\n\r\n\r\nConfig for \"server\" Envoy:\r\n```json\r\n{\r\n    \"listeners\": [{\r\n        \"address\": \"tcp://0.0.0.0:10001\",\r\n        \"filters\": [{\r\n            \"type\": \"read\",\r\n            \"name\": \"tcp_proxy\",\r\n            \"config\": {\r\n              \"stat_prefix\": \"ingress_tcp\",\r\n              \"route_config\": {\r\n                  \"routes\": [{\r\n                    \"cluster\": \"service_local\"\r\n               }]\r\n            }\r\n          }\r\n        }],\r\n        \"ssl_context\": {\r\n          \"cert_chain_file\": \"certs/server.crt\",\r\n          \"private_key_file\": \"certs/server.key\"\r\n        }\r\n    }],\r\n    \"admin\": {\r\n        \"access_log_path\": \"/tmp/admin_access.log\",\r\n        \"address\": \"tcp://127.0.0.1:9901\"\r\n    },\r\n    \"cluster_manager\": {\r\n        \"clusters\": [{\r\n            \"name\": \"service_local\",\r\n            \"connect_timeout_ms\": 2500,\r\n            \"type\": \"static\",\r\n            \"lb_type\": \"round_robin\",\r\n            \"hosts\": [{\r\n                \"url\": \"tcp://127.0.0.1:8080\"\r\n            }]\r\n        }]\r\n    }\r\n}\r\n```\r\n\r\nConfig for \"client\" Envoy:\r\n```json\r\n{\r\n    \"listeners\": [{\r\n        \"address\": \"tcp://0.0.0.0:10000\",\r\n        \"filters\": [{\r\n            \"type\": \"read\",\r\n            \"name\": \"tcp_proxy\",\r\n            \"config\": {\r\n                \"stat_prefix\": \"ingress_tcp\",\r\n                \"route_config\": {\r\n                    \"routes\": [{\r\n                            \"cluster\": \"service_remote\"\r\n                    }]\r\n                }\r\n            }\r\n        }]\r\n    }],\r\n    \"admin\": {\r\n        \"access_log_path\": \"/tmp/admin_access.log\",\r\n        \"address\": \"tcp://127.0.0.1:9901\"\r\n    },\r\n    \"cluster_manager\": {\r\n        \"clusters\": [{\r\n            \"name\": \"service_remote\",\r\n            \"connect_timeout_ms\": 2500,\r\n            \"type\": \"static\",\r\n            \"lb_type\": \"round_robin\",\r\n            \"hosts\": [{\r\n                \"url\": \"tcp://172.17.0.2:10001\"\r\n            }],\r\n            \"ssl_context\": {\r\n              \"ca_cert_file\": \"certs/ca.crt\"\r\n            }\r\n        }]\r\n    }\r\n}\r\n```\r\n\r\n\r\nAny idea why this setup doesn't work consistently?\r\n\r\nThanks,\r\nAngela and @rosenhouse\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/1189/comments",
    "author": "angelachin",
    "comments": [
      {
        "user": "mattklein123",
        "created_at": "2017-06-29T14:42:37Z",
        "body": "When you run `date | nc -v 127.0.0.1 10000` the connection will be closed after writing the data. Envoy detects this close and terminates the proxied connection leading to a timing issue. Basically, Envoy does not currently support this case in the TCP proxy (accept client connection, write data, and guarantee that all data gets written if client closes connection). \r\n\r\nIs this a real use case or just a test? If just a test, try `telnet` or `nc` without a pipe so you can keep the connection open."
      },
      {
        "user": "angelachin",
        "created_at": "2017-06-29T16:27:58Z",
        "body": "Ok, that makes sense. It was just a test-- works fine when we don't pipe. Thanks!\r\n\r\n- Angela and @rosenhouse"
      }
    ],
    "satisfaction_conditions": [
      "Identification of connection closure timing issues in Envoy's TCP proxy implementation",
      "Clarification of Envoy's behavior with short-lived client connections in TCP proxy scenarios",
      "Guidance on valid testing methodologies for Envoy TCP proxy with TLS"
    ]
  },
  {
    "number": 10701,
    "title": "thrift proxy test driver dependencies",
    "created_at": "2020-04-08T17:25:22Z",
    "closed_at": "2022-04-26T00:54:08Z",
    "labels": [
      "tech debt",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/10701",
    "body": "Envoy's thrift proxy network filter uses some python code to generate a variety of requests and responses in the various combinations of thrift transport and protocol that are support. One of the supported protocols, colloquially known as \"ttwitter\", is no longer supported in python and is not compatible with python3, blocking #4552.\r\n\r\nThe actual python3 incompatibility is in the twitter.common.rpc package and involves a type check against `long`, which no longer exists in python3. The fix is simple, but as the package is no longer support I don't expect we'll see an update. \r\n\r\nThis issue enumerates so possible paths forward:\r\n\r\n1. It's fairly simple to patch the library to remove the check for `long`. I think this is reasonable in the short-term.\r\n\r\n2. Bring the unsupported twitter.common.rpc code into the envoyproxy org (not necessarily envoyproxy/envoy), and fix it. I dislike this path because the entire point of using external libraries was to test against a different implementation of the protocol. If the protocol were ever updated (and it does have a versioning provision) we'd be implementing both sides of the integration test.\r\n\r\n3. Thrift supports other languages besides python and it should be possible to rewrite the code in `test/extensions/filters/network/thrift_proxy/driver` in another language. Java seems the mostly likely candidate since it's supported by bazel and has support for all the variations of thrift. I think we'd want to put that code in a new repository (under the envoyproxy org) and treat the entire payload generating structure as an external dependency.\r\n\r\n4. Deprecate ttwitter support and delete usage of the abandoned libraries. I don't have a sense of how much the ttwitter thrift protocol is used in conjunction with Envoy so I don't know how to gauge how painful this would be to end users.\r\n",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/10701/comments",
    "author": "zuercher",
    "comments": [
      {
        "user": "zuercher",
        "created_at": "2020-04-08T17:29:17Z",
        "body": "#10702 implements the first option above."
      },
      {
        "user": "rgs1",
        "created_at": "2022-04-25T23:00:58Z",
        "body": "I think we should get started with option 4 (deprecate ttwitter and remove the abandoned tests), I am happy to drive. I pinged @mattklein123 offline to get a sense of whether other deployments are relying on ttwitter.\r\n\r\ncc: @fishcakez @davinci26 @tkovacs-2 @caitong93  \n\n---\n\n@zuercher I think we can close this now that #20466 is done. \r\n\r\ncc: @phlax "
      },
      {
        "user": "zuercher",
        "created_at": "2022-04-26T00:54:08Z",
        "body": "For anyone going across this bug in the future -- we chose option 4 and have deprecated the ttwitter protocol."
      }
    ],
    "satisfaction_conditions": [
      "Resolve Python3 incompatibility blocking test execution",
      "Eliminate dependency on unmaintained third-party libraries",
      "Avoid maintaining redundant protocol implementations",
      "Assess impact on end-users before deprecating protocol support"
    ]
  },
  {
    "number": 5412,
    "title": "create easier way to get a core dump locally and in CI",
    "created_at": "2018-12-24T17:03:28Z",
    "closed_at": "2019-11-08T22:24:47Z",
    "labels": [
      "area/build",
      "help wanted"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/5412",
    "body": "I was trying to debug the hot restart test and get a core dump, and it took me an embarrassingly long time to figure out how to do it under bazel (the main issue was that my shell `ulimit` settings don't transfer to the test invocation I think because bazel is running the test and not my shell). It would be really nice if we could do two things:\r\n1) Have a better way to get a core dump from a test run locally. Perhaps this is a run_under thing (per @htuch) that correctly sets up ulimit and also make sets the core dump output location to something known and then sets it back, and I guess also deals with sandboxing correctly (i.e., gives you very clear instructions on how to look at the core dump with the right symbols/binary).\r\n2) Also enables this in CI somehow where we could get a core dump saved as an artifact with the binary that created it. ",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/5412/comments",
    "author": "mattklein123",
    "comments": [
      {
        "user": "lizan",
        "created_at": "2019-11-08T22:13:00Z",
        "body": "@mattklein123 re: 1) after #8488 you can run test binary outside bazel without anything special (just invoke `bazel-bin/test/<binary>`). only general core dump configuration are needed."
      },
      {
        "user": "mattklein123",
        "created_at": "2019-11-08T22:24:46Z",
        "body": "Cool SGTM will close. Thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Enable core dump generation for local test runs without manual environment configuration (e.g., ulimit settings)"
    ]
  },
  {
    "number": 3819,
    "title": "improve syscall API wrappers",
    "created_at": "2018-07-09T20:24:01Z",
    "closed_at": "2020-01-23T17:10:39Z",
    "labels": [
      "tech debt",
      "help wanted",
      "beginner"
    ],
    "url": "https://github.com/envoyproxy/envoy/issues/3819",
    "body": "Per discussion in #3813 it'd be good if the doRead call didn't latch errno several levels away from where the actual syscall was performed.  We should change the APIs to return {rc, errnno} in some form to better futureproof in case the intermediate function calls add their own errno-corrupting debug logging.\r\n\r\nWe could also get really fancy and insist that errno is latched via fix_format fixes, as also discussed there.",
    "comments_url": "https://api.github.com/repos/envoyproxy/envoy/issues/3819/comments",
    "author": "alyssawilk",
    "comments": [
      {
        "user": "venilnoronha",
        "created_at": "2018-07-10T16:25:23Z",
        "body": "Hi @alyssawilk, I'd like to give it a try."
      },
      {
        "user": "mattklein123",
        "created_at": "2018-07-10T17:11:50Z",
        "body": "@venilnoronha go for it!"
      },
      {
        "user": "PiotrSikora",
        "created_at": "2018-07-25T00:25:50Z",
        "body": "Erm, looking at the already merged and outstanding PRs, the errno seems to be latched one level too high (inside functions calling syscall wrappers and not in the syscall wrappers themselves).\r\n\r\nIs there any reason not to do it in the syscall wrappers and return tuple from them?"
      },
      {
        "user": "venilnoronha",
        "created_at": "2018-07-25T00:50:43Z",
        "body": "@PiotrSikora that's next in the plan of action. Briefly, I wanted to start off one level higher to keep the PRs smaller."
      },
      {
        "user": "alyssawilk",
        "created_at": "2020-01-23T17:10:39Z",
        "body": "I think the final fix was solid, and we forgot to close this one out.  Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Errno must be captured at the syscall wrapper level",
      "API must return both return code and errno in a unified structure",
      "Solution must prevent future errno corruption from intermediate debug logging"
    ]
  }
]