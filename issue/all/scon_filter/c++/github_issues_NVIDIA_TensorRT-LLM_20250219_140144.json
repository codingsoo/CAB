[
  {
    "number": 2506,
    "title": "Medusa max_draft_len overhead impact",
    "created_at": "2024-11-26T22:50:45Z",
    "closed_at": "2024-12-03T08:49:20Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2506",
    "body": "### System Info\n\nCPU architecture: x86_64\nGPU: 8 NVIDIA H200\nLibraries\nTensorRT-LLM: v0.14.0\nCUDA: 12.4\nNVIDIA driver version: 550.127.05\n\n### Setup Info\n\nI'm attempting to use Medusa with TensorRT-LLM to accelerate inference of a fine-tuned Llama 3.1 70B model originally in FP16 precision. To achieve this, I first converted the model to FP8 precision and built it using the following commands:\n\n```\nquantize.py --model_dir=<FINE-TUNED MODEL DIR> --dtype=float16 --tp_size=1 --output_dir=<QUANTIZED MODEL DIR> --qformat=fp8 --kv_cache_dtype=fp8 --calib_dataset=<CALIB DATASET> --calib_size=512 --batch_size=8 --calib_max_seq_length=1024\n\ntrtllm-build --checkpoint_dir=<QUANTIZED MODEL DIR> --max_beam_width=1 --max_seq_len=131072 --max_input_len=130560 --max_num_tokens=32768 --max_batch_size=8 --context_fmha=enable --output_dir=<OUT DIR> --use_fp8_context_fmha=disable\n```\n\nI used this FP8 model to distill a dataset and then trained 3 Medusa heads. When evaluated on a validation dataset, the Medusa heads achieved the following token prediction accuracies wrt the tokens generated by the original FP16 fine-tuned model:\n```\nTopK=0\n> Head 0 Accuracy=0.6837761270606081\n> Head 1 Accuracy=0.32617484167971394\n> Head 2 Accuracy=0.1807497640902462\n\nTopK=4\n> Head 0 Accuracy=0.8547368890673448\n> Head 1 Accuracy=0.5451475708643937\n> Head 2 Accuracy=0.35749212612899667\n```\nThese results indicate that the Medusa heads are correctly predicting tokens.\n\nNext, I built an FP8 model with Medusa heads and set `max_draft_len=1`:\n```\nquantize.py --model_dir=<FINE-TUNED MODEL DIR> --dtype=float16 --tp_size=1 --output_dir=<QUANTIZED MODEL DIR> --qformat=fp8 --kv_cache_dtype=fp8 --calib_dataset=<CALIB DATASET> --calib_size=512 --batch_size=8 --calib_max_seq_length=1024 --max_draft_len=1 --num_medusa_heads=3 --num_medusa_layers=1 --medusa_model_dir=<MEDUSA MODEL DIR>\n\ntrtllm-build --checkpoint_dir=<QUANTIZED MODEL DIR> --max_beam_width=1 --max_seq_len=131072 --max_input_len=130560 --max_num_tokens=32768 --max_batch_size=8 --context_fmha=enable --output_dir=<OUT DIR> --use_fp8_context_fmha=disable --speculative_decoding_mode=medusa --max_draft_len=1\n```\n\nRunning this model built with Medusa and a comparable model built without Medusa in a framework that utilizes TensorRT-LLM's implementation of inflight batching, I observed the following inference p99 latencies:\n- FP8 model without Medusa: 2.526s\n- FP8 model with Medusa and `medusa_choices=\"[[0]]\"`: 2.271s\n\n\nI'm adding the `medusa_choices` in the code as follows:\n```\ndecoding_config = trtllm.DecodingConfig()\nif medusa_choices is not None:\n    decoding_config.medusa_choices = ast.literal_eval(medusa_choices)\n\nexecutor_config = trtllm.ExecutorConfig(\n    max_beam_width=max_beam_width,\n    max_batch_size=max_batch_size,\n    max_num_tokens=max_num_tokens,\n    batching_type=trtllm.BatchingType.INFLIGHT,\n    scheduler_config=trtllm.SchedulerConfig(trtllm.CapacitySchedulerPolicy.GUARANTEED_NO_EVICT),\n    kv_cache_config=kv_cache_config,\n    decoding_config=decoding_config,\n    enable_chunked_context=enable_chunked_context,\n    gpu_weights_percent=1\n)\n\nsession = trtllm.Executor(model_path, trtllm.ModelType.DECODER_ONLY, executor_config)\n```\n\nand creating the `trtllm.Request` like so:\n```\nimport tensorrt_llm.bindings.executor as trtllm\n\ntokens = self.tokenizer.encode(prompt, add_special_tokens=True,\n                               max_length=self.config.build_config.max_input_len, truncation=True)\n\noutput_config = trtllm.OutputConfig()\noutput_config.exclude_input_from_output = True\n\nsampling_conf = trtllm.SamplingConfig(\n    temperature=1 if self.medusa else 0.1,\n    top_k=1 if self.medusa else 50,\n    top_p=0.9,\n    random_seed=self.seed,\n    beam_width=1 if self.medusa else self.max_beam_width\n)\n\ntrt_request = trtllm.Request(\n    input_token_ids = tokens,\n    max_new_tokens = self.max_output_len,\n    pad_id = self.tokenizer.pad_token_id,\n    end_id = self.tokenizer.eos_token_id,\n    streaming = True,\n    sampling_config = sampling_conf,\n    output_config = output_config\n) \n```\n\n### Questions\n\n1. When I build a similar engine with `max_draft_len=17` and run it with the same `medusa_choices=\"[[0]]\"`, I notice a clear increase in inference latency (p99 of 2.918s). Is this expected behavior due to the increased `max_draft_len`, even though I'm specifying to use only topk 0 of the first head?\n2. Do you have any benchmarks that demonstrate the overhead introduced by increasing the Medusa choice tree size (and the `max_draft_len` with it)?",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2506/comments",
    "author": "ValeGian",
    "comments": [
      {
        "user": "rakib-hasan",
        "created_at": "2024-12-02T22:16:58Z",
        "body": "Hi @ValeGian \nFor (1), I think it is expected for the following reason. (It is a matter of compile-time-known vs runtime-known dimension)\nFor `max_draft_len=1`, TRT could choose some kernel where there is no need for any overhead (e.g. loop). \nvs\nFor `max_draft_len=17` and running with 1 medusa choice, TRT will still need to build an engine that is valid for all draft lengths from 1 to 17. So, it could choose a different kernel with an extra loop and an entirely different optimization strategy that is optimal for all values from 1 to 17, not just 1 as in the previous case. Adding that flexibility and balanced performance across all possible shapes, it can cost some performance.\n\nFor (2), unfortunately, we do not have any benchmarks yet that demonstrate the impact of these parameters. Maybe we can add it in the near future."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how `max_draft_len` parameter affects inference latency even when using limited Medusa choices",
      "Clarification on whether TRT-LLM engine compilation constraints introduce fixed overhead regardless of runtime Medusa configuration"
    ]
  },
  {
    "number": 2322,
    "title": "C++ Executor Leader Mode",
    "created_at": "2024-10-12T08:56:12Z",
    "closed_at": "2024-10-16T06:15:44Z",
    "labels": [
      "question",
      "triaged",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/2322",
    "body": "Is it possible to have one process with multiple Executors with Leader mode inside if i have one gpu device? \n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/2322/comments",
    "author": "Guangjun-A",
    "comments": [
      {
        "user": "pcastonguay",
        "created_at": "2024-10-15T14:44:59Z",
        "body": "It is possible, but not officially supported. You need to be careful when setting the `KvCacheConfig` `maxTokens` or `freeGpuMemoryFraction` and there could be performance implications when sharing a single gpu between multiple executor instances."
      },
      {
        "user": "Guangjun-A",
        "created_at": "2024-10-16T06:15:44Z",
        "body": "> It is possible, but not officially supported. You need to be careful when setting the `KvCacheConfig` `maxTokens` or `freeGpuMemoryFraction` and there could be performance implications when sharing a single gpu between multiple executor instances.\n\nget it, thanks."
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of feasibility for running multiple Leader mode Executors in a single process with one GPU",
      "Consideration of resource allocation strategies for GPU memory management",
      "Identification of performance trade-offs when sharing GPU resources"
    ]
  },
  {
    "number": 1996,
    "title": "Question: node_sharding_weight / edge_resharding_weight",
    "created_at": "2024-07-22T03:48:56Z",
    "closed_at": "2024-07-23T00:43:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1996",
    "body": "Hi, I am trying to understand the underlying code for auto_parallel. \r\n\r\nthe solver.py inside auto_parallel uses node_sharding_weight and edge_resharding_weight to add weight coefficients to \r\neach node communication cost and edge resharding costs. \r\n\r\nthe node_sharding_weight and node_resharding_weight gets incremented as follows: \r\n```       \r\nfor layer_name in layer_mapping.values():\r\n   node = self.get_node(layer_name)\r\n   node.sharding_weight += 1\r\n   node.resharding_weight += 1\r\n```\r\n\r\nMay I ask the purpose of applying these weights? \r\n\r\nI believe that such work is important and appreciate your help and the works. \r\nThanks :)\r\n",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1996/comments",
    "author": "saeyoonoh",
    "comments": [
      {
        "user": "yuxianq",
        "created_at": "2024-07-22T08:55:56Z",
        "body": "@saeyoonoh Because we simplify the graph before passing it to the solver (see tensorrt_llm/auto_parallel/simplifier.py). LLM usually contains a lot of repeated blocks, we can simplify the model by elimating most blocks (e.g. from N blocks to 2 blocks) to reduce computation cost of the solver. To correctly estimate the cost of each node/edge, we assign a `sharding_weight` for each of them to make the total cost equal to the original graph before simplified. For example, if the simplified graph contains 2 blocks, the `sharding_weight` of nodes/edges in one block should be 1, and those in another block should be N-1, which simulates the original N-block case. The `layer_mapping` records the mapping from the elimated layers to the reserved layers."
      },
      {
        "user": "saeyoonoh",
        "created_at": "2024-07-23T00:43:07Z",
        "body": "Thanks for the detailed answer. It helps me a lot."
      }
    ],
    "satisfaction_conditions": [
      "Explains the relationship between graph simplification and cost estimation accuracy",
      "Clarifies how weights compensate for structural reductions in the simplified graph",
      "Describes the purpose of tracking layer mappings in cost calculations",
      "Addresses the balance between solver efficiency and model accuracy"
    ]
  },
  {
    "number": 1778,
    "title": "`Parameter transformer.layers.N.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method`",
    "created_at": "2024-06-13T13:06:39Z",
    "closed_at": "2024-06-13T16:07:01Z",
    "labels": [
      "bug",
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1778",
    "body": "### System Info\r\n\r\nWhile trying to debug poor quality of outputs from TRT LLM for Llama3 70b tp=4 (compared to vLLM and HF), I ran into the following message when building bfloat16 engine.\r\n\r\n```\r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network\r\n```\r\n\r\n(repeated for each layer)\r\n\r\nIs this message harmless?\r\n\r\nThe commands I run:\r\n\r\n```sh\r\npython convert_checkpoint.py \\\r\n--model_dir /workspace/llama3-70b \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--dtype bfloat16 \\\r\n--tp_size 4\r\n\r\ntrtllm-build \\\r\n--checkpoint_dir /workspace/llama3-70b-bf16-tp4 \\\r\n--output_dir /workspace/llama3-70b-bf16-tp4-engine \\\r\n--gpt_attention_plugin bfloat16 \\\r\n--gemm_plugin bfloat16 \\\r\n--use_custom_all_reduce disable \\\r\n--max_num_tokens 32768 \\\r\n--max_batch_size 48 \\\r\n--max_input_len 8192 \\\r\n--max_output_len 4096\r\n```\r\n\r\nThe full logs:\r\n\r\n```\r\n[TensorRT-LLM] TensorRT-LLM version: 0.11.0.dev2024060400                                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set bert_attention_plugin to auto.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.                                                                                                                                                                                                                              \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set nccl_plugin to auto.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lookup_plugin to None.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set lora_plugin to None.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set moe_plugin to auto.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.                                                                                                                                                                                                                                   \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha to True.                                                                                                                                                                                                                                          \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_kv_cache to True.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set remove_input_padding to True.                                                                                                                                                                                                                                  \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multi_block_mode to False.                                                                                                                                                                                                                                     \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set enable_xqa to True.                                                                                                                                                                                                                                            \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set tokens_per_block to 64.                                                                                                                                                                                                                                        \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_paged_context_fmha to False.                                                                                                                                                                                                                               \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set use_fp8_context_fmha to False.                                                                                                                                                                                                                                 \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set multiple_profiles to False.                                                                                                                                                                                                                                    \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set paged_state to True.                                                                                                                                                                                                                                           \r\n[06/13/2024-13:01:14] [TRT-LLM] [I] Set streamingllm to False.                                                                                                                                                                                                                                         \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Specifying a `max_num_tokens` larger than 16384 is usually not recommended, we do not expect perf gain with that and too large `max_num_tokens` could possibly exceed the TensorRT tensor volume, causing runtime errors. Got `max_num_tokens` = 32768             \r\n[06/13/2024-13:01:14] [TRT-LLM] [W] Implicitly setting LLaMAConfig.mup_width_multiplier = 1.0                                                                                                                                                                                                          \r\n[06/13/2024-13:01:15] [TRT-LLM] [I] Set dtype to bfloat16.                                                                                                                                                                                                                                             \r\n[06/13/2024-13:01:15] [TRT] [I] [MemUsageChange] Init CUDA: CPU +17, GPU +0, now: CPU 160, GPU 528 (MiB)                                                                                                                                                                                               \r\n[06/13/2024-13:01:19] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4312, GPU +1150, now: CPU 4607, GPU 1678 (MiB)                                                                                                                                                                      \r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.\r\n[06/13/2024-13:01:19] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.                                                                                                                                                              [138/782]\r\n[06/13/2024-13:01:19] [TRT-LLM] [W] allreduce algorithm is selected automatically during execution now. use_custom_all_reduce will be deprecated in future releases.                                                                                                                                   \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set nccl_plugin to bfloat16.                                                                                                                                                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Set use_custom_all_reduce to False.                                                                                                                                                                                                                                \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.0.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.1.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.2.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.3.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.4.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.5.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.6.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.7.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.8.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.9.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                       \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.10.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.11.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.12.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.13.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.14.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                                                                                      \r\n[06/13/2024-13:01:19] [TRT-LLM] [I] Parameter transformer.layers.15.attention.embed_positions (1, 8192, 128) float32 was created but unused in forward method, so not materialized to TRT network                                         \r\n```\r\n\r\n### Who can help?\r\n\r\n@byshiue \r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n4xH100 SXM\r\n\r\n### Expected behavior\r\n\r\nUnsure, maybe the message is harmless\r\n\r\n### actual behavior\r\n\r\nN/A\r\n\r\n### additional notes\r\n\r\nN/A",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1778/comments",
    "author": "DreamGenX",
    "comments": [
      {
        "user": "nv-guomingz",
        "created_at": "2024-06-13T14:11:46Z",
        "body": "This is a known issue introduced by new feature weightless engine, it's just a warning message and harmless. Please ignore it and we'll fix it in the coming release."
      },
      {
        "user": "DreamGenX",
        "created_at": "2024-06-13T15:32:50Z",
        "body": "Thanks!"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that the parameter unused warning is harmless",
      "Clarification about potential impact on model performance/output quality",
      "Information about planned resolution timeline"
    ]
  },
  {
    "number": 1748,
    "title": "\u2018cudaStream_t\u2019 has not been declared when building tensorrt_llm v0.10.0",
    "created_at": "2024-06-06T06:09:39Z",
    "closed_at": "2024-06-06T06:29:02Z",
    "labels": [
      "question",
      "not a bug"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1748",
    "body": "### System Info\n\nCPU: INTEL\r\nGPU Name: A100-SXM4-80GB\r\nTensorRT-LLM: tag v0.10.0\r\nContainer Used: No\r\nDriver Version: 535.54.03\r\nCUDA Version: 12.1\r\nOS: Ubuntu 22.04\r\nOthers: tensorrt==10.0.1.16, pytorch==2.2.0+cu121, python==3.10.12\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\npython scripts/build_wheel.py --trt_root=/usr/local/tensorrt --clean --install --cuda_architectures=\"80-real\"\n\n### Expected behavior\n\nWorks fine\n\n### actual behavior\n\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:48:37: error: \u2018cudaStream_t\u2019 has not been declared\r\n   48 | void invokeRGLRU(lruParams& params, cudaStream_t stream);\r\n      |                                     ^~~~~~~~~~~~\r\nxxx/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h:51:43: error: \u2018cudaStream_t\u2019 has not been declared\r\n   51 | void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);\r\n      |                                           ^~~~~~~~~~~~\n\n### additional notes\n\nNo",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1748/comments",
    "author": "zhangts20",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:12:26Z",
        "body": "Env issue\r\nTry this container: nvcr.io/nvidia/tritonserver:24.05-trtllm-python-py3"
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:22:10Z",
        "body": "@hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks"
      },
      {
        "user": "hijkzzz",
        "created_at": "2024-06-06T06:29:35Z",
        "body": "> @hijkzzz Can you share the solution? I want to install tensorrt_llm in my own docker env, thanks\r\n\r\nYou could build the container that come FROM this container\r\nOr use pip install tensorrt_llm==xxx in your container."
      },
      {
        "user": "zhangts20",
        "created_at": "2024-06-06T06:46:14Z",
        "body": "@hijkzzz thanks! I have installed it successfully using pip install tensorrt_llm==xxx"
      },
      {
        "user": "yorickvP",
        "created_at": "2024-06-12T13:19:32Z",
        "body": "In case anyone else wants to build from source, add `#include <cuda_runtime.h>` to `cpp/tensorrt_llm/kernels/lruKernel.h`"
      }
    ],
    "satisfaction_conditions": [
      "Resolves missing CUDA runtime declarations during compilation",
      "Provides a way to install tensorrt_llm without requiring source compilation",
      "Ensures compatibility with user's existing CUDA 12.1 and driver stack",
      "Works in user's custom Docker environment without mandatory use of NVIDIA's container",
      "Addresses header inclusion order/dependencies in TensorRT-LLM source code"
    ]
  },
  {
    "number": 1523,
    "title": "can trtllm-build process on cpu? ",
    "created_at": "2024-04-29T09:02:41Z",
    "closed_at": "2024-04-30T03:43:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1523",
    "body": "### System Info\n\nNVIDIA A800 40G\n\n### Who can help?\n\n@byshiue \n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\ncan trtllm-build process on cpu? like parameter load_model_on_cpu in convert_checkpoint.py\n\n### Expected behavior\n\nnone\n\n### actual behavior\n\nnone\n\n### additional notes\n\nnone",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1523/comments",
    "author": "thend-wk",
    "comments": [
      {
        "user": "hijkzzz",
        "created_at": "2024-04-29T14:04:27Z",
        "body": "trtllm-build requires NVIDIA GPUs to timing the kernel perf."
      },
      {
        "user": "thend-wk",
        "created_at": "2024-04-30T01:27:55Z",
        "body": "> trtllm-build requires NVIDIA GPUs to timing the kernel perf.\r\n\r\ni get it, thanks"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation of whether trtllm-build requires NVIDIA GPUs for core functionality",
      "Clarification about GPU-bound operations in trtllm-build (e.g., kernel performance timing)"
    ]
  },
  {
    "number": 1410,
    "title": "What is the meaning for the benchmark output `tokens_per_sec` and `generation_tokens_per_second`? ",
    "created_at": "2024-04-07T07:43:18Z",
    "closed_at": "2024-04-10T09:11:21Z",
    "labels": [
      "question",
      "triaged"
    ],
    "url": "https://github.com/NVIDIA/TensorRT-LLM/issues/1410",
    "body": "I run benchmark like this:\r\n```\r\nmpirun -n 2 --allow-run-as-root python benchmark.py \\\r\n    -m llama_13b \\\r\n    --mode plugin \\\r\n    --batch_size \"1;8;16\" \\\r\n    --input_output_len \"710,190\" \\\r\n    --max_input_len 750 --max_output_len 200\r\n```\r\nI got this:\r\n```\r\n[BENCHMARK] model_name llama_13b world_size 2 num_heads 40 num_kv_heads 40 num_layers 40 hidden_size 5120 vocab_size 32000 precision float16 batch_size 1 input_length 710 output_length 190 gpu_peak_mem(gb) 0.0 build_time(s) 116.39 tokens_per_sec 43.09 percentile95(ms) 5120.208 percentile99(ms) 5120.208 latency(ms) 4409.816 compute_cap sm80 quantization QuantMode.0 generation_time(ms) 3751.546 total_generated_tokens 189.0 generation_tokens_per_second 50.379\r\n```\r\n\r\nI see there are two token per sec numbers, which is correct? and what is the meaning for each of them?\r\n\r\nI can't find any documentation mentioning that.",
    "comments_url": "https://api.github.com/repos/NVIDIA/TensorRT-LLM/issues/1410/comments",
    "author": "sleepwalker2017",
    "comments": [
      {
        "user": "byshiue",
        "created_at": "2024-04-09T07:57:54Z",
        "body": "`tokens_per_sec` means the throughput of end to end inference. It is computed by `generated_tokens / total_latency`. In your case, it is computed by `190 / 4409.816 * 1000 = 43.09`.\r\n\r\n`generation_tokens_per_second` only consider the generation. It means the thorughput during generation and computed by `generated_tokens / generation_time`. In your case, it is computed by  `189 / 3751.546 * 1000 = 50.379`. "
      },
      {
        "user": "sleepwalker2017",
        "created_at": "2024-04-09T08:02:32Z",
        "body": "> `tokens_per_sec` means the throughput of end to end inference. It is computed by `generated_tokens / total_latency`. In your case, it is computed by `190 / 4409.816 * 1000 = 43.09`.\r\n> \r\n> `generation_tokens_per_second` only consider the generation. It means the thorughput during generation and computed by `generated_tokens / generation_time`. In your case, it is computed by `189 / 3751.546 * 1000 = 50.379`.\r\n\r\nGot it, the lower number includes the prefill stage."
      },
      {
        "user": "YiandLi",
        "created_at": "2024-04-26T07:55:08Z",
        "body": "what about  `gpu_peak_mem` mean ? It is 0 in my case.\r\n"
      },
      {
        "user": "byshiue",
        "created_at": "2024-05-09T07:05:06Z",
        "body": "Could you take a try on latest main branch? "
      }
    ],
    "satisfaction_conditions": [
      "Clear distinction between end-to-end inference throughput and generation-only throughput",
      "Explanation of what time periods/components each metric measures",
      "Calculation rationale for both metrics",
      "Contextual definition of benchmark phases (prefill vs generation)"
    ]
  }
]