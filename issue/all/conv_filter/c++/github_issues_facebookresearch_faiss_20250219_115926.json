[
  {
    "number": 2953,
    "title": "IndexFlatL2 multithread is slower than single thread",
    "created_at": "2023-07-14T09:33:48Z",
    "closed_at": "2024-06-30T22:34:20Z",
    "labels": [
      "question",
      "Performance"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2953",
    "body": "python faiss-cpu 1.7.4 installed with pip3.x\r\nMultithread performance is pool on my 32-processor machine\r\n\r\nmodel name\t: Intel(R) Xeon(R) Platinum 8255C CPU @ 2.50GHz\r\n************ nthread= 1\r\n*********** nq= 100\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=1.393 ms (\u00b1 0.1564)\r\nsearch k= 10 t=2.679 ms (\u00b1 0.0422)\r\nsearch k=100 t=6.473 ms (\u00b1 0.4788)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=11.656 ms (\u00b1 23.1539)\r\nsearch k= 10 t=3.664 ms (\u00b1 0.4651)\r\nsearch k=100 t=6.653 ms (\u00b1 0.6943)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=4.447 ms (\u00b1 0.4957)\r\nsearch k= 10 t=4.460 ms (\u00b1 0.0903)\r\nsearch k=100 t=8.210 ms (\u00b1 0.8620)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=7.682 ms (\u00b1 1.1851)\r\nsearch k= 10 t=8.133 ms (\u00b1 1.1031)\r\nsearch k=100 t=10.987 ms (\u00b1 1.5985)\r\nrestab=\r\n 1.39302\t2.67902\t6.4728\r\n11.6563\t3.66396\t6.65313\r\n4.44698\t4.45956\t8.20962\r\n7.68209\t8.13305\t10.9866\r\n*********** nq= 10000\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.080 s (\u00b1 0.0044)\r\nsearch k= 10 t=0.257 s (\u00b1 0.0085)\r\nsearch k=100 t=0.564 s (\u00b1 0.0193)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.259 s (\u00b1 0.0097)\r\nsearch k= 10 t=0.321 s (\u00b1 0.0092)\r\nsearch k=100 t=0.635 s (\u00b1 0.0237)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.368 s (\u00b1 0.0306)\r\nsearch k= 10 t=0.410 s (\u00b1 0.0379)\r\nsearch k=100 t=0.681 s (\u00b1 0.0412)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.599 s (\u00b1 0.0144)\r\nsearch k= 10 t=0.645 s (\u00b1 0.0107)\r\nsearch k=100 t=0.921 s (\u00b1 0.0569)\r\nrestab=\r\n 0.0801447\t0.257458\t0.56392\r\n0.259316\t0.321337\t0.635152\r\n0.368472\t0.410237\t0.680965\r\n0.599093\t0.644711\t0.921228\r\n************ nthread= 32\r\n*********** nq= 100\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=12.850 ms (\u00b1 7.3587)\r\nsearch k= 10 t=326.201 ms (\u00b1 9.8362)\r\nsearch k=100 t=331.151 ms (\u00b1 16.7528)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=181.012 ms (\u00b1 20.5017)\r\nsearch k= 10 t=325.893 ms (\u00b1 12.7326)\r\nsearch k=100 t=325.874 ms (\u00b1 24.1845)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=181.696 ms (\u00b1 14.6625)\r\nsearch k= 10 t=329.945 ms (\u00b1 17.0235)\r\nsearch k=100 t=329.392 ms (\u00b1 14.8352)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 100 B 10000 T 0\r\nsearch k=  1 t=176.828 ms (\u00b1 9.2367)\r\nsearch k= 10 t=326.336 ms (\u00b1 16.2117)\r\nsearch k=100 t=325.248 ms (\u00b1 13.9408)\r\nrestab=\r\n 12.8498\t326.201\t331.151\r\n181.012\t325.893\t325.874\r\n181.696\t329.945\t329.392\r\n176.828\t326.336\t325.248\r\n*********** nq= 10000\r\n========== d= 16\r\ndataset in dimension 16, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.027 s (\u00b1 0.0119)\r\nsearch k= 10 t=0.980 s (\u00b1 0.0149)\r\nsearch k=100 t=1.029 s (\u00b1 0.0168)\r\n========== d= 32\r\ndataset in dimension 32, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.524 s (\u00b1 0.0138)\r\nsearch k= 10 t=0.986 s (\u00b1 0.0122)\r\nsearch k=100 t=1.066 s (\u00b1 0.0379)\r\n========== d= 64\r\ndataset in dimension 64, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.572 s (\u00b1 0.0328)\r\nsearch k= 10 t=0.999 s (\u00b1 0.0171)\r\nsearch k=100 t=1.090 s (\u00b1 0.0780)\r\n========== d= 128\r\ndataset in dimension 128, with metric L2, size: Q 10000 B 10000 T 0\r\nsearch k=  1 t=0.721 s (\u00b1 0.0103)\r\nsearch k= 10 t=1.059 s (\u00b1 0.0262)\r\nsearch k=100 t=1.147 s (\u00b1 0.0235)\r\nrestab=\r\n 0.0267251\t0.979833\t1.02869\r\n0.523988\t0.985733\t1.0658\r\n0.571997\t0.999151\t1.09039\r\n0.721175\t1.05897\t1.14676\r\n\r\n# Reproduction instructions\r\n\r\nbench_index_flat.py \r\nI modified faiss.cvar.distance_compute_min_k_reservoir from 5 to 100",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2953/comments",
    "author": "RongchunYao",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-07-24T07:20:39Z",
        "body": "Please install Faiss with conda to make sure that the proper MKL version is installed. \r\nOn intel, we sometimes observe worse MKL perf with nthread = nb cores. Please try 16 threads"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-07-24T12:28:27Z",
        "body": "> \r\nIt tried out that nthread = nb cores/2 works good for me on another server which has 16 amd processors (both training and query). Thank you so much && I wonder why the performance is bad  with nthread = nb cores :-)"
      },
      {
        "user": "alexanderguzhva",
        "created_at": "2023-07-24T16:41:19Z",
        "body": "@RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores.\r\nHope it helps. \r\n"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-07-25T02:19:01Z",
        "body": "> @RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores. Hope it helps.\r\n\r\nThank you!"
      },
      {
        "user": "RongchunYao",
        "created_at": "2023-11-30T15:30:14Z",
        "body": "> @RongchunYao the performance is likely bad because of the hyper-threading. As you know, typically the hyper-threading is about having two virtual CPU cores sharing the same compute resources of a single real core. And such a sharing is not efficient for linear-algebra ops within Faiss. So, by specifying \"nthread = nb codes / 2\" you make sure that there's no fight among two virtual CPU cores. Hope it helps.\r\n\r\nHi, I recently run faiss with openblas that compiled with omp, and I set the omp thread to 32. I run the jobs in batch on some computing platform, most machines gain great acceleration, but some machine runs very slow (each machine has similar\r\n workload). What's stranger is that part of the slow machine has a high cpu utilization ( same as normal machine ).\r\n\r\nI wonder the potential reasons, could the tasks submited to the machine by other users be a great influence factor?\r\nLooking forward to your reply."
      }
    ]
  },
  {
    "number": 2792,
    "title": "Reconstructing all vectors with Arbitrary ID mapping",
    "created_at": "2023-03-27T02:30:36Z",
    "closed_at": "2024-06-30T21:34:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2792",
    "body": "# Summary\r\n\r\nHow do I reconstruct all vectors from an Index with ID mapping enabled? The IDs are non-contiguous arbitrary integers in my case, and calling `reconstruct_n(0, index.ntotal)` throws a Fatal Python Error which I assume is because faiss is reconstructing the vectors based on my non-contiguous ID mapping.\r\n\r\nIf I understand this correctly, I should be able to get pass the ID maps and call `reconstruct_n` directly on the Index, which I assume still uses incremental IDs starting at 0.\r\n\r\nI'm aware that I can always loop through the IDs and call `reconstruct` on each item, but I believe there must be a better way?\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2792/comments",
    "author": "Isaac-the-Man",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-03-27T15:42:04Z",
        "body": "Please use `reconstruct_batch` with the ids you want to reconstruct. "
      },
      {
        "user": "Isaac-the-Man",
        "created_at": "2023-03-28T09:10:07Z",
        "body": "Thanks for the quick response, `reconstruct_batch` works perfectly for me! \r\n\r\nI'd still like to know if there is any way to bypass ID Mapping and call all the `reconstruct_x` methods directly on the default incremental ID?"
      }
    ]
  },
  {
    "number": 2370,
    "title": "IndexShards ignores ids in shards",
    "created_at": "2022-06-30T12:33:28Z",
    "closed_at": "2022-07-01T18:53:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2370",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS:\r\n\r\nFaiss version: 1.7.2\r\n\r\nInstalled from:\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n\r\nI did not expect IndexShards to ignore the ID's added to sub-indices, and I don't see how to efficiently work around this. So, I wanted to ask if this is the expected behavior, and - if so - how can I add shards with existing ID's to an IndexShards or IndexBinaryShards?\r\n\r\nI see that IndexShards has an add_with_ids, but this would require me to reconstruct an existing index's data. This would be difficult to use because I'm loading each index from disk with the IO_FLAG_MMAP to deal with memory constraints.\r\n\r\nHere is a POC of the behavior, the second assert fails, while I expected it to pass:\r\n```\r\nimport faiss\r\nimport numpy\r\n\r\n\r\ndef make_shard(dimension, data, id_0):\r\n    id_f = id_0 + data.shape[0]\r\n    print(f\"Make shard dim. {dimension} data shape {data.shape} ids {id_0}-{id_f - 1}\")\r\n    shard = faiss.IndexFlatL2(dimension)\r\n    shard_map = faiss.IndexIDMap(shard)\r\n    ids = numpy.arange(id_0, id_f)\r\n    shard_map.add_with_ids(data, numpy.arange(id_0, id_f))\r\n    return shard_map\r\n\r\n\r\ndef make_sharded_index(dimension, shards):\r\n    index_shards = faiss.IndexShards(dimension)\r\n    for i, shard in enumerate(shards):\r\n        index_shards.add_shard(shard)\r\n    return index_shards\r\n\r\n\r\ndimension = 32\r\nshard_cnt = 5\r\nshard_sz = 10\r\nkcnt = shard_sz + 1\r\nquery_row = 0\r\n\r\ndata = numpy.random.randn(shard_cnt * shard_sz, dimension).astype(numpy.float32)\r\n\r\nall_shards = [make_shard(dimension, data[i:i + shard_sz], i * shard_sz) for i in range(shard_cnt)]\r\n\r\ndata_query = data[query_row:query_row + 1]\r\n\r\nprint(f\"\\nQuery row {query_row} for each shard\")\r\nfor i, shard in enumerate(all_shards):\r\n    dists, ids = shard.search(data_query, kcnt)\r\n    print(f\"shard {i}: dist {dists[0]}\")\r\n    print(f\"shard {i}: ids {ids[0]}\\n\")\r\n\r\nprint(f\"Query row {query_row} in sharded index, in created order\")\r\nindex_shards = make_sharded_index(dimension, all_shards)\r\ndists, ids = index_shards.search(data_query, kcnt)\r\nprint(f\"shards dist {dists[0]}\")\r\nprint(f\"shards ids {ids[0]}\\n\")\r\nassert(ids[0][0] == query_row)\r\n\r\nprint(f\"Query row {query_row} in sharded index, out of order\")\r\nindex_shards = make_sharded_index(dimension, reversed(all_shards))\r\ndists, ids = index_shards.search(data_query, kcnt)\r\nprint(f\"shards rev dist {dists[0]}\")\r\nprint(f\"shards rev ids {ids[0]}\\n\")\r\nassert(ids[0][0] == query_row)\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2370/comments",
    "author": "mmaps",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-30T16:58:58Z",
        "body": "IndexShards has flag `successive_ids` to indicate whether the ids of each sub-index is relative to the last index of the previous shard. There is no way when the sub-indexes are built externally to tell if they are successive, and successive_ids is True by default. You should set is explicitly at construction time (or afterwards) with\r\n\r\n```\r\nindex_shards = IndexShards(dim, False, False) \r\n```\r\nthe first False is to indicate if search should be threaded.\r\n"
      },
      {
        "user": "mmaps",
        "created_at": "2022-07-01T18:53:39Z",
        "body": "Thanks! This fixes my issue. I had seen `successive_ids`, but didn't realize how it would affect existing ID's until I read your explanation.\r\n\r\nIf I set exaggerated (like offset +100) ID's in the sub-indexes, its more obvious that IndexShards is picking those up and not counting from 0. So, I wonder why it doesn't ignore `successive_ids` because it doesn't need to number them?"
      }
    ]
  },
  {
    "number": 2361,
    "title": "Clone not supported for this type of IndexIVF",
    "created_at": "2022-06-19T19:27:41Z",
    "closed_at": "2022-06-28T16:41:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2361",
    "body": "# Summary\r\n\r\nI'm trying to move a trained composite index to a GPU, so that adding embeddings (~5.8B) to the index is faster. However, my IndexIVF cannot be cloned onto the GPU. Here's a minimal reproducing snippet:\r\n\r\n```\r\nimport faiss\r\n\r\nindex = faiss.index_factory(128, \"OPQ4_64,IVF16384_HNSW32,PQ16x4fs\")\r\nxt = faiss.rand((20000, 128))\r\nindex.train(xt)\r\n\r\nfaiss.index_cpu_to_all_gpus(index)\r\n```\r\n\r\nwhich yields:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tmp.py\", line 7, in <module>\r\n    faiss.index_cpu_to_all_gpus(index)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 887, in index_cpu_to_all_gpus\r\n    index_gpu = index_cpu_to_gpus_list(index, co=co, gpus=None, ngpu=ngpu)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 899, in index_cpu_to_gpus_list\r\n    index_gpu = index_cpu_to_gpu_multiple_py(res, index, co, gpus)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/__init__.py\", line 882, in index_cpu_to_gpu_multiple_py\r\n    index = index_cpu_to_gpu_multiple(vres, vdev, index, co)\r\n  File \"/opt/conda/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py\", line 10278, in index_cpu_to_gpu_multiple\r\n    return _swigfaiss_avx2.index_cpu_to_gpu_multiple(provider, devices, index, options)\r\nRuntimeError: Error in virtual faiss::IndexIVF* faiss::Cloner::clone_IndexIVF(const faiss::IndexIVF*) at /root/miniconda3/conda-bld/faiss-pkg_1641228905850/work/faiss/clone_index.cpp:71: clone not supported for this type of IndexIVF\r\n```\r\n\r\nIs this expected behavior? The IndexIVF I'm using doesn't seem to be special. I've also tried:\r\n\r\n```\r\nindex_ivf = faiss.extract_index_ivf(index)\r\nindex_ivf = faiss.index_cpu_to_all_gpus(index_ivf)\r\n```\r\n\r\nwith similar results.\r\n\r\n# Platform\r\n\r\nOS: `Linux 53143a0863f8 5.4.0-94-generic #106~18.04.1-Ubuntu SMP Fri Jan 7 07:23:53 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux`\r\n(Docker image `nvidia/cuda:11.3.0-devel-ubuntu20.04`)\r\n\r\nFaiss version: \r\n\r\n```\r\nroot@fddb9798ebfc:/src# conda list\r\n# packages in environment at /opt/conda:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main\r\n_openmp_mutex             4.5                       1_gnu\r\nattrs                     21.4.0                   pypi_0    pypi\r\nblas                      1.0                         mkl\r\nbrotlipy                  0.7.0           py38h27cfd23_1003\r\nca-certificates           2022.4.26            h06a4308_0\r\ncertifi                   2022.5.18.1      py38h06a4308_0\r\ncffi                      1.15.0           py38hd667e15_1\r\ncharset-normalizer        2.0.4              pyhd3eb1b0_0\r\ncolorama                  0.4.4              pyhd3eb1b0_0\r\nconda                     4.13.0           py38h06a4308_0\r\nconda-content-trust       0.1.1              pyhd3eb1b0_0\r\nconda-package-handling    1.8.1            py38h7f8727e_0\r\ncryptography              37.0.1           py38h9ce1e76_0\r\ncudatoolkit               11.3.1               h2bc3f7f_2\r\neinops                    0.4.1                    pypi_0    pypi\r\nfaiss-gpu                 1.7.2           py3.8_h28a55e0_0_cuda11.3    pytorch\r\nfilelock                  3.7.1                    pypi_0    pypi\r\nfire                      0.4.0                    pypi_0    pypi\r\nhuggingface-hub           0.7.0                    pypi_0    pypi\r\nidna                      3.3                pyhd3eb1b0_0\r\nimportlib-metadata        4.11.1                   pypi_0    pypi\r\nintel-openmp              2021.4.0          h06a4308_3561\r\njsonlines                 3.0.0                    pypi_0    pypi\r\nld_impl_linux-64          2.35.1               h7274673_9\r\nlibfaiss                  1.7.2           hfc2d529_0_cuda11.3    pytorch\r\nlibffi                    3.3                  he6710b0_2\r\nlibgcc-ng                 9.3.0               h5101ec6_17\r\nlibgomp                   9.3.0               h5101ec6_17\r\nlibstdcxx-ng              9.3.0               hd4cf53a_17\r\nlibuv                     1.40.0               h7b6447c_0\r\nmkl                       2021.4.0           h06a4308_640\r\nmkl-service               2.4.0            py38h7f8727e_0\r\nmkl_fft                   1.3.1            py38hd3c417c_0\r\nmkl_random                1.2.2            py38h51133e4_0\r\nncurses                   6.3                  h7f8727e_2\r\nnumpy                     1.22.3           py38he7a7128_0\r\nnumpy-base                1.22.3           py38hf524024_0\r\nopenssl                   1.1.1o               h7f8727e_0\r\npackaging                 21.3                     pypi_0    pypi\r\npip                       21.2.4           py38h06a4308_0\r\npycosat                   0.6.3            py38h7b6447c_1\r\npycparser                 2.21               pyhd3eb1b0_0\r\npyopenssl                 22.0.0             pyhd3eb1b0_0\r\npyparsing                 3.0.9                    pypi_0    pypi\r\npysocks                   1.7.1            py38h06a4308_0\r\npython                    3.8.13               h12debd9_0\r\npytorch                   1.10.2          py3.8_cuda11.3_cudnn8.2.0_0    pytorch\r\npytorch-mutex             1.0                        cuda    pytorch\r\npyyaml                    6.0                      pypi_0    pypi\r\nreadline                  8.1.2                h7f8727e_1\r\nregex                     2022.6.2                 pypi_0    pypi\r\nrequests                  2.27.1             pyhd3eb1b0_0\r\nretro-pytorch             0.3.7                    pypi_0    pypi\r\nruamel_yaml               0.15.100         py38h27cfd23_0\r\nsentencepiece             0.1.96                   pypi_0    pypi\r\nsetuptools                61.2.0           py38h06a4308_0\r\nsix                       1.16.0             pyhd3eb1b0_1\r\nsqlite                    3.38.2               hc218d9a_0\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\ntk                        8.6.11               h1ccaba5_0\r\ntokenizers                0.12.1                   pypi_0    pypi\r\ntqdm                      4.63.0             pyhd3eb1b0_0\r\ntransformers              4.20.0                   pypi_0    pypi\r\ntyping_extensions         4.1.1              pyh06a4308_0\r\ntzdata                    2022a                hda174b7_0\r\nurllib3                   1.26.8             pyhd3eb1b0_0\r\nwheel                     0.37.1             pyhd3eb1b0_0\r\nxz                        5.2.5                h7b6447c_0\r\nyaml                      0.2.5                h7b6447c_0\r\nzipp                      3.8.0                    pypi_0    pypi\r\nzlib                      1.2.12               h7f8727e_1\r\n```\r\n\r\nInstalled from: Anaconda\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [X] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2361/comments",
    "author": "mitchellgordon95",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-27T23:50:11Z",
        "body": "The index type that you build here is tuned for CPU indexing. \r\n\r\n\"OPQ4_64,IVF16384_HNSW32,PQ16x4fs\"\r\n\r\n- IVFx_HNSW is not supported (and not necessary) on GPU: use IVF16386\r\n\r\n- the \"fs\" variant of PQ is not supported on GPU. Only 8-bit PQ is supported (and more accurate anyways). \r\n\r\nSo this boils down to \"OPQ8_64,IVF16386,PQ8\"\r\n"
      },
      {
        "user": "mitchellgordon95",
        "created_at": "2022-06-28T16:41:30Z",
        "body": "Ah, thank you for replying! This answers my question so I will close the issue. \r\n\r\nI am not sure I will be able to get away with \"IVF...,PQ8\" since I have >5B vectors, but I will do some benchmarking to see what works for me.\r\n\r\n"
      }
    ]
  },
  {
    "number": 2346,
    "title": "How to use single thread when do batch search",
    "created_at": "2022-06-07T02:20:36Z",
    "closed_at": "2022-08-31T09:24:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2346",
    "body": "# Summary\r\n\r\nwe want to do ivfpq search by single thread, so we use pthread function to  bind ivfpq search on a cpu core, how to do it.\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: ubuntu 18.04\r\n\r\nFaiss version: last\r\n\r\nInstalled from: compiled \r\n\r\n\r\n\r\nRunning on:\r\n- [ \u00d7 ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ \u00d7] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2346/comments",
    "author": "jackhouchina",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-06-07T09:05:31Z",
        "body": "you can call omp_set_num_threads(1) to avoid the openmp overhead. "
      },
      {
        "user": "jackhouchina",
        "created_at": "2022-06-07T09:16:24Z",
        "body": "ok,ths"
      }
    ]
  },
  {
    "number": 2285,
    "title": "ProductQuantizer  compute_codes get wrong codes when nbits not 8",
    "created_at": "2022-04-04T00:18:25Z",
    "closed_at": "2022-04-04T12:31:01Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2285",
    "body": "    \r\n\r\n\r\n    d = 10\r\n    n = 400000\r\n    cs = 5\r\n    np.random.seed(123)\r\n    x = np.random.random(size=(n, d)).astype('float32')\r\n    testInputs=np.random.random(size=(1, d)).astype('float32')\r\n    print(testInputs)\r\n    pq = faiss.ProductQuantizer(d, cs,6)\r\n    pq.verbose=True\r\n    pq.train(x)\r\n    codes=pq.compute_codes(testInputs)\r\n    #here expect 5 code range from 0-64, but get 4 and also code number not range 0-64\r\n    print(codes.shape)\r\n  \r\n   ",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2285/comments",
    "author": "jasstionzyf",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-04-04T07:51:04Z",
        "body": "This is because the codes are packed into ceil(5 * 6 / 8) = 4 bytes.  \r\nTo access the individual codes, use `BitstringReader`: \r\n\r\n```python\r\nbs = faiss.BitstringReader(faiss.swig_ptr(codes[0]), codes.shape[1])\r\nfor i in range(cs): \r\n    print(bs.read(6))  # read 6 bits at a time\r\n````\r\n\r\nAdmittedly, the `BitstringReader` API could be made more python friendly."
      },
      {
        "user": "jasstionzyf",
        "created_at": "2022-04-04T12:28:55Z",
        "body": "@mdouze   thanks very much!"
      }
    ]
  },
  {
    "number": 2259,
    "title": "Chain an existing OPQMatrix with a new IVFPQ index",
    "created_at": "2022-03-15T08:01:49Z",
    "closed_at": "2022-03-16T03:47:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2259",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nI have trained an IVFOPQ index and I want to migrate the OPQMatrix to the top of a new(untrained) IVFPQ index. Here is my code:\r\n```\r\nimport faiss\r\n\r\nold = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # suppose it is already trained, the opqmatrix is not empty\r\nnew = faiss.index_factory(128, \"IVF4,PQ16\")   # a new index that I want to prepend an OPQMatrix to\r\n\r\nvector_transform = faiss.downcast_VectorTransform(old.chain.at(0))\r\nold_opq_matrix = vector_transform.A\r\nold_opq_array = faiss.vector_to_array(old_opq_matrix)\r\n\r\nnew_opq_matrix = faiss.OPQMatrix(vector_transform.d_in, 1, vector_transform.d_out)\r\nfaiss.copy_array_to_vector(old_opq_array, new_opq_matrix.A)\r\nnew_index = faiss.IndexPreTransform(new_opq_matrix, new)\r\n```\r\nI don't think it's a good idea that we should copy the vector to a new array then copy them back. Is there a easier way to do this? I just need to chain the **old** VectorTransform and a **new** IVFPQ. \r\n\r\nI tried the following but none of them worked (throwing segmentation error when adding embeddings to the new index):\r\n```\r\nnew_index = faiss.IndexPreTransform(old.chain.at(0), new)\r\nnew_index = faiss.IndexPreTransform(faiss.downcast_VectorTransform(old.chain.at(0)), new)\r\n```\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: 1.7.1 <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: pip <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2259/comments",
    "author": "namespace-Pt",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-03-15T17:04:49Z",
        "body": "Maybe the easiest is to do \r\n```\r\nold = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # suppose it is already trained, the opqmatrix is not empty\r\nnew = faiss.index_factory(128, \"OPQ16,IVF4,PQ16\")   # a new index that I want to prepend an OPQMatrix to\r\n\r\n... train old opq\r\n\r\nopq_old = faiss.downcast_VectorTransform(old.chain.at(0))\r\nopq_new = faiss.downcast_VectorTransform(new.chain.at(0))\r\nopq_new.A = opq_old.A\r\nopq_new.b = opq_old.b\r\nopq_new.is_trained = opq_old.is_trained\r\n```\r\n"
      },
      {
        "user": "namespace-Pt",
        "created_at": "2022-03-16T03:47:51Z",
        "body": "Got it. Thank you."
      },
      {
        "user": "namespace-Pt",
        "created_at": "2022-03-16T03:48:39Z",
        "body": "@mdouze BTW, I wonder is there an Inner Product version of HNSWPQ?"
      },
      {
        "user": "mdouze",
        "created_at": "2022-03-31T11:25:51Z",
        "body": "no"
      }
    ]
  },
  {
    "number": 2057,
    "title": "QUESTION: Can I create an IndexIVFPQ object with custom centroids?",
    "created_at": "2021-09-21T15:23:20Z",
    "closed_at": "2021-09-21T16:32:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2057",
    "body": "Hello. I'm trying to run an experiment that involves using some custom centroids (that I generate) with the IVFPQ indexing structure. Since faiss provides highly optimised infrastructure and support for IVFPQ indexing, I would like to use it to perform my experiments.\r\n\r\nIs it possible to to create an `IndexIVFPQ` object whose coarse and fine quantizer centroids are initialised to vectors I provide?\r\n\r\nHere's what I tried doing to achieve this:\r\n\r\n```python\r\nquantizer = faiss.IndexFlatL2(d)  \r\nindex = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\r\ncustom_coarse_centroids = <a numpy array>\r\ncustom_pq_centroids = <a numpy array>\r\nquantizer.add(custom_coarse_centroids)\r\nindex.train(custom_coarse_centroids)\r\nfaiss.copy_array_to_vector(custom_pq_centroids.ravel(), index.pq.centroids)\r\n```\r\n\r\nAfter doing this, I verified by reading the corresponding centroids using `index.quantizer.reconstruct_n(0, index.nlist)` and `faiss.vector_to_array(index.pq.centroids).reshape(index.pq.M, index.pq.ksub, index.pq.dsub)` that the centroids are correctly set to what I want them to be. However, when I try to perform a query, I get nonsensical results such as negative distance estimates.\r\n\r\n```python\r\nindex.add(xb)\r\nD, I = index.search(xb[:5], k) # sanity check\r\nprint(I)\r\nprint(D)\r\n```\r\nI understand that certain distances and inner products are precomputed and stored inside an `IndexIVFPQ` object when the index is trained. Am I correct in thinking that what remains to be done to make my custom `IndexIVFPQ` object work correctly is to perform those precomputations? How can I make the `IndexIVFPQ` object carry out the relevant precomputations with the centroids I've just inserted?\r\n\r\nAlternatively, is there a better way to achieve this? My end goal is to create a queryable `IndexIVFPQ` object with my own custom centroids instead of relying on `.train()` to learn them.\r\n\r\nThanks in advance for any help you can offer!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2057/comments",
    "author": "anirudhajith",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-09-21T15:43:29Z",
        "body": "A few things to keep in mind: \r\n\r\n- by default the IVFPQ encodes the residual of the vectors wrt. the centroids they are assigned to, not the vectors themselves\r\n\r\n- the precomputed tables (used only for L2 search with residuals) are initialized after training so if you update the coarse or fine centroids after training you should call \r\n\r\n```\r\nindex.verbose = True # to see what happens\r\nindex.precompute_table()\r\n```"
      },
      {
        "user": "anirudhajith",
        "created_at": "2021-09-21T16:32:32Z",
        "body": "`index.precompute_table()` is exactly what I was looking for! It's working exactly as expected now. Thanks a lot!\r\n\r\nI'm aware of the bit about the residuals being encoded, thanks."
      }
    ]
  },
  {
    "number": 1973,
    "title": "Why does IndexIVFPQFastScan support only 4-bits-per-index cases?",
    "created_at": "2021-07-02T07:09:54Z",
    "closed_at": "2022-01-19T10:41:19Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1973",
    "body": "# Summary\r\n\r\nIn the beginning of IndexIVFPQFastScan.cpp, it checks for `FAISS_THROW_IF_NOT(nbits_per_idx == 4);`. It seems that FastScan shows better performance than normal IndexIVF search since it sorts QC with coarse list number beforehand. If this is the case, why is FastScan only applied to cases where it requires 4-bits per index? Is it also worth considering to apply this technique, sorting the queries beforehand based on coarse quantization results, to other cases, e.g., 8-bits-per-index cases, as well?\r\n\r\n# Platform\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [ ] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1973/comments",
    "author": "sunhongmin225",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-07-02T21:59:30Z",
        "body": "The difference with the default PQ implementation is that the look-up tables are stored in registers, but registers are too small to host 256-entry LUTs."
      },
      {
        "user": "sunhongmin225",
        "created_at": "2021-07-03T02:15:15Z",
        "body": "I see. Thanks for your reply."
      }
    ]
  },
  {
    "number": 1937,
    "title": "K-Means IP has increasing objective, but better performance - logging issue? ",
    "created_at": "2021-06-08T22:27:55Z",
    "closed_at": "2021-06-10T02:26:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1937",
    "body": "# Summary\r\n\r\nWhen running k-means with `spherical=True`, final classification results are improved compared to using an L2 distance metric when the features are unit normed. This is expected. \r\n\r\nHowever, when inspecting training loss with with `.obj` attribute, the loss increases with each iteration. I'm not sure what's causing this discrepancy. As I'm using k-means++ by initializing the centroids manually with `nredo=1` and selecting the best of multiple runs, the `.obj` attribute needs to be accurate to select the lowest loss model. \r\n\r\n\r\n# Platform\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\nRun any unit normed dataset and inspect the `.obj` attribute with `spherical=True`. It will be increasing per iteration, although the final model will perform well. \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1937/comments",
    "author": "GerardMaggiolino",
    "comments": [
      {
        "user": "GerardMaggiolino",
        "created_at": "2021-06-08T22:29:25Z",
        "body": "Alternatively, is it possible to supply multiple runs to `init_centroids` to the `.train()` function to have a set of centroids per iteration of `nredo`? \r\n\r\nIf `init_centroids` is specified, current behavior seems to be to use those centroids for all runs. "
      },
      {
        "user": "mdouze",
        "created_at": "2021-06-09T04:56:16Z",
        "body": "The objective is the sum of \"distances\" returned by the clustering index.\r\nFor an IP index the distances are actually dot products, that are better when higher, so it makes sense that the objective is increasing. \r\nNB that spherical k-means and IP search there is no clear guarantee or loss that k-means optimizes. \r\n\r\nFor the nredo / init_centroids: indeed it's a bit useless to use the combination of both.... A workaround is to run the optimization several times in an external loop."
      },
      {
        "user": "GerardMaggiolino",
        "created_at": "2021-06-10T02:26:25Z",
        "body": "@mdouze Thank you, this solves my question :) "
      }
    ]
  },
  {
    "number": 1853,
    "title": "Windows delete by IDs",
    "created_at": "2021-04-29T12:02:33Z",
    "closed_at": "2021-04-29T15:41:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1853",
    "body": "# Platform\r\nOS:\r\n- [x] Windows 10 (Error)\r\n- [x] OSX 10.15.7 (Working)\r\n\r\nFaiss version: 1.7.0\r\n\r\nInstalled from: Anaconda\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nI have problems with `delete_ids` on Windows.\r\n```python\r\nxb = np.random.randn(10, 256)\r\nxb = xb.astype(np.float32)\r\nindex = faiss.IndexFlatL2(xb.shape[1])\r\nindex.remove_ids(np.array([0]))\r\n-------------\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\__init__.py\", line 381, in replacement_remove_ids\r\n    sel = IDSelectorBatch(x.size, swig_ptr(x))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\swigfaiss.py\", line 4843, in __init__\r\n    _swigfaiss.IDSelectorBatch_swiginit(self, _swigfaiss.new_IDSelectorBatch(n, indices))\r\nTypeError: in method 'new_IDSelectorBatch', argument 2 of type 'faiss::IDSelector::idx_t const *'\r\n```\r\n\r\nAlso, I've tried to use `IndexIDMap`\r\n```python\r\nindex = faiss.IndexFlatL2(xb.shape[1])\r\nindex2 = faiss.IndexIDMap(index)\r\nindex2.add_with_ids(xb, ids)\r\n--------------\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\__init__.py\", line 212, in replacement_add_with_ids\r\n    self.add_with_ids_c(n, swig_ptr(x), swig_ptr(ids))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\swigfaiss.py\", line 4661, in add_with_ids\r\n    return _swigfaiss.IndexIDMap_add_with_ids(self, n, x, xids)\r\nTypeError: in method 'IndexIDMap_add_with_ids', argument 4 of type 'faiss::IndexIDMapTemplate< faiss::Index >::idx_t const *'\r\n```\r\n\r\nBut have the same code samples working on OSX. How can I properly delete items from `IndexFlatL2`?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1853/comments",
    "author": "hadhoryth",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-04-29T15:08:21Z",
        "body": "please make sure it is an array of int64s\r\n```\r\nindex.remove_ids(np.array([0], dtype='int64'))\r\n```"
      },
      {
        "user": "hadhoryth",
        "created_at": "2021-04-29T15:41:51Z",
        "body": "Yes, you are right. Everything is working. Thank you."
      }
    ]
  },
  {
    "number": 1569,
    "title": "Is the cosine distance normalized to 0-1 and if so how?",
    "created_at": "2020-12-10T11:45:12Z",
    "closed_at": "2020-12-15T16:46:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1569",
    "body": "I built an inner_product index with L2 normalized vectors, with the goal to search by cosine distance. The question that I have is whether this distance is in the typical -1 tot 1 range, or whether it has been normalized to 0-1, and if so - how?\r\n\r\nThanks in advance",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1569/comments",
    "author": "BramVanroy",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-12-15T16:20:44Z",
        "body": "so it's cosine similarity, which is between -1 and 1 like the normal cosine function."
      },
      {
        "user": "BramVanroy",
        "created_at": "2020-12-15T16:46:30Z",
        "body": "Alright, that is clear. Thank you."
      }
    ]
  },
  {
    "number": 1539,
    "title": "Is IMI a good index for GPU?",
    "created_at": "2020-11-22T08:53:40Z",
    "closed_at": "2020-11-23T17:35:59Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1539",
    "body": "# Summary\r\n\r\nHi, \r\n\r\nI have noticed that IMI is not implemented in the GPU Faiss. I am trying to guess the reason, is that because IMI's memory access pattern does not favor GPUs? Since IMI partitioned the vector set in a very fine-grained manner, it can lead to many small random memory accesses during the searching process, which is not friendly to GPU because these accesses may lead to bank conflicts on GPU global memory (unbalanced workload on each bank). I am just curious if this guess is correct.\r\n\r\nThanks",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1539/comments",
    "author": "WenqiJiang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-11-23T14:16:12Z",
        "body": "Yes it is. Moreover, on the GPU it is more feasible to use expensive exhaustive search for the coarse quantizer."
      },
      {
        "user": "WenqiJiang",
        "created_at": "2020-11-23T17:35:53Z",
        "body": "Thanks, great to hear that!"
      }
    ]
  },
  {
    "number": 1505,
    "title": "Clearing Cache",
    "created_at": "2020-11-05T01:55:05Z",
    "closed_at": "2020-11-05T06:12:28Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1505",
    "body": "Gooday all,\r\n\r\nIs there a way to clear cache after a query? (Using on-disk faiss)\r\nI noticed the ram usage started to buildup as repeated random queries are performed.\r\n\r\nI would like it to clear cache whenever the program used up more than 90% of total ram.\r\n\r\nThank you.\r\n\r\n- Stefan",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1505/comments",
    "author": "stefanjuang",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-11-05T05:41:35Z",
        "body": "Cache control is a system-level functionality. Cache can be disabled with \r\n```\r\nsync && sudo sh -c 'echo 3 >/proc/sys/vm/drop_caches'\r\n```\r\n"
      },
      {
        "user": "stefanjuang",
        "created_at": "2020-11-05T06:13:20Z",
        "body": "Thank you!"
      }
    ]
  },
  {
    "number": 1469,
    "title": "How to add a function in C++ and use it in python code in benches",
    "created_at": "2020-10-15T14:20:22Z",
    "closed_at": "2020-11-06T15:55:04Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1469",
    "body": "I want to add a function in C++ file and then use this function in python code in benches. I successfully compile the C++ code by 'cmake' and 'make'. But I failed to call this function in Python. Could you please tell me how to fix it? \r\nThank you",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1469/comments",
    "author": "Hap-Hugh",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-10-15T14:31:31Z",
        "body": "The function should appear in the python interface. If this is not the case, you probably forgot to install with setup.py."
      },
      {
        "user": "mdouze",
        "created_at": "2020-10-15T15:34:56Z",
        "body": "For ref, here is a one-liner I use to compile + run a test in the tests directory without installing anything: \r\n\r\n```\r\n make -C build VERBOSE=1 swigfaiss &&  (cd build/faiss/python/ ; python setup.py build ) && (pp=$PWD/build/faiss/python/build/lib; cd\r\n tests/ ; PYTHONPATH=$pp python -m unittest -v test_index )\r\n```"
      },
      {
        "user": "Hap-Hugh",
        "created_at": "2020-10-15T16:13:21Z",
        "body": "Thank you so much dear Matthijs. I run a simple test based on the new function I wrote. Fortunately, it works.\r\n\r\nThe other question is, in Link and Code bench, the code use 'import faiss'. But, if I want to use the function defined myself, I have to 'import swigfaiss'. ('import faiss' use every function that can not be modified) Is there any way that I can reconstruct this 'Link and Code' base on swigfaiss?"
      },
      {
        "user": "Hap-Hugh",
        "created_at": "2020-10-16T08:52:28Z",
        "body": "The last comment is solved. Please read the draft in mdouze's answer carefully. There is a manual wrapper, and it's really useful. So just change the python-path to build/faiss/python/build/lib and import the faiss. This will be the updated one.\r\n\r\nBy the way, do I have to run 'make clean' every time I modify the code?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-10-19T16:38:01Z",
        "body": "no."
      },
      {
        "user": "mdouze",
        "created_at": "2020-11-06T15:55:04Z",
        "body": "no activity, closing."
      }
    ]
  },
  {
    "number": 1199,
    "title": "Question Regarding How Faiss Search Neighbors",
    "created_at": "2020-05-04T16:28:58Z",
    "closed_at": "2020-05-05T20:54:50Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1199",
    "body": "Hi, I have some questions about how Faiss search for neighbors:\r\n\r\n1. For HNSW, why faiss allowed searching k > ef ?\r\n2. For IndexLSH, what is the searching algorithm? Return top k data in the bucket that the query data belong to? What if k > size(bucket that query data belongs to)?\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1199/comments",
    "author": "IhaveAquestionHere",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-05-04T21:35:55Z",
        "body": "1. Why not? When there are not enough search results, the missing entries are set to -1.\r\n2. no. The IndexLSH just binarizes the input vector and does exhaustive search on the binary vectors (there are no buckets)."
      },
      {
        "user": "IhaveAquestionHere",
        "created_at": "2020-05-05T14:57:44Z",
        "body": "Thank you very much for your reply! For HNSW, what will happen when the query number k is larger than ef (the dynamic list of neighbors)?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-05T20:48:23Z",
        "body": "hen there are not enough search results, the missing entries are set to -1"
      },
      {
        "user": "IhaveAquestionHere",
        "created_at": "2020-05-05T20:54:50Z",
        "body": "Thank you very much!"
      }
    ]
  },
  {
    "number": 1107,
    "title": "Explanation of IVF65536_HNSW32,PQ64 index structure",
    "created_at": "2020-02-16T12:53:31Z",
    "closed_at": "2020-02-21T09:15:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1107",
    "body": "# Summary\r\n\r\nDo I correctly understand, that with this index we firstly find $nprobe nearest InvertedFile clusters using HNSW to speed up the search process and then we compute distances to all vectors in found clusters using ProductQuantization code books, ordering results by distances and return it to user?\r\n\r\nIf I want to use HNSW at the second level (inside IVF clusters), which index structure should I use?\r\n\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1107/comments",
    "author": "sgjurano",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2020-02-20T10:11:24Z",
        "body": "The explanation is correct. \r\nUsing HNSW inside the IVF clusters is not implemented. It is likely that it would be less efficient than using a single HNSW on all the vectors to index."
      },
      {
        "user": "sgjurano",
        "created_at": "2020-02-21T09:15:45Z",
        "body": "Thank you for the answer."
      }
    ]
  },
  {
    "number": 1072,
    "title": "Single Server | GpuIndexFlatL2 Write Strategy",
    "created_at": "2019-12-30T09:50:25Z",
    "closed_at": "2020-01-02T10:29:51Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1072",
    "body": "Running on:\r\n- [ ] CPU\r\n- [yes] GPU\r\n\r\nInterface: \r\n- [ yes] C++\r\n- [ ] Python\r\n\r\nAbout my app:\r\n1. Multi threaded http server based application\r\n2. Accepts id and vector for /add request\r\n3. Provide GpuIndexFlatL2 search functionality\r\n\r\nHowever all the adding and search is happending in memory and if the application closes or crashes the data is lost. My question is since faiss supports writing index via:\r\n\r\n```\r\n    const char *name = \"index.bin\";\r\n    faiss::write_index(faiss::gpu::index_gpu_to_cpu(index), name);\r\n```\r\n\r\nhow do i implement the most efficient index saving strategy ?\r\n\r\n1. Block all requests while index is being written to file for every new vector added\r\nThis will lead to decrease in performance.\r\n\r\n2. Periodically update the index in the background after every 10,000 new vectors\r\nif application crashes unwritten new vectors will be lost\r\n\r\n3. Other strategy ?\r\n\r\nPlease help me. I have been scratching my head for the last 2 weeks regarding this problem.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1072/comments",
    "author": "dexception",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-12-31T06:57:12Z",
        "body": "It really depends on the operating conditions. \r\nOne approach is with two indexes: one big one with most of the vectors, and one in which you add new vectors. At search time, you search in both.\r\n\r\nThen you can save every 10k adds with: \r\n\r\n1. save small index with (fast) with incremental file names\r\n\r\n2. merge small index into big one (fast, in RAM)\r\n\r\n3. clear small index.\r\n\r\nAt recover time, you then need to load the small indexes to reconstruct the big one. You could have a background job that merges the small indexes on disk.\r\n"
      },
      {
        "user": "dexception",
        "created_at": "2019-12-31T07:07:08Z",
        "body": "Thanks i think this would work without data loss in case of failure. \r\n\r\nOther question is how do you handle meta data for the vectors because when the results for distance search are achieved that might not be revelant. For example,\r\n\r\nIn our application we have clientId,categoryId for each vector and other attributes as well. So when the topK results are returned that might not be for that clientID. Is there an Index that suports adding attributes for vectors inside the index as well ?"
      },
      {
        "user": "mdouze",
        "created_at": "2020-01-02T10:26:50Z",
        "body": "No, you need to put metadata in a separate conventional table. \r\nRationale in #641\r\n"
      },
      {
        "user": "dexception",
        "created_at": "2020-01-02T10:29:51Z",
        "body": "Resolved."
      }
    ]
  },
  {
    "number": 1069,
    "title": "Any plan on python wrapper for faiss::InvertedLists::prefetch_lists",
    "created_at": "2019-12-25T13:33:17Z",
    "closed_at": "2020-01-10T07:54:30Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1069",
    "body": "# Summary\r\ni guess **python** can not call **faiss::InvertedLists::prefetch_lists** for now.\r\nare there any plans on adding it?\r\n\r\n# example\r\ncode:\r\n```\r\ninvlists = faiss.OnDiskInvertedLists(100, 256, \"merged_index.ivfdata\")\r\npf = np.array(range(10)).astype('int')\r\ninvlists.prefetch_lists(pf, 10)\r\n```\r\n\r\nresult:\r\n```\r\nreturn _swigfaiss.OnDiskInvertedLists_prefetch_lists(self, list_nos, nlist)\r\nTypeError: in method 'OnDiskInvertedLists_prefetch_lists', argument 2 of type 'faiss::InvertedLists::idx_t const *'\r\n```\r\n\r\n# Platform\r\n\r\nOS: macOS .\r\n\r\nRunning on:\r\n- CPU\r\n\r\nInterface: \r\n- Python\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1069/comments",
    "author": "Prymon",
    "comments": [
      {
        "user": "Prymon",
        "created_at": "2019-12-25T13:35:46Z",
        "body": "i m using trick below \r\n```\r\nindex.nprob = index.nlist\r\nindex.search(np.random.random(1,128), 1)\r\nindex.nprob = 1\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2019-12-31T06:45:54Z",
        "body": "Yes python can call it. However you have to use the low-level wrapper. \r\n```\r\ninvlists.prefetch_lists(faiss.swig_ptr(pf), 10)\r\n```"
      },
      {
        "user": "Prymon",
        "created_at": "2020-01-10T07:54:30Z",
        "body": "thanks a lot !"
      }
    ]
  },
  {
    "number": 979,
    "title": "How can I get the samples number of each centroid in python?",
    "created_at": "2019-10-09T03:35:07Z",
    "closed_at": "2019-10-30T06:36:42Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/979",
    "body": "I have trained an index as below setting, and added 3.5 million data in the index.\r\nd = 1x4x4x1024\r\nquantizer = faiss.index_factory(d,'PCA4978,IVF32768_HNSW64,SQ8')\r\n\r\nBut when I search a query, time is very long, about 30s. I think maybe a lot of samples were clustered into one centroid, so I want to know the samples number of each centroid.\r\nAnd is there other reasons result in  the very long search time?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/979/comments",
    "author": "fengsky401",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-10-09T19:36:30Z",
        "body": "A good thing to check is indeed whether the lists are balanced. You can call\r\n\r\nfaiss.extract_index_ivf(quantizer).display()\r\n\r\nto display some stats, or \r\n\r\nil = faiss.extract_index_ivf(quantizer).invlists\r\nlist_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n\r\nto get all the list sizes"
      },
      {
        "user": "fengsky401",
        "created_at": "2019-10-10T10:57:28Z",
        "body": "> A good thing to check is indeed whether the lists are balanced. You can call\r\n> \r\n> faiss.extract_index_ivf(quantizer).display()\r\n> \r\n> to display some stats, or\r\n> \r\n> il = faiss.extract_index_ivf(quantizer).invlists\r\n> list_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n> \r\n> to get all the list sizes\r\n\r\nThank you!\r\nThe second method:\r\nil = faiss.extract_index_ivf(quantizer).invlists\r\nlist_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n\r\n is worked, searching a query feature in 4 million data used 66 ms, is it normal?\r\nThe index set is as below:\r\nd = 1x4x4x1024\r\nquantizer = faiss.index_factory(d,'PCA4978,IVF32768_HNSW64,SQ8')\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "fengsky401",
        "created_at": "2019-10-10T10:58:18Z",
        "body": "> A good thing to check is indeed whether the lists are balanced. You can call\r\n> \r\n> faiss.extract_index_ivf(quantizer).display()\r\n> \r\n> to display some stats, or\r\n> \r\n> il = faiss.extract_index_ivf(quantizer).invlists\r\n> list_sizes = [il.list_size(i) for i in range(il.nlist)]\r\n> \r\n> to get all the list sizes\r\n\r\nThe first method reported error:\r\nFile \"test_faiss6.py\", line 21, in <module>\r\n    faiss.extract_index_ivf(global_faiss_quantizer).display()\r\n  File \"/data/anaconda3/envs/queenie_python3/lib/python3.6/site-packages/faiss/swigfaiss_avx2.py\", line 3206, in <lambda>\r\n    __getattr__ = lambda self, name: _swig_getattr(self, IndexIVF, name)\r\n  File \"/data/anaconda3/envs/queenie_python3/lib/python3.6/site-packages/faiss/swigfaiss_avx2.py\", line 80, in _swig_getattr\r\n    raise AttributeError(\"'%s' object has no attribute '%s'\" % (class_type.__name__, name))\r\nAttributeError: 'IndexIVF' object has no attribute 'display'\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-14T11:22:07Z",
        "body": "Sorry, I meant: \r\nfaiss.extract_index_ivf(quantizer).print_stats ()"
      },
      {
        "user": "mdouze",
        "created_at": "2019-10-30T06:36:42Z",
        "body": "no activity, closing. "
      }
    ]
  },
  {
    "number": 916,
    "title": "Very high matrice dimentionality makes crash the machine when indexing",
    "created_at": "2019-08-16T09:53:10Z",
    "closed_at": "2019-08-23T12:56:45Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/916",
    "body": "# Platform\r\n\r\nOS: Linux Ubuntu 19.04\r\nConfig machine:\r\n- 8 GPU V100\r\n- CPU 96 cores\r\n- 614 Go RAM\r\n\r\nFaiss version: 1.5.3 (commit 656368b5eda4d376177a3355673d217fa95000b6)\r\nFaiss compilation options: `./configure --with-cuda=/usr/local/cuda-10.0 --prefix=/opt/faiss --with-python=/usr/lib/python3.7/` with MKL version 2019.2.187-1\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\nHere the minimal piece of code to reproduce the issue:\r\n\r\n```\r\nimport numpy as np\r\nimport faiss\r\nimport time\r\nk = 10\r\nd = 1536\r\nnb = 30000000\r\nnq = 1\r\nnp.random.seed(1234) \r\nxb = np.random.random((nb, d)).astype('float32')\r\nxb[:, 0] += np.arange(nb) / 1000.\r\nxq = np.random.random((nq, d)).astype('float32')\r\nxq[:, 0] += np.arange(nq) / 1000.\r\n\r\nindex = faiss.IndexFlatL2(d)\r\nco = faiss.GpuMultipleClonerOptions()\r\nco.shard = True\r\nco.useFloat16 = True\r\nindex = faiss.index_cpu_to_all_gpus(index, co, ngpu=4)\r\nindex.add(xb) ##### WHERE IT CRASHS\r\nstart_time = time.process_time()\r\nD, I = index.search(xq, k)\r\nprint(time.process_time() - start_time, \" seconds\")\r\nprint(I)\r\n```\r\n\r\nWhen running this piece of code the line ```index.add()``` makes crash the machine without any error message, the machine just hangs and then restarts.\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/916/comments",
    "author": "jplu",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-08-22T13:17:23Z",
        "body": "Right, it is a bit harsh that it does not report a usable error message. However, you are trying to add 11G of data at once. Could you try to add the data by slices of 1M elements?\r\n\r\n"
      },
      {
        "user": "jplu",
        "created_at": "2019-08-23T12:56:45Z",
        "body": "Cool! Thanks a lot!!! Adding by slice of 1M works perfectly."
      }
    ]
  },
  {
    "number": 892,
    "title": "display a vector at an index",
    "created_at": "2019-07-17T10:28:21Z",
    "closed_at": "2019-07-17T16:43:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/892",
    "body": "i have index of type Index faiss.IndexIVFFlat\r\ni need to retrieve or display a vector at a particular index\r\ni am working on cpu\r\ncan anyone help me in this issue\r\nthanks in advance",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/892/comments",
    "author": "Ravikiran2611",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-17T16:43:02Z",
        "body": "You can call `index.reconstruct(id)`."
      },
      {
        "user": "Ravikiran2611",
        "created_at": "2019-07-18T09:50:04Z",
        "body": "thanks you so much @beauby "
      },
      {
        "user": "yuyifan1991",
        "created_at": "2020-12-09T10:52:32Z",
        "body": "> You can call `index.reconstruct(id)`.\r\n\r\nHi, when I use the _index.reconstruct(id)_ , error is: _RuntimeError: Error in virtual void faiss::IndexIVF::reconstruct(faiss::Index::idx_t, float*) const at IndexIVF.cpp:191: Error: 'direct_map.size() == ntotal' failed: direct map is not initialized_  \r\nWhen I use the _index.make_direct_map()_ , error is : _RuntimeError: Error in void faiss::IndexIVF::make_direct_map(bool) at IndexIVF.cpp:159: Error: '0 <= idlist [ofs] && idlist[ofs] < ntotal' failed: direct map supported only for seuquential ids_\r\nI have the hash ids for the index."
      }
    ]
  },
  {
    "number": 884,
    "title": "AttributeError: 'IndexPreTransform' object has no attribute 'invlists'",
    "created_at": "2019-07-05T10:19:52Z",
    "closed_at": "2019-07-05T11:48:36Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/884",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform centOS\r\n\r\nFaiss version:  1.5.2\r\n\r\n\r\nRunning on: CPU\r\n\r\nInterface:   Python\r\n\r\n\r\n I changed the \"IVF4096,Flat\" to  'OPQ20_80,IMI2x12,PQ20'  and run demo_ondisk_ivf.py failed.\r\n\r\nerror information : \r\nAttributeError: 'IndexPreTransform' object has no attribute 'invlists'\r\n\r\nIf I use 'OPQ20_80,IMI2x12,PQ20'  how to merge the index file.\r\n\r\nThanks\r\n \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/884/comments",
    "author": "winself",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-07-05T11:48:36Z",
        "body": "You're adding an index pre-transform, so you'll need to call `index.index.invlists`."
      },
      {
        "user": "winself",
        "created_at": "2019-07-05T12:10:44Z",
        "body": "but I meet \"AttributeError: 'Index' object has no attribute 'invlists'\" "
      },
      {
        "user": "winself",
        "created_at": "2019-07-05T12:18:11Z",
        "body": "@beauby \r\nwhen i call index.index.invlists\r\n I meet \"AttributeError: 'Index' object has no attribute 'invlists'\"\r\nthanks"
      },
      {
        "user": "beauby",
        "created_at": "2019-07-05T12:55:58Z",
        "body": "`faiss.downcast_Index(index.index).invlists`"
      },
      {
        "user": "winself",
        "created_at": "2019-07-08T07:20:25Z",
        "body": "@beauby \r\nas you say , I use the faiss.downcast_Index(index.index).invlists . it works \r\nbut i meet another problem .\r\n**RuntimeError: Error in size_t faiss::OnDiskInvertedLists::merge_from(const faiss::InvertedLists\\**, int, bool) at OnDiskInvertedLists.cpp:602: Error: 'il->nlist == nlist && il->code_size == code_size' failed**\r\n\r\ni try print nlist  of  the train.index and block_x.index .they are same. \r\n\r\ncan you give some sugestion ? \r\n\r\n # my code  ( just changed the \"IVF4096,Flat\" to 'OPQ20_80,IMI2x12,PQ20') \r\n\r\n`ivfs = []\r\nfor bno in range(2):\r\n    print(\"read \" + indexdir + \"block_%d.index\" % bno)\r\n    index = faiss.read_index(indexdir + \"block_%d.index\" % bno,\r\n                             faiss.IO_FLAG_MMAP)\r\n    ivfs.append(faiss.downcast_index(index.index).invlists)\r\n    index.own_invlists = False\r\n\r\nindex = faiss.read_index(\"./data/trained.index\")\r\n\r\ninvlists = faiss.OnDiskInvertedLists(\r\n    faiss.downcast_index(index.index).nlist, faiss.downcast_index(index.index).code_size,\r\n    indexdir + \"merged_index.ivfdata\")\r\n\r\nivf_vector = faiss.InvertedListsPtrVector()\r\n\r\nfor ivf in ivfs:\r\n    ivf_vector.push_back(ivf)\r\n\r\n\r\nprint(\"merge %d inverted lists \" % ivf_vector.size())\r\nntotal = invlists.merge_from(ivf_vector.data(), ivf_vector.size())\r\n\r\nindex.ntotal = ntotal\r\nfaiss.downcast_index(index.index).replace_invlists(invlists)\r\n\r\nprint(\"write \" + indexdir + \"all.index\")\r\nfaiss.write_index(index, indexdir + \"all.index\")`\r\n"
      }
    ]
  },
  {
    "number": 870,
    "title": "Support for double precision vectors ?",
    "created_at": "2019-06-21T14:48:34Z",
    "closed_at": "2019-06-25T14:34:02Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/870",
    "body": "Hello,\r\n\r\nI could not use the library because I have double precision vectors and all train() methods use float in their signature.\r\nI think I need to write overrides for all methods that contain floats.\r\n\r\nIs this work worth to be added in a PR ?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/870/comments",
    "author": "unmeshvrije",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-06-21T21:01:03Z",
        "body": "Just convert/round your data to single precision floating point before passing it to us.\r\n\r\nWe almost certainly won't change any of the compute to allow native double precision in the math kernels, so it's just a question of whether you do the conversion or we do, and letting you do the conversion makes the most sense to me.\r\n"
      },
      {
        "user": "unmeshvrije",
        "created_at": "2019-06-24T14:48:26Z",
        "body": "@wickedfoo , Thank you for the suggestion. Is there any technical reason why \"native double precision in the math kernels\" is not allowed ?"
      },
      {
        "user": "mdouze",
        "created_at": "2019-06-25T14:22:02Z",
        "body": "@unmeshvrije, Faiss focuses on high-performance search and most indexes are approximate. In this context, we benefit from the more compact and faster operations on float32 numbers. The added precision of float64 is of no use to Faiss."
      },
      {
        "user": "unmeshvrije",
        "created_at": "2019-06-25T14:34:02Z",
        "body": "Thank you @mdouze !"
      }
    ]
  },
  {
    "number": 859,
    "title": "how to guaranteed uniqueness of id in index with add_with_ids",
    "created_at": "2019-06-12T13:59:44Z",
    "closed_at": "2019-06-13T01:23:23Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/859",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: Centos 7.5\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\nHi,\r\nI  try try to add vector with a special id into index by add_with_ids api, also I do not want to add duplicate vector(identified by id) into index. \r\nBut i find this index allow duplicate id exist, so i have to maintain an id set to decision whether exist or not. \r\nSo, my questions :\r\n1. Is there some api of index can be used to decision whether some id exist or not. \r\n2. Is there some api guaranteed uniqueness of id\r\n\r\n<pre><code>\r\nimport faiss\r\nimport numpy as np\r\n\r\nv = np.random.rand(1,128).astype('float32')\r\nindex = faiss.IndexFlatL2(128)\r\nindex = faiss.IndexIDMap(index)\r\n\r\nindex.add_with_ids(v, np.array([1001]))\r\nprint(index.ntotal) # 1\r\nindex.add_with_ids(v, np.array([1001]))\r\nprint(index.ntotal) # 2</code></pre>\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/859/comments",
    "author": "handsomefun",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-06-12T23:48:35Z",
        "body": "You would have to keep track of it yourself and enforce it. There is no requirement that the IDs are unique, in fact some use cases may desire that multiple vectors have the same identifier."
      },
      {
        "user": "handsomefun",
        "created_at": "2019-06-13T01:23:23Z",
        "body": "> You would have to keep track of it yourself and enforce it. There is no requirement that the IDs are unique, in fact some use cases may desire that multiple vectors have the same identifier.\r\n\r\nOk, thanks"
      }
    ]
  },
  {
    "number": 850,
    "title": "HNSW support range_search?",
    "created_at": "2019-06-03T14:23:38Z",
    "closed_at": "2019-06-04T02:03:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/850",
    "body": "Hi,\r\n\r\nI am curious if HNSW supports range_search. I just know the IndexIVFFlat support range_search function.\r\n\r\nI am looking forward to your reply!\r\nThank you.",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/850/comments",
    "author": "UpCoder",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2019-06-03T15:10:48Z",
        "body": "No range_search is not supported because the exploration strategy of nearest neighbors is tuned for knn-search. So it is not easy to add either.\r\n"
      },
      {
        "user": "UpCoder",
        "created_at": "2019-06-04T02:03:03Z",
        "body": "OK, Thanks"
      }
    ]
  },
  {
    "number": 822,
    "title": "Make py -- SyntaxError: invalid syntax",
    "created_at": "2019-05-09T05:03:54Z",
    "closed_at": "2019-05-13T07:58:51Z",
    "labels": [
      "question",
      "install"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/822",
    "body": "## when I run \"make py\", the following error appears\r\n\r\n```\r\nmake[1]: Entering directory 'path_to/faiss/python'\r\npython -c++ -Doverride= -I../ -DGPU_WRAPPER -o swigfaiss.cpp swigfaiss.swig\r\n  File \"<string>\", line 1\r\n    ++\r\n     ^\r\nSyntaxError: invalid syntax\r\nMakefile:17: recipe for target 'swigfaiss.cpp' failed\r\nmake[1]: [swigfaiss.cpp] Error 1 (ignored)\r\ng++ -std=c++11 -DFINTEGER=int  -fopenmp -I/usr/local/cuda-10.0/include  -fPIC -m64 -Wno-sign-compare -g -O3 -Wall -Wextra -msse4 -mpopcnt -I \\\r\n               -I../ -c swigfaiss.cpp -o swigfaiss.o\r\ng++: error: swigfaiss.cpp: No such file or directory\r\ng++: fatal error: no input files\r\ncompilation terminated.\r\nMakefile:20: recipe for target 'swigfaiss.o' failed\r\nmake[1]: *** [swigfaiss.o] Error 1\r\nmake[1]: Leaving directory '/opt/Faiss/faiss/python'\r\nMakefile:82: recipe for target 'py' failed\r\nmake: *** [py] Error 2\r\n```\r\n# Env\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nFaiss version: up to date with 'origin/master'\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\nInterface: \r\n- [x] C++\r\n- [x] Python\r\n\r\n# Previous steps done:\r\n\r\n----\r\nswig -version\r\nSWIG Version 4.0.0\r\nCompiled with g++ [x86_64-pc-linux-gnu]\r\n---\r\n\r\n$ ./configure --with-cuda=/usr/local/cuda-10.0  --with-python=/usr/bin/python3\r\n\r\n```\r\n./configure --with-cuda=/usr/local/cuda-10.0  --with-python=/usr/bin/python3\r\nchecking for g++... g++\r\nchecking whether the C++ compiler works... yes\r\nchecking for C++ compiler default output file name... a.out\r\nchecking for suffix of executables...\r\nchecking whether we are cross compiling... no\r\nchecking for suffix of object files... o\r\nchecking whether we are using the GNU C++ compiler... yes\r\nchecking whether g++ accepts -g... yes\r\nchecking whether g++ supports C++11 features with -std=c++11... yes\r\nchecking for gcc... gcc\r\nchecking whether we are using the GNU C compiler... yes\r\nchecking whether gcc accepts -g... yes\r\nchecking for gcc option to accept ISO C89... none needed\r\nchecking how to run the C preprocessor... gcc -E\r\nchecking whether make sets $(MAKE)... yes\r\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\r\nchecking for /usr/bin/python3... no\r\nchecking for Python C flags... ./configure: line 4138: -c: command not found\r\n\r\nchecking for swig... no\r\nchecking how to run the C++ preprocessor... g++ -std=c++11 -E\r\nchecking for grep that handles long lines and -e... /bin/grep\r\nchecking for egrep... /bin/grep -E\r\nchecking for ANSI C header files... yes\r\nchecking for sys/types.h... yes\r\nchecking for sys/stat.h... yes\r\nchecking for stdlib.h... yes\r\nchecking for string.h... yes\r\nchecking for memory.h... yes\r\nchecking for strings.h... yes\r\nchecking for inttypes.h... yes\r\nchecking for stdint.h... yes\r\nchecking for unistd.h... yes\r\nchecking for nvcc... /usr/local/cuda-10.0/bin/nvcc\r\nchecking cuda.h usability... yes\r\nchecking cuda.h presence... yes\r\nchecking for cuda.h... yes\r\nchecking for cublasAlloc in -lcublas... yes\r\nchecking for cudaSetDevice in -lcudart... yes\r\nchecking float.h usability... yes\r\nchecking float.h presence... yes\r\nchecking for float.h... yes\r\nchecking limits.h usability... yes\r\nchecking limits.h presence... yes\r\nchecking for limits.h... yes\r\nchecking stddef.h usability... yes\r\nchecking stddef.h presence... yes\r\nchecking for stddef.h... yes\r\nchecking for stdint.h... (cached) yes\r\nchecking for stdlib.h... (cached) yes\r\nchecking for string.h... (cached) yes\r\nchecking sys/time.h usability... yes\r\nchecking sys/time.h presence... yes\r\nchecking for sys/time.h... yes\r\nchecking for unistd.h... (cached) yes\r\nchecking for stdbool.h that conforms to C99... no\r\nchecking for _Bool... no\r\nchecking for inline... inline\r\nchecking for int32_t... yes\r\nchecking for int64_t... yes\r\nchecking for C/C++ restrict keyword... __restrict\r\nchecking for size_t... yes\r\nchecking for uint16_t... yes\r\nchecking for uint32_t... yes\r\nchecking for uint64_t... yes\r\nchecking for uint8_t... yes\r\nchecking for stdlib.h... (cached) yes\r\nchecking for GNU libc compatible malloc... yes\r\nchecking for stdlib.h... (cached) yes\r\nchecking for unistd.h... (cached) yes\r\nchecking for sys/param.h... yes\r\nchecking for getpagesize... yes\r\nchecking for working mmap... yes\r\nchecking for clock_gettime... yes\r\nchecking for floor... yes\r\nchecking for gettimeofday... yes\r\nchecking for memmove... yes\r\nchecking for memset... yes\r\nchecking for munmap... yes\r\nchecking for pow... yes\r\nchecking for sqrt... yes\r\nchecking for strerror... yes\r\nchecking for strstr... yes\r\nchecking for g++ -std=c++11 option to support OpenMP... -fopenmp\r\nchecking build system type... x86_64-pc-linux-gnu\r\nchecking host system type... x86_64-pc-linux-gnu\r\nchecking if sgemm_ is being linked in already... no\r\nchecking for sgemm_ in -lmkl_intel_lp64... no\r\nchecking for sgemm_ in -lmkl... no\r\nchecking for sgemm_ in -lopenblas... yes\r\nchecking for cheev_... yes\r\nchecking target system type... x86_64-pc-linux-gnu\r\nchecking for cpu arch... x86_64-pc-linux-gnu CPUFLAGS+=-msse4 -mpopcnt CXXFLAGS+=-m64\r\nconfigure: creating ./config.status\r\nconfig.status: creating makefile.inc\r\n```\r\n\r\n$ make\r\n$ make install\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/822/comments",
    "author": "0xhanh",
    "comments": [
      {
        "user": "Santiago810",
        "created_at": "2019-05-09T08:22:11Z",
        "body": "\r\nthe first line show some flag var are wrong\r\nthe second line show swig is not installed.\r\n\r\nI also fail when making py.\r\n```\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\nswigfaiss.swig:301: Warning 302: Identifier 'IndexShards' redefined (ignored) (Renamed from 'IndexShardsTemplate< faiss::Index >'),\r\n../IndexShards.h:79: Warning 302: previous definition of 'IndexShards'.\r\nswigfaiss.swig:302: Warning 302: Identifier 'IndexBinaryShards' redefined (ignored) (Renamed from 'IndexShardsTemplate< faiss::IndexBinary >'),\r\n../IndexShards.h:80: Warning 302: previous definition of 'IndexBinaryShards'.\r\nswigfaiss.swig:305: Warning 302: Identifier 'IndexReplicas' redefined (ignored) (Renamed from 'IndexReplicasTemplate< faiss::Index >'),\r\n../IndexReplicas.h:86: Warning 302: previous definition of 'IndexReplicas'.\r\nswigfaiss.swig:306: Warning 302: Identifier 'IndexBinaryReplicas' redefined (ignored) (Renamed from 'IndexReplicasTemplate< faiss::IndexBinary >'),\r\n../IndexReplicas.h:87: Warning 302: previous definition of 'IndexBinaryReplicas'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n../Index.h:63: Warning 315: Nothing known about 'long'.\r\n../IndexBinary.h:38: Warning 315: Nothing known about 'Index::idx_t'.\r\n```\r\nthis warning lead to the idx_t undefined  when compile the swigfaiss.cpp.\r\nwhen I try to explicit typedefine idx_t, it still get error about other undefine functions.Needing help"
      },
      {
        "user": "beauby",
        "created_at": "2019-05-09T10:00:44Z",
        "body": "@hanhfgia Swig does not seem to be in your path."
      },
      {
        "user": "beauby",
        "created_at": "2019-05-09T10:01:09Z",
        "body": "@Santiago810 Would you mind opening a separate issue?"
      },
      {
        "user": "0xhanh",
        "created_at": "2019-05-10T06:54:32Z",
        "body": "> @hanhfgia Swig does not seem to be in your path.\r\n\r\nThanks, reload env missed :). It's done"
      },
      {
        "user": "chenqiu01",
        "created_at": "2020-04-17T09:17:07Z",
        "body": "> > @hanhfgia Swig does not seem to be in your path.\r\n> \r\n> Thanks, reload env missed :). It's done\r\n\r\nExcuse me, What's the Path which i need to join in?"
      },
      {
        "user": "rookiezed",
        "created_at": "2022-09-27T02:06:06Z",
        "body": "> > > @hanhfgia Swig does not seem to be in your path.\r\n> > \r\n> > \r\n> > Thanks, reload env missed :). It's done\r\n> \r\n> Excuse me, What's the Path which i need to join in?\r\n\r\ntry install swig, this fix my problem"
      }
    ]
  },
  {
    "number": 804,
    "title": "How to understand the nlist parameter\uff1f",
    "created_at": "2019-04-24T11:44:31Z",
    "closed_at": "2019-04-29T13:02:03Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/804",
    "body": "# Summary\r\nthe sample code of cpp tutorial\uff0c like this, how to understand the nlist ?\r\n\r\n```\r\nint nlist = 100;\r\nint k = 4;\r\nint m = 8;                             // bytes per vector\r\nfaiss::IndexFlatL2 quantizer(d);       // the other index\r\nfaiss::IndexIVFPQ index(&quantizer, d, nlist, m, 8);\r\n// here we specify METRIC_L2, by default it performs inner-product search\r\nindex.train(nb, xb);\r\nindex.add(nb, xb);\r\n```\r\n\r\nRunning on:\r\n- [ ] CPU\r\n\r\nInterface: \r\n- [ ] C++\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/804/comments",
    "author": "yuxingfirst",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2019-04-24T11:49:43Z",
        "body": "All IVF index work by splitting the vectors into `nlist` clusters, according to the quantizer. During search time, only `nprobe` clusters are searched."
      },
      {
        "user": "yuxingfirst",
        "created_at": "2019-04-24T13:03:10Z",
        "body": "> All IVF index work by splitting the vectors into `nlist` clusters, according to the quantizer. During search time, only `nprobe` clusters are searched.\r\n\r\nThanks your reply\uff0c i got that."
      }
    ]
  },
  {
    "number": 620,
    "title": "TypeError: in method 'IndexPreTransform_reconstruct', argument 2 of type 'faiss::Index::idx_t'",
    "created_at": "2018-10-18T07:47:14Z",
    "closed_at": "2018-10-22T02:39:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/620",
    "body": "I am using faiss-cpu version with python interface, when I am trying to reconstruct a vector from an idx, i meet an error below: \r\n```\r\nTypeError: in method 'IndexPreTransform_reconstruct', argument 2 of type 'faiss::Index::idx_t'\r\n```\r\n\r\nThe code I use is \r\n```\r\nfeat = np.load('feat.npy')\r\nd = 2048\r\nindex = faiss.index_factory(d, 'PCAR128,IMI2x10,SQ8')\r\nfaiss.ParameterSpace().set_index_parameter(index, 'nprobe', 100)\r\nindex.train(feat)\r\nindex.add(feat)\r\n\r\nquery_feat = np.random.rand(1, d)\r\nk = 10\r\nD, I  = index.search(query_feat, k)\r\nreconstruct_feat = index.reconstruct(I[0, 0]) # I[0, 0] is not -1\r\n```",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/620/comments",
    "author": "animebing",
    "comments": [
      {
        "user": "beauby",
        "created_at": "2018-10-19T11:12:57Z",
        "body": "Could you post the full stack trace?"
      },
      {
        "user": "animebing",
        "created_at": "2018-10-19T11:19:54Z",
        "body": "@beauby, below is the whole stack trace\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-101-6c9926c73508> in <module>()\r\n     30 for i in range(search_num):\r\n     31     tmp_idx = I[0, i]\r\n---> 32     tm_index.reconstruct(tmp_idx)\r\n     33     tmp_img_path = database_info_list[tmp_idx].strip('\\n').split(' ')[0]\r\n     34     tmp_img = Image.open(tmp_img_path)\r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/faiss/__init__.py in replacement_reconstruct(self, key)\r\n    151     def replacement_reconstruct(self, key):\r\n    152         x = np.empty(self.d, dtype=np.float32)\r\n--> 153         self.reconstruct_c(key, swig_ptr(x))\r\n    154         return x\r\n    155 \r\n\r\n~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/faiss/swigfaiss.py in reconstruct(self, key, recons)\r\n   1917 \r\n   1918     def reconstruct(self, key, recons):\r\n-> 1919         return _swigfaiss.IndexPreTransform_reconstruct(self, key, recons)\r\n   1920 \r\n   1921     def reconstruct_n(self, i0, ni, recons):\r\n\r\nTypeError: in method 'IndexPreTransform_reconstruct', argument 2 of type 'faiss::Index::idx_t'\r\n```"
      },
      {
        "user": "mdouze",
        "created_at": "2018-10-20T16:45:59Z",
        "body": "probably a weird interaction between numpy and swig. Try casting -> \r\n\r\n```\r\nindex.reconstruct(int(I[0, 0]))\r\n```"
      },
      {
        "user": "animebing",
        "created_at": "2018-10-22T02:39:39Z",
        "body": "@mdouze, thanks for your reply, it works right now."
      },
      {
        "user": "Prymon",
        "created_at": "2019-12-25T12:28:51Z",
        "body": "try below:\r\n    query_feat = np.random.rand((1, d))\r\n\r\n    rand((a,b))    not    rand(a,b)"
      }
    ]
  },
  {
    "number": 495,
    "title": "Nested Indexes",
    "created_at": "2018-06-19T18:32:46Z",
    "closed_at": "2018-06-20T16:34:48Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/495",
    "body": "# Summary\r\nI am trying to run the demo_ondisk_ivf.py, I want to try the PCA dimension reduction, I replaced this line\r\nindex = faiss.index_factory(xt.shape[1], \"IVF4096,Flat\")\r\n\r\nto \r\n\r\nindex = faiss.index_factory(xt.shape[1], \"PCAR8,IVF4096,Flat\")\r\n\r\nBut then, when in stage 5, how can I merge the images. Now the index is VectorTransform, not a IVFIndex, there's no index.invlists, how can I get index.invlists filed\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [ ] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/495/comments",
    "author": "kwaibun",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-20T15:56:47Z",
        "body": "Hi\r\nIt is `faiss.downcast_Index(index.index).invlists`."
      },
      {
        "user": "kwaibun",
        "created_at": "2018-06-20T16:34:45Z",
        "body": "Cool, thanks!"
      }
    ]
  },
  {
    "number": 483,
    "title": "Faiss is optimized for batch search, but looks like during query time, the searches are done in parrel in different threads in OMP",
    "created_at": "2018-06-07T01:34:45Z",
    "closed_at": "2018-07-06T09:11:46Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/483",
    "body": "# Summary\r\nwhy matrix-matrix multiplication is not used in final query. I can see that knn_L2sqr_blas is implemented for IndexFlat search, and this is used to pick up piles of centroids during search. After getting the nprobes of clusters, seperate queries, say, 20 queries are conducted in vector-vector L2 distance comparison with OMP. Is there a reason for this? is blas not efficient for small blocks matrix compution?\r\n\r\n# Platform\r\n\r\nOS: <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\n\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/483/comments",
    "author": "fishbell",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-06-08T09:54:57Z",
        "body": "Hi \r\nYou cannot map this to matrix-matrix product unless several vectors get quantized to the same centroids."
      },
      {
        "user": "fishbell",
        "created_at": "2018-06-13T01:48:23Z",
        "body": "yes I did not notice this. Thanks for your answer!"
      }
    ]
  },
  {
    "number": 458,
    "title": "Libgomp: Thread creation failed: Resource temporarily unavailable",
    "created_at": "2018-05-23T09:48:52Z",
    "closed_at": "2018-06-12T10:15:20Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/458",
    "body": "Released a faiss service with thrift, my thrift service opened 100 threads, requests more than one, it will give an error:\r\nLibgomp: Thread creation failed: Resource temporarily unavailable\r\n\r\nulimit -u 65535",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/458/comments",
    "author": "fuchao01",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-05-23T22:31:03Z",
        "body": "Hi \r\nYou may want to compile Faiss without threading if you are using thrift to do the multi-threading. OpenMP has a non-trivial overhead when a new non-openmp thread is started.\r\n"
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T02:53:27Z",
        "body": "@mdouze Thank you for your reply.How to compile faiss without threads"
      },
      {
        "user": "mdouze",
        "created_at": "2018-05-25T08:11:55Z",
        "body": "In `makefile.inc` in the `CFLAGS` variable replace `-fopenmp` with `-fno-openmp`. Adding `-Wno-error=unknown-pragmas` will quiet all the warnings. "
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T09:54:26Z",
        "body": "This really does. But there is a problem, performance is not as good as before. Can you specify the maximum number of openmp threads?"
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T10:05:55Z",
        "body": "faiss.omp_set_num_threads() This parameter is not set openmp open thread number?"
      },
      {
        "user": "fuchao01",
        "created_at": "2018-05-25T15:30:26Z",
        "body": "I have 200w indexed data, qps 100/s, thrift server 100 threads. The faiss flat index is used. The server is basically running at full capacity. 32-core cpu, 128g memory, load reaches 40+. Is the amount of data too large for the index?"
      },
      {
        "user": "mdouze",
        "created_at": "2018-06-12T10:15:20Z",
        "body": "No clear question. Closing."
      }
    ]
  },
  {
    "number": 376,
    "title": "Access `nprobe` attribute for an `IndexPreTransform` ",
    "created_at": "2018-03-25T20:17:00Z",
    "closed_at": "2018-03-26T11:53:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/376",
    "body": "# Summary\r\n\r\nFind `nprobe` attribute for an `IndexPreTransform`, such as `OPQ64_256,IVF4096,PQ64`.\r\n\r\n# Platform\r\n\r\nOS: Linux\r\n\r\nFaiss version: 4d440b6698fcc7b08607534bc622902b52bf9c49\r\n\r\nFaiss compilation options: from pytorch/faiss-cpu\r\n\r\nRunning on :\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\n# Reproduction instructions\r\n\r\nI was able to set/get `nprobe` attribute for an `IndexIVFFlat`, or `IndexIVFScalarQuantizer`, but for an index constructed through factory, or `faiss.load_index()`, such as `OPQ64_256,IVF4096,PQ64`, how can I achieve the same attribute?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/376/comments",
    "author": "terencezl",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-26T11:45:53Z",
        "body": "Hi \r\nYou can do:\r\n```\r\nfaiss.ParameterSpace().set_index_parameter(index, \"nprobe\", 123)\r\n```\r\nor\r\n```\r\nfaiss.downcast_index(index.index).nprobe = 123\r\n```"
      },
      {
        "user": "terencezl",
        "created_at": "2018-03-26T11:53:31Z",
        "body": "Thanks!"
      }
    ]
  },
  {
    "number": 375,
    "title": "Running on GPU slower than CPU?",
    "created_at": "2018-03-23T04:48:37Z",
    "closed_at": "2018-03-26T14:19:36Z",
    "labels": [
      "question",
      "GPU"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/375",
    "body": "# Summary\r\n\r\nI use faiss for my own dataset.\r\nFirst, I try IndexFlatL2 on cpu, it takes around 90 seconds for my dataset\r\nAnd then, I try multiple gpus by the code below, and it takes around 400 seconds for my dataset.\r\n\r\n```python\r\ncpu_index = faiss.IndexFlatL2(d)\r\n\r\ngpu_index = faiss.index_cpu_to_all_gpus(  # build the index\r\n    cpu_index\r\n)\r\n```\r\n\r\nSo, for the normal index like IndexFlat2D, how can I optimize the performance?",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/375/comments",
    "author": "hminle",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2018-03-23T12:08:36Z",
        "body": "Hi,\r\nWhat is the number of vectors, their dimension and how are you performing the searches (by batch or one by one)?"
      },
      {
        "user": "wickedfoo",
        "created_at": "2018-03-23T15:54:21Z",
        "body": "Also, how are you timing the search on the GPU? Are you including the copy of the index to the GPUs?\r\n\r\n"
      },
      {
        "user": "hminle",
        "created_at": "2018-03-26T07:34:59Z",
        "body": "@mdouze Hi, the size of my embeddings is (23600, 128)\r\nD = 128\r\nI perform the search one by one, not by batch\r\n"
      },
      {
        "user": "hminle",
        "created_at": "2018-03-26T07:38:06Z",
        "body": "@wickedfoo I run my script on my own dataset, \r\nFirst, I run it with simple index (IndexFlat2D).\r\nAnd then I modify my code to transfer the index to the gpu, and run my script again.\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2018-03-26T12:15:44Z",
        "body": "If you run the search one by one, you cannot take advantage of the GPU because of insufficient inherent parallelism and the synchronization and memory transfer overheads. "
      },
      {
        "user": "hminle",
        "created_at": "2018-03-26T14:19:26Z",
        "body": "@mdouze Thank you a lot. I got it."
      }
    ]
  },
  {
    "number": 208,
    "title": "what is in the GPU global memory when i use GpuIndexIVFPQ for search?",
    "created_at": "2017-09-08T03:30:53Z",
    "closed_at": "2017-09-08T13:58:55Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/208",
    "body": "my test is that:  d=128  trainset10Million,  c1_centroids = 4*sqrt(nt)(adout 12000) subM=8,  32floating points ,  and i add 100Millon 128D data into the gpuivfpq-index.  my query set is 2000*128D.   query-batch=500.    Gpu is one Tesla P4 with Memory 7606MiB.    after add the 100Million*12D, i use the api  gpu_ivfpq_index.reclaimMemory() and get the result 668274176 .  but when nvidia-smi to check the GPU, GPU global memory used about 4513M.  (when it's 200Million*128D,the number is 1336548352,  definately 2times . and Gpu 6419M ) so here is my questions:\r\n1.what's the content of 668274176?     i think the index mainly has invert-list-indexs and  codes. each vector has 8 Byte index and 8 Byte PQ codes.  there's  100Million * (8+8)Byte, isn't it ?\r\n2.what 's in the GPU global memory  (use 4513M)? \r\n\r\nplease help me , thank you",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/208/comments",
    "author": "bzwqq",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2017-09-08T13:58:55Z",
        "body": "`reclaimMemory` returns the amount of memory reclaimed in bytes, not the amount of memory in use. This is done by exactly sizing lists for storage.\r\n\r\nGPU Faiss reserves ~18% of the GPU's memory for temporary calculations. This is adjustable in `StandardGpuResources`, so about 1370 MB is used for that.\r\n\r\nYou are correct, the size of the index in memory for 8 byte indices and 8 byte PQ codes is roughly N * (8 + 8).\r\n\r\nIf you have precomputed codes enabled, then there is potentially a lot of memory outstanding for that. So the memory you have in use is your list storage + precomputed codes + temp memory reservation + some other smaller, miscellaneous things.\r\n\r\n\r\n"
      },
      {
        "user": "bzwqq",
        "created_at": "2017-09-10T04:03:39Z",
        "body": "ok,thank you very much! i really appreciate it"
      }
    ]
  },
  {
    "number": 174,
    "title": "How can I set ClusteringParameters for GpuIndexIVFFlat  in python ?",
    "created_at": "2017-08-07T11:18:58Z",
    "closed_at": "2017-08-10T05:59:35Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/174",
    "body": "",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/174/comments",
    "author": "djy4713",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2017-08-07T11:34:58Z",
        "body": "Hi \r\n\r\nwith eg. `index.cp.niter = 50`"
      },
      {
        "user": "djy4713",
        "created_at": "2017-08-07T12:02:11Z",
        "body": "but on gpu edition, it can not work.   eg. GpuIndexIVFFlat object.\r\nI just modify the GpuIndexIVF.h file, change the \"cp_\" variable from projected to public and recompile, then i can work.  eg. gpu_index.cp_.niter = 50"
      },
      {
        "user": "wickedfoo",
        "created_at": "2017-08-07T20:57:20Z",
        "body": "I am changing the GPU code to expose ClusteringParameters in the same way as the CPU code, as a public member. Once the push is made, you should be able to just use `index.cp`.\r\n"
      },
      {
        "user": "djy4713",
        "created_at": "2017-08-08T07:11:11Z",
        "body": "thank you."
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-09T18:22:31Z",
        "body": "Ok, the push is done in the latest Faiss version."
      },
      {
        "user": "mdouze",
        "created_at": "2017-08-10T05:59:35Z",
        "body": "Seems to solve the problem. Closing."
      }
    ]
  },
  {
    "number": 2894,
    "title": "TypeError: in method 'IndexFlat_range_search', argument 4 of type 'float'",
    "created_at": "2023-06-05T18:34:02Z",
    "closed_at": "2023-06-06T17:24:48Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2894",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\nI have been using the `range_search` functionality with great success within the Python interpreter. However, when I attempt to call it through a bash interface, I get prompted the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/path_to_script/test_faiss_cmd.py\", line 24, in <module>\r\n    lim, D, I = idx.range_search(X, thresh=r)\r\n  File \"/home/sebastiaan/miniconda3/envs/knn_tcr/lib/python3.9/site-packages/faiss/__init__.py\", line 492, in replacement_range_search\r\n    self.range_search_c(n, swig_ptr(x), thresh, res)\r\n  File \"/home/sebastiaan/miniconda3/envs/knn_tcr/lib/python3.9/site-packages/faiss/swigfaiss_avx2.py\", line 1631, in range_search\r\n    return _swigfaiss_avx2.IndexFlat_range_search(self, n, x, radius, result)\r\nTypeError: in method 'IndexFlat_range_search', argument 4 of type 'float'\r\n```\r\nRunning the exact same code in a Python interpreter does not produce the error, it only occurs from a command line interface.\r\n\r\n# Platform\r\n\r\nOS: Ubuntu 20.04.5 LTS\r\n\r\nFaiss version: faiss 1.7.2 py39h44b29b8_3_cpu conda-forge\r\n\r\nInstalled from: anaconda \r\n\r\nFaiss compilation options: /\r\n\r\nRunning on:\r\n- [X] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [X] Python\r\n\r\n# Reproduction instructions\r\n\r\n```python\r\nimport faiss\r\n\r\n# Generate random input array of shape (n, d)\r\nn = 500\r\nd = 272python3 test_faiss_cmd.py --n_vecs 100 --n_dims 272 --radius 50\r\nvecs = np.random.rand(n,d).astype(\"float32\")\r\n\r\n# Build Flat Index\r\nidx = faiss.IndexFlatL2(272)\r\nidx.train(vecs)\r\nidx.add(vecs)\r\n\r\n# Search Flat Index\r\nr = 24\r\nX = np.random.rand(1,d).astype(\"float32\")\r\nlim, D, I = idx.range_search(X, thresh=r)\r\n```\r\n\r\nThis example runs perfectly in a Python interpreter. However, in the following situation, this script fails and prompts the error that was mentioned previously.\r\n\r\n`argparse` script (test_faiss_cmd.py):\r\n\r\n```python\r\nimport faiss\r\nimport numpy as np\r\nimport argparse\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--n_vecs', type=int)\r\nparser.add_argument('--n_dims', type=int)\r\nparser.add_argument('--radius')\r\nargs = parser.parse_args()\r\n\r\n# Generate random input array of shape (n, d)\r\nn = args.n_vecs\r\nd = args.n_dims\r\nvecs = np.random.rand(n,d).astype(\"float32\")\r\n\r\n# Build Flat Index\r\nidx = faiss.IndexFlatL2(args.n_dims)\r\nidx.train(vecs)\r\nidx.add(vecs)\r\n\r\n# Search Flat Index\r\nr = args.radius\r\nX = np.random.rand(1,d).astype(\"float32\")\r\nlim, D, I = idx.range_search(X, thresh=r)\r\n```\r\nCommand line:\r\n`python3 test_faiss_cmd.py --n_vecs 100 --n_dims 272 --radius 50`\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2894/comments",
    "author": "svalkiers",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2023-06-06T09:12:15Z",
        "body": "radius is a string......"
      },
      {
        "user": "svalkiers",
        "created_at": "2023-06-06T17:24:48Z",
        "body": "Wow, I can't believe I did not realize this. Issue solved."
      }
    ]
  },
  {
    "number": 2377,
    "title": "Getting Cosine similarity different for \"Flat\" & \"HNSW32Flat\" Indexes",
    "created_at": "2022-07-07T05:45:32Z",
    "closed_at": "2024-07-24T18:25:52Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2377",
    "body": "# Summary\r\n\r\n<!-- Facebook has a bounty program for the safe disclosure of security bugs. In\r\nthose cases, please go through the process outlined on that page and do not\r\nfile a public issue. -->\r\n\r\n# Platform\r\n\r\n<!-- if the question/problem is not platform-specific, please ignore this -->\r\n\r\nOS: linux <!-- e.g. macOS 10.13.3 -->\r\n\r\nFaiss version: <!-- git commit, e.g. 56383610bcb982d6591e2e2bea3516cb7723e04a -->\r\n\r\nInstalled from: <!-- anaconda? compiled by yourself ? --> \r\n\r\nFaiss compilation options: <!-- e.g. using MKL with compile flags ... -->\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [x] C++\r\n- [x] Python\r\n\r\n# Reproduction instructions\r\n\r\n<!-- Please provide specific and comprehensive instructions to reproduce the\r\ndescribed behavior. -->\r\n\r\n<!-- Please *do not* post screenshots of logs. They are not searchable. Copy/paste \r\nthe text or make a gist if the text is too bulky. --> \r\nHello,\r\n\r\nI am trying to find the cosine similarity with HNSW.\r\nBut the cosine similarity found to be incorrect below is the code and comparison of \"Flat\", \"HNSW\" & \"scipy\"\r\n```\r\nimport faiss\r\nemb1 = np.fromfile(\"emb1.raw\", dtype=np.float32)\r\nemb2 = np.fromfile(\"emb2.raw\", dtype=np.float32)\r\n```\r\nScipy code & result\r\n\r\n```\r\nfrom scipy import spatial\r\nresult = 1 - spatial.distance.cosine(emb1, emb2)\r\nprint('Cosine Similarity by scipy:{}'.format(result))\r\n```\r\nResult:\r\n`Cosine Similarity by scipy::0.991761326789856`\r\n\r\nIndexFlatL2/Flat code & result\r\n```\r\nxb = np.expand_dims(emb1,axis=0)\r\nxq = np.expand_dims(emb2,axis=0)\r\n\r\nindex = faiss.index_factory(128, \"Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex.ntotal\r\nfaiss.normalize_L2(xb)\r\nindex.add(xb)\r\nfaiss.normalize_L2(xq)\r\ndistance, index = index.search(xq, 1)\r\nprint('[FAISS] Cosine Similarity by Flat:{}'.format(distance))\r\n```\r\nResult:\r\n`[FAISS] Cosine Similarity by Flat:[[0.9917611]]`\r\n\r\nIndexHNSWFlat/HNSW32Flat code & result\r\n\r\n```\r\nxb = np.expand_dims(emb1,axis=0)\r\nxq = np.expand_dims(emb2,axis=0)\r\n\r\nindex = faiss.index_factory(128, \"HNSW32Flat\", faiss.METRIC_INNER_PRODUCT)\r\nindex.ntotal\r\nfaiss.normalize_L2(xb)\r\nindex.add(xb)\r\nfaiss.normalize_L2(xq)\r\ndistance, index = index.search(xq, 1)\r\nprint('[FAISS] Cosine Similarity by HNSW32Flat:{}'.format(distance))\r\n```\r\nResult:\r\n`[FAISS] Cosine Similarity by HNSW32Flat:[[0.01647742]]`\r\n\r\n**The results of Scipy & Flat are matching.\r\nWhereas the result is incorrect for HNSW.\r\nVerified the results using C++ & Python API's**",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2377/comments",
    "author": "Kapil-23",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2022-07-08T08:39:07Z",
        "body": "This is with an old version of Faiss, HNSW32Flat is not a valid index_factory string, it should be HNSW32,Flat. \r\nIn addition, the faiss.METRIC_INNER_PRODUCT is not taken into account, so it computes L2 distances. \r\nThis is fine, it just requires to do the translation to cosine similarity: \r\n\r\n2 - 2 * 0.9917611 = 0.0164778"
      },
      {
        "user": "Kapil-23",
        "created_at": "2022-07-08T10:33:53Z",
        "body": "@mdouze Thanks for your reply !!!\r\n\r\nYes the faiss python version that was installed was (1.5.3) after upgrading to 1.7.2 the issue resolved. \r\nUpdated the api \r\n`faiss.index_factory(128, \"HNSW32,Flat\", faiss.METRIC_INNER_PRODUCT)`\r\nCorrect Result : `0.9917613`\r\n\r\n**Note : Results are direct from API (Not used: 2 - 2 * 0.9917611 = 0.0164778)**\r\n\r\nWith respect to C++ I am facing the same issue of incorrect results (i.e getting Euclidean distance) instead of cosine similarity.\r\nI am using the following code.\r\nFaiss compiled from repo : latest version\r\n```\r\nfaiss::IndexHNSWFlat index(128,64);\r\nindex.metric_type = faiss::METRIC_INNER_PRODUCT;\r\n\r\nnormalize(xb)\r\nindex.add(xb)\r\nnormalize(xq)\r\n\r\nindex.search(...)\r\n```\r\nResult: `-0.0164774` \r\n"
      }
    ]
  },
  {
    "number": 2109,
    "title": "How can I update FAISS index that on disk ?",
    "created_at": "2021-11-14T22:37:18Z",
    "closed_at": "2021-11-17T20:01:25Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/2109",
    "body": "# Summary\r\nQ1\r\nI am trying to update my blocks on disk with respect to the index that is currently running on the script. I am capable to add new blocks but I have doubts on does FAISS saves all of the index to the new block or does it save only newly added data to the new block?  \r\nQ2\r\nIs there any way to save FAISS index (loaded from the disk /from blocks) after adding new data without creating new blocks?  \r\n\r\n\r\nFaiss version: <faiss-gpu=1.7.1>\r\n\r\nInstalled from: <pip> \r\n\r\n\r\nRunning on:\r\n- [ ] CPU\r\n- [x] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/2109/comments",
    "author": "abdullahbas",
    "comments": [
      {
        "user": "mdouze",
        "created_at": "2021-11-15T08:43:25Z",
        "body": "What are blocks? Are you referring to an OnDisk index built from several indexes?"
      },
      {
        "user": "abdullahbas",
        "created_at": "2021-11-15T09:21:36Z",
        "body": "Yes. We have data that does not fit in RAM. Hence, we created several indexes and then save them on the disk. Lastly merged them on one index. What should we do if we want to update our index on disk with newly added data? Should I save it as new? "
      },
      {
        "user": "mdouze",
        "created_at": "2021-11-17T10:42:13Z",
        "body": "It is not practical to add vectors to an OnDisk index. I would suggest that you keep an in-RAM index for the additional vectors and merge the results from the static OnDisk index and the in-RAM index."
      },
      {
        "user": "abdullahbas",
        "created_at": "2021-11-17T20:01:25Z",
        "body": "Ok, I will update my pipe like that. Thanks for the help and FAISS. "
      },
      {
        "user": "gustavz",
        "created_at": "2024-06-05T12:26:42Z",
        "body": ">  It is not practical to add vectors to an OnDisk index. I would suggest that you keep an in-RAM index for the additional vectors and merge the results from the static OnDisk index and the in-RAM index.\r\n\r\n@mdouze is this still the preferred approach or are there now supported ways to update OnDisk indexes?\r\nI assume it is building new OnDisk indexes and then merging?\r\n"
      }
    ]
  },
  {
    "number": 1119,
    "title": "Regarding the IndexFlatIP",
    "created_at": "2020-02-28T14:03:05Z",
    "closed_at": "2020-04-01T12:43:41Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1119",
    "body": "# Summary\r\n\r\nHi ,May I please know how can I get Cosine similarities not Cosine Distances while searching for similar documents. I've used IndexFlatIP as indexes,as it gives inner product.\r\n\r\n`distances, indices = index.search(query_vectors, k)\r\n`\r\n\r\nRunning on:\r\n- [x] CPU\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] C++\r\n- [x] Python\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1119/comments",
    "author": "MaheshChandrra",
    "comments": [
      {
        "user": "MaheshChandrra",
        "created_at": "2020-03-09T10:17:37Z",
        "body": "When I try to do a search I'm getting be below values:\r\n```\r\nresults = index.search(query_vector, 10)\r\nprint(results)#prints distances and similar ids\r\n\r\n(array([[267.5353 , 234.20415, 227.57852, 226.83115, 225.78455, 220.038  ,\r\n         218.0101 , 217.20752, 217.03021, 215.2745 , 215.01762, 214.11276,\r\n         213.06128, 212.98251, 212.56494, 210.98376, 210.3661 , 209.87708,\r\n         209.74539, 209.55539]], dtype=float32),\r\n array([[  3205711,   5535941,   5639730,   5572735,   5803736,   5819228,\r\n           5692490,   2974726,  11847732,   3104495,   2989770,   5845608,\r\n           3132981, 127403668, 127401208,   5728888,   5799607,   5799609,\r\n           5669756,   5579338]]))\r\n\r\n```\r\nCan someone please help me in understanding the distances which I received in the above list(distances,id's),how do I get Cosine similarity in the range or 0 to 1.\r\n\r\n"
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-03-09T13:11:06Z",
        "body": "You need to normalize your query vectors and the search space vectors. Something like this should do.\r\n\r\n```python\r\nnum_vectors = 1000000\r\nvector_dim = 1024\r\nvectors = np.random.rand(num_vectors, vector_dim)\r\n\r\n#sample index code\r\nquantizer = faiss.IndexFlatIP(1024)\r\nindex = faiss.IndexIVFFlat(quantizer, vector_dim, int(np.sqrt(num_vectors)), faiss.METRIC_INNER_PRODUCT)\r\ntrain_vectors = vectors[:int(num_vectors/2)].copy()\r\nfaiss.normalize_L2(train_vectors)\r\nindex.train(train_vectors)\r\nfaiss.normalize_L2(vectors)\r\nindex.add(vectors)\r\n#index creation done\r\n\r\n#let's search\r\nquery_vector = np.random.rand(10, 1024)\r\nfaiss.normalize_L2(query_vector)\r\nD, I = index.search(query_vector, 100)\r\n\r\nprint(D)\r\n```\r\n\r\nPlease note:- <b>faiss.normalize_L2() changes the input vector itself. No copy is created. Hence there it returns None.</b> In case you want to use the original vector you need to create a copy of it by yourself before calling faiss.normalize_L2().\r\nHope this helps."
      },
      {
        "user": "MaheshChandrra",
        "created_at": "2020-03-09T14:19:04Z",
        "body": "Hi EvilPort2,Thanks for  the quick response,may I please know why are we doing index.train for the first half corpus and then adding the complete corpus,is there any possible way of normalizing all the vectors at once without doing a train??\r\n\r\nThanks in advance."
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-03-10T05:33:24Z",
        "body": "I am not exactly sure as to what algorithm IndexIVFFlat uses underneath. But as far as I know, it uses something called KD tree for doing approximate search (@mdouze feel free to correct me). In a KD tree you first create some k clusters using the points in the corpus i.e the vector search space. The **training is done for this clustering** to happen. Now to search a vector you see which of the k clusters is nearest to the query vector by measuring the distance between the query and the cluster centroid. The cluster which is nearest to the query vector is now searched for the top nearest points hence reducing the search space. I have chosen k = square_root(number of vectors in the corpus). \r\nWhen your vector search space is huge and you don't have enough RAM you can take a part of the corpus and train. Ideally you should train with all the vectors and not half of them like I have shown. Hence the ideal code should be something like this.\r\n```python\r\nfaiss.normalize_L2(vectors)\r\nindex.train(vectors)\r\nindex.add(vectors)\r\n```"
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-03-10T07:09:02Z",
        "body": "Also, just a small note. Since you want cosine similarity, it will range from -1 to +1. "
      },
      {
        "user": "MaheshChandrra",
        "created_at": "2020-03-11T05:00:31Z",
        "body": "My bad, forgot about negative similarity,Thanks for addressing.\r\nOne last query does faiss work well in creating indexes on a corpus of 6M embeddings?\r\n\r\nThanks for the quick response and the fix @EvilPort2 , got it fixed."
      },
      {
        "user": "mdouze",
        "created_at": "2020-04-01T12:43:41Z",
        "body": "no activity, closing."
      },
      {
        "user": "ucasiggcas",
        "created_at": "2020-05-31T05:19:40Z",
        "body": "> You need to normalize your query vectors and the search space vectors. Something like this should do.\r\n> \r\n> ```python\r\n> num_vectors = 1000000\r\n> vector_dim = 1024\r\n> vectors = np.random.rand(num_vectors, vector_dim)\r\n> \r\n> #sample index code\r\n> quantizer = faiss.IndexFlatIP(1024)\r\n> index = faiss.IndexIVFFlat(quantizer, vector_dim, int(np.sqrt(num_vectors)), faiss.METRIC_INNER_PRODUCT)\r\n> train_vectors = vectors[:int(num_vectors/2)].copy()\r\n> faiss.normalize_L2(train_vectors)\r\n> index.train(train_vectors)\r\n> faiss.normalize_L2(vectors)\r\n> index.add(vectors)\r\n> #index creation done\r\n> \r\n> #let's search\r\n> query_vector = np.random.rand(10, 1024)\r\n> faiss.normalize_L2(query_vector)\r\n> D, I = index.search(query_vector, 100)\r\n> \r\n> print(D)\r\n> ```\r\n> \r\n> Please note:- faiss.normalize_L2() changes the input vector itself. No copy is created. Hence there it returns None. In case you want to use the original vector you need to create a copy of it by yourself before calling faiss.normalize_L2().\r\n> Hope this helps.\r\n\r\nhi,dear\r\nhave tried the codes,but\r\n```\r\nTraceback (most recent call last):\r\n  File \"faiss_method_.py\", line 266, in <module>\r\n    faiss.normalize_L2(train_vectors)\r\n  File \"/home/xulm1/anaconda3/lib/python3.7/site-packages/faiss/__init__.py\", line 674, in normalize_L2\r\n    fvec_renorm_L2(x.shape[1], x.shape[0], swig_ptr(x))\r\n  File \"/home/xulm1/anaconda3/lib/python3.7/site-packages/faiss/swigfaiss.py\", line 886, in fvec_renorm_L2\r\n    return _swigfaiss.fvec_renorm_L2(d, nx, x)\r\nTypeError: in method 'fvec_renorm_L2', argument 3 of type 'float *'\r\n```\r\nSO could you pls help me?\r\nthx\r\n"
      },
      {
        "user": "mdouze",
        "created_at": "2020-05-31T20:45:14Z",
        "body": "train_vectors should be of dtype float32"
      },
      {
        "user": "EvilPort2",
        "created_at": "2020-05-31T21:31:53Z",
        "body": "> My bad, forgot about negative similarity,Thanks for addressing.\r\n> One last query does faiss work well in creating indexes on a corpus of 6M embeddings?\r\n> \r\n> Thanks for the quick response and the fix @EvilPort2 , got it fixed.\r\n\r\nFaiss is awesome for searching in a huge number of vectors. I think the search time will vary on your vector size and also the type of index you use. I think for 6M vectors you can either go for IVFFlat or HNSW index type. Or you can take a mixture of the both (which I don't know how it works) called IVF65536_HNSW32."
      }
    ]
  },
  {
    "number": 1001,
    "title": "IndexIVFFlat on 2M embeddings from FaceNet is giving poor results",
    "created_at": "2019-10-23T16:53:21Z",
    "closed_at": "2019-10-23T21:31:22Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/facebookresearch/faiss/issues/1001",
    "body": "# Summary\r\nI am using embeddings computed from the popular FaceNet model. I have calculate about 2.5M embeddings in d=512 and am looking at performance of the `IndexIVFFlat` compared to the simple `Flat` index. Even with large `k` I see flat results in the recall\r\n\r\nRunning on:\r\n- [ ] GPU\r\n\r\nInterface: \r\n- [ ] Python\r\n\r\n# Reproduction instructions\r\n```\r\nxb = np.ascontiguousarray(X[::2][:2*1000*1000])\r\nxq = np.ascontiguousarray(X[1::2][:10*1000])\r\nd = xq.shape[1]\r\n\r\n# compute gt\r\nflat_index = faiss.index_factory(d, \"Flat\")\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.index_cpu_to_gpu(res, 0, flat_index, None)\r\nflat_index.train(xb)\r\nflat_index.add(xb)\r\nD, gt = flat_index.search(xq, k)\r\n\r\n# try an approximate method\r\nindex = faiss.index_factory(d, \"IVF<n_centroids>,Flat\")\r\nres = faiss.StandardGpuResources()\r\nindex = faiss.index_cpu_to_gpu(res, 0, index, None)\r\nindex.train(xb)\r\nindex.add(xb)\r\n\r\ndef evaluate(index, xq, gt, k):\r\n    nq = xq.shape[0]\r\n    t0 = time.time()\r\n    D, I = index.search(xq, k)  # noqa: E741\r\n    t1 = time.time()\r\n    recalls = {}\r\n    i = 1\r\n    while i <= k:\r\n        recalls[i] = (I[:, :i] == gt[:, :1]).sum() / float(nq)\r\n        i *= 10\r\n\r\n    return (t1 - t0) * 1000.0 / nq, recalls\r\n\r\nevaluate(flat_index, xq, gt, 1000)\r\n>>\r\n(2.1849388122558593, \r\n {1: 0.99850000000000005, \r\n  10: 1.0, \r\n  100: 1.0, \r\n  1000: 1.0})\r\n\r\nevaluate(index, xq, gt, 1000)\r\n\r\n>>\r\n(0.038869810104370114,\r\n {1: 0.35210000000000002,\r\n  10: 0.35289999999999999,\r\n  100: 0.35289999999999999,\r\n  1000: 0.35299999999999998})\r\n```\r\nNotice how the recall is not increasing as k increases.\r\n\r\nI have tried many ,<n_centroids>, between  4096 to 20000 and I do not see any improvement. \r\n\r\n### Questions:\r\n1. Is it possible that the data distribution is not conducive to this method? \r\n\r\n2. Am I possibly splitting my query and training set incorrectly?\r\n",
    "comments_url": "https://api.github.com/repos/facebookresearch/faiss/issues/1001/comments",
    "author": "ljstrnadiii",
    "comments": [
      {
        "user": "wickedfoo",
        "created_at": "2019-10-23T20:12:05Z",
        "body": "You are only looking in a single IVF list, as `nprobe` is by default 1.\r\n\r\nIncrease `nprobe` rather than `k`.\r\n"
      },
      {
        "user": "ljstrnadiii",
        "created_at": "2019-10-23T21:31:22Z",
        "body": "of course, merci beaucoup!\r\n\r\nI did want to ask about the typical strategy to split your datasets. In some examples I have noticed that you build an xb, xt, xq dataset: one for training, one for adding and the last for query (equivalent to a test set). I am not sure what is the typical split for this field. Do you usually train on xt, add [xt, xb] (or does xb already contain xt?) to the index, and search with xt? It is hard to tell how you have constructed your memmap files. What proportion of the whole dataset is xq, xt and xb typically?\r\n\r\nthanks for such a killer project!"
      }
    ]
  }
]