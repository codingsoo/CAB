[
  {
    "number": 1919,
    "title": "Add time frames",
    "created_at": "2024-03-04T13:30:38Z",
    "closed_at": "2024-03-10T14:01:53Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1919",
    "body": "Tell me, in the example it is indicated\r\n./main -m models/ggml-large-v1.bin -l ru --no-timestamps -f ~/output.wav -of output -otxt\r\nThis is to remove the time frames, but how can we make sure they still exist? If I delete --no-timestamps, then they are still not there? How can I make them exist?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1919/comments",
    "author": "DittmerOk",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2024-03-05T16:04:24Z",
        "body": "In the function located in main.cpp, you'll notice that when we direct the output to a text file, we only include the textual content, excluding any special tokens. If you wish to include special tokens in the text file output, please make use of the additional option `--print-special`"
      },
      {
        "user": "DittmerOk",
        "created_at": "2024-03-10T14:01:53Z",
        "body": "yes, thanks. it's work"
      }
    ]
  },
  {
    "number": 1684,
    "title": "Can't compile with cuBLAS with non-standard CUDA setup on linux",
    "created_at": "2023-12-23T15:24:00Z",
    "closed_at": "2023-12-27T20:36:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1684",
    "body": "I've installed CUDA packages from the Negativo17 repo on a Fedora 38 system (because I'm using their nvidia-driver on this Lenovo P50 laptop with its discrete Nvidia GPU, which used to be tricky to setup when I first got this machine, and this was the driver that worked), and I'm having problems compiling whisper.cpp with cuBLAS enabled. I get a barrage of weird errors like \r\n\r\n```\r\n/usr/include/cuda/std/detail/libcxx/include/__cuda/atomic_prelude.h(29): error: identifier \"ATOMIC_LLONG_LOCK_FREE\" is undefined\r\n      static_assert(ATOMIC_LLONG_LOCK_FREE == 2, \"\");\r\n                    ^\r\n\r\n/usr/include/cuda/std/detail/libcxx/include/__cuda/atomic_prelude.h(30): error: identifier \"ATOMIC_POINTER_LOCK_FREE\" is undefined\r\n      static_assert(ATOMIC_POINTER_LOCK_FREE == 2, \"\");\r\n                    ^\r\n\r\nggml-cuda.cu(6519): error: namespace \"std\" has no member \"atomic_flag\"\r\n      std::atomic_flag& lock;\r\n           ^\r\n\r\nggml-cuda.cu(6520): error: namespace \"std\" has no member \"atomic_flag\"\r\n      scoped_spin_lock(std::atomic_flag& lock) : lock(lock) {\r\n```\r\n\r\nThe problem seems to be that these packages install the CUDA headers and libraries under /usr/ instead of /usr/local/cuda (like the official cuda-toolkit from Nvidia does). Apart from whisper.cpp (and llama.cpp for that matter) the setup seems to be functional, e.g. I could compile and run the examples from cuda-samples.\r\n\r\nDoes anyone have any ideas or experience on how to fix this?",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1684/comments",
    "author": "pjuhasz",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-12-23T16:14:31Z",
        "body": "> The problem seems to be that these packages install the CUDA headers and libraries under /usr/ instead of /usr/local/cuda (like the official cuda-toolkit from Nvidia does). \r\n\r\nTry `CMake` and see if that helps.\r\n\r\n`cmake -S [source] -B [build] -DWHISPER_CUBLAS=1`\r\n\r\n`cmake --build [build] --config release`\r\n"
      },
      {
        "user": "pjuhasz",
        "created_at": "2023-12-23T17:49:28Z",
        "body": "It didn't help:\r\n```\r\nCMake Error at /usr/share/cmake/Modules/CMakeDetermineCUDACompiler.cmake:227 (message):\r\n  Couldn't find CUDA library root.\r\n```\r\n"
      },
      {
        "user": "pjuhasz",
        "created_at": "2023-12-27T20:36:32Z",
        "body": "After reinstalling the machine with the official Nvidia driver packages, the errors went away and I could compile the program. So, PEBKAC, I guess."
      },
      {
        "user": "stevestorey",
        "created_at": "2024-01-15T15:46:20Z",
        "body": "Just for anyone else who gets here prior to something being changed in the negativo17 packaging - the current workaround is to comment the line `INCLUDES += \"-I/usr/include/cuda\"` from the `/usr/bin/nvcc.profile` file after which compilation works."
      }
    ]
  },
  {
    "number": 1301,
    "title": "still low performances with CoreML model (using ANE) compared to Vosk (Kaldi) using CPU",
    "created_at": "2023-09-17T09:19:15Z",
    "closed_at": "2023-09-18T12:22:21Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/1301",
    "body": "Hi,\r\n\r\nusing the large CoreML encoder  provided by huggingface I still have performances very low compared to Vosk with Kaldi and I don't get why.\r\n\r\nwhen I run it:\r\n\r\nwhisper_init_state: Core ML model loaded\r\nsystem_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | COREML = 1 | OPENVINO = 0 | \r\n\r\nso everything is finely set and in theory I should use ANE that should be really performant.\r\n\r\nfor converting 3h audio (16khz and 1channel) it took 1h, while with Vosk using Kaldi (I use only the CPU), same quality but it took 11min, how is it possible? Am I missing something?\r\n\r\nThank you\r\nLuca",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/1301/comments",
    "author": "xcottos",
    "comments": [
      {
        "user": "bobqianic",
        "created_at": "2023-09-17T10:08:53Z",
        "body": ">  I still have performances very low compared to Vosk with Kaldi and I don't get why.\r\n\r\nCompared to the Whisper large model, which is 2.9 GiB, the Vosk and Kaldi models are relatively small. In addition, the `KAIDI GigaSpeech ASR` models were trained on datasets that are an order of magnitude smaller than `Whisper` (10,000 hours vs 680,000 hours). If you're aiming for faster transcription speeds, you might want to consider using `Metal` instead of `CoreML` or other smaller models (eg. small, medium).\r\n\r\n> In this work we close that gap, scaling weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data. We call our approach Whisper."
      },
      {
        "user": "xcottos",
        "created_at": "2023-09-17T11:30:31Z",
        "body": "Thank you bobqianic,\r\n\r\nI can see, indeed, that the transcription with whisper is much accurate (writes numbers as numbers, the punctuation is much better etc) so I assume the better accuracy is also the result of a larger training set/model.\r\n\r\nI am studying the options I have and I would like to push whisper at its max performances on my M1 Max (64GB Ram and 32 cores GPU). Now I started studying the acceleration with my Mac and as far as I understood ANE should be even more performant compared to Metal but it's still only theory for me that's why I'm experimenting.\r\n\r\nFor using ANE the only way (as far as I could get) is using CoreML and apparently I am using it now (so in theory I am using the NPU)\r\n\r\nIs there already the support for Metal with whisper to test its performances on my GPU?\r\n\r\nThank you\r\nLuca"
      },
      {
        "user": "bobqianic",
        "created_at": "2023-09-17T11:41:19Z",
        "body": "> Is there already the support for Metal with whisper to test its performances on my GPU?\r\n\r\nYes. See #1270"
      },
      {
        "user": "xcottos",
        "created_at": "2023-09-18T12:20:52Z",
        "body": "well... great improvement it took 20 min now with Metal  :)\r\n\r\nlarge model and 3h audio in Italian\r\n\r\nI still don't get why ANE is less performant...\r\n\r\nThanks for your suggestion bobqianic, really appreciate it\r\n\r\nLuca"
      }
    ]
  },
  {
    "number": 578,
    "title": "What is the audio length in the stream?",
    "created_at": "2023-03-07T00:34:03Z",
    "closed_at": "2023-03-07T21:53:32Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/ggerganov/whisper.cpp/issues/578",
    "body": "```\r\n./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\r\n```\r\nin the --help it says\r\n>            --length N      [10000  ] audio length in milliseconds\r\n\r\nI thought is the max length of a segment?  but it is not that\r\nthe length to run the stream? (I know it isn't that)\r\nis it the time allowed to process it?\r\n",
    "comments_url": "https://api.github.com/repos/ggerganov/whisper.cpp/issues/578/comments",
    "author": "G2G2G2G",
    "comments": [
      {
        "user": "ggerganov",
        "created_at": "2023-03-07T19:35:02Z",
        "body": "It's the length of each segment.\r\nEach transcribed line by the `stream` example is a segment and it corresponds to `--length` ms of audio."
      },
      {
        "user": "G2G2G2G",
        "created_at": "2023-03-07T21:53:23Z",
        "body": "ohh ok the segment of voice that turns into 1 line of text?"
      }
    ]
  }
]