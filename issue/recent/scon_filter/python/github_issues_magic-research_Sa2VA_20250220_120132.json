[
  {
    "number": 16,
    "title": "[SEG] Hidden State usage",
    "created_at": "2025-02-07T17:01:36Z",
    "closed_at": "2025-02-10T07:08:58Z",
    "labels": [],
    "url": "https://github.com/magic-research/Sa2VA/issues/16",
    "body": "Thank you for the great work. As I read the code, if I understand correctly, it seems that If the model predicts the next token is [SEG], it will take the final layer hidden state and feed through an MLP connector and feed it to SAM2 for decoding.\n\nHowever, if the language model predicts [SEG] as the next token, it means the embedding has the highest (or very high if sampling is used) similarity with [SEG] embedding. Doesn't that mean this embedding fed to SAM2 is always very close to [SEG] no matter what the rest of the sentence/context is? How does the model utilize the context with very similar embeddings for segmenting different objects?\n\nPlease let me know if I misunderstood. Thank you!",
    "comments_url": "https://api.github.com/repos/magic-research/Sa2VA/issues/16/comments",
    "author": "seermer",
    "comments": [
      {
        "user": "HarborYuan",
        "created_at": "2025-02-10T06:31:53Z",
        "body": "Your understanding is correct. But I would like to point out that [SEG]'s embedding has two different MLP/projection layers, outputing to the classifier or visual prompt respectively. When considering an embedding, for text classifiers, they may be similar, but for prompt encoder, these differences are used to segment different objects."
      },
      {
        "user": "seermer",
        "created_at": "2025-02-10T07:08:58Z",
        "body": "Thank you so much for your help! "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how the same [SEG] token embedding can produce different segmentation results based on context",
      "Clarification of distinct processing pathways for the [SEG] token's hidden state",
      "Description of how semantic differences in embeddings are preserved/amplified for segmentation"
    ]
  }
]