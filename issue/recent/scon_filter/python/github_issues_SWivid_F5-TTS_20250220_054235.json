[
  {
    "number": 792,
    "title": "FileNotFoundError: [WinError 2]",
    "created_at": "2025-02-17T06:21:27Z",
    "closed_at": "2025-02-17T17:13:51Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/792",
    "body": "### Checks\n\n- [x] This template is only for usage issues encountered.\n- [x] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [x] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [x] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\nwhen i click synthesize i get this error :(\n\n\n\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n### Steps to Reproduce\n\ni did everything the documentation said exactly and everything was going fine until i actually tried to run the synthesize button\n\n### \u2714\ufe0f Expected Behavior\n\nWorking, generating audio\n\n### \u274c Actual Behavior\n\nError, nothing happens",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/792/comments",
    "author": "SulaimanBasal",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:23:18Z",
        "body": "> FileNotFoundError: [WinError 2] The system cannot find the file specified\n\nmake sure you have fully upload the audio"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T06:26:20Z",
        "body": "@SWivid i did, i can even listen to it in the ui... (thanks for the fast reply)\n"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:29:37Z",
        "body": "you could probably provide more details as suggest by the issue template.\notherwise we could hardly help cuz no info"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T06:36:24Z",
        "body": "@SWivid yea i'm sorry about that, i'm a total noob at this... \n\nthis is the full message when i upload, will that help?\n\n\nC:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\nTraceback (most recent call last):\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n    response = await route_utils.call_process_api(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 2051, in process_api\n    result = await self.call_function(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\blocks.py\", line 1598, in call_function\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2461, in run_sync_in_worker_thread\n    return await future\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 962, in run\n    result = context.run(func, *args)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\gradio\\utils.py\", line 883, in wrapper\n    response = f(*args, **kwargs)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\infer_gradio.py\", line 241, in basic_tts\n    audio_out, spectrogram_path, ref_text_out = infer(\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\infer_gradio.py\", line 131, in infer\n    ref_audio, ref_text = preprocess_ref_audio_text(ref_audio_orig, ref_text, show_info=show_info)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\f5_tts\\infer\\utils_infer.py\", line 294, in preprocess_ref_audio_text\n    aseg = AudioSegment.from_file(ref_audio_orig)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\audio_segment.py\", line 728, in from_file\n    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py\", line 274, in mediainfo_json\n    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\subprocess.py\", line 971, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\SUL\\.conda\\envs\\f5\\lib\\subprocess.py\", line 1456, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n\n\n\n\n\n---\n\n@SWivid also when starting the gradio app  i get this before given the local link:\nStarting app...\nINFO: Could not find files for the given pattern(s).\n\nwould that be what's causing the issue?"
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-17T06:52:06Z",
        "body": "> C:\\Users\\SUL.conda\\envs\\f5\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n\nmay try #739 \ninstall ffmpeg"
      },
      {
        "user": "SulaimanBasal",
        "created_at": "2025-02-17T17:13:51Z",
        "body": "@SWivid Yep!!! that was the issue hahaha, it's weird that it didn't tell me that it was missing as an error but anyway thanks for the fast reply, installing the ffmpeg and the ffmpeg python fixed it and now works like a charm! thank you again, and great job on the tool, much appreciate it!"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the missing dependency causing the FileNotFoundError during audio processing",
      "Explains why the error occurs despite valid UI file uploads",
      "Ensures audio processing tools are properly accessible in the execution environment"
    ]
  },
  {
    "number": 772,
    "title": "Improve prepare_csv_wavs.py",
    "created_at": "2025-02-07T20:13:05Z",
    "closed_at": "2025-02-09T06:34:11Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/772",
    "body": "This PR enhances the dataset preparation pipeline with major performance and reliability improvements. \r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/772/comments",
    "author": "hcsolakoglu",
    "comments": [
      {
        "user": "hcsolakoglu",
        "created_at": "2025-02-08T04:35:46Z",
        "body": "I actually wrote this for my own use. Goal here is to efficiently prepare large datasets for training. For example, in a dataset with 116,180 samples, the old version of the code took 4 hours, but now it has been reduced to 1 hour. If you're interested, I can finish adding it for everyone to use. @SWivid "
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-08T06:06:48Z",
        "body": "Yes, if you are willing to share"
      },
      {
        "user": "justinjohn0306",
        "created_at": "2025-02-08T06:57:45Z",
        "body": "> I actually wrote this for my own use. Goal here is to efficiently prepare large datasets for training. For example, in a dataset with 116,180 samples, the old version of the code took 4 hours, but now it has been reduced to 1 hour. If you're interested, I can finish adding it for everyone to use. @SWivid\r\n\r\nI really love your PR. Please share the final thing, it would be much appreciated. Thank you :)"
      },
      {
        "user": "atlonxp",
        "created_at": "2025-02-08T19:01:24Z",
        "body": "looking forward to the merging"
      },
      {
        "user": "hcsolakoglu",
        "created_at": "2025-02-09T16:04:34Z",
        "body": "Since I was working, I couldn't return, but I see that @SWivid merged it for me. I wish everyone fast and successful training runs! @justinjohn0306 @atlonxp"
      }
    ],
    "satisfaction_conditions": [
      "Significant reduction in processing time for large datasets",
      "Maintains or improves reliability for dataset preparation",
      "Generalizable solution usable by multiple users"
    ]
  },
  {
    "number": 676,
    "title": "Bug Fix: Parsing Argument --finetune always True",
    "created_at": "2024-12-28T11:19:46Z",
    "closed_at": "2024-12-29T13:29:50Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/676",
    "body": "I use f5-tts_finetune-gradio and found out that the finetune checkbox will always send finetune=true to the argument, eventhought the checkbox are off.\r\n\r\nAfter some investigation turns out using type=bool on ArgumentParser always return true because in python:\r\n`bool(\"False\")` are `True`.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/676/comments",
    "author": "hndrbrm",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-28T11:36:47Z",
        "body": "@hndrbrm seems bugs in first commit\r\n\r\nfeel free to test if works, if so will merge\r\nthanks again~"
      },
      {
        "user": "hndrbrm",
        "created_at": "2024-12-30T16:22:48Z",
        "body": "Its working, thanks."
      }
    ],
    "satisfaction_conditions": [
      "Ensures boolean arguments from UI checkboxes are parsed correctly",
      "Handles 'off' state mapping to False in argument values",
      "Maintains compatibility with existing argument parsing logic"
    ]
  },
  {
    "number": 630,
    "title": "Checkpoint saving differences",
    "created_at": "2024-12-15T09:30:49Z",
    "closed_at": "2024-12-17T02:16:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/630",
    "body": "### Checks\r\n\r\n- [X] This template is only for question, not feature requests or bug reports.\r\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\r\n- [X] I have searched for existing issues, including closed ones, no similar questions.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Question details\r\n\r\nWhen having `grad_accumulation_steps` set to a different value than `1`, the checkpoint saving is a bit unexpected:\r\n\r\nIn `trainer.py` for setting `save_per_updates`:\r\n\r\n```python\r\nif global_step % (self.save_per_updates * self.grad_accumulation_steps) == 0:\r\n    self.save_checkpoint(global_step)\r\n```\r\n\r\nfor setting `last_per_steps`:\r\n\r\n```python\r\nif global_step % self.last_per_steps == 0:\r\n    self.save_checkpoint(global_step, last=True)\r\n```\r\n\r\nConsequently, for my global batch-size of `19200*8` the value of `save_per_updates` needs to be divided by `8` to be comparable to the setting of `last_per_steps` and the overall variable `global_step` shown via `tqdm`.\r\n\r\nIs this intended ?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/630/comments",
    "author": "lumpidu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-15T10:11:43Z",
        "body": "yes, intended\r\nstep and update are different stuffs"
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-16T23:46:34Z",
        "body": "Similar subject as in #632, closing"
      }
    ],
    "satisfaction_conditions": [
      "Clarification of the intended relationship between gradient accumulation steps and checkpoint saving logic",
      "Explanation of the conceptual distinction between 'steps' and 'updates' in the training process",
      "Confirmation of whether the observed parameter scaling requirement is intentional design"
    ]
  },
  {
    "number": 594,
    "title": "Does the vocabulary size impact the amount of data required for training?",
    "created_at": "2024-12-06T04:19:55Z",
    "closed_at": "2024-12-06T16:27:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/594",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI've been experimenting with this recently, there seems to be some great potential here.  It does clearly take a lot of data to work well.  I was wondering then, if it would train more quickly or with less data if the vocab.txt file is smaller?  Has anyone tried this out?  I realize it would reduce the multi-language capability of the resultant model.  ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/594/comments",
    "author": "DrBrule",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T04:58:31Z",
        "body": "no idea,\r\nthought the hard part is to learn speech-text alignment but word embed seems no conflict with that\r\nand if we use big vocab size but just use part of it, the gradient will just go for the used ones, thus the only bad thing is wasted params and sparse weights (low utilization rate)?"
      },
      {
        "user": "DrBrule",
        "created_at": "2024-12-06T16:27:25Z",
        "body": "That's clear, thank you!  Seems like it wouldn't make much of a difference in practice. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how vocabulary size relates to training data requirements and computational efficiency",
      "Clarification of whether unused vocabulary entries impact model training dynamics",
      "Analysis of speech-text alignment challenges vs vocabulary-related factors in training difficulty"
    ]
  },
  {
    "number": 545,
    "title": "issue training F5tts small model after new training code",
    "created_at": "2024-11-28T04:22:22Z",
    "closed_at": "2024-11-28T04:36:55Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/545",
    "body": "### Checks\r\n\r\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\r\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\r\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Environment Details\r\n\r\npython 3.10.12 (wsl)\r\n\r\n### Steps to Reproduce\r\n\r\n1. initialize training using the new config-based training code following the readme.\r\nIn my case I'm trying to train the f5tts small model and have modified the training config to point to my dataset.\r\nI receive the following error:\r\n\r\n```\r\nError executing job with overrides: []                                                                                  \r\nTraceback (most recent call last):                                                                                      \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/train/train.py\", line 35, in main                                                \r\n    model = CFM(                                                                                                        \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/model/cfm.py\", line 55, in __init__                                              \r\n    self.mel_spec = default(mel_spec_module, MelSpec(**mel_spec_kwargs))                                                \r\nTypeError: MelSpec.__init__() got an unexpected keyword argument 'is_local_vocoder'                                     \r\n```\r\nAnother issue I'm experiencing when using the new prepare_ljspeech script and trying to train using that dataset with char is the following, let me know if I should use a different tokenizer in this senario though.\r\n\r\n```\r\nTraceback (most recent call last):                                                                                      \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/train/train.py\", line 26, in main                                                \r\n    vocab_char_map, vocab_size = get_tokenizer(tokenizer_path, tokenizer)                                               \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/model/utils.py\", line 118, in get_tokenizer                                      \r\n    assert vocab_char_map[\" \"] == 0, \"make sure space is of idx 0 in vocab.txt, cuz 0 is used for unknown char\"         \r\nAssertionError: make sure space is of idx 0 in vocab.txt, cuz 0 is used for unknown char                                \r\n```\r\n\r\nThanks for all of your work on these models.\r\n\r\n\r\n### \u2714\ufe0f Expected Behavior\r\n\r\nmodel training should proceed.\r\n\r\n### \u274c Actual Behavior\r\n\r\ntracebacks as above, which I assume are related to the new training code.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/545/comments",
    "author": "danielw97",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-28T04:37:54Z",
        "body": "Hi @danielw97\r\nThanks for reporting the bug!\r\nCheck if it works now"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T04:45:21Z",
        "body": "Thanks a lot for the quick fix!\r\nThat fixed my first issue with training, however am still having the second one with the prepare_ljspeech vocab.txt that I also mentioned.\r\nIf it's easier I can open another issue for that.\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-28T04:53:23Z",
        "body": "It would be fine with this issue, we will check for the LJSpeech vocab"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T04:55:01Z",
        "body": "Thanks, in my tests I'm using ljspeech 1.1\r\nIf there's any more info you need, let me know."
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-28T05:22:00Z",
        "body": "Hi @danielw97 \r\nthe second one is because the ljspeech has /n at end, which is not expected to include in the vocab\r\nfixed with adding strip() during processing\r\n"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T05:24:25Z",
        "body": "Thanks for the quick resolution, and explanation of my second issue."
      }
    ],
    "satisfaction_conditions": [
      "Resolve unexpected keyword argument errors in MelSpec initialization",
      "Ensure proper formatting of vocab.txt for tokenizer compatibility",
      "Maintain compatibility with LJSpeech dataset processing",
      "Provide clear error resolution guidance for config-based training flow"
    ]
  },
  {
    "number": 471,
    "title": "Moving storage, but f5 still load from old location",
    "created_at": "2024-11-15T23:48:34Z",
    "closed_at": "2024-11-16T02:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/471",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI cloned my f5 installation to a new drive (E:\\ -> Y:\\) and ran f5-tts_finetune-gradio from the new location.\r\nSeems the path for my data checkpoints is still mapped to E:\\. I need help to re-map, so I can delete the data on the source drive.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/471/comments",
    "author": "AlpacaManAlpha",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-16T01:36:20Z",
        "body": "`pip uninstall f5-tts`\r\nDo `pip install -e .` at new path."
      },
      {
        "user": "AlpacaManAlpha",
        "created_at": "2024-11-16T02:00:54Z",
        "body": "This worked, thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Ensures the application references the new installation path for data storage",
      "Provides a method to update internal path references without manual file editing",
      "Allows safe removal of original data from the old drive"
    ]
  },
  {
    "number": 459,
    "title": "load finetune data",
    "created_at": "2024-11-13T09:26:17Z",
    "closed_at": "2024-11-13T11:59:53Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/459",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\ntrain on 4 a800\n\n### Steps to Reproduce\n\nI encountered an issue when trying to load my fine-tuned checkpoint. During the load_state_dict() call, I received the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 151, in <module>\r\n    infer_tts()\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 72, in infer_tts\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 216, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, dtype=dtype, use_ema=use_ema)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 168, in load_checkpoint\r\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\r\n  File \"/remote-home1/ycyuan/conda/anaconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n    Missing key(s) in state_dict: \"transformer.time_embed.time_mlp.0.weight\", \"transformer.time_embed.time_mlp.0.bias\", \"transformer.time_embed.time_mlp.2.weight\", \"transformer.time_embed.time_mlp.2.bias\", \"transformer.text_embed.text_embed.weight\", \"transformer.text_embed.text_blocks.0.dwconv.weight\", \"transformer.text_embed.text_blocks.0.dwconv.bias\", \"transformer.text_embed.text_blocks.0.norm.weight\", \"transformer.text_embed.text_blocks.0.norm.bias\", \"transformer.text_embed.text_blocks.0.pwconv1.weight\", \"transformer.text_embed.text_blocks.0.pwconv1.bias\", \"transformer.text_embed.text_blocks.0.grn.gamma\", \"transformer.text_embed.text_blocks.0.grn.beta\", \"transformer.text_embed.text_blocks.0.pwconv2.weight\", \"transformer.text_embed.text_blocks.0.pwconv2.bias\", \"transformer.text_embed.text_blocks.1.dwconv.weight\", \"transformer.text_embed.text_blocks.1.dwconv.bias\", \"transformer.text_embed.text_blocks.1.norm.weight\", \"transformer.text_embed.text_blocks.1.norm.bias\"...\r\n    Unexpected key(s) in state_dict: \"module.transformer.time_embed.time_mlp.0.weight\", \"module.transformer.time_embed.time_mlp.0.bias\", \"module.transformer.time_embed.time_mlp.2.weight\", \"module.transformer.time_embed.time_mlp.2.bias\", \"module.transformer.text_embed.text_embed.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.bias\", \"module.transformer.text_embed.text_blocks.0.norm.weight\", \"module.transformer.text_embed.text_blocks.0.norm.bias\", \"module.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"module.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"module.transformer.text_embed.text_blocks.0.grn.gamma\", \"module.transformer.text_embed.text_blocks.0.grn.beta\",...\r\n```\r\nHowever, when I print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected:\r\n```\r\nOrderedDict([('transformer.time_embed.time_mlp.0.weight', tensor([[-0.0007, -0.0009, -0.0007,  ..., -0.0355,  0.0119,  0.0061], ...], device='cuda:3')),\r\n            ('transformer.time_embed.time_mlp.0.bias', tensor([ 0.0139,  0.0171,  0.0399,  ..., -0.0508,  0.0258, -0.0191], device='cuda:3'))]),....\r\n```\r\nHowever, when I attempt to load the checkpoint without any wrapping, the model loading fails due to the error above. If I wrap the model with DataParallel before loading the checkpoint (like this: model = torch.nn.DataParallel(model)), the loading works fine, but this leads to subsequent errors during inference.\r\n\r\nAdditionally, I have tried to remove the module prefix using the following function:\r\n```\r\ndef remove_module_prefix(state_dict):\r\n    new_state_dict = {}\r\n    for k, v in state_dict.items():\r\n        # Remove the 'module.' prefix\r\n        print(k)\r\n        new_k = k.replace('module.', '') if 'module.' in k else k\r\n        new_state_dict[new_k] = v\r\n    return new_state_dict\r\n```\r\nHowever, this approach does not work because, when I print the keys, they don't actually contain the module. prefix. Despite this, when loading the checkpoint, I still encounter a mismatch error regarding the module. prefix.\r\n\r\nI have not modified the checkpoint saving function. Here\u2019s the relevant code that I use to save checkpoints:\r\n```\r\ndef save_checkpoint(self, step, last=False):\r\n    self.accelerator.wait_for_everyone()\r\n    if self.is_main:\r\n        checkpoint = dict(\r\n            model_state_dict=self.accelerator.unwrap_model(self.model).state_dict(),\r\n            optimizer_state_dict=self.accelerator.unwrap_model(self.optimizer).state_dict(),\r\n            ema_model_state_dict=self.ema_model.state_dict(),\r\n            scheduler_state_dict=self.scheduler.state_dict(),\r\n            step=step,\r\n        )\r\n        if not os.path.exists(self.checkpoint_path):\r\n            os.makedirs(self.checkpoint_path)\r\n        rank = self.accelerator.process_index\r\n\r\n        if rank == 0:\r\n            if last:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_last.pt\")\r\n                print(f\"Saved last checkpoint at step {step}\")\r\n            else:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_{step}.pt\")\r\n```\r\n\r\nIs there a way to handle the mismatch of the module. prefix in the checkpoint file?\n\n### \u2714\ufe0f Expected Behavior\n\n_No response_\n\n### \u274c Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/459/comments",
    "author": "southwindyong",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-13T09:33:23Z",
        "body": "maybe you could provide the additional `File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\"` for more info to figure out the issue."
      },
      {
        "user": "southwindyong",
        "created_at": "2024-11-13T10:31:33Z",
        "body": "This is the file of ``/remote-home1/ycyuan/TTS/F5-TTS/inference.py``\r\n```\r\nimport os\r\nimport random\r\nimport json\r\nimport codecs\r\nimport numpy as np\r\nimport soundfile as sf\r\nimport tomli\r\nfrom pathlib import Path\r\nfrom f5_tts.infer.utils_infer import (\r\n    infer_process,\r\n    load_model,\r\n    load_vocoder,\r\n    preprocess_ref_audio_text,\r\n    remove_silence_for_generated_wav,\r\n)\r\nfrom f5_tts.model import DiT, UNetT\r\nimport re\r\n\r\ndef infer_tts():\r\n    # Directly specify the variables without argparse\r\n    print(\"\u5f00\u59cb TTS \u63a8\u7406\u8fc7\u7a0b...\")\r\n\r\n    config_path = Path(\"f5_tts\") / \"infer/examples/basic\" / \"basic.toml\"  \r\n    model = \"F5-TTS\"  # Choose \"F5-TTS\" or \"E2-TTS\"\r\n    ckpt_file = \"/remote-home1/ycyuan/TTS/F5-TTS/ckpts/F5TTS_Base/libritts_25s_1/model_last.pt\"  # Provide the path to checkpoint if necessary\r\n    vocab_file = \"\"  # Provide the vocab file path if necessary\r\n    text_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_text_2-6\"  # Text directory\r\n    audio_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_prompt_2_10s\"  # Audio directory\r\n    output_dir = \"./generate_data/libritts_25s_1\"  # Path to output folder\r\n    remove_silence = False  # Whether to remove silence from generated audio\r\n    vocoder_name = \"vocos\"  # Choose between \"vocos\" or \"bigvgan\"\r\n    load_vocoder_from_local = False  # Whether to load vocoder from local path\r\n    speed = 1.0  # Adjust audio generation speed (default is 1.0)\r\n\r\n    # Load configuration from the specified TOML file\r\n    print(f\"\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 {config_path}...\")\r\n    config = tomli.load(open(config_path, \"rb\"))\r\n    \r\n    # Use provided values or defaults from the config file\r\n    output_dir = output_dir if output_dir else config[\"output_dir\"]\r\n    model = model if model else config[\"model\"]\r\n    ckpt_file = ckpt_file if ckpt_file else \"\"\r\n    vocab_file = vocab_file if vocab_file else \"\"\r\n    remove_silence = remove_silence if remove_silence else config[\"remove_silence\"]\r\n    speed = speed\r\n    wave_path = Path(output_dir) / \"infer_cli_out.wav\"\r\n    \r\n    # Set vocoder local path based on the selected vocoder\r\n    print(f\"\u9009\u62e9\u7684 vocoder \u6a21\u578b: {vocoder_name}...\")\r\n    if vocoder_name == \"vocos\":\r\n        vocoder_local_path = \"../checkpoints/vocos-mel-24khz\"\r\n    elif vocoder_name == \"bigvgan\":\r\n        vocoder_local_path = \"../checkpoints/bigvgan_v2_24khz_100band_256x\"\r\n    \r\n    # Load the vocoder model\r\n    print(\"\u52a0\u8f7d vocoder \u6a21\u578b...\")\r\n    vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=load_vocoder_from_local, local_path=vocoder_local_path)\r\n\r\n    # Load the TTS model\r\n    print(f\"\u52a0\u8f7d TTS \u6a21\u578b: {model}...\")\r\n    if model == \"F5-TTS\":\r\n        model_cls = DiT\r\n        model_cfg = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"F5-TTS\"\r\n            exp_name = \"F5TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    elif model == \"E2-TTS\":\r\n        model_cls = UNetT\r\n        model_cfg = dict(dim=1024, depth=24, heads=16, ff_mult=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"E2-TTS\"\r\n            exp_name = \"E2TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    \r\n    # Initialize the model\r\n    print(f\"\u4f7f\u7528 {model} \u6a21\u578b\u8fdb\u884c\u63a8\u7406...\")\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n    print(\"\u6a21\u578b\u52a0\u8f7d\u6210\u529f\u3002\")\r\n\r\n    # Load a random JSON file containing text\r\n    print(f\"\u4ece {text_dir} \u4e2d\u52a0\u8f7d\u968f\u673a JSON \u6587\u4ef6...\")\r\n    json_files = [f for f in os.listdir(text_dir) if f.endswith('.json')]\r\n    random_json_file = random.choice(json_files)\r\n    json_file_path = os.path.join(text_dir, random_json_file)\r\n\r\n    with open(json_file_path, 'r') as file:\r\n        json_data = json.load(file)\r\n        text = json_data[\"\u5b8c\u6574\u6587\u672c\"]\r\n        text_turns = json_data[\"\u6587\u4ef6\u6570\"]\r\n\r\n    # From the audio directory, select a random folder and corresponding audio file\r\n    print(f\"\u4ece {audio_dir} \u4e2d\u9009\u62e9\u4e00\u4e2a\u968f\u673a\u6587\u4ef6\u5939...\")\r\n    folders = [f for f in os.listdir(audio_dir) if os.path.isdir(os.path.join(audio_dir, f))]\r\n    while True:\r\n        random_folder = random.choice(folders)\r\n        folder_path = os.path.join(audio_dir, random_folder)\r\n        lab_files = [f for f in os.listdir(folder_path) if f.endswith('.lab')]\r\n        if not lab_files:\r\n            continue  # Skip folders without `.lab` files\r\n\r\n        random_lab_file = random.choice(lab_files)\r\n        lab_file_path = os.path.join(folder_path, random_lab_file)\r\n\r\n        # Read the corresponding `.lab` file to get prompt text\r\n        with open(lab_file_path, 'r') as lab_file:\r\n            prompt_text = lab_file.read().strip()\r\n        prompt_turns = len(prompt_text.splitlines())  # Number of lines = number of turns\r\n\r\n        if text_turns % 2 == prompt_turns % 2 and prompt_turns <= 4:  # Ensure matching turns and prompt_turns <= 4\r\n            break\r\n\r\n    # Choose the corresponding audio file\r\n    print(f\"\u9009\u62e9\u7684\u97f3\u9891\u6587\u4ef6: {random_lab_file}...\")\r\n    audio_file = random_lab_file.replace('.lab', '.wav')\r\n    ref_audio = os.path.join(folder_path, audio_file)\r\n    if not os.path.exists(ref_audio):\r\n        raise FileNotFoundError(f\"\u672a\u627e\u5230\u97f3\u9891\u6587\u4ef6 {ref_audio}\")\r\n\r\n    # Call the inference process\r\n    def main_process(ref_audio, text, gen_text, model_obj, mel_spec_type, remove_silence, speed):\r\n        print(f\"\u6b63\u5728\u751f\u6210\u97f3\u9891: {text}\")\r\n        generated_audio_segments = []\r\n        reg1 = r\"(?=\\[\\w+\\])\"\r\n        chunks = re.split(reg1, gen_text)\r\n        reg2 = r\"\\[(\\w+)\\]\"\r\n\r\n        # Process each chunk of the generated text\r\n        for text_chunk in chunks:\r\n            if not text_chunk.strip():\r\n                continue\r\n            match = re.match(reg2, text_chunk)\r\n            if match:\r\n                voice = match[1]\r\n            else:\r\n                print(\"\u6ca1\u6709\u627e\u5230\u8bed\u97f3\u6807\u7b7e\uff0c\u4f7f\u7528\u9ed8\u8ba4\u8bed\u97f3\u3002\")\r\n                voice = \"main\"\r\n            \r\n            print(f\"\u8bed\u97f3\u9009\u62e9: {voice}\")\r\n            gen_text = text_chunk.strip()\r\n            audio, final_sample_rate, spectrogram = infer_process(\r\n                ref_audio, text, gen_text, model_obj, vocoder, mel_spec_type=mel_spec_type, speed=speed\r\n            )\r\n            generated_audio_segments.append(audio)\r\n\r\n        if generated_audio_segments:\r\n            final_wave = np.concatenate(generated_audio_segments)\r\n\r\n            if not os.path.exists(output_dir):\r\n                os.makedirs(output_dir)\r\n\r\n            with open(wave_path, \"wb\") as f:\r\n                sf.write(f.name, final_wave, final_sample_rate)\r\n                if remove_silence:\r\n                    print(\"\u6b63\u5728\u53bb\u9664\u751f\u6210\u97f3\u9891\u4e2d\u7684\u9759\u97f3\u90e8\u5206...\")\r\n                    remove_silence_for_generated_wav(f.name)\r\n                print(f\"\u751f\u6210\u7684\u97f3\u9891\u5df2\u4fdd\u5b58\u5230 {f.name}\")\r\n\r\n    main_process(ref_audio, text, text, ema_model, vocoder_name, remove_silence, speed)\r\n\r\n# Call the infer_tts function to start the process\r\nif __name__ == \"__main__\":\r\n    infer_tts()\r\n```\r\nI think the text and prompt wav file is not important because it's all right"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-13T10:58:49Z",
        "body": "> print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected\r\n\r\nmaybe you have changed the code related with ema_model setup?\r\n\r\ntry do the removal of the module prefix using the function you got to `checkpoint['ema_model_state_dict']` see if works"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the checkpoint keys have 'module.' prefix despite no DataParallel usage",
      "Solution for loading checkpoints saved with distributed training artifacts",
      "Method to handle EMA model state_dict alignment",
      "Approach that maintains inference functionality after loading",
      "Clarification on accelerator.unwrap_model() behavior"
    ]
  },
  {
    "number": 412,
    "title": "\u52a0\u8f7d\u897f\u73ed\u7259\u548c\u65e5\u8bed\u6a21\u578b\u7684\u65f6\u5019\uff0c\u62a5\u9519\uff1aRuntimeError: Error(s) in loading state_dict for CFM:",
    "created_at": "2024-11-07T02:20:30Z",
    "closed_at": "2024-11-07T05:19:54Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/412",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n\u52a0\u8f7dDiscussions\u4e2d\u63d0\u4f9b\u7684\u8bad\u7ec3\u597d\u7684\u65e5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u6a21\u578b\u7684\u65f6\u5019\u62a5\u9519\uff1a\r\n\u65e5\u8bed\u62a5\u9519\uff1a\r\nmodel :  /root/F5-TTS/ckpts/.../model_1108224.pt\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\n\u897f\u73ed\u7259\u8bed\u62a5\u9519\uff1a\r\nmodel :  /root/F5-TTS/ckpts/jpgallegoar/model_1200000.safetensors\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        Missing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\". ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/412/comments",
    "author": "liuhui881125",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-07T05:18:36Z",
        "body": "> RuntimeError: Error(s) in loading state_dict for CFM:\r\n> size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\nUse corresponding vocab.txt\r\n\r\n> RuntimeError: Error(s) in loading state_dict for CFM:\r\nMissing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\".\r\n\r\npull latest repo commit"
      },
      {
        "user": "liuhui881125",
        "created_at": "2024-11-07T05:19:54Z",
        "body": "ok.fix .good"
      }
    ],
    "satisfaction_conditions": [
      "Resolve vocabulary size mismatch between pretrained model and current implementation",
      "Ensure codebase compatibility with model architecture requirements"
    ]
  },
  {
    "number": 359,
    "title": "small update gradio finetune",
    "created_at": "2024-11-01T09:23:40Z",
    "closed_at": "2024-11-01T10:22:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/359",
    "body": "@SWivid just small fix stuff \r\nso now when audio stereo always get duraction mono and resample\r\nafter train take case stereo to mono and resample\r\nand just fix error speling the bf16",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/359/comments",
    "author": "lpscr",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T10:22:32Z",
        "body": "@lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification.\r\nJust remove the num_channel in eq is fine."
      },
      {
        "user": "lpscr",
        "created_at": "2024-11-01T10:28:24Z",
        "body": "> @lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification. Just remove the num_channel in eq is fine.\r\n\r\nok so the first comit was ok then , yes after i confuse and chnage again thank you the fix"
      }
    ],
    "satisfaction_conditions": [
      "Ensures audio duration calculation does not involve resampling",
      "Uses correct sample rate for audio processing calculations",
      "Properly handles stereo-to-mono conversion without unnecessary processing steps",
      "Maintains correct parameter/variable naming conventions"
    ]
  },
  {
    "number": 337,
    "title": "RuntimeError: Error(s) in loading state_dict for EMA",
    "created_at": "2024-10-30T17:01:15Z",
    "closed_at": "2024-10-31T01:26:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/337",
    "body": "\u6c42\u52a9\uff0c\u662f\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u4e0d\u5bf9\u5417\uff0c\u6211\u53ea\u6539\u4e86cache_path\u4e3a\u672c\u5730\u7684\u7edd\u5bf9\u8def\u5f84\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 161, in <module>\r\n    main()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 154, in main\r\n    trainer.train(\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 248, in train\r\n    start_step = self.load_checkpoint()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 168, in load_checkpoint\r\n    self.ema_model.load_state_dict(checkpoint[\"ema_model_state_dict\"])\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 512]) from checkpoint, the shape in current model is torch.Size([3413, 512]).\r\nTraceback (most recent call last):\r\n  File \"/data/env/f5-tts/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\r\n    args.func(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1168, in launch_command\r\n    simple_launcher(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 763, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/data/env/f5-tts/bin/python', 'src/f5_tts/train/finetune_cli.py', '--exp_name', 'F5TTS_Base', '--learning_rate', '1e-05', '--batch_size_per_gpu', '2400', '--batch_size_type', 'frame', '--max_samples', '64', '--grad_accumulation_steps', '1', '--max_grad_norm', '1', '--epochs', '18107', '--num_warmup_updates', '14', '--save_per_updates', '26', '--last_per_steps', '6', '--dataset_name', 'yn_1030', '--finetune', 'True', '--tokenizer', 'pinyin', '--log_samples', 'True', '--logger', 'wandb']' returned non-zero exit status 1.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/337/comments",
    "author": "russell-shu",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T00:51:44Z",
        "body": "vocab\u5927\u5c0f\u4e0d\u5bf9\uff0c\u5982\u679c\u60f3finetune\u7684\u8bdd\uff0c\u4e0d\u8981\u66f4\u6539vocab"
      },
      {
        "user": "russell-shu",
        "created_at": "2024-10-31T01:11:57Z",
        "body": "\u975e\u5e38\u611f\u8c22\uff0c\u679c\u7136\u662f\u6211\u6539\u4e86vocab\u7684\u539f\u56e0\r\n\r\n> vocab\u5927\u5c0f\u4e0d\u5bf9\uff0c\u5982\u679c\u60f3finetune\u7684\u8bdd\uff0c\u4e0d\u8981\u66f4\u6539vocab\r\n\r\n"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T01:26:16Z",
        "body": "great"
      }
    ],
    "satisfaction_conditions": [
      "Ensures vocabulary size consistency between the pre-trained model and finetuning configuration",
      "Preserves original model architecture parameters during finetuning",
      "Explains relationship between tokenizer configuration and model architecture"
    ]
  },
  {
    "number": 322,
    "title": "no such file f5-tts_infer-gradio",
    "created_at": "2024-10-30T01:20:47Z",
    "closed_at": "2024-10-30T11:16:59Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/322",
    "body": "# Launch a Gradio app (web interface)\r\nf5-tts_infer-gradio\r\nThere is no such file in the repo. cannot run it.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/322/comments",
    "author": "michaeltran33",
    "comments": [
      {
        "user": "justinjohn0306",
        "created_at": "2024-10-30T02:26:23Z",
        "body": "> # Launch a Gradio app (web interface)\r\n> f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n\r\ncd F5-TTS\r\npip install -e ."
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T10:42:55Z",
        "body": "> > # Launch a Gradio app (web interface)\r\n> > f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n> \r\n> cd F5-TTS pip install -e .\r\nno such file regardless. Best to review the repo. \r\n"
      },
      {
        "user": "danielw97",
        "created_at": "2024-10-30T10:46:01Z",
        "body": "Please note that this command is available when f5tts is installed as a pip package, not before.\r\nHope this helps a bit."
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T11:17:00Z",
        "body": "lemme try. found it. it is exe form. thx. \n\n---\n\nit works now. thanks in a bunch."
      }
    ],
    "satisfaction_conditions": [
      "Clarify that the command requires installation of the package first",
      "Explain the relationship between the repository files and the installed package",
      "Address the executable availability mechanism"
    ]
  },
  {
    "number": 285,
    "title": "Podcast generation",
    "created_at": "2024-10-26T11:00:31Z",
    "closed_at": "2024-10-26T12:10:26Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/285",
    "body": "I love this but a bit disappointed that the podcast generation has been taken out of the update really used that a lot, how can I get it back is there code for it I could include in the script.\r\nthanks for your work!!!!!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/285/comments",
    "author": "Cal-DCosta",
    "comments": [
      {
        "user": "samni728",
        "created_at": "2024-10-26T11:18:18Z",
        "body": "in  Multiple Speech-Type Generation \r\n\r\nExample Input 2:\r\n{Speaker1_Happy} Hello, I'd like to order a sandwich please.\r\n{Speaker2_Regular} Sorry, we're out of bread.\r\n{Speaker1_Sad} I really wanted a sandwich though...\r\n{Speaker2_Whisper} I'll give you the last one I was hiding."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-26T11:26:25Z",
        "body": "Hi @Cal-DCosta ~\r\nMultiple Speech-Type Generation is just for it, and you can generate for a talk (podcast) with more guests invited now lol"
      },
      {
        "user": "Cal-DCosta",
        "created_at": "2024-10-26T11:39:02Z",
        "body": "Ah! sorry did not realise, thanks!!!!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-26T12:10:26Z",
        "body": "@Cal-DCosta Yeah, have fun~\r\nWill close as solved, feel free to open if further questions"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that podcast-like functionality still exists in the system",
      "Clear documentation of multi-speaker interaction syntax",
      "Support for dynamic conversations with multiple participants"
    ]
  },
  {
    "number": 197,
    "title": "Minimal GPU Memory Requirements for Running the Model??",
    "created_at": "2024-10-21T07:10:33Z",
    "closed_at": "2024-10-21T11:24:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/197",
    "body": "I would like to know:\r\n\r\nWhat is the minimal GPU memory requirement for running this model smoothly?\r\nIs there a recommended batch size or other configurations (like precision or memory optimizations) I should use to reduce memory usage?\r\ni managed to change batch_size of trainer.py script, utils_infer.py still encountering cuda oom!! can you tell me the minimal requirements of GPU mmeory??",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/197/comments",
    "author": "sachin-seisei",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:05:56Z",
        "body": "the minimal GPU memory is like 2G (just loading F5/E2 TTS model, and not leverage ASR model to do transcription)\r\nbtw you were to do training or inference.\r\n"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:08:13Z",
        "body": "@SWivid so inference only i am doing as of now!! thats why i want to know in inference also why its showing cuda oom!!\r\nPS: i am not leveraging ASR even i am providing ref text also and not an empty string!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:15:49Z",
        "body": "which script are you using? gradio_app.py or inference-cli.py"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:16:16Z",
        "body": "inference-cli.py \r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:48:26Z",
        "body": "how many gpu mem do you have.\r\ni just tweak the `inference-cli.py` script, make sure it is not loading unneeded asr pipeline.\r\nit takes 1622M gpu mem for me running it"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T11:24:08Z",
        "body": "the issue has been resolved that's why i am closing this.. thanks @SWivid "
      }
    ],
    "satisfaction_conditions": [
      "Clear specification of minimal GPU memory required for inference-only usage",
      "Guidance on avoiding unnecessary model component loading",
      "Identification of common pitfalls causing OOM despite meeting minimal specs"
    ]
  },
  {
    "number": 192,
    "title": "RuntimeError: Input type (c10::Half) and bias type (float) should be the same",
    "created_at": "2024-10-21T01:57:10Z",
    "closed_at": "2024-10-21T17:55:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/192",
    "body": "I just started getting this error with the latest build:\r\n```\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/ttspod/speech/f5.py\", line 214, in infer_batch\r\n    generated_wave = self.vocos.decode(generated_mel_spec.cpu())\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/models.py\", line 79, in forward\r\n    x = self.embed(x)\r\n        ^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 308, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 304, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/192/comments",
    "author": "ajkessel",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T02:32:21Z",
        "body": "`generated = generated.to(torch.float32)` should be added as we done along with .half() for model and input\r\n```\r\n# Final result\r\ngenerated = generated.to(torch.float32)\r\ngenerated = generated[:, ref_audio_len:, :]\r\ngenerated_mel_spec = generated.permute(0, 2, 1)\r\ngenerated_wave = vocos.decode(generated_mel_spec.cpu())\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Resolve data type mismatch between model input and bias parameters",
      "Maintain compatibility with existing model quantization/half-precision usage",
      "Ensure tensor type alignment before decoding operations",
      "Address type consistency without breaking subsequent processing steps"
    ]
  },
  {
    "number": 190,
    "title": "Broken on MPS",
    "created_at": "2024-10-20T19:29:57Z",
    "closed_at": "2024-10-20T20:06:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/190",
    "body": "Not sure what happened, but looks like the app is broken on Macs at the moment.\r\n\r\nJust did a fresh install and the app itself runs, but the resulting audio is empty.\r\n\r\nAlso I am not sure if the logs are useful but pasting just in case:\r\n\r\n```\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\r\n  warnings.warn(\r\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\r\n\r\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\ngen_text 0 Reference text will be automatically transcribed with Whisper if not provided. For best results, keep your reference clips short\r\nBuilding prefix dict from the default dictionary ...\r\nLoading model from cache /Users/x/pinokio/cache/TMPDIR/jieba.cache\r\nLoading model cost 0.426 seconds.\r\nPrefix dict has been built successfully.\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:574: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\r\n  warnings.warn(warning.format(data.dtype))\r\n/Users/x/pinokio/api/e2-f5-tts.git/app/env/lib/python3.10/site-packages/gradio/processing_utils.py:577: RuntimeWarning: invalid value encountered in cast\r\n  data = data.astype(np.int16)\r\n```\r\n\r\nI am not completely sure but I don't remember seeing this many warning messages previously.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/190/comments",
    "author": "cocktailpeanut",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-20T19:35:37Z",
        "body": "a default fp16 inference setting was added.\r\nsee if the last commit works d3badb95cf1b97a61472d65d4787a35cddf9c908"
      },
      {
        "user": "cocktailpeanut",
        "created_at": "2024-10-20T19:48:06Z",
        "body": "@SWivid this worked, thank you!\r\n\r\nCould you share what the switch to fp16 means from end user's point of view (performance, etc.)? Appreciate it!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-20T20:05:25Z",
        "body": "> Could you share what the switch to fp16 means from end user's point of view (performance, etc.)? Appreciate it!\r\n\r\nA bit faster than using fp32, ~half graphics card usage (the %), and more environmentally friendly maybe lol\r\nCompared to a more aggressive int8 quantization, it can be seen as no performance (quality) penalty."
      },
      {
        "user": "cocktailpeanut",
        "created_at": "2024-10-20T20:06:10Z",
        "body": "Thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Resolves empty audio output issue on macOS",
      "Explains performance implications of technical changes in non-technical terms",
      "Addresses compatibility with Apple's Metal Performance Shaders (MPS)",
      "Maintains audio quality while optimizing resource usage"
    ]
  },
  {
    "number": 165,
    "title": "I get a mismatch error from using a finetune model while using the inference file",
    "created_at": "2024-10-18T17:51:27Z",
    "closed_at": "2024-10-18T20:22:53Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/165",
    "body": "I edit the inference-cli.py file to include the checkpoint path\r\n\r\n```\r\ndef load_model(repo_name, exp_name, model_cls, model_cfg, ckpt_step):\r\n    ckpt_path = f\"/home/user/F5-TTS/ckpts/Finetune/model_last.pt\" # .pt | \r\n.safetensors\r\n```\r\nI left this as it is\r\n\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Emilia_ZH_EN\", \"pinyin\")\r\n```\r\n\r\nThen I changed it after I got a message message\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Finetune\", \"pinyin\")\r\n```\r\n\r\nThen I use the inference file\r\n```\r\npython inference-cli.py \\\r\n--ref_audio \"/home/user/F5-TTS Test/Template/Input.wav\" \\\r\n--ref_text \"Robinson Industries, the world's leading scientific research and design factory. My dad runs the company. They mass produce his inventions. His motto, keep moving forward. It's what he does.\" \\\r\n--gen_text \"I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall. But always remember, I am mighty and enduring. Respect me and I'll nurture you; ignore me and you shall face the consequences.\"\r\n```\r\n\r\nThen I get this error message\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 392, in <module>\r\n    process(ref_audio, ref_text, gen_text, model, remove_silence)\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 374, in process\r\n    audio, spectragram = infer(ref_audio, ref_text, gen_text, model, remove_silence)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 346, in infer\r\n    return infer_batch((audio, sr), ref_text, gen_text_batches, model, remove_silence, cross_fade_duration)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 190, in infer_batch\r\n    ema_model = load_model(model, \"F5TTS_Base\", DiT, F5TTS_model_cfg, 1200000)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 148, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, use_ema = True)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/model/utils.py\", line 575, in load_checkpoint\r\n    ema_model.load_state_dict(checkpoint['ema_model_state_dict'])\r\n  File \"/home/user/F5-TTS/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\r\n    raise RuntimeError(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        Missing key(s) in state_dict: \"ema_model.transformer.text_embed.text_blocks.0.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.0.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.0.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.0.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.0.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.0.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.1.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.1.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.1.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.1.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.2.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.2.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.2.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.2.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.3.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.3.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.3.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.3.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.bias\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.bias\", \"ema_model.transformer.norm_out.linear.weight\", \"ema_model.transformer.norm_out.linear.bias\". \r\n        Unexpected key(s) in state_dict: \"ema_model.transformer.layers.0.1.g\", \"ema_model.transformer.layers.0.2.to_q.weight\", \"ema_model.transformer.layers.0.2.to_q.bias\", \"ema_model.transformer.layers.0.2.to_k.weight\", \"ema_model.transformer.layers.0.2.to_k.bias\", \"ema_model.transformer.layers.0.2.to_v.weight\", \"ema_model.transformer.layers.0.2.to_v.bias\", \"ema_model.transformer.layers.0.2.to_out.0.weight\", \"ema_model.transformer.layers.0.2.to_out.0.bias\", \"ema_model.transformer.layers.0.3.g\", \"ema_model.transformer.layers.0.4.ff.0.0.weight\", \"ema_model.transformer.layers.0.4.ff.0.0.bias\", \"ema_model.transformer.layers.0.4.ff.2.weight\", \"ema_model.transformer.layers.0.4.ff.2.bias\", \"ema_model.transformer.layers.1.1.g\", \"ema_model.transformer.layers.1.2.to_q.weight\", \"ema_model.transformer.layers.1.2.to_q.bias\", \"ema_model.transformer.layers.1.2.to_k.weight\", \"ema_model.transformer.layers.1.2.to_k.bias\", \"ema_model.transformer.layers.1.2.to_v.weight\", \"ema_model.transformer.layers.1.2.to_v.bias\", \"ema_model.transformer.layers.1.2.to_out.0.weight\", \"ema_model.transformer.layers.1.2.to_out.0.bias\", \"ema_model.transformer.layers.1.3.g\", \"ema_model.transformer.layers.1.4.ff.0.0.weight\", \"ema_model.transformer.layers.1.4.ff.0.0.bias\", \"ema_model.transformer.layers.1.4.ff.2.weight\", \"ema_model.transformer.layers.1.4.ff.2.bias\", \"ema_model.transformer.layers.2.1.g\", \"ema_model.transformer.layers.2.2.to_q.weight\", \"ema_model.transformer.layers.2.2.to_q.bias\", \"ema_model.transformer.layers.2.2.to_k.weight\", \"ema_model.transformer.layers.2.2.to_k.bias\", \"ema_model.transformer.layers.2.2.to_v.weight\", \"ema_model.transformer.layers.2.2.to_v.bias\", \"ema_model.transformer.layers.2.2.to_out.0.weight\", \"ema_model.transformer.layers.2.2.to_out.0.bias\", \"ema_model.transformer.layers.2.3.g\", \"ema_model.transformer.layers.2.4.ff.0.0.weight\", \"ema_model.transformer.layers.2.4.ff.0.0.bias\", \"ema_model.transformer.layers.2.4.ff.2.weight\", \"ema_model.transformer.layers.2.4.ff.2.bias\", \"ema_model.transformer.layers.3.1.g\", \"ema_model.transformer.layers.3.2.to_q.weight\", \"ema_model.transformer.layers.3.2.to_q.bias\", \"ema_model.transformer.layers.3.2.to_k.weight\", \"ema_model.transformer.layers.3.2.to_k.bias\", \"ema_model.transformer.layers.3.2.to_v.weight\", \"ema_model.transformer.layers.3.2.to_v.bias\", \"ema_model.transformer.layers.3.2.to_out.0.weight\", \"ema_model.transformer.layers.3.2.to_out.0.bias\", \"ema_model.transformer.layers.3.3.g\", \"ema_model.transformer.layers.3.4.ff.0.0.weight\", \"ema_model.transformer.layers.3.4.ff.0.0.bias\", \"ema_model.transformer.layers.3.4.ff.2.weight\", \"ema_model.transformer.layers.3.4.ff.2.bias\", \"ema_model.transformer.layers.4.1.g\", \"ema_model.transformer.layers.4.2.to_q.weight\", \"ema_model.transformer.layers.4.2.to_q.bias\", \"ema_model.transformer.layers.4.2.to_k.weight\", \"ema_model.transformer.layers.4.2.to_k.bias\", \"ema_model.transformer.layers.4.2.to_v.weight\", \"ema_model.transformer.layers.4.2.to_v.bias\", \"ema_model.transformer.layers.4.2.to_out.0.weight\", \"ema_model.transformer.layers.4.2.to_out.0.bias\", \"ema_model.transformer.layers.4.3.g\", \"ema_model.transformer.layers.4.4.ff.0.0.weight\", \"ema_model.transformer.layers.4.4.ff.0.0.bias\", \"ema_model.transformer.layers.4.4.ff.2.weight\", \"ema_model.transformer.layers.4.4.ff.2.bias\", \"ema_model.transformer.layers.5.1.g\", \"ema_model.transformer.layers.5.2.to_q.weight\", \"ema_model.transformer.layers.5.2.to_q.bias\", \"ema_model.transformer.layers.5.2.to_k.weight\", \"ema_model.transformer.layers.5.2.to_k.bias\", \"ema_model.transformer.layers.5.2.to_v.weight\", \"ema_model.transformer.layers.5.2.to_v.bias\", \"ema_model.transformer.layers.5.2.to_out.0.weight\", \"ema_model.transformer.layers.5.2.to_out.0.bias\", \"ema_model.transformer.layers.5.3.g\", \"ema_model.transformer.layers.5.4.ff.0.0.weight\", \"ema_model.transformer.layers.5.4.ff.0.0.bias\", \"ema_model.transformer.layers.5.4.ff.2.weight\", \"ema_model.transformer.layers.5.4.ff.2.bias\", \"ema_model.transformer.layers.6.1.g\", \"ema_model.transformer.layers.6.2.to_q.weight\", \"ema_model.transformer.layers.6.2.to_q.bias\", \"ema_model.transformer.layers.6.2.to_k.weight\", \"ema_model.transformer.layers.6.2.to_k.bias\", \"ema_model.transformer.layers.6.2.to_v.weight\", \"ema_model.transformer.layers.6.2.to_v.bias\", \"ema_model.transformer.layers.6.2.to_out.0.weight\", \"ema_model.transformer.layers.6.2.to_out.0.bias\", \"ema_model.transformer.layers.6.3.g\", \"ema_model.transformer.layers.6.4.ff.0.0.weight\", \"ema_model.transformer.layers.6.4.ff.0.0.bias\", \"ema_model.transformer.layers.6.4.ff.2.weight\", \"ema_model.transformer.layers.6.4.ff.2.bias\", \"ema_model.transformer.layers.7.1.g\", \"ema_model.transformer.layers.7.2.to_q.weight\", \"ema_model.transformer.layers.7.2.to_q.bias\", \"ema_model.transformer.layers.7.2.to_k.weight\", \"ema_model.transformer.layers.7.2.to_k.bias\", \"ema_model.transformer.layers.7.2.to_v.weight\", \"ema_model.transformer.layers.7.2.to_v.bias\", \"ema_model.transformer.layers.7.2.to_out.0.weight\", \"ema_model.transformer.layers.7.2.to_out.0.bias\", \"ema_model.transformer.layers.7.3.g\", \"ema_model.transformer.layers.7.4.ff.0.0.weight\", \"ema_model.transformer.layers.7.4.ff.0.0.bias\", \"ema_model.transformer.layers.7.4.ff.2.weight\", \"ema_model.transformer.layers.7.4.ff.2.bias\", \"ema_model.transformer.layers.8.1.g\", \"ema_model.transformer.layers.8.2.to_q.weight\", \"ema_model.transformer.layers.8.2.to_q.bias\", \"ema_model.transformer.layers.8.2.to_k.weight\", \"ema_model.transformer.layers.8.2.to_k.bias\", \"ema_model.transformer.layers.8.2.to_v.weight\", \"ema_model.transformer.layers.8.2.to_v.bias\", \"ema_model.transformer.layers.8.2.to_out.0.weight\", \"ema_model.transformer.layers.8.2.to_out.0.bias\", \"ema_model.transformer.layers.8.3.g\", \"ema_model.transformer.layers.8.4.ff.0.0.weight\", \"ema_model.transformer.layers.8.4.ff.0.0.bias\", \"ema_model.transformer.layers.8.4.ff.2.weight\", \"ema_model.transformer.layers.8.4.ff.2.bias\", \"ema_model.transformer.layers.9.1.g\", \"ema_model.transformer.layers.9.2.to_q.weight\", \"ema_model.transformer.layers.9.2.to_q.bias\", \"ema_model.transformer.layers.9.2.to_k.weight\", \"ema_model.transformer.layers.9.2.to_k.bias\", \"ema_model.transformer.layers.9.2.to_v.weight\", \"ema_model.transformer.layers.9.2.to_v.bias\", \"ema_model.transformer.layers.9.2.to_out.0.weight\", \"ema_model.transformer.layers.9.2.to_out.0.bias\", \"ema_model.transformer.layers.9.3.g\", \"ema_model.transformer.layers.9.4.ff.0.0.weight\", \"ema_model.transformer.layers.9.4.ff.0.0.bias\", \"ema_model.transformer.layers.9.4.ff.2.weight\", \"ema_model.transformer.layers.9.4.ff.2.bias\", \"ema_model.transformer.layers.10.1.g\", \"ema_model.transformer.layers.10.2.to_q.weight\", \"ema_model.transformer.layers.10.2.to_q.bias\", \"ema_model.transformer.layers.10.2.to_k.weight\", \"ema_model.transformer.layers.10.2.to_k.bias\", \"ema_model.transformer.layers.10.2.to_v.weight\", \"ema_model.transformer.layers.10.2.to_v.bias\", \"ema_model.transformer.layers.10.2.to_out.0.weight\", \"ema_model.transformer.layers.10.2.to_out.0.bias\", \"ema_model.transformer.layers.10.3.g\", \"ema_model.transformer.layers.10.4.ff.0.0.weight\", \"ema_model.transformer.layers.10.4.ff.0.0.bias\", \"ema_model.transformer.layers.10.4.ff.2.weight\", \"ema_model.transformer.layers.10.4.ff.2.bias\", \"ema_model.transformer.layers.11.1.g\", \"ema_model.transformer.layers.11.2.to_q.weight\", \"ema_model.transformer.layers.11.2.to_q.bias\", \"ema_model.transformer.layers.11.2.to_k.weight\", \"ema_model.transformer.layers.11.2.to_k.bias\", \"ema_model.transformer.layers.11.2.to_v.weight\", \"ema_model.transformer.layers.11.2.to_v.bias\", \"ema_model.transformer.layers.11.2.to_out.0.weight\", \"ema_model.transformer.layers.11.2.to_out.0.bias\", \"ema_model.transformer.layers.11.3.g\", \"ema_model.transformer.layers.11.4.ff.0.0.weight\", \"ema_model.transformer.layers.11.4.ff.0.0.bias\", \"ema_model.transformer.layers.11.4.ff.2.weight\", \"ema_model.transformer.layers.11.4.ff.2.bias\", \"ema_model.transformer.layers.12.0.weight\", \"ema_model.transformer.layers.12.1.g\", \"ema_model.transformer.layers.12.2.to_q.weight\", \"ema_model.transformer.layers.12.2.to_q.bias\", \"ema_model.transformer.layers.12.2.to_k.weight\", \"ema_model.transformer.layers.12.2.to_k.bias\", \"ema_model.transformer.layers.12.2.to_v.weight\", \"ema_model.transformer.layers.12.2.to_v.bias\", \"ema_model.transformer.layers.12.2.to_out.0.weight\", \"ema_model.transformer.layers.12.2.to_out.0.bias\", \"ema_model.transformer.layers.12.3.g\", \"ema_model.transformer.layers.12.4.ff.0.0.weight\", \"ema_model.transformer.layers.12.4.ff.0.0.bias\", \"ema_model.transformer.layers.12.4.ff.2.weight\", \"ema_model.transformer.layers.12.4.ff.2.bias\", \"ema_model.transformer.layers.13.0.weight\", \"ema_model.transformer.layers.13.1.g\", \"ema_model.transformer.layers.13.2.to_q.weight\", \"ema_model.transformer.layers.13.2.to_q.bias\", \"ema_model.transformer.layers.13.2.to_k.weight\", \"ema_model.transformer.layers.13.2.to_k.bias\", \"ema_model.transformer.layers.13.2.to_v.weight\", \"ema_model.transformer.layers.13.2.to_v.bias\", \"ema_model.transformer.layers.13.2.to_out.0.weight\", \"ema_model.transformer.layers.13.2.to_out.0.bias\", \"ema_model.transformer.layers.13.3.g\", \"ema_model.transformer.layers.13.4.ff.0.0.weight\", \"ema_model.transformer.layers.13.4.ff.0.0.bias\", \"ema_model.transformer.layers.13.4.ff.2.weight\", \"ema_model.transformer.layers.13.4.ff.2.bias\", \"ema_model.transformer.layers.14.0.weight\", \"ema_model.transformer.layers.14.1.g\", \"ema_model.transformer.layers.14.2.to_q.weight\", \"ema_model.transformer.layers.14.2.to_q.bias\", \"ema_model.transformer.layers.14.2.to_k.weight\", \"ema_model.transformer.layers.14.2.to_k.bias\", \"ema_model.transformer.layers.14.2.to_v.weight\", \"ema_model.transformer.layers.14.2.to_v.bias\", \"ema_model.transformer.layers.14.2.to_out.0.weight\", \"ema_model.transformer.layers.14.2.to_out.0.bias\", \"ema_model.transformer.layers.14.3.g\", \"ema_model.transformer.layers.14.4.ff.0.0.weight\", \"ema_model.transformer.layers.14.4.ff.0.0.bias\", \"ema_model.transformer.layers.14.4.ff.2.weight\", \"ema_model.transformer.layers.14.4.ff.2.bias\", \"ema_model.transformer.layers.15.0.weight\", \"ema_model.transformer.layers.15.1.g\", \"ema_model.transformer.layers.15.2.to_q.weight\", \"ema_model.transformer.layers.15.2.to_q.bias\", \"ema_model.transformer.layers.15.2.to_k.weight\", \"ema_model.transformer.layers.15.2.to_k.bias\", \"ema_model.transformer.layers.15.2.to_v.weight\", \"ema_model.transformer.layers.15.2.to_v.bias\", \"ema_model.transformer.layers.15.2.to_out.0.weight\", \"ema_model.transformer.layers.15.2.to_out.0.bias\", \"ema_model.transformer.layers.15.3.g\", \"ema_model.transformer.layers.15.4.ff.0.0.weight\", \"ema_model.transformer.layers.15.4.ff.0.0.bias\", \"ema_model.transformer.layers.15.4.ff.2.weight\", \"ema_model.transformer.layers.15.4.ff.2.bias\", \"ema_model.transformer.layers.16.0.weight\", \"ema_model.transformer.layers.16.1.g\", \"ema_model.transformer.layers.16.2.to_q.weight\", \"ema_model.transformer.layers.16.2.to_q.bias\", \"ema_model.transformer.layers.16.2.to_k.weight\", \"ema_model.transformer.layers.16.2.to_k.bias\", \"ema_model.transformer.layers.16.2.to_v.weight\", \"ema_model.transformer.layers.16.2.to_v.bias\", \"ema_model.transformer.layers.16.2.to_out.0.weight\", \"ema_model.transformer.layers.16.2.to_out.0.bias\", \"ema_model.transformer.layers.16.3.g\", \"ema_model.transformer.layers.16.4.ff.0.0.weight\", \"ema_model.transformer.layers.16.4.ff.0.0.bias\", \"ema_model.transformer.layers.16.4.ff.2.weight\", \"ema_model.transformer.layers.16.4.ff.2.bias\", \"ema_model.transformer.layers.17.0.weight\", \"ema_model.transformer.layers.17.1.g\", \"ema_model.transformer.layers.17.2.to_q.weight\", \"ema_model.transformer.layers.17.2.to_q.bias\", \"ema_model.transformer.layers.17.2.to_k.weight\", \"ema_model.transformer.layers.17.2.to_k.bias\", \"ema_model.transformer.layers.17.2.to_v.weight\", \"ema_model.transformer.layers.17.2.to_v.bias\", \"ema_model.transformer.layers.17.2.to_out.0.weight\", \"ema_model.transformer.layers.17.2.to_out.0.bias\", \"ema_model.transformer.layers.17.3.g\", \"ema_model.transformer.layers.17.4.ff.0.0.weight\", \"ema_model.transformer.layers.17.4.ff.0.0.bias\", \"ema_model.transformer.layers.17.4.ff.2.weight\", \"ema_model.transformer.layers.17.4.ff.2.bias\", \"ema_model.transformer.layers.18.0.weight\", \"ema_model.transformer.layers.18.1.g\", \"ema_model.transformer.layers.18.2.to_q.weight\", \"ema_model.transformer.layers.18.2.to_q.bias\", \"ema_model.transformer.layers.18.2.to_k.weight\", \"ema_model.transformer.layers.18.2.to_k.bias\", \"ema_model.transformer.layers.18.2.to_v.weight\", \"ema_model.transformer.layers.18.2.to_v.bias\", \"ema_model.transformer.layers.18.2.to_out.0.weight\", \"ema_model.transformer.layers.18.2.to_out.0.bias\", \"ema_model.transformer.layers.18.3.g\", \"ema_model.transformer.layers.18.4.ff.0.0.weight\", \"ema_model.transformer.layers.18.4.ff.0.0.bias\", \"ema_model.transformer.layers.18.4.ff.2.weight\", \"ema_model.transformer.layers.18.4.ff.2.bias\", \"ema_model.transformer.layers.19.0.weight\", \"ema_model.transformer.layers.19.1.g\", \"ema_model.transformer.layers.19.2.to_q.weight\", \"ema_model.transformer.layers.19.2.to_q.bias\", \"ema_model.transformer.layers.19.2.to_k.weight\", \"ema_model.transformer.layers.19.2.to_k.bias\", \"ema_model.transformer.layers.19.2.to_v.weight\", \"ema_model.transformer.layers.19.2.to_v.bias\", \"ema_model.transformer.layers.19.2.to_out.0.weight\", \"ema_model.transformer.layers.19.2.to_out.0.bias\", \"ema_model.transformer.layers.19.3.g\", \"ema_model.transformer.layers.19.4.ff.0.0.weight\", \"ema_model.transformer.layers.19.4.ff.0.0.bias\", \"ema_model.transformer.layers.19.4.ff.2.weight\", \"ema_model.transformer.layers.19.4.ff.2.bias\", \"ema_model.transformer.layers.20.0.weight\", \"ema_model.transformer.layers.20.1.g\", \"ema_model.transformer.layers.20.2.to_q.weight\", \"ema_model.transformer.layers.20.2.to_q.bias\", \"ema_model.transformer.layers.20.2.to_k.weight\", \"ema_model.transformer.layers.20.2.to_k.bias\", \"ema_model.transformer.layers.20.2.to_v.weight\", \"ema_model.transformer.layers.20.2.to_v.bias\", \"ema_model.transformer.layers.20.2.to_out.0.weight\", \"ema_model.transformer.layers.20.2.to_out.0.bias\", \"ema_model.transformer.layers.20.3.g\", \"ema_model.transformer.layers.20.4.ff.0.0.weight\", \"ema_model.transformer.layers.20.4.ff.0.0.bias\", \"ema_model.transformer.layers.20.4.ff.2.weight\", \"ema_model.transformer.layers.20.4.ff.2.bias\", \"ema_model.transformer.layers.21.0.weight\", \"ema_model.transformer.layers.21.1.g\", \"ema_model.transformer.layers.21.2.to_q.weight\", \"ema_model.transformer.layers.21.2.to_q.bias\", \"ema_model.transformer.layers.21.2.to_k.weight\", \"ema_model.transformer.layers.21.2.to_k.bias\", \"ema_model.transformer.layers.21.2.to_v.weight\", \"ema_model.transformer.layers.21.2.to_v.bias\", \"ema_model.transformer.layers.21.2.to_out.0.weight\", \"ema_model.transformer.layers.21.2.to_out.0.bias\", \"ema_model.transformer.layers.21.3.g\", \"ema_model.transformer.layers.21.4.ff.0.0.weight\", \"ema_model.transformer.layers.21.4.ff.0.0.bias\", \"ema_model.transformer.layers.21.4.ff.2.weight\", \"ema_model.transformer.layers.21.4.ff.2.bias\", \"ema_model.transformer.layers.22.0.weight\", \"ema_model.transformer.layers.22.1.g\", \"ema_model.transformer.layers.22.2.to_q.weight\", \"ema_model.transformer.layers.22.2.to_q.bias\", \"ema_model.transformer.layers.22.2.to_k.weight\", \"ema_model.transformer.layers.22.2.to_k.bias\", \"ema_model.transformer.layers.22.2.to_v.weight\", \"ema_model.transformer.layers.22.2.to_v.bias\", \"ema_model.transformer.layers.22.2.to_out.0.weight\", \"ema_model.transformer.layers.22.2.to_out.0.bias\", \"ema_model.transformer.layers.22.3.g\", \"ema_model.transformer.layers.22.4.ff.0.0.weight\", \"ema_model.transformer.layers.22.4.ff.0.0.bias\", \"ema_model.transformer.layers.22.4.ff.2.weight\", \"ema_model.transformer.layers.22.4.ff.2.bias\", \"ema_model.transformer.layers.23.0.weight\", \"ema_model.transformer.layers.23.1.g\", \"ema_model.transformer.layers.23.2.to_q.weight\", \"ema_model.transformer.layers.23.2.to_q.bias\", \"ema_model.transformer.layers.23.2.to_k.weight\", \"ema_model.transformer.layers.23.2.to_k.bias\", \"ema_model.transformer.layers.23.2.to_v.weight\", \"ema_model.transformer.layers.23.2.to_v.bias\", \"ema_model.transformer.layers.23.2.to_out.0.weight\", \"ema_model.transformer.layers.23.2.to_out.0.bias\", \"ema_model.transformer.layers.23.3.g\", \"ema_model.transformer.layers.23.4.ff.0.0.weight\", \"ema_model.transformer.layers.23.4.ff.0.0.bias\", \"ema_model.transformer.layers.23.4.ff.2.weight\", \"ema_model.transformer.layers.23.4.ff.2.bias\", \"ema_model.transformer.norm_out.g\". \r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 100]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n        size mismatch for ema_model.transformer.input_embed.proj.weight: copying a param with shape torch.Size([1024, 300]) from checkpoint, the shape in current model is torch.Size([1024, 712]).\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/165/comments",
    "author": "GUUser91",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-18T18:37:27Z",
        "body": "make sure you use corrrect model_type  also **F5TTS_Base** or **E2TTS_Base** this you finetune\r\n\r\n"
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T20:10:36Z",
        "body": "@lpscr\r\nI used the finetune_gradio.py file and selected E2TTS_Base for finetuning. Then I clicked on auto settings. Then I click on start training.\r\nI also used finetune-cli.py and the same thing happened.\n\n---\n\n@lpscr\r\nI used the finetune_gradio.py file again and finetuned with the F5TTS_Base model and now I don't have the error if I used this finetuned F5TTS_Base model. But I want to finetune with E2TTS_Base, can you try finetuning with E2TTS_Base on your end to see if it works?"
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-18T20:13:30Z",
        "body": "yes because you need to change in\r\nstart the interface-cli -m E2-TTS\r\n\r\n\r\nparser.add_argument(\r\n    \"-m\",\r\n    \"--model\",\r\n    help=\"F5-TTS | E2-TTS\",\r\n)\n\n---\n\nwhat gpu you have and memory  ? because i working in auto setting , "
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T20:22:54Z",
        "body": "@lpscr\r\nThat did it. Thank you. I'm using a 4090. I have 64GB Ram."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of model architecture compatibility between finetuning and inference",
      "Clarification of model type specification requirements"
    ]
  },
  {
    "number": 145,
    "title": "Run time error while fine tuning",
    "created_at": "2024-10-17T15:26:06Z",
    "closed_at": "2024-10-21T07:00:03Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/145",
    "body": "I am trying to fine tune the model. And I am stuck with this error \r\n\r\n>`Epoch 1/10:  23% 34/145 [00:24<01:21,  1.36step/s, loss=1.92, step=34]\r\nTraceback (most recent call last):\r\n  File \"/content/F5-TTS/train.py\", line 94, in <module>\r\n    main()\r\n  File \"/content/F5-TTS/train.py\", line 88, in main\r\n    trainer.train(train_dataset,\r\n  File \"/content/F5-TTS/model/trainer.py\", line 229, in train\r\n    loss, cond, pred = self.model(mel_spec, text=text_inputs, lens=mel_lengths, noise_scheduler=self.noise_scheduler)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 820, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 808, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/cfm.py\", line 273, in forward\r\n    pred = self.transformer(x = \u03c6, cond = cond, text = text, time = time, drop_audio_cond = drop_audio_cond, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 162, in forward\r\n    text_embed = self.text_embed(text, seq_len, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 57, in forward\r\n    text = self.text_embed(text) # b n -> b n d\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\", line 164, in forward\r\n    return F.embedding(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2267, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)`\r\n\r\n\r\n\r\n\r\nthese are the inputs I am using.\r\n\r\ntarget_sample_rate = 24000\r\nn_mel_channels = 100\r\nhop_length = 256\r\n\r\ntokenizer = \"pinyin\" # 'pinyin', 'char', or 'custom'\r\ntokenizer_path = None # if tokenizer = 'custom', define the path to the tokenizer you want to use (should be vocab.txt)\r\ndataset_name = \"My_Dataset\"\r\n\r\n# -------------------------- Training Settings -------------------------- #\r\n\r\nexp_name = \"E2TTS_Base\"  # F5TTS_Base | E2TTS_Base\r\n\r\nlearning_rate = 5e-06\r\n\r\nbatch_size_per_gpu = 38400  # 8 GPUs, 8 * 38400 = 307200\r\nbatch_size_type = \"frame\"  # \"frame\" or \"sample\"\r\nmax_samples = 2  # max sequences per batch if use frame-wise batch_size. we set 32 for small models, 64 for base models\r\ngrad_accumulation_steps = 1  # note: updates = steps / grad_accumulation_steps\r\nmax_grad_norm = 1.\r\n\r\nepochs = 10  # use linear decay, thus epochs control the slope\r\nnum_warmup_updates = 20  # warmup steps\r\nsave_per_updates = 500  # save checkpoint per steps\r\nlast_per_steps = 5000  # save last checkpoint per steps\r\n\r\n\r\n\r\nIs this error something related to my dataset or my input? Can anyone help?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/145/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-19T02:12:00Z",
        "body": "'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\r\n\r\nmaybe you need to check the tensor's type"
      },
      {
        "user": "MilanaShhanukova",
        "created_at": "2024-10-19T19:48:36Z",
        "body": "@rasheed-aidetic I may guess that you have an empty text sample, check it out. If it is intended, you may add the type converter in TextEmbedding\r\n\r\n```\r\n        if text.dtype is not torch.long:\r\n            text = text.long()\r\n```\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the root cause of the tensor type mismatch in the embedding layer",
      "Explains how to validate text input preprocessing",
      "Provides guidance on tensor type conversion requirements"
    ]
  },
  {
    "number": 144,
    "title": "Shape mismatch error while fine tuning (non-singleton dimension 1)",
    "created_at": "2024-10-17T12:37:13Z",
    "closed_at": "2024-10-17T14:56:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/144",
    "body": "Hi This project is incredible guys. Great work.\r\n\r\nI am trying to finetune this model and facing an error on shape mismatch. Can anyone help?\r\n\r\n`Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\r\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/content/F5-TTS/model/dataset.py\", line 117, in __getitem__\r\n    mel_spec = rearrange(mel_spec, '1 d t -> d t')\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\r\n    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\r\n    raise EinopsError(message + \"\\n {}\".format(e))\r\neinops.EinopsError:  Error while processing rearrange-reduction pattern \"1 d t -> d t\".\r\n Input tensor shape: torch.Size([2, 100, 1407]). Additional info: {}.\r\n Shape mismatch, 2 != 1`\r\n\r\nI am trying to fine tune the model with my own voice model. I have created the dataset using the prepare_csv_wavs.py script. Please let me know if I need to make any changes in the input section of train.py\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/144/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-17T12:57:14Z",
        "body": "i think the problem is you have stereo you need to  resample your audio to **mono** **24000 hz**\r\n\r\nrun this script please first make back up because replace your original files !\r\n\r\n```\r\nimport os\r\nimport glob\r\nfrom pydub import AudioSegment\r\n\r\ndef convert_wav_to_mono(folder_path):\r\n    wav_files = glob.glob(os.path.join(folder_path, '*.wav'))\r\n    for file_path in wav_files:\r\n        filename = os.path.basename(file_path)\r\n        audio = AudioSegment.from_wav(file_path)\r\n        mono_audio = audio.set_channels(1)\r\n        mono_audio = mono_audio.set_frame_rate(24000)\r\n        new_file_path = os.path.join(folder_path, f\"mono_{filename}\")\r\n        mono_audio.export(new_file_path, format=\"wav\")\r\n\r\nfolder_path = 'your_folder_path'\r\nconvert_wav_to_mono(folder_path)\r\n```\r\n"
      },
      {
        "user": "rasheed-aidetic",
        "created_at": "2024-10-17T14:56:13Z",
        "body": "Working after the change. Thank you so much for the quick reply."
      }
    ],
    "satisfaction_conditions": [
      "Ensures audio input is mono-channel",
      "Validates audio sample rate compatibility",
      "Resolves tensor shape mismatch in mel spectrogram processing"
    ]
  },
  {
    "number": 117,
    "title": "Python version",
    "created_at": "2024-10-16T08:06:20Z",
    "closed_at": "2024-10-16T08:13:26Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/117",
    "body": "It would be a good idea to specify the version of python. From my tests the project works for` >3.9`",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/117/comments",
    "author": "Mateleo",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T08:11:48Z",
        "body": "yes, we have the badge in readme showing that python 3.10 is recommended"
      }
    ],
    "satisfaction_conditions": [
      "Explicit documentation of Python version compatibility",
      "Clarification of minimum supported version vs. recommended version",
      "Visibility of version requirements in project documentation"
    ]
  },
  {
    "number": 110,
    "title": "Offloading Whisper to CPU",
    "created_at": "2024-10-16T02:18:29Z",
    "closed_at": "2024-10-16T05:08:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/110",
    "body": "I'm using the Gradio app on a laptop with 6 GB VRAM.\n\nWith the current settings, the model overflows into system RAM reducing generation speed significantly, but if I offload Whisper to CPU, it fits in about 5.2 GB. Can we have a cmdline swich / config file setting / in-app setting to do this automatically? Or perhaps unload Whisper when not in use?\n\nThe model itself is amazing by the way.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/110/comments",
    "author": "athu16",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T03:02:54Z",
        "body": "Thanks for using. `inference-cli.py` may with your need"
      },
      {
        "user": "athu16",
        "created_at": "2024-10-16T05:08:40Z",
        "body": "Thanks, just what I needed!"
      }
    ],
    "satisfaction_conditions": [
      "Provides an automated way to offload Whisper to the CPU during inference",
      "Supports configuration via CLI, config file, or in-app settings",
      "Ensures the model remains within available VRAM limits during operation"
    ]
  },
  {
    "number": 33,
    "title": "precompute_max_pos = 4096 \u76ee\u524d\u6700\u5927\u53ea\u80fd\u652f\u6301\u52304096token\u7684\u6587\u672c\u957f\u5ea6\u4e48\uff1f",
    "created_at": "2024-10-13T13:20:08Z",
    "closed_at": "2024-10-14T02:46:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/33",
    "body": "      if conv_layers > 0:\r\n          self.extra_modeling = True\r\n          self.precompute_max_pos = 4096  # ~44s of 24khz audio\r\n          self.register_buffer(\"freqs_cis\", precompute_freqs_cis(text_dim, self.precompute_max_pos), persistent=False)\r\n          self.text_blocks = nn.Sequential(*[ConvNeXtV2Block(text_dim, text_dim * conv_mult) for _ in range(conv_layers)])\r\n\r\n\u8bf7\u95ee\u540e\u7eed\u662f\u5426\u6709\u8ba1\u5212\u652f\u6301\u66f4\u5927\u7684\u957f\u5ea6\u3002",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/33/comments",
    "author": "pengyong94",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T02:46:10Z",
        "body": "\u5b9e\u9645\u4e0a\u76ee\u524d\u8bad\u7ec3\u96c6\u6700\u957f\u662f30\u79d2\uff0c\u6240\u4ee54096 ~44s\u5b8c\u5168\u8986\u76d6\u4e86\uff08\u56e0\u4e3atext\u662fpad\u5230\u548cmel\u4e00\u6837\u957f\uff0c\u800cmel\u6700\u957f\u5c31\u662f30*24000/256\u76ee\u524d\uff09\r\n\u76ee\u524d\u901a\u8fc7\u5206\u5757\u751f\u6210\uff0c30s\u5e94\u8be5\u591f\u7528\uff0c\u4e4b\u540e\u5927\u6982\u7387\u4f1a\u5f80\u6d41\u5f0f\u7684\u65b9\u5411\u8d70\n\n---\n\nI'll close this issue, if you have other questions you can reopen this issue."
      },
      {
        "user": "pengyong94",
        "created_at": "2024-10-14T04:21:20Z",
        "body": "ok, thanks for your reply ,and your work is great !"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the current text length limitation exists",
      "Clarification of future development direction for longer text handling",
      "Rationale for current sufficiency of the implementation"
    ]
  },
  {
    "number": 6,
    "title": "Can it be used to edit speech?",
    "created_at": "2024-10-10T10:06:20Z",
    "closed_at": "2024-10-11T01:28:09Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/6",
    "body": "Can this model to be used to edit speech?\r\n\r\nIt follow VoiceBox, but why E2-TTS do not mention its ability of redact speech, or it just can not. Or, I missed some thing?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/6/comments",
    "author": "chenht2021",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-10T16:43:40Z",
        "body": "As it was trained with in-filling task, editing ability should be inherited.\r\nSimply mask out the part for edit, synthesize and replace into this part.\r\n\r\nI'll try it out and if it works I'll add it to the script.\n\n---\n\n@chenht2021 It works well\r\nsee `python test_infer_single_edit.py`\r\nhave fun~"
      },
      {
        "user": "chenht2021",
        "created_at": "2024-10-11T01:28:02Z",
        "body": "Thanks, I figure out I made a mistake sending wrong mask to model.\r\nThanks for open source this model. Nice work!"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates the model's capability for speech editing through in-filling tasks",
      "Validates functionality through testable examples"
    ]
  }
]