[
  {
    "number": 35,
    "title": "Clarification: proxy or library for cot_decoding??",
    "created_at": "2024-09-24T18:05:42Z",
    "closed_at": "2024-09-24T22:47:34Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/codelion/optillm/issues/35",
    "body": "The documentation is unclear.  It looks like the proxy code doesn't support cot_decoding at all, right?  So it's only available as a library and the python notebook demo right now?\r\n\r\nOr are you saying that cot_decoding will work with the proxy if a suitable server is used that provides multiple choices in the chat response, like ollama (but not llama.cpp).\r\n\r\nI think it's the former, since the proxy code doesn't seem to include cot_decoding, but would like a clear statement on this.\r\n\r\nThanks!",
    "comments_url": "https://api.github.com/repos/codelion/optillm/issues/35/comments",
    "author": "lee-b",
    "comments": [
      {
        "user": "codelion",
        "created_at": "2024-09-24T20:53:39Z",
        "body": "cot_decoding cannot be implemented via a proxy with only API access to model. It need to look at the logits of tokens before they are decoded. I have implemented it using transformers library so the model needs to be loaded in using `AutoModelForCausalLM.from_pretrained(model_name)` as shown in the colab.\r\n\r\nMultiple responses when they are returned via api are already decoded using some existing strategy like beam search. To implement it in llama.cpp or vllm  we will need to integrate it into the upstream library that does the decoding."
      }
    ],
    "satisfaction_conditions": [
      "Clarifies which environments (proxy/library/notebook) support cot_decoding",
      "Explains the technical requirement for logit access to implement cot_decoding",
      "Identifies dependencies on specific server capabilities for proxy implementation",
      "Differentiates between API-level access vs direct model control"
    ]
  }
]