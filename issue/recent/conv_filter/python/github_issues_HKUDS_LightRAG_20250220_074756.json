[
  {
    "number": 855,
    "title": "Run examples/lightrag_zhipu_postgres_demo.py failed:  RuntimeWarning: coroutine 'LightRAG.initialize_storages' was never awaited",
    "created_at": "2025-02-19T08:49:00Z",
    "closed_at": "2025-02-19T09:38:18Z",
    "labels": [
      "question",
      "lightrag-server"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/855",
    "body": "```\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ~~~~~~~~~~^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 720, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"/private/var/www/LightRAG/hello2.py\", line 32, in main\n    rag = LightRAG(\n        working_dir=WORKING_DIR,\n    ...<15 lines>...\n        vector_storage=\"PGVectorStorage\",\n    )\n  File \"<string>\", line 34, in __init__\n  File \"/private/var/www/LightRAG/lightrag/lightrag.py\", line 563, in __post_init__\n    loop.run_until_complete(self.initialize_storages())\n    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 696, in run_until_complete\n    self._check_running()\n    ~~~~~~~~~~~~~~~~~~~^^\n  File \"/Users/sunny/.local/share/uv/python/cpython-3.13.1-macos-x86_64-none/lib/python3.13/asyncio/base_events.py\", line 632, in _check_running\n    raise RuntimeError('This event loop is already running')\nRuntimeError: This event loop is already running\nINFO:Creating a new event loop in main thread.\n<sys>:0: RuntimeWarning: coroutine 'LightRAG.initialize_storages' was never awaited\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/855/comments",
    "author": "tevooli",
    "comments": [
      {
        "user": "YanSte",
        "created_at": "2025-02-19T09:11:33Z",
        "body": "Are you using a notebook ?\n\n``\nimport nest_asyncio\nnest_asyncio.apply()\n``\n"
      },
      {
        "user": "tevooli",
        "created_at": "2025-02-19T09:32:33Z",
        "body": "@YanSte Thank you. This line of code solved this error."
      }
    ]
  },
  {
    "number": 715,
    "title": "Fix: AttributeError in NanoVectorDB Initialization",
    "created_at": "2025-02-05T08:12:34Z",
    "closed_at": "2025-02-07T05:12:43Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/715",
    "body": "Issue\r\nThe following error occurred during the initialization of NanoVectorDB in nano_vector_db_impl.py:\r\n\r\nFile \".../LightRAG/LightRAG/lightrag/kg/nano_vector_db_impl.py\", line 92, in __post_init__\r\n    self.embedding_func.embedding_dim, storage_file=self._client_file_name\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'function' object has no attribute 'embedding_dim'\r\n\r\nRoot Cause\r\nThe embedding_func variable was declared locally instead of being assigned to self.embedding_func.\r\nAs a result, self.embedding_func was referring to a function instead of an instance of EmbeddingFunc, causing the AttributeError.\r\n\r\nChanges Made\r\n1. Explicitly Initialized self.embedding_func in __post_init__ and query using:\r\n\r\nself.embedding_func = EmbeddingFunc(\r\n    embedding_dim=4096, max_token_size=8192, func=gpt_4o_mini_complete\r\n)\r\n\r\n2. Imported EmbeddingFunc and gpt_4o_mini_complete to avoid reference errors.\r\n\r\n\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/715/comments",
    "author": "Vasundhhara",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2025-02-05T12:42:38Z",
        "body": "Thanks for your contribution!  But there are some linting errors. Please make sure to run `pre-commit run --all-files` before submitting to ensure all linting checks pass."
      },
      {
        "user": "danielaskdd",
        "created_at": "2025-02-05T15:30:12Z",
        "body": "@LarFii the embedding function is setup by post_init of LightRAG according to user preference. It can not be set to a fix method in the DB implementation."
      },
      {
        "user": "ArnoChenFx",
        "created_at": "2025-02-05T15:35:53Z",
        "body": "It's not a good idea to create the embedding_func in NanoVectorDBStorage. The embedding_func of VectorDB is automatically passed in by LightRAG. You only need to pass the embedding_func as a parameter when creating LightRAG.\r\n\r\n```python\r\nfrom lightrag.llm.openai import gpt_4o_complete, openai_embed\r\n\r\nrag = LightRAG(\r\n    llm_model_func=gpt_4o_complete,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1024, \r\n        max_token_size=8192, \r\n        func=lambda texts: openai_embed(\r\n            texts,\r\n            model=\"text-embedding-3-small\"\r\n        )\r\n    )\r\n)\r\n```"
      },
      {
        "user": "LarFii",
        "created_at": "2025-02-05T18:04:39Z",
        "body": "> @LarFii the embedding function is setup by post_init of LightRAG according to user preference. It can not be set to a fix method in the DB implementation.\r\n\r\nOh, I got it. Thanks!"
      },
      {
        "user": "Vasundhhara",
        "created_at": "2025-02-06T08:12:16Z",
        "body": "@ArnoChenFx This solved the error without having to initialize the embedding_func in NanoVectorDBStorage. Thanks."
      }
    ]
  },
  {
    "number": 662,
    "title": "AttributeError: 'function' object has no attribute 'embedding_dim' in \"lightrag_openai_demo.py\" (PLEASE EXPLAIN THE FIX)",
    "created_at": "2025-01-27T22:04:01Z",
    "closed_at": "2025-01-28T14:37:05Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/662",
    "body": "Hi\n\nI am getting the following error in the lightrag_openai_demo.py. Can someone please help me fix it?\n\nTraceback (most recent call last):\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\examples\\lightrag_openai_demo.py\", line 11, in <module>\n    rag = LightRAG(\n          ^^^^^^^^^\n  File \"<string>\", line 32, in __init__\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\lightrag\\lightrag.py\", line 254, in __post_init__\n    self.entities_vdb = self.vector_db_storage_cls(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\lightrag\\lightrag.py\", line 80, in import_class\n    return cls(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"<string>\", line 8, in __init__\n  File \"C:\\Users\\arpit\\LLM Projects\\RAG\\Tutorial - GraphRAG\\LightRAG\\lightrag\\kg\\nano_vector_db_impl.py\", line 84, in __post_init__\n    self.embedding_func.embedding_dim, storage_file=self._client_file_name\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'function' object has no attribute 'embedding_dim'",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/662/comments",
    "author": "ArpitNarain",
    "comments": [
      {
        "user": "napoleon9000",
        "created_at": "2025-01-28T01:31:32Z",
        "body": "Same here"
      },
      {
        "user": "gutama",
        "created_at": "2025-01-28T03:03:51Z",
        "body": "```\nimport os\n\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.llm.openai import gpt_4o_mini_complete\nfrom lightrag.utils import EmbeddingFunc\nfrom lightrag.llm.openai import openai_embed  # or your custom embedding\n\nWORKING_DIR = \"./dickens\"\n\nif not os.path.exists(WORKING_DIR):\n    os.mkdir(WORKING_DIR)\n\nrag = LightRAG(\n    working_dir=WORKING_DIR,\n    llm_model_func=gpt_4o_mini_complete,\n    # llm_model_func=gpt_4o_complete\n    embedding_func=EmbeddingFunc(\n        embedding_dim=1536,\n        max_token_size=8192,\n        func=lambda texts: openai_embed(texts, model=\"text-embedding-3-small\")\n    )\n)\n\n\nwith open(\"./book.txt\", \"r\", encoding=\"utf-8\") as f:\n    rag.insert(f.read())\n\n\n# Perform naive search\n# print(\n#     rag.query(query=\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\", ))\n# )\n\n# Perform local search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n)\n\n# Perform global search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n)\n\n# Perform hybrid search\nprint(\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n)\n\nprint(\n    rag.query(query=\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n)\n```\n\n\nshould be work\n"
      },
      {
        "user": "ArpitNarain",
        "created_at": "2025-01-28T14:37:05Z",
        "body": "Thank you so much, it worked."
      }
    ]
  },
  {
    "number": 562,
    "title": "Error running Postgres Demo - SyntaxError",
    "created_at": "2025-01-09T17:05:10Z",
    "closed_at": "2025-02-17T11:02:10Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/562",
    "body": "Error running DEMO . I'm using Python 3.11. \r\n\r\n/home/asksuite/asksuite-dev/test-lightrag/venv311/bin/python /home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py \r\nTraceback (most recent call last):\r\n  File \"/home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py\", line 5, in <module>\r\n    from lightrag.kg.postgres_impl import PostgreSQLDB\r\n  File \"/home/asksuite/asksuite-dev/test-lightrag/venv311/lib/python3.11/site-packages/lightrag/kg/postgres_impl.py\", line 406\r\n    sql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({\",\".join([f\"'{_id}'\" for _id in data])})\"\r\n                                                                               ^\r\nSyntaxError: f-string: expecting '}'",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/562/comments",
    "author": "pavei",
    "comments": [
      {
        "user": "WellTung666",
        "created_at": "2025-01-10T02:57:07Z",
        "body": "same error\uff01 "
      },
      {
        "user": "WellTung666",
        "created_at": "2025-01-10T03:45:48Z",
        "body": "> Error running DEMO . I'm using Python 3.11.\r\n> \r\n> /home/asksuite/asksuite-dev/test-lightrag/venv311/bin/python /home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py Traceback (most recent call last): File \"/home/asksuite/asksuite-dev/test-lightrag/lightrag_openai_demo_costao_postgres.py\", line 5, in from lightrag.kg.postgres_impl import PostgreSQLDB File \"/home/asksuite/asksuite-dev/test-lightrag/venv311/lib/python3.11/site-packages/lightrag/kg/postgres_impl.py\", line 406 sql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({\",\".join([f\"'{_id}'\" for _id in data])})\" ^ SyntaxError: f-string: expecting '}'\r\n\r\nThis seems to solve the problem.\r\n\r\n```\r\nid_list = \",\".join([f\"'{_id}'\" for _id in data])\r\nsql = f\"SELECT id FROM LIGHTRAG_DOC_STATUS WHERE workspace=$1 AND id IN ({id_list})\"\r\n```"
      },
      {
        "user": "pavei",
        "created_at": "2025-01-10T18:32:57Z",
        "body": "Yes, it works! "
      }
    ]
  },
  {
    "number": 359,
    "title": "Update NanoVectorDBStorage upsert to use hash values for IDs",
    "created_at": "2024-11-30T12:55:00Z",
    "closed_at": "2024-12-02T10:02:12Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/pull/359",
    "body": "- In the delete_entity method, a hash value is calculated when deleting an entity.\r\n- On the other hand, in the upsert method, the index was used directly as the ID when adding an entity.\r\n- Therefore, I modified the upsert method to also calculate a hash value.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/359/comments",
    "author": "FatRicePaddyyyy",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-12-02T08:01:21Z",
        "body": "Thanks for your suggestion! But the hash value is already calculated in `operate.py` before calling the `upsert` method."
      },
      {
        "user": "FatRicePaddyyyy",
        "created_at": "2024-12-02T10:01:50Z",
        "body": "I see. Sorry for jumping to conclusions. I'll cancel the pull request."
      }
    ]
  },
  {
    "number": 249,
    "title": "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
    "created_at": "2024-11-11T08:49:13Z",
    "closed_at": "2024-11-19T07:19:54Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/249",
    "body": "I am using a huggingface demo, but with a local model.How to deal with it?please help me , thank you very much!!\r\n**code**:\r\nimport os\r\n\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import hf_model_complete, hf_embedding\r\nfrom lightrag.utils import EmbeddingFunc\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=hf_model_complete,\r\n    llm_model_name=\"/data/Qwen2.5-14B-Instruct\",\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1024,\r\n        max_token_size=5000,\r\n        func=lambda texts: hf_embedding(\r\n            texts,\r\n            tokenizer=AutoTokenizer.from_pretrained(\r\n                r\"/data/project/raag/bge-large-zh-v1.5/\", model_max_length=512\r\n            ),\r\n            embed_model=AutoModel.from_pretrained(\r\n                r\"/data/project/raag/bge-large-zh-v1.5/\"\r\n            ),\r\n        ),\r\n    ),\r\n)\r\n\r\nwith open(r\"/data/project/raag/caiwu.txt\", \"r\", encoding=\"utf-8\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n)\r\n\r\n# Perform local search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\r\n)\r\n\r\n# Perform global search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\r\n)\r\n\r\n# Perform hybrid search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\r\n)\r\nerror:\r\nroot@c15e0721d1a6:~# CUDA_VISIBLE_DEVICES=0  python /data/project/raag/light_rag.py\r\nINFO:lightrag:Logger initialized for working directory: /data/project/raag/dickens\r\nDEBUG:lightrag:LightRAG init with param:\r\n  working_dir = /data/project/raag/dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 1024, 'max_token_size': 5000, 'func': <function <lambda> at 0x7f5eaa0bfd90>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function hf_model_complete at 0x7f5dc1cd6b00>,\r\n  llm_model_name = /data/Qwen2.5-14B-Instruct,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x7f5dc1cbfeb0>\r\n\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:lightrag:Loaded graph from /data/project/raag/dickens/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\r\nINFO:nano-vectordb:Load (0, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Load (2, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_chunks.json'} 2 data\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nINFO:lightrag:[New Chunks] inserting 2 chunks\r\nINFO:lightrag:Inserting 2 vectors to chunks\r\nINFO:lightrag:[Entity Extraction]...\r\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\r\n/usr/local/lib/python3.10/site-packages/accelerate/utils/modeling.py:1390: UserWarning: Current model requires 12544 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\r\n  warnings.warn(\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:07<00:00,  1.00it/s]\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"/data/project/raag/light_rag.py\", line 33, in <module>\r\n    rag.insert(f.read())\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/lightrag.py\", line 164, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/lightrag.py\", line 211, in ainsert\r\n    maybe_new_kg = await extract_entities(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/operate.py\", line 331, in extract_entities\r\n    results = await asyncio.gather(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/operate.py\", line 270, in _process_single_content\r\n    final_result = await use_llm_func(hint_prompt)\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/utils.py\", line 87, in wait_func\r\n    result = await func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/llm.py\", line 377, in hf_model_complete\r\n    return await hf_model_if_cache(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/llm.py\", line 286, in hf_model_if_cache\r\n    output = hf_model.generate(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2215, in generate\r\n    result = self._sample(\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3206, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1164, in forward\r\n    outputs = self.model(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 854, in forward\r\n    inputs_embeds = self.embed_tokens(input_ids)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\r\n    return F.embedding(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/functional.py\", line 2551, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/249/comments",
    "author": "Z-oo883",
    "comments": [
      {
        "user": "CooFlow",
        "created_at": "2024-11-11T12:49:00Z",
        "body": "I solved it by change the code in llm.py\r\n\r\n`device_map='auto'` to `device_map=None`\r\n\r\n```\r\n@lru_cache(maxsize=1)\r\ndef initialize_hf_model(model_name):\r\n    hf_tokenizer = AutoTokenizer.from_pretrained(\r\n        model_name, device_map=None, trust_remote_code=True\r\n    )\r\n    hf_model = AutoModelForCausalLM.from_pretrained(\r\n        model_name, device_map=None, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\r\n    if hf_tokenizer.pad_token is None:\r\n        hf_tokenizer.pad_token = hf_tokenizer.eos_token\r\n\r\n    return hf_model, hf_tokenizer\r\n```"
      },
      {
        "user": "Z-oo883",
        "created_at": "2024-11-12T02:31:02Z",
        "body": "> torch_dtype=torch.bfloat16).cuda()\r\n\r\nthank you very much!"
      }
    ]
  },
  {
    "number": 202,
    "title": "Some questions about graph output comparison with GraphRAG",
    "created_at": "2024-11-04T03:37:59Z",
    "closed_at": "2024-11-07T15:19:32Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/202",
    "body": "While reproducing, I noticed that the graph in the output file was slightly different from GraphRAG.\r\n\r\nAre the steps for building a graph in LightRAG the same as those for building a graph in GraphRAG? For example, are the same interfaces or logic used?\r\n\r\nIf so, how should I use the outputs generated by the two projects with each other? Is it possible?\r\n\r\n If not, what is the difference?\r\n \r\n Could you please give me more details? Thanks a lot.",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/202/comments",
    "author": "WinstonCHEN1",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-11-07T07:12:43Z",
        "body": "The main difference between LightRAG and GraphRAG in the extraction process is that LightRAG omits communities and instead introduces keywords, making them incompatible with each other. For more details, please refer to our paper."
      },
      {
        "user": "WinstonCHEN1",
        "created_at": "2024-11-07T15:19:32Z",
        "body": "Thanks a lot!!!"
      }
    ]
  },
  {
    "number": 163,
    "title": "RuntimeWarning: coroutine 'LightRAG.ainsert' was never awaited   print(f\"An error occurred: {e}\") ",
    "created_at": "2024-10-29T08:02:04Z",
    "closed_at": "2024-10-30T02:42:42Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/163",
    "body": "\u8fd0\u884clightrag_openai_compatible_demo.py\u62a5\u9519",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/163/comments",
    "author": "small122",
    "comments": [
      {
        "user": "dhdsjy",
        "created_at": "2024-10-29T12:32:21Z",
        "body": "\u6211\u5728win\u4e0b\u8fd0\u884c\u4e5f\u662f\u8fd9\u79cd\u9519\u8bef\uff0c\u63d0\u793a\u662f\u534f\u7a0b\u7684\u95ee\u9898\uff0c\u4f46\u4e5f\u6ca1\u53d1\u73b0\u5177\u4f53\u9519\u5728\u54ea\u91cc"
      },
      {
        "user": "Dormiveglia-elf",
        "created_at": "2024-10-29T15:52:52Z",
        "body": "Please refer my PR #168 , I think you can run the program smoothly without error now."
      },
      {
        "user": "small122",
        "created_at": "2024-10-30T02:42:40Z",
        "body": "> Please refer my PR #168 , I think you can run the program smoothly without error now.\r\n\r\nthank you\uff0ci get it"
      }
    ]
  },
  {
    "number": 124,
    "title": "How to construct the Overall Performance Table in example/batch_eval.py?",
    "created_at": "2024-10-25T06:39:35Z",
    "closed_at": "2024-10-30T01:40:15Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/124",
    "body": "How to get the score in Overall Performance Table by running example/batch_eval.py?\r\n1. How are the **result_files** obtained? Are they obtained by querying different classes (in reproduce/Step_3.py)?\r\n2. Why is it unnecessary to consider the **retrieval context** and **ground truth** ? And what's the ground_truth of the generated queries?\r\n\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/124/comments",
    "author": "SetonLiang",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-28T07:00:10Z",
        "body": "Yes, the result_files are the results generated by Step 3. Specifically, they are the answers provided by different RAG systems to the queries produced in Step 2. As for question 2, you can refer to Section 4.1 of the paper for more details."
      },
      {
        "user": "SetonLiang",
        "created_at": "2024-10-28T12:18:49Z",
        "body": "Got it. So, should I understand that instead of considering the ground truth, the approach is to compare different answers across multiple dimensions and then calculate the win rate, using only the **document content without (query, answer) pairs** in the UltraDomain Benchmark dataset during evaluation?"
      },
      {
        "user": "LarFii",
        "created_at": "2024-10-29T02:13:52Z",
        "body": "You're correct. Our approach focuses on evaluating model performance across multiple dimensions. This method directly follows the GraphRAG."
      }
    ]
  },
  {
    "number": 105,
    "title": "AttributeError: module 'ollama' has no attribute 'embeddings'",
    "created_at": "2024-10-22T23:18:40Z",
    "closed_at": "2024-10-25T11:17:23Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/105",
    "body": "I'm running the following adaptation to the ollama example:\r\n\r\n```\r\nimport os\r\n\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import ollama_model_complete, ollama_embedding\r\nfrom lightrag.utils import EmbeddingFunc\r\n\r\nWORKING_DIR = \"light_rag/dickens\"\r\nMODEL_NAME = \"llama3.2:3b\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=ollama_model_complete,\r\n    llm_model_name=MODEL_NAME,\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=768,\r\n        max_token_size=8192,\r\n        func=lambda texts: ollama_embedding(texts, embed_model=\"nomic-embed-text\"),\r\n    ),\r\n)\r\n\r\n\r\nwith open(\"light_rag/dickens/book.txt\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n)\r\n\r\n# Perform local search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\r\n)\r\n\r\n# Perform global search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\r\n)\r\n\r\n# Perform hybrid search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\r\n)\r\n```\r\n\r\nUnfortunately, I'm getting the following error:\r\n\r\n```\r\nINFO:lightrag:Logger initialized for working directory: light_rag/dickens\r\nDEBUG:lightrag:LightRAG init with param:\r\n  working_dir = light_rag/dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 768, 'max_token_size': 8192, 'func': <function <lambda> at 0x1268d1440>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function ollama_model_complete at 0x142289800>,\r\n  llm_model_name = llama3.2:3b,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x141af6980>\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'light_rag/dickens/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'light_rag/dickens/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': 'light_rag/dickens/vdb_chunks.json'} 0 data\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nINFO:lightrag:[New Chunks] inserting 42 chunks\r\nINFO:lightrag:Inserting 42 vectors to chunks\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"light_rag/src/example.py\", line 26, in <module>\r\n    rag.insert(f.read())\r\n  File \"site-packages/lightrag/lightrag.py\", line 162, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\r\n    return future.result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"site-packages/lightrag/lightrag.py\", line 206, in ainsert\r\n    await self.chunks_vdb.upsert(inserting_chunks)\r\n  File \"site-packages/lightrag/storage.py\", line 92, in upsert\r\n    embeddings_list = await asyncio.gather(\r\n                      ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/site-packages/lightrag/utils.py\", line 87, in wait_func\r\n    result = await func(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"site-packages/lightrag/utils.py\", line 43, in __call__\r\n    return await self.func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"site-packages/lightrag/llm.py\", line 421, in ollama_embedding\r\n    data = ollama.embeddings(model=embed_model, prompt=text)\r\n           ^^^^^^^^^^^^^^^^^\r\nAttributeError: module 'ollama' has no attribute 'embeddings'\r\n```",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/105/comments",
    "author": "rcontesti",
    "comments": [
      {
        "user": "LarFii",
        "created_at": "2024-10-23T04:01:22Z",
        "body": "You can try creating a new virtual environment and then install LightRAG in it."
      },
      {
        "user": "rcontesti",
        "created_at": "2024-10-23T13:44:06Z",
        "body": "> You can try creating a new virtual environment and then install LightRAG in it.\r\n\r\nThanks, I was able to solve it that way. I'm surprised that it works that way since in both env I have installed the same ollama version. Any clues of what might be going on with ollama? "
      }
    ]
  },
  {
    "number": 51,
    "title": "Can't run any example",
    "created_at": "2024-10-18T19:25:37Z",
    "closed_at": "2024-10-18T23:59:08Z",
    "labels": [],
    "url": "https://github.com/HKUDS/LightRAG/issues/51",
    "body": "I'm not able to run the example.\r\n\r\nI've tried this with python 3.9, 3.10, and 3.11. The initial bit of the examples work up until I run\r\n```python\r\nwith open(\"./book.txt\") as f:\r\n    rag.insert(f.read())\r\n\r\n```\r\n\r\nWhen `rag.insert(f.read())`, is run the following error appears every time:\r\n\r\n```logs\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[9], line 1\r\n----> 1 rag.insert(our_docs)\r\n\r\nFile /home/jupyter/projects/LightRAG/lightrag/lightrag.py:166, in LightRAG.insert(self, string_or_strings)\r\n    164 def insert(self, string_or_strings):\r\n    165     loop = always_get_an_event_loop()\r\n--> 166     return loop.run_until_complete(self.ainsert(string_or_strings))\r\n\r\nFile /opt/conda/envs/lightrag/lib/python3.11/asyncio/base_events.py:626, in BaseEventLoop.run_until_complete(self, future)\r\n    615 \"\"\"Run until the Future is done.\r\n    616 \r\n    617 If the argument is a coroutine, it is wrapped in a Task.\r\n   (...)\r\n    623 Return the Future's result, or raise its exception.\r\n    624 \"\"\"\r\n    625 self._check_closed()\r\n--> 626 self._check_running()\r\n    628 new_task = not futures.isfuture(future)\r\n    629 future = tasks.ensure_future(future, loop=self)\r\n\r\nFile /opt/conda/envs/lightrag/lib/python3.11/asyncio/base_events.py:586, in BaseEventLoop._check_running(self)\r\n    584 def _check_running(self):\r\n    585     if self.is_running():\r\n--> 586         raise RuntimeError('This event loop is already running')\r\n    587     if events._get_running_loop() is not None:\r\n    588         raise RuntimeError(\r\n    589             'Cannot run the event loop while another loop is running')\r\n\r\nRuntimeError: This event loop is already running\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/51/comments",
    "author": "wrosko",
    "comments": [
      {
        "user": "jannikdev",
        "created_at": "2024-10-18T20:06:01Z",
        "body": "Hi, the problem is that you are running this in a Jupyter notebook which is itself already running a loop. To do this in a notebook add \r\n`import nest_asyncio` and\r\n`nest_asyncio.apply()`\r\nbefore trying to run the async loop.\r\n\r\nIf you are wondering where you are trying to run something asynchronously: insert just wraps around ainsert and waits for completion. It will always start an event loop."
      },
      {
        "user": "ashishjaddu",
        "created_at": "2024-10-18T20:51:00Z",
        "body": "Hi, I tried it this way in my notebook and it worked for me. Hope this helps you.\r\n\r\n```python\r\nfrom lightrag.utils import EmbeddingFunc\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import gpt_4o_mini_complete, gpt_4o_complete\r\nimport os\r\nimport nest_asyncio\r\n\r\nWORKING_DIR = \"./dickens\"\r\nnest_asyncio.apply()\r\n\r\nos.environ[\"OPENAI_API_KEY\"] = \"sk-proj-\"\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nfrom lightrag.llm import hf_model_complete, hf_embedding\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\nbook_file_path = os.path.join(WORKING_DIR, \"book.txt\")\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=gpt_4o_mini_complete\r\n)\r\n\r\nasync def insert_book_content():\r\n    with open(book_file_path, 'r') as f:\r\n        await rag.insert(f.read()) \r\n\r\nawait insert_book_content()\r\n"
      },
      {
        "user": "wrosko",
        "created_at": "2024-10-18T23:59:09Z",
        "body": "> Hi, the problem is that you are running this in a Jupyter notebook which is itself already running a loop. To do this in a notebook add `import nest_asyncio` and `nest_asyncio.apply()` before trying to run the async loop.\r\n> \r\n> If you are wondering where you are trying to run something asynchronously: insert just wraps around ainsert and waits for completion. It will always start an event loop.\r\n\r\nThanks @jannikdev this solves the issue. I appreciate it"
      }
    ]
  }
]