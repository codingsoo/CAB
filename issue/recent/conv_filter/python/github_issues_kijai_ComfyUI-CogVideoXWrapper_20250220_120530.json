[
  {
    "number": 267,
    "title": "Error using Tora example workflow: RuntimeError: Input type (struct c10::BFloat16) and bias type (struct c10::Half) should be the same",
    "created_at": "2024-11-20T14:27:54Z",
    "closed_at": "2024-11-20T15:27:21Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/267",
    "body": "Hi,\r\nThanks for all of your work on this! \r\nI am fully updated as of 30 minutes ago, and other workflows I've tried are working well. However when trying the Tora example workflow in your repository, I get the below error.\r\nIf it matters, I am on Python 3.11.9, Torch 2.51, and CUDA 12.4. \r\nThank you!\r\n\r\n```\r\nDownloading model to: D:\\ComfyUI\\models\\CogVideo\\CogVideoX-Fun-V1.1-5b-InP\r\nFetching 5 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [04:25<00:00, 53.12s/it]\r\nThe config attributes {'add_noise_in_inpaint_model': True} were passed to CogVideoXTransformer3DModel, but are not expected and will be ignored. Please verify your config.json configuration file.\r\nend_vram - start_vram: 13008260046 - 1867276110 = 11140983936\r\n#80 [DownloadAndLoadCogVideoModel]: 274.52s - vram 11140983936b\r\nDownloading Fuser model to: D:\\ComfyUI\\models\\CogVideo\\CogVideoX-5b-Tora\\fuser\\fuser.safetensors\r\nFetching 1 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:32<00:00, 32.42s/it]\r\nDownloading trajectory extractor model to: D:\\ComfyUI\\models\\CogVideo\\CogVideoX-5b-Tora\\traj_extractor\\traj_extractor.safetensors\r\nFetching 1 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.51s/it]\r\nend_vram - start_vram: 14446958702 - 13008260046 = 1438698656\r\n#75 [DownloadAndLoadToraModel]: 35.23s - vram 1438698656b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#72 [LoadImage]: 0.02s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#73 [ImageResizeKJ]: 0.00s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#60 [SplineEditor]: 0.11s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#67 [GetMaskSizeAndCount]: 0.00s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#85 [SplineEditor]: 0.07s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#82 [SplineEditor]: 0.07s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#83 [AppendStringsToList]: 0.00s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#86 [AppendStringsToList]: 0.00s - vram 0b\r\nreceived 3 trajectorie(s)\r\nvideo_flow shape after encoding: torch.Size([1, 16, 13, 60, 90])\r\nend_vram - start_vram: 16117826884 - 14446958702 = 1670868182\r\n#78 [ToraEncodeTrajectory]: 4.74s - vram 1670868182b\r\nend_vram - start_vram: 14724537294 - 14724537294 = 0\r\n#66 [VHS_VideoCombine]: 0.32s - vram 0b\r\nend_vram - start_vram: 14724537294 - 14724537294 = 0\r\n#65 [CreateShapeImageOnPath]: 0.21s - vram 0b\r\nEncoded latents shape: torch.Size([1, 13, 16, 60, 90])\r\nend_vram - start_vram: 16395405476 - 14724537294 = 1670868182\r\n#93 [CogVideoImageEncodeFunInP]: 3.57s - vram 1670868182b\r\nend_vram - start_vram: 14726924094 - 14726924094 = 0\r\n#20 [CLIPLoader]: 2.17s - vram 0b\r\nRequested to load SD3ClipModel_\r\nLoading 1 new model\r\nloaded partially 64.0 32.38671875 0\r\nend_vram - start_vram: 24471274066 - 14726924094 = 9744349972\r\n#30 [CogVideoTextEncode]: 12.29s - vram 9744349972b\r\nUnloading models for lowram load.\r\n1 models unloaded.\r\nLoading 1 new model\r\nloaded partially 6805.310731887817 6779.38671875 0\r\nend_vram - start_vram: 24251545410 - 24251545410 = 0\r\n#31 [CogVideoTextEncode]: 5.73s - vram 0b\r\nReceived 13 image conditioning frames\r\nContext schedule disabled\r\nTora trajectory length: 13\r\nSampling 49 frames in 13 latent frames at 720x480 with 40 inference steps\r\n  0%|                                                                                                        | 0/40 [00:00<?, ?it/s]\r\n!!! Exception during processing !!! Input type (struct c10::BFloat16) and bias type (struct c10::Half) should be the same\r\nTraceback (most recent call last):\r\n  File \"D:\\ComfyUI\\execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"D:\\ComfyUI\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\nodes.py\", line 696, in process\r\n    latents = model[\"pipe\"](\r\n              ^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\pipeline_cogvideox.py\", line 757, in __call__\r\n    noise_pred = self.transformer(\r\n                 ^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\custom_cogvideox_transformer_3d.py\", line 688, in forward\r\n    hidden_states, encoder_hidden_states = block(\r\n                                           ^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\custom_cogvideox_transformer_3d.py\", line 266, in forward\r\n    h = fuser(h, video_flow_feature.to(h), T=T)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\tora\\traj_module.py\", line 287, in forward\r\n    gamma_flow = self.flow_gamma_spatial(flow)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 554, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 549, in _conv_forward\r\n    return F.conv2d(\r\n           ^^^^^^^^^\r\nRuntimeError: Input type (struct c10::BFloat16) and bias type (struct c10::Half) should be the same\r\n\r\nend_vram - start_vram: 16169275134 - 14726924094 = 1442351040\r\n#79 [CogVideoSampler]: 0.66s - vram 1442351040b\r\n```",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/267/comments",
    "author": "EnragedAntelope",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-11-20T14:42:15Z",
        "body": "Thanks for the report, indeed I broke it with last update, should be fixed now."
      },
      {
        "user": "EnragedAntelope",
        "created_at": "2024-11-20T15:27:21Z",
        "body": "Confirmed fixed, thank you!"
      }
    ]
  },
  {
    "number": 245,
    "title": "1.5test branch AssertionError('First input (fp16) and second input (bf16) must have the same dtype!')",
    "created_at": "2024-11-18T08:00:10Z",
    "closed_at": "2024-11-18T08:13:45Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/245",
    "body": "   noise_pred = self.transformer(\r\n                 ^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 685, in forward\r\n    hidden_states, encoder_hidden_states = block(\r\n                                           ^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 282, in forward\r\n    attn_hidden_states, attn_encoder_hidden_states = self.attn1(\r\n                                                     ^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/diffusers/models/attention_processor.py\", line 495, in forward\r\n    return self.processor(\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 129, in __call__\r\n    hidden_states = sageattn_func(query, key, value, attn_mask=attention_mask, dropout_p=0.0,is_causal=False)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 451, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 50, in sageattn_func\r\n    return sageattn(query, key, value, attn_mask=attn_mask, dropout_p=dropout_p,is_causal=is_causal)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/sageattention/core.py\", line 106, in sageattn\r\n    o = attn_true(q_int8, k_int8, v, q_scale, k_scale, tensor_layout=tensor_layout, output_dtype=dtype)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/sageattention/attn_qk_int8_per_block.py\", line 113, in forward\r\n    _attn_fwd[grid](\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/runtime/jit.py\", line 167, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/runtime/jit.py\", line 416, in run\r\n    self.cache[device][key] = compile(\r\n                              ^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/compiler/compiler.py\", line 191, in compile\r\n    module = src.make_ir(options)\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/compiler/compiler.py\", line 117, in make_ir\r\n    return ast_to_ttir(self.fn, self, options=options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/compiler/code_generator.py\", line 1231, in ast_to_ttir\r\n    raise CompilationError(fn.src, node, repr(e)) from e\r\ntriton.compiler.errors.CompilationError: at 39:55:    O_block_ptr = Out + (off_z * stride_oz + off_h * stride_oh) + offs_m[:, None] * stride_on + offs_k[None, :]\r\n\r\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\r\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\r\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\r\n\r\n    q = tl.load(Q_ptrs, mask = offs_m[:, None] < qo_len)\r\n    q_scale = tl.load(Q_scale_ptr)\r\n    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, kv_len, K_ptrs, K_scale_ptr, V_ptrs, stride_kn, stride_vn,\r\n                                    start_m,\r\n                                    BLOCK_M, HEAD_DIM, BLOCK_N,\r\n                                    4 - STAGE, offs_m, offs_n\r\n                                                       ^\r\nAssertionError('First input (fp16) and second input (bf16) must have the same dtype!')\r\n\r\ncommit 6f9e4ff6477d51ef29e2f7eea9ff2bbd6986b007 (HEAD -> 1.5_test, origin/1.5_test)\r\nAuthor: kijai <40791699+kijai@users.noreply.github.com>\r\nDate:   Sun Nov 17 22:23:40 2024 +0200\r\n\r\n    Update custom_cogvideox_transformer_3d.py\r\n\r\ncommit e70da23ac2b4724624537e503b0cdaf93d24a74e\r\nAuthor: kijai <40791699+kijai@users.noreply.",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/245/comments",
    "author": "magicwang1111",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-11-18T08:09:53Z",
        "body": "I think I had this too with sageattention 1.0.5, reverting back to 1.0.3 made it work again. "
      },
      {
        "user": "magicwang1111",
        "created_at": "2024-11-18T08:13:33Z",
        "body": "> I think I had this too with sageattention 1.0.5, reverting back to 1.0.3 made it work again.\r\n\r\nthx,this error is fixed.you can add it in requirements."
      }
    ]
  },
  {
    "number": 136,
    "title": "Error when running fastmode",
    "created_at": "2024-10-07T12:06:11Z",
    "closed_at": "2024-10-07T18:06:43Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/136",
    "body": "When I try activating fastmode, I get the error CUDA error: \r\n\r\nCUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`.\r\n\r\nI suspect my CUDA version is not correct, I have v11.8. Which one do I need?\r\n\r\nRTX 4090, Windows 11.",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/136/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-10-07T12:41:03Z",
        "body": "You need torch that's compiled for cuda 124."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-07T17:19:04Z",
        "body": "> You need torch that's compiled for cuda 124.\r\n\r\nThank you, that worked."
      }
    ]
  },
  {
    "number": 119,
    "title": "CogVideoX-Fun-V1.1",
    "created_at": "2024-10-01T21:00:40Z",
    "closed_at": "2024-10-03T18:05:59Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/119",
    "body": "is the CogVideoX-Fun-V1.1 not compatible with I2V like the first one? it completely distorts and warps images, no matter the noise strength \"wich i have no idea how it works\" or cfg, i had a poor cat turn into a strange matter akin to a demon from the depths of hell so many times that i had to delete the poor thing, i downloaded the model directly from HF, could that be the reason tho? i might re-download it from the loader tomorrow maybe.",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/119/comments",
    "author": "hassakajin",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-10-01T21:08:41Z",
        "body": "It's a lot stronger because of that noise, less noise would make it more stable. Generally the issue with the 1.0 was that it most of the time just generated no motion at all, just slow zooms, and their 1.1 was mostly to address this. I can't say I've seen anything like you describe yet though, but there's been some results with pretty much garbled motion.\r\n\r\nThat said, if you can run the model then it's working as intended, if there was any issue with download etc. it would simply error out."
      },
      {
        "user": "hassakajin",
        "created_at": "2024-10-01T21:38:00Z",
        "body": "> It's a lot stronger because of that noise, less noise would make it more stable. \r\n\r\nyeah really cranking the noise down + lowering cfg made it way more stable, thanks! i had the idea that normal noise would be like 0.4, but around 0.010 it started getting better!\r\n\r\n> Generally the issue with the 1.0 was that it most of the time just generated no motion at all, just slow zooms, and their 1.1 was mostly to address this.\r\n\r\nyeah that was truly an issue, though i think 1.1 was a bit overtuned in that aspect, maybe that's just me... 2b works wonders though strangely, time to get testing everything that works again i guess."
      },
      {
        "user": "cheezecrisp",
        "created_at": "2024-10-02T08:15:16Z",
        "body": "The v1.1 model does produce more significant human dynamics, but is unable to maintain proper human body structure. Attempted to reduce the 'noise_aug_strength' were not effective in improving this, and perhaps the v1.1 model needs further training."
      },
      {
        "user": "hassakajin",
        "created_at": "2024-10-03T06:35:30Z",
        "body": "Before anything else... what is the default \"noise_aug_strength\"? I've been trying to figure this out for the first fun5b model release, is it 0.7 or straight up 0? for me so far the previous model were giving me nice motion, so there is a little trick to it. Perhaps when i'm done setting up my workflow i can share with you, to use it however you like it.\r\n\r\nAlso in regards to the v1.1 version, it works well in pair with caption models like florence, it gives the image motion enough to no be still, but inputting any type of human motion like jumping or walking warps everything,"
      },
      {
        "user": "kijai",
        "created_at": "2024-10-03T09:01:26Z",
        "body": "> Before anything else... what is the default \"noise_aug_strength\"? I've been trying to figure this out for the first fun5b model release, is it 0.7 or straight up 0? for me so far the previous model were giving me nice motion, so there is a little trick to it. Perhaps when i'm done setting up my workflow i can share with you, to use it however you like it.\r\n> \r\n> Also in regards to the v1.1 version, it works well in pair with caption models like florence, it gives the image motion enough to no be still, but inputting any type of human motion like jumping or walking warps everything,\r\n\r\nThe default is the default when you create the node, so 0.056. The value is from the original code."
      },
      {
        "user": "hassakajin",
        "created_at": "2024-10-03T18:05:59Z",
        "body": "thanks"
      }
    ]
  },
  {
    "number": 97,
    "title": "1Torch was not compiled with flash attention",
    "created_at": "2024-09-25T06:30:07Z",
    "closed_at": "2024-10-01T02:07:44Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/97",
    "body": "I know this most likely has nothing to do with Cog, but I'm getting the following:\r\n```ComfyUI\\comfy\\ldm\\modules\\attention.py:407: UserWarning: 1Torch was not compiled with flash attention.```\r\nIt still runs okay, I'm just wondering if this is compromising my quality or speed or anything.\r\n\r\nThanks in advance for any help.",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/97/comments",
    "author": "Vektor369",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-09-25T06:35:05Z",
        "body": "It's normal for Windows currently as flash_attn isn't supported by the prebuilt torch installs. CogVideoX itself doesn't benefit from it either, as far as I know."
      },
      {
        "user": "Vektor369",
        "created_at": "2024-09-25T06:52:35Z",
        "body": "Awesome! Thank you so much kijai!"
      }
    ]
  },
  {
    "number": 95,
    "title": "OneDiff Support",
    "created_at": "2024-09-24T17:03:29Z",
    "closed_at": "2024-09-30T22:39:48Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/95",
    "body": "Hello! \r\nI was running all sample workflows on my system (Linux, PyTorch 2.4, onediff, RTX 4090). \r\nThey all work, but onediff is not being used. \r\n\r\nWhat do I need to trigger it? \r\n\r\nThank you! ",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/95/comments",
    "author": "sandor-lisn",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-09-24T17:06:27Z",
        "body": "You have enabled onediff as the compile option in the model loader node? Note that onediff doesn't work with GGUF so it's only available in the normal model loader node."
      },
      {
        "user": "sandor-lisn",
        "created_at": "2024-09-25T04:21:09Z",
        "body": "Hello, \r\nthank you very much for your lightning fast answer! \r\nMy bad, I should have seen this option myself...\r\n\r\nWhen I enable it, I get a python error. The stacktrace is at the botton of this message. \r\n\r\nI followed the instructions on your main project page. My conda environment contains: \r\n- torch 2.4.0.dev20240324+cu121\r\n- nexfort 0.1.dev271\r\n- onediff 1.2.1.dev24\r\n- onediffx 1.2.1.dev24\r\n\r\nDo you have a suggestion? Thank you!\r\n\r\n========\r\nUnable to load nexfort.{extension} module. Is it compatible with your PyTorch installation?\r\n!!! Exception during processing !!! /home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/nexfort/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK3c105Error4whatEv\r\nTraceback (most recent call last):\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n  File \"/home/sandor/workspace/ComfyUI-flux/custom_nodes/ComfyUI-CogVideoXWrapper/nodes.py\", line 176, in loadmodel\r\n    pipe = compile_pipe(\r\n  File \"/home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/onediffx/compilers/diffusion_pipeline_compiler.py\", line 75, in compile_pipe\r\n    pipe = convert_pipe_to_memory_format(\r\n  File \"/home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/onediffx/compilers/diffusion_pipeline_compiler.py\", line 120, in convert_pipe_to_memory_format\r\n    from nexfort.utils.attributes import multi_recursive_apply\r\n  File \"/home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/nexfort/__init__.py\", line 22, in <module>\r\n    exec(f\"import nexfort.{extension} as {extension}\")\r\n  File \"<string>\", line 1, in <module>\r\nImportError: /home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/nexfort/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK3c105Error4whatEv\r\n"
      },
      {
        "user": "kijai",
        "created_at": "2024-09-25T11:02:20Z",
        "body": "Some issue with nexfort install, not seen that one before. Torch 2.4.1 is on stable release now so I'd suggest installing that, as it works for me using it."
      },
      {
        "user": "sandor-lisn",
        "created_at": "2024-09-26T10:11:16Z",
        "body": "Updating to Torch 2.4.1 did the trick. Thanks so much for your great volunteer efforts! "
      }
    ]
  },
  {
    "number": 85,
    "title": "Question about decoder",
    "created_at": "2024-09-21T21:39:28Z",
    "closed_at": "2024-09-22T09:47:04Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/85",
    "body": "Apologies if this is a stupid question, I have pretty much zero knowledge about how video models work compared to image models.\r\n\r\nIs the way the sampler and decoder currently work somehow similar to generating images in batches with SA or Flux? If so, would it be theoretically possible to unbatch the latents or specify which frame(s) to send to the decoder by index?\r\n\r\nOr am I totally off the mark and the decoder already handles this already, and doesn't decode all frames at once.\r\n\r\nThe reason I got curious about this is because with Fun-xB models (I assume probably caused by the models themselves, not the wrapper) I noticed that it will output extra/duplicated frames at the beginning of image sequence (ie: it outputs 16 frames when selecting 13 frames as length), so I was wondering if it was possible to remove those before sending to the decoder, thus somehow saving on a bit of resources along the way when decoding.\r\n\r\nI tried a few different nodes to separate latents from batch/select by index but obviously it doesn't work ;)",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/85/comments",
    "author": "thelemuet",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-09-21T21:54:28Z",
        "body": "CogVideoX uses a 3D VAE, meaning it also compresses the images temporally: 4 images into one latent. This causes the disparancies you noticed with the frame counts. The decoding is done 2 \"frames\" at the time by design, it can't be less. If memory for decoding is a concern, the VAE tiling works pretty well with the tile size set to half of the dimensions."
      },
      {
        "user": "thelemuet",
        "created_at": "2024-09-21T22:33:20Z",
        "body": "Ah, makes completes sense, thank you for the explanation.\r\n\r\nFunnily enough, another reason I was messing with the latents is because yesterday I had issues with the VAE tiling resulting in very obvious seams. As an alternative I was splitting the latents with some padding using the core \"crop latent\" node before sending to the decoder, then stitching the images back after that myself. Clearly the latent shape was wrong, cropping the Width was cropping the Height and cropping the Height did nothing, but it did actually work even if not as intended hehe,\r\n\r\nBut looks like there are no more visible seams with VAE tiling after updating today, so thank you, definitely much more convenient ;)"
      },
      {
        "user": "kijai",
        "created_at": "2024-09-21T23:23:00Z",
        "body": "> Ah, makes completes sense, thank you for the explanation.\n> \n> Funnily enough, another reason I was messing with the latents is because yesterday I had issues with the VAE tiling resulting in very obvious seams. As an alternative I was splitting the latents with some padding using the core \"crop latent\" node before sending to the decoder, then stitching the images back after that myself. Clearly the latent shape was wrong, cropping the Width was cropping the Height and cropping the Height did nothing, but it did actually work even if not as intended hehe,\n> \n> But looks like there are no more visible seams with VAE tiling after updating today, so thank you, definitely much more convenient ;)\n\nYes they defaults were just awful before, I got the values from the initial code before these new models and as I never really used it myself, I didn't realise they should be completely different. 96x96 tiles made no sense in pixel space especially, half of the image dimension seems fine now and no seams with 0.2 overlap."
      }
    ]
  },
  {
    "number": 77,
    "title": "Error occurred when executing DownloadAndLoadCogVideoModel",
    "created_at": "2024-09-20T16:59:48Z",
    "closed_at": "2024-09-20T17:45:45Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/77",
    "body": "When I tried CogVideoX-5b-I2V, I encountered this problem. Does anyone know how to solve it\uff1f\r\n![Uploading PixPin_2024-09-21_00-55-09.jpg\u2026]()\r\n",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/77/comments",
    "author": "TryingToDoBetter25",
    "comments": [
      {
        "user": "TryingToDoBetter25",
        "created_at": "2024-09-20T17:01:37Z",
        "body": "Traceback (most recent call last):\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\ComfyUI\\execution.py\", line 152, in recursive_execute\r\n    output_data, output_ui = get_output_data(obj, input_data_all)\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\ComfyUI\\execution.py\", line 82, in get_output_data\r\n    return_values = map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\ComfyUI\\execution.py\", line 75, in map_node_over_list\r\n    results.append(getattr(obj, func)(**slice_dict(input_data_all, i)))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\nodes.py\", line 98, in loadmodel\r\n    transformer = CogVideoXTransformer3DModel.from_pretrained(base_path, subfolder=\"transformer\")\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\python_embeded\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\python_embeded\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 774, in from_pretrained\r\n    accelerate.load_checkpoint_and_dispatch(\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\python_embeded\\Lib\\site-packages\\accelerate\\big_modeling.py\", line 613, in load_checkpoint_and_dispatch\r\n    load_checkpoint_in_model(\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 1821, in load_checkpoint_in_model\r\n    set_module_tensor_to_device(\r\n  File \"F:\\1AI\\1\\ComfyUI_CogVideoX-\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 341, in set_module_tensor_to_device\r\n    raise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")\r\nValueError: CogVideoXPatchEmbed(\r\n  (proj): Conv2d(32, 3072, kernel_size=(2, 2), stride=(2, 2))\r\n  (text_proj): Linear(in_features=4096, out_features=3072, bias=True)\r\n) does not have a parameter or a buffer named pos_embedding.\r\n"
      },
      {
        "user": "kijai",
        "created_at": "2024-09-20T17:25:44Z",
        "body": "Did you update diffusers to 0.30.3?"
      },
      {
        "user": "TryingToDoBetter25",
        "created_at": "2024-09-20T17:45:37Z",
        "body": "> Did you update diffusers to 0.30.3?\r\n\r\nI reinstalled it and it ran successfully. Thank you very much"
      }
    ]
  },
  {
    "number": 62,
    "title": "Error occurred when executing DownloadAndLoadCogVideoModel:  CogVideoXPatchEmbed( (proj): Conv2d(32, 3072, kernel_size=(2, 2), stride=(2, 2)) (text_proj): Linear(in_features=4096, out_features=3072, bias=True) ) does not have a parameter or a buffer named pos_embedding.",
    "created_at": "2024-09-18T21:05:16Z",
    "closed_at": "2024-09-18T21:15:36Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/62",
    "body": "Error occurred when executing DownloadAndLoadCogVideoModel:\r\n\r\nCogVideoXPatchEmbed(\r\n(proj): Conv2d(32, 3072, kernel_size=(2, 2), stride=(2, 2))\r\n(text_proj): Linear(in_features=4096, out_features=3072, bias=True)\r\n) does not have a parameter or a buffer named pos_embedding.\r\n\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 317, in execute\r\noutput_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 192, in get_output_data\r\nreturn_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\nprocess_inputs(input_dict, i)\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 158, in process_inputs\r\nresults.append(getattr(obj, func)(**inputs))\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\nodes.py\", line 92, in loadmodel\r\ntransformer = CogVideoXTransformer3DModel.from_pretrained(base_path, subfolder=\"transformer\")\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\r\nreturn fn(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 774, in from_pretrained\r\naccelerate.load_checkpoint_and_dispatch(\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\big_modeling.py\", line 613, in load_checkpoint_and_dispatch\r\nload_checkpoint_in_model(\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 1821, in load_checkpoint_in_model\r\nset_module_tensor_to_device(\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 341, in set_module_tensor_to_device\r\nraise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/62/comments",
    "author": "brad12d3",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-09-18T21:07:50Z",
        "body": "I2V model requires diffusers version 0.30.3 which came out yesterday."
      },
      {
        "user": "brad12d3",
        "created_at": "2024-09-18T21:15:34Z",
        "body": "Yup that was it, thanks. I guess I missed that.\r\n"
      }
    ]
  },
  {
    "number": 21,
    "title": "\"Prompt outputs failed validation: Value not in list\"",
    "created_at": "2024-08-28T07:19:18Z",
    "closed_at": "2024-08-28T15:51:50Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/21",
    "body": "I've updated ComfyUI, and I installed the latest CogVideoXWrapper through ComfyUI manager via this Git's URL. I've loaded the \"cogvideox_5b_example_01.json\" workflow, and pointed the Load Clip node to my existing model (t5xxl_fp8_e4m3fn.safetensors).\r\n\r\nAfter hitting queue prompt, I get this error:\r\n\r\nPrompt outputs failed validation: Value not in list: format: 'video/nvenc_h264-mp4' not in ['image/gif', 'image/webp', 'video/ProRes', 'video/av1-webm', 'video/h264-mp4', 'video/h265-mp4', 'video/webm']\r\nVHS_VideoCombine:\r\n    - Value not in list: format: 'video/nvenc_h264-mp4' not in ['image/gif', 'image/webp', 'video/ProRes', 'video/av1-webm', 'video/h264-mp4', 'video/h265-mp4', 'video/webm']",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/21/comments",
    "author": "Gyramuur",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-08-28T12:49:51Z",
        "body": "This is because I forgot the nvenc format selected on the video combine node, it's not supported by all platforms. Just change to some other encode format or recreate the video combine node to get past that."
      },
      {
        "user": "Gyramuur",
        "created_at": "2024-08-28T15:51:50Z",
        "body": "That fixed it, thanks. :) Now getting a separate error but I'll open a different issue."
      }
    ]
  },
  {
    "number": 9,
    "title": "Results from this are different to official CogVideoX demo and I can't figure out how to match it.",
    "created_at": "2024-08-10T07:37:18Z",
    "closed_at": "2024-08-11T06:55:11Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/9",
    "body": "Using the default settings provided with the example workflow, with 16 frames at 8 fps, it only renders a two second video (as expected). But by default, CogVideoX does 6 second clips. However, upping the amount of frames to 48 does not have the intended effect and instead generates three disjointed clips, like three 2 second clips stuck together. \r\n\r\nHow do I get a single 6 second video like it's supposed to do?",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/9/comments",
    "author": "Gyramuur",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-08-10T14:12:41Z",
        "body": "Which workflow exactly? I added the feature to use temporal tiling (t_tile) as context windows, if the value is lower than your frame count it creates the video in what you call \"disjointed clips\", so you'd want to put that to 48 to get the max clips. The point of the t_tile is that you can also go above the 48, though the clips are usually not joined too well together, sometimes it works nicely."
      },
      {
        "user": "Gyramuur",
        "created_at": "2024-08-11T06:55:11Z",
        "body": "> Which workflow exactly? I added the feature to use temporal tiling (t_tile) as context windows, if the value is lower than your frame count it creates the video in what you call \"disjointed clips\", so you'd want to put that to 48 to get the max clips. The point of the t_tile is that you can also go above the 48, though the clips are usually not joined too well together, sometimes it works nicely.\r\n\r\nI'm using the default example_01.json provided in the examples folder. Also increasing t_tile_length along with num_frames fixes it, thank you. :) I wasn't sure what the t_tile_length parameter did.\r\n\r\nAlso it's unrelated to this issue, so I'll close it out anyway, but when you load the aforementioned workflow, it throws an error saying the t_tile_overlap of 2 is less than the minimum of 8."
      }
    ]
  }
]