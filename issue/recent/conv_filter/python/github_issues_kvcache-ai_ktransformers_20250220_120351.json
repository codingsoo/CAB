[
  {
    "number": 293,
    "title": "\u662f\u5426\u652f\u6301 A6000 \u6216\u8005 A100 \u5355\u5361\uff1f",
    "created_at": "2025-02-14T11:39:20Z",
    "closed_at": "2025-02-16T12:01:45Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/293",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/293/comments",
    "author": "Liangdi",
    "comments": [
      {
        "user": "vc5409ftu",
        "created_at": "2025-02-15T23:45:47Z",
        "body": "\u53ef\u4ee5"
      },
      {
        "user": "Liangdi",
        "created_at": "2025-02-16T12:01:45Z",
        "body": "> \u53ef\u4ee5\n\n\u8c22\u8c22"
      }
    ]
  },
  {
    "number": 100,
    "title": "Does ktransformers support deepseek V2.5\uff1f",
    "created_at": "2024-10-12T11:34:10Z",
    "closed_at": "2024-10-15T06:37:28Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/100",
    "body": null,
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/100/comments",
    "author": "huliangbing",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-10-14T06:58:02Z",
        "body": "Yes\uff5eyou can run v2.5 with deepseekv2\u2018s yaml."
      },
      {
        "user": "huliangbing",
        "created_at": "2024-10-14T17:39:28Z",
        "body": "Thank you very much!"
      }
    ]
  },
  {
    "number": 95,
    "title": "Suggestion to add DeepSeek v2.5 support",
    "created_at": "2024-09-24T08:40:58Z",
    "closed_at": "2024-09-25T08:49:30Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/95",
    "body": "Recently DeepSeek 2.5 MoE model was released. Please consider adding a support for it. (Q5_K_M or Q4_K_M)",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/95/comments",
    "author": "arisau",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-24T10:31:11Z",
        "body": "It appears to be supported automatically, as some users have already tested it successfully."
      },
      {
        "user": "arisau",
        "created_at": "2024-09-24T21:04:31Z",
        "body": "Perfect. Thank you. \r\njust to confirm, only Q4_K_M works?"
      },
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-25T04:45:04Z",
        "body": "We have supported all Qx_km format~"
      },
      {
        "user": "arisau",
        "created_at": "2024-09-25T08:49:30Z",
        "body": "Understood. Thank you once again. "
      }
    ]
  },
  {
    "number": 82,
    "title": "Seg Fault on long replies",
    "created_at": "2024-09-10T14:44:09Z",
    "closed_at": "2024-09-11T13:43:47Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/82",
    "body": "I am trying to use this as a llama.cpp replacement as it is a lot faster.  I did modify the backend args file (`ktransformers/server/backend/args.py`) as max_new_tokens isn't an option for the sever:\r\n```\r\n-    max_new_tokens: int = Field(500, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n+    max_new_tokens: int = Field(2040, description=\"Max new tokens per completion. For this example applies to all jobs\")\r\n```\r\n\r\nto allow for longer responses as 500 tokens was causing a lot of my stuff to get cut off half way through.\r\nIt does generate some tokens (feels like about the 500 limit) and then hard crashes.  So maybe there is some other limit that I need to adjust?\r\n\r\nI am using DeekSeek-V2.5 in Q4_K_M, I did also try it with WizardLM 8x22B and the same thing happens.\r\n\r\nHardware: 1x 3090, Epyc 7402, 512 Gb Ram\r\n\r\n```\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\r\n../ktransformers.sh: line 12: 2152563 Segmentation fault      (core dumped) ktransformers --model_path /nvmes/models/DeepSeek-V2.5/ --gguf_path /nvmes/models/DeepSeek-V2.5-GGUF2/ --port 8081 --cpu_infer 32\r\n```",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/82/comments",
    "author": "matthusby",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-11T06:38:22Z",
        "body": "Hi, maybe this one will help #73 "
      },
      {
        "user": "matthusby",
        "created_at": "2024-09-11T13:43:47Z",
        "body": "Thank you, yeah changing the `cache_lens` param is what was causing my problem.  I guess my chat was at just the right length where the longer max tokens would trigger that error.\r\n\r\nFor reference I doubled it and now when I try to run DeepSeek-V2.5 I get a out of memory crash, but on Wizard 8x22 its working great.\r\n\r\nIt looks like there are plans to address some of this soon, so I will close this issue."
      }
    ]
  },
  {
    "number": 73,
    "title": "When the input token exceeds 4096, an error will occur.",
    "created_at": "2024-09-02T06:23:20Z",
    "closed_at": "2024-09-03T11:34:18Z",
    "labels": [],
    "url": "https://github.com/kvcache-ai/ktransformers/issues/73",
    "body": "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/fastapi/routing.py\", line 210, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/api/openai/endpoints/chat.py\", line 32, in chat_completion\r\n    async for token in interface.inference(input_message,id):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 323, in inference\r\n    for t in self.prefill(input_ids,self.check_is_new(thread_id)):\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\r\n    response = gen.send(None)\r\n               ^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/interfaces/transformers.py\", line 272, in prefill\r\n    logits = self.model(\r\n             ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1731, in forward\r\n    outputs = self.model(\r\n              ^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/operators/models.py\", line 651, in forward\r\n    causal_mask = self._update_causal_mask(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/models/modeling_deepseek.py\", line 1624, in _update_causal_mask\r\n    padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\r\n                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nRuntimeError: The size of tensor a (4096) must match the size of tensor b (8122) at non-singleton dimension 3",
    "comments_url": "https://api.github.com/repos/kvcache-ai/ktransformers/issues/73/comments",
    "author": "fengyang95",
    "comments": [
      {
        "user": "Azure-Tang",
        "created_at": "2024-09-03T09:17:30Z",
        "body": "Hi, I haven\u2019t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?"
      },
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T09:30:43Z",
        "body": "> Hi, I haven\u2019t encountered this issue in our most recent code update. Could you please try using the latest version and let me know if the problem persists?\r\n\r\nI compiled it using the most recent code, and my configs are: \r\n``` yaml\r\n- match:\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding\r\n  replace:\r\n    class: ktransformers.operators.RoPE.YarnRotaryEmbedding\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\.(?!.*self_attn\\\\.kv_b_proj).*$\"  # regular expression\r\n    class: torch.nn.Linear  # only match modules matching name and class simultaneously\r\n  replace:\r\n    class: ktransformers.operators.linear.KTransformersLinear  # optimized Kernel on quantized data types\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n      generate_op: \"KLinearMarlin\"\r\n      prefill_op: \"KLinearTorch\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp$\"\r\n    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE\r\n  replace:\r\n    class: ktransformers.operators.experts.KDeepseekV2MoE     # mlp module with custom forward function\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.mlp\\\\.experts$\"\r\n  replace:\r\n    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism\r\n    kwargs:\r\n      prefill_device: \"cuda\"\r\n      prefill_op: \"KExpertsTorch\"\r\n      generate_device: \"cpu\"\r\n      generate_op: \"KExpertsCPU\"\r\n      out_device: \"cuda\"\r\n  recursive: False # don't recursively inject submodules of this module\r\n- match:\r\n    name: \"^model\\\\.layers\\\\..*\\\\.self_attn$\"\r\n  replace:\r\n    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation\r\n    kwargs:\r\n      generate_device: \"cuda\"\r\n      prefill_device: \"cuda\"\r\n- match:\r\n    name: \"^model$\"\r\n  replace:\r\n    class: \"ktransformers.operators.models.KDeepseekV2Model\"\r\n    kwargs:\r\n      per_layer_prefill_intput_threshold: 0 # 0 is close layer wise prefill\r\n- match:\r\n    name: \"^model.embed_tokens\"\r\n  replace:\r\n    class: \"default\"\r\n    kwargs:\r\n      generate_device: \"cpu\"\r\n      prefill_device: \"cuda\"\r\n```\r\n\r\n\r\n"
      },
      {
        "user": "UnicornChan",
        "created_at": "2024-09-03T10:05:26Z",
        "body": "I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release.\r\nIf you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens"
      },
      {
        "user": "fengyang95",
        "created_at": "2024-09-03T11:34:15Z",
        "body": "> I'm very sorry. For the convenience of server testing, we set the cache_lens parameter to 4096, which has caused errors when the cache length exceeds this value. This configuration item is hardcoded as \"cache_lens\" in ktransformers/server/backend/args.py. We will make these configuration items configurable in the next release. If you want to support more tokens now, I suggest to modify the config in `/home/tiger/.pyenv/versions/3.11.2/lib/python3.11/site-packages/ktransformers/server/backend/args.py` line 93, cache_lens\r\n\r\nLGTM"
      }
    ]
  }
]