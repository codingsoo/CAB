[
  {
    "number": 18,
    "title": "12GB VRAM getting OOM with Q8_0, but not in Forge.",
    "created_at": "2024-08-15T23:32:44Z",
    "closed_at": "2024-08-16T04:25:56Z",
    "labels": [],
    "url": "https://github.com/city96/ComfyUI-GGUF/issues/18",
    "body": "Able to use Forge with default settings to run the Q8_0 variant on a 3060 w 12GB VRAM and 32GB sys RAM.\r\n\r\nBut with this node in Comfy I always get OOM trying to load the models up for inference. Tried --lowvram mode too.",
    "comments_url": "https://api.github.com/repos/city96/ComfyUI-GGUF/issues/18/comments",
    "author": "caustiq",
    "comments": [
      {
        "user": "city96",
        "created_at": "2024-08-16T02:37:29Z",
        "body": "Just pushed a patch that should fix this. Please test it!"
      },
      {
        "user": "NHLOCAL",
        "created_at": "2024-08-16T04:00:24Z",
        "body": "I also had the same problem with lowvram, now the problem is solved with the update push!\r\n\r\nBut a comment is still received when loading the model:\r\n```\r\nC:\\Users\\Public\\StableSwarmUI\\StableSwarmUI-master\\dlbackend\\comfy\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\dequant.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n  data = torch.tensor(tensor.data)\r\nC:\\Users\\Public\\StableSwarmUI\\StableSwarmUI-master\\dlbackend\\comfy\\ComfyUI\\comfy\\ldm\\modules\\attention.py:407: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\r\n```"
      },
      {
        "user": "city96",
        "created_at": "2024-08-16T04:25:56Z",
        "body": "Those log messages can be safely ignored."
      },
      {
        "user": "caustiq",
        "created_at": "2024-08-16T07:49:32Z",
        "body": "Confirmed working here.\r\n\r\nNice one man!\n\n---\n\nedit: ran all night with no issues\r\n\r\n==\r\nI did get an OOM after about 20-30 gens of smooth sailing. \r\nI turned off Xorg and ran headless to free up a little more cause I was at 94-96% VRAM usage.\r\nBut it seems like Comfy will just use even more if it can, so still riding close to the edge.\r\nDunno if it's related to your changes so just thought I'd report that one OOM.\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Resolves OOM errors when loading Q8_0 models on 12GB VRAM GPUs in ComfyUI",
      "Maintains stable VRAM usage during prolonged operation",
      "Compatibility with system resource constraints (e.g., headless operation)",
      "Clear distinction between critical errors and ignorable warnings"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 01:47:58"
    }
  }
]