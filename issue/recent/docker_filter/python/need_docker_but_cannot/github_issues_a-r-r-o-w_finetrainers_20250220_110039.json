[
  {
    "number": 143,
    "title": "nan loss",
    "created_at": "2024-12-24T08:21:25Z",
    "closed_at": "2024-12-26T01:43:07Z",
    "labels": [],
    "url": "https://github.com/a-r-r-o-w/finetrainers/issues/143",
    "body": "### System Info / \u7cfb\u7d71\u4fe1\u606f\r\n\r\nCUDA:  12.4  \r\npython: 3.10\r\ndiffusers: 0.33.0.dev0\r\n\r\n### Information / \u95ee\u9898\u4fe1\u606f\r\n\r\n- [ ] The official example scripts / \u5b98\u65b9\u7684\u793a\u4f8b\u811a\u672c\r\n- [X] My own modified scripts / \u6211\u81ea\u5df1\u4fee\u6539\u7684\u811a\u672c\u548c\u4efb\u52a1\r\n\r\n### Reproduction / \u590d\u73b0\u8fc7\u7a0b\r\n\r\n1. I randomly selected a few videos to fine-tune hunyuanvideo\r\n2. But I ended up with nan loss at the first step.\r\n3. the log is \r\n`Training steps:   2%|\u2588\u2588                                                                                                        | 1/50 [00:56<46:19, 56.73s/it, loss=nan, lr=2e-7]`\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior / \u671f\u5f85\u8868\u73b0\r\n\r\nGet the correct loss value.",
    "comments_url": "https://api.github.com/repos/a-r-r-o-w/finetrainers/issues/143/comments",
    "author": "tanshuai0219",
    "comments": [
      {
        "user": "a-r-r-o-w",
        "created_at": "2024-12-25T22:24:39Z",
        "body": "@tanshuai0219 I believe we interacted in the Diffusers issue too. Upgrading pytorch to 2.5.1 has been confirmed to fix the nan training loss problem by atleast 3 other people now. Could you give it a try?"
      },
      {
        "user": "tanshuai0219",
        "created_at": "2024-12-26T01:43:07Z",
        "body": "> @tanshuai0219 I believe we interacted in the Diffusers issue too. Upgrading pytorch to 2.5.1 has been confirmed to fix the nan training loss problem by atleast 3 other people now. Could you give it a try?\r\n\r\nYes,it works for me~ The loss value is normal now. Thanks!"
      },
      {
        "user": "BlackTea-c",
        "created_at": "2025-01-17T03:42:20Z",
        "body": "same problem.butmy torch version is 2.5.1"
      }
    ],
    "satisfaction_conditions": [
      "Identifies and resolves the root cause of NaN loss during training",
      "Ensures framework/library version compatibility"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 01:58:12"
    }
  }
]