[
  {
    "number": 249,
    "title": "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
    "created_at": "2024-11-11T08:49:13Z",
    "closed_at": "2024-11-19T07:19:54Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/HKUDS/LightRAG/issues/249",
    "body": "I am using a huggingface demo, but with a local model.How to deal with it?please help me , thank you very much!!\r\n**code**:\r\nimport os\r\n\r\nfrom lightrag import LightRAG, QueryParam\r\nfrom lightrag.llm import hf_model_complete, hf_embedding\r\nfrom lightrag.utils import EmbeddingFunc\r\nfrom transformers import AutoModel, AutoTokenizer\r\n\r\nWORKING_DIR = \"./dickens\"\r\n\r\nif not os.path.exists(WORKING_DIR):\r\n    os.mkdir(WORKING_DIR)\r\n\r\nrag = LightRAG(\r\n    working_dir=WORKING_DIR,\r\n    llm_model_func=hf_model_complete,\r\n    llm_model_name=\"/data/Qwen2.5-14B-Instruct\",\r\n    embedding_func=EmbeddingFunc(\r\n        embedding_dim=1024,\r\n        max_token_size=5000,\r\n        func=lambda texts: hf_embedding(\r\n            texts,\r\n            tokenizer=AutoTokenizer.from_pretrained(\r\n                r\"/data/project/raag/bge-large-zh-v1.5/\", model_max_length=512\r\n            ),\r\n            embed_model=AutoModel.from_pretrained(\r\n                r\"/data/project/raag/bge-large-zh-v1.5/\"\r\n            ),\r\n        ),\r\n    ),\r\n)\r\n\r\nwith open(r\"/data/project/raag/caiwu.txt\", \"r\", encoding=\"utf-8\") as f:\r\n    rag.insert(f.read())\r\n\r\n# Perform naive search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\r\n)\r\n\r\n# Perform local search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\r\n)\r\n\r\n# Perform global search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\r\n)\r\n\r\n# Perform hybrid search\r\nprint(\r\n    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\r\n)\r\nerror:\r\nroot@c15e0721d1a6:~# CUDA_VISIBLE_DEVICES=0  python /data/project/raag/light_rag.py\r\nINFO:lightrag:Logger initialized for working directory: /data/project/raag/dickens\r\nDEBUG:lightrag:LightRAG init with param:\r\n  working_dir = /data/project/raag/dickens,\r\n  chunk_token_size = 1200,\r\n  chunk_overlap_token_size = 100,\r\n  tiktoken_model_name = gpt-4o-mini,\r\n  entity_extract_max_gleaning = 1,\r\n  entity_summary_to_max_tokens = 500,\r\n  node_embedding_algorithm = node2vec,\r\n  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},\r\n  embedding_func = {'embedding_dim': 1024, 'max_token_size': 5000, 'func': <function <lambda> at 0x7f5eaa0bfd90>},\r\n  embedding_batch_num = 32,\r\n  embedding_func_max_async = 16,\r\n  llm_model_func = <function hf_model_complete at 0x7f5dc1cd6b00>,\r\n  llm_model_name = /data/Qwen2.5-14B-Instruct,\r\n  llm_model_max_token_size = 32768,\r\n  llm_model_max_async = 16,\r\n  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,\r\n  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,\r\n  vector_db_storage_cls_kwargs = {},\r\n  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,\r\n  enable_llm_cache = True,\r\n  addon_params = {},\r\n  convert_response_to_json_func = <function convert_response_to_json at 0x7f5dc1cbfeb0>\r\n\r\nINFO:lightrag:Load KV full_docs with 0 data\r\nINFO:lightrag:Load KV text_chunks with 0 data\r\nINFO:lightrag:Load KV llm_response_cache with 0 data\r\nINFO:lightrag:Loaded graph from /data/project/raag/dickens/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges\r\nINFO:nano-vectordb:Load (0, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_entities.json'} 0 data\r\nINFO:nano-vectordb:Load (0, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_relationships.json'} 0 data\r\nINFO:nano-vectordb:Load (2, 1024) data\r\nINFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '/data/project/raag/dickens/vdb_chunks.json'} 2 data\r\nINFO:lightrag:Creating a new event loop in a sub-thread.\r\nINFO:lightrag:[New Docs] inserting 1 docs\r\nINFO:lightrag:[New Chunks] inserting 2 chunks\r\nINFO:lightrag:Inserting 2 vectors to chunks\r\nINFO:lightrag:[Entity Extraction]...\r\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\r\n/usr/local/lib/python3.10/site-packages/accelerate/utils/modeling.py:1390: UserWarning: Current model requires 12544 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\r\n  warnings.warn(\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:07<00:00,  1.00it/s]\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\r\n  warnings.warn(\r\n/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\r\n  warnings.warn(\r\nINFO:lightrag:Writing graph with 0 nodes, 0 edges\r\nTraceback (most recent call last):\r\n  File \"/data/project/raag/light_rag.py\", line 33, in <module>\r\n    rag.insert(f.read())\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/lightrag.py\", line 164, in insert\r\n    return loop.run_until_complete(self.ainsert(string_or_strings))\r\n  File \"/usr/local/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n    return future.result()\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/lightrag.py\", line 211, in ainsert\r\n    maybe_new_kg = await extract_entities(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/operate.py\", line 331, in extract_entities\r\n    results = await asyncio.gather(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/operate.py\", line 270, in _process_single_content\r\n    final_result = await use_llm_func(hint_prompt)\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/utils.py\", line 87, in wait_func\r\n    result = await func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/llm.py\", line 377, in hf_model_complete\r\n    return await hf_model_if_cache(\r\n  File \"/usr/local/lib/python3.10/site-packages/lightrag/llm.py\", line 286, in hf_model_if_cache\r\n    output = hf_model.generate(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2215, in generate\r\n    result = self._sample(\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3206, in _sample\r\n    outputs = self(**model_inputs, return_dict=True)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1164, in forward\r\n    outputs = self.model(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 854, in forward\r\n    inputs_embeds = self.embed_tokens(input_ids)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 190, in forward\r\n    return F.embedding(\r\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/functional.py\", line 2551, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\r\n",
    "comments_url": "https://api.github.com/repos/HKUDS/LightRAG/issues/249/comments",
    "author": "Z-oo883",
    "comments": [
      {
        "user": "CooFlow",
        "created_at": "2024-11-11T12:49:00Z",
        "body": "I solved it by change the code in llm.py\r\n\r\n`device_map='auto'` to `device_map=None`\r\n\r\n```\r\n@lru_cache(maxsize=1)\r\ndef initialize_hf_model(model_name):\r\n    hf_tokenizer = AutoTokenizer.from_pretrained(\r\n        model_name, device_map=None, trust_remote_code=True\r\n    )\r\n    hf_model = AutoModelForCausalLM.from_pretrained(\r\n        model_name, device_map=None, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\r\n    if hf_tokenizer.pad_token is None:\r\n        hf_tokenizer.pad_token = hf_tokenizer.eos_token\r\n\r\n    return hf_model, hf_tokenizer\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Ensures all model components are on the same compute device",
      "Provides guidance on proper device allocation for local Hugging Face models",
      "Explains how to control model loading location in Hugging Face pipelines",
      "Addresses proper GPU utilization for model components"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 02:05:45"
    }
  }
]