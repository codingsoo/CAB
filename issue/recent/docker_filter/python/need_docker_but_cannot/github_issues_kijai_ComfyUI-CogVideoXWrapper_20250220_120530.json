[
  {
    "number": 136,
    "title": "Error when running fastmode",
    "created_at": "2024-10-07T12:06:11Z",
    "closed_at": "2024-10-07T18:06:43Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/136",
    "body": "When I try activating fastmode, I get the error CUDA error: \r\n\r\nCUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`.\r\n\r\nI suspect my CUDA version is not correct, I have v11.8. Which one do I need?\r\n\r\nRTX 4090, Windows 11.",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/136/comments",
    "author": "jpgallegoar",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-10-07T12:41:03Z",
        "body": "You need torch that's compiled for cuda 124."
      },
      {
        "user": "jpgallegoar",
        "created_at": "2024-10-07T17:19:04Z",
        "body": "> You need torch that's compiled for cuda 124.\r\n\r\nThank you, that worked."
      }
    ],
    "satisfaction_conditions": [
      "Identifies CUDA version compatibility requirements for the RTX 4090 GPU",
      "Specifies PyTorch/CUDA version alignment requirements"
    ],
    "_classification": {
      "category": "Requires build environment but hard to be dockerized",
      "timestamp": "2025-04-05 01:50:21"
    }
  }
]