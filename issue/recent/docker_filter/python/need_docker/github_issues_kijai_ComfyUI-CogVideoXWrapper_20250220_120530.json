[
  {
    "number": 267,
    "title": "Error using Tora example workflow: RuntimeError: Input type (struct c10::BFloat16) and bias type (struct c10::Half) should be the same",
    "created_at": "2024-11-20T14:27:54Z",
    "closed_at": "2024-11-20T15:27:21Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/267",
    "body": "Hi,\r\nThanks for all of your work on this! \r\nI am fully updated as of 30 minutes ago, and other workflows I've tried are working well. However when trying the Tora example workflow in your repository, I get the below error.\r\nIf it matters, I am on Python 3.11.9, Torch 2.51, and CUDA 12.4. \r\nThank you!\r\n\r\n```\r\nDownloading model to: D:\\ComfyUI\\models\\CogVideo\\CogVideoX-Fun-V1.1-5b-InP\r\nFetching 5 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [04:25<00:00, 53.12s/it]\r\nThe config attributes {'add_noise_in_inpaint_model': True} were passed to CogVideoXTransformer3DModel, but are not expected and will be ignored. Please verify your config.json configuration file.\r\nend_vram - start_vram: 13008260046 - 1867276110 = 11140983936\r\n#80 [DownloadAndLoadCogVideoModel]: 274.52s - vram 11140983936b\r\nDownloading Fuser model to: D:\\ComfyUI\\models\\CogVideo\\CogVideoX-5b-Tora\\fuser\\fuser.safetensors\r\nFetching 1 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:32<00:00, 32.42s/it]\r\nDownloading trajectory extractor model to: D:\\ComfyUI\\models\\CogVideo\\CogVideoX-5b-Tora\\traj_extractor\\traj_extractor.safetensors\r\nFetching 1 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.51s/it]\r\nend_vram - start_vram: 14446958702 - 13008260046 = 1438698656\r\n#75 [DownloadAndLoadToraModel]: 35.23s - vram 1438698656b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#72 [LoadImage]: 0.02s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#73 [ImageResizeKJ]: 0.00s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#60 [SplineEditor]: 0.11s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#67 [GetMaskSizeAndCount]: 0.00s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#85 [SplineEditor]: 0.07s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#82 [SplineEditor]: 0.07s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#83 [AppendStringsToList]: 0.00s - vram 0b\r\nend_vram - start_vram: 14446958702 - 14446958702 = 0\r\n#86 [AppendStringsToList]: 0.00s - vram 0b\r\nreceived 3 trajectorie(s)\r\nvideo_flow shape after encoding: torch.Size([1, 16, 13, 60, 90])\r\nend_vram - start_vram: 16117826884 - 14446958702 = 1670868182\r\n#78 [ToraEncodeTrajectory]: 4.74s - vram 1670868182b\r\nend_vram - start_vram: 14724537294 - 14724537294 = 0\r\n#66 [VHS_VideoCombine]: 0.32s - vram 0b\r\nend_vram - start_vram: 14724537294 - 14724537294 = 0\r\n#65 [CreateShapeImageOnPath]: 0.21s - vram 0b\r\nEncoded latents shape: torch.Size([1, 13, 16, 60, 90])\r\nend_vram - start_vram: 16395405476 - 14724537294 = 1670868182\r\n#93 [CogVideoImageEncodeFunInP]: 3.57s - vram 1670868182b\r\nend_vram - start_vram: 14726924094 - 14726924094 = 0\r\n#20 [CLIPLoader]: 2.17s - vram 0b\r\nRequested to load SD3ClipModel_\r\nLoading 1 new model\r\nloaded partially 64.0 32.38671875 0\r\nend_vram - start_vram: 24471274066 - 14726924094 = 9744349972\r\n#30 [CogVideoTextEncode]: 12.29s - vram 9744349972b\r\nUnloading models for lowram load.\r\n1 models unloaded.\r\nLoading 1 new model\r\nloaded partially 6805.310731887817 6779.38671875 0\r\nend_vram - start_vram: 24251545410 - 24251545410 = 0\r\n#31 [CogVideoTextEncode]: 5.73s - vram 0b\r\nReceived 13 image conditioning frames\r\nContext schedule disabled\r\nTora trajectory length: 13\r\nSampling 49 frames in 13 latent frames at 720x480 with 40 inference steps\r\n  0%|                                                                                                        | 0/40 [00:00<?, ?it/s]\r\n!!! Exception during processing !!! Input type (struct c10::BFloat16) and bias type (struct c10::Half) should be the same\r\nTraceback (most recent call last):\r\n  File \"D:\\ComfyUI\\execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"D:\\ComfyUI\\execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\nodes.py\", line 696, in process\r\n    latents = model[\"pipe\"](\r\n              ^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\pipeline_cogvideox.py\", line 757, in __call__\r\n    noise_pred = self.transformer(\r\n                 ^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\custom_cogvideox_transformer_3d.py\", line 688, in forward\r\n    hidden_states, encoder_hidden_states = block(\r\n                                           ^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\custom_cogvideox_transformer_3d.py\", line 266, in forward\r\n    h = fuser(h, video_flow_feature.to(h), T=T)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\tora\\traj_module.py\", line 287, in forward\r\n    gamma_flow = self.flow_gamma_spatial(flow)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 554, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\ComfyUI\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 549, in _conv_forward\r\n    return F.conv2d(\r\n           ^^^^^^^^^\r\nRuntimeError: Input type (struct c10::BFloat16) and bias type (struct c10::Half) should be the same\r\n\r\nend_vram - start_vram: 16169275134 - 14726924094 = 1442351040\r\n#79 [CogVideoSampler]: 0.66s - vram 1442351040b\r\n```",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/267/comments",
    "author": "EnragedAntelope",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-11-20T14:42:15Z",
        "body": "Thanks for the report, indeed I broke it with last update, should be fixed now."
      },
      {
        "user": "EnragedAntelope",
        "created_at": "2024-11-20T15:27:21Z",
        "body": "Confirmed fixed, thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Resolves data type mismatch between model layers",
      "Maintains compatibility with user's specified environment (Python 3.11.9/Torch 2.51/CUDA 12.4)",
      "Ensures Tora example workflow executes without runtime errors"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:49:56"
    }
  },
  {
    "number": 245,
    "title": "1.5test branch AssertionError('First input (fp16) and second input (bf16) must have the same dtype!')",
    "created_at": "2024-11-18T08:00:10Z",
    "closed_at": "2024-11-18T08:13:45Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/245",
    "body": "   noise_pred = self.transformer(\r\n                 ^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 685, in forward\r\n    hidden_states, encoder_hidden_states = block(\r\n                                           ^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 282, in forward\r\n    attn_hidden_states, attn_encoder_hidden_states = self.attn1(\r\n                                                     ^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/diffusers/models/attention_processor.py\", line 495, in forward\r\n    return self.processor(\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 129, in __call__\r\n    hidden_states = sageattn_func(query, key, value, attn_mask=attention_mask, dropout_p=0.0,is_causal=False)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 451, in _fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/ComfyUI/custom_nodes/ComfyUI-CogVideoXWrapper/custom_cogvideox_transformer_3d.py\", line 50, in sageattn_func\r\n    return sageattn(query, key, value, attn_mask=attn_mask, dropout_p=dropout_p,is_causal=is_causal)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/sageattention/core.py\", line 106, in sageattn\r\n    o = attn_true(q_int8, k_int8, v, q_scale, k_scale, tensor_layout=tensor_layout, output_dtype=dtype)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/sageattention/attn_qk_int8_per_block.py\", line 113, in forward\r\n    _attn_fwd[grid](\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/runtime/jit.py\", line 167, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/runtime/jit.py\", line 416, in run\r\n    self.cache[device][key] = compile(\r\n                              ^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/compiler/compiler.py\", line 191, in compile\r\n    module = src.make_ir(options)\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/compiler/compiler.py\", line 117, in make_ir\r\n    return ast_to_ttir(self.fn, self, options=options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/wangxi/miniconda3/envs/comfyuimain/lib/python3.11/site-packages/triton/compiler/code_generator.py\", line 1231, in ast_to_ttir\r\n    raise CompilationError(fn.src, node, repr(e)) from e\r\ntriton.compiler.errors.CompilationError: at 39:55:    O_block_ptr = Out + (off_z * stride_oz + off_h * stride_oh) + offs_m[:, None] * stride_on + offs_k[None, :]\r\n\r\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\r\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\r\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\r\n\r\n    q = tl.load(Q_ptrs, mask = offs_m[:, None] < qo_len)\r\n    q_scale = tl.load(Q_scale_ptr)\r\n    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, kv_len, K_ptrs, K_scale_ptr, V_ptrs, stride_kn, stride_vn,\r\n                                    start_m,\r\n                                    BLOCK_M, HEAD_DIM, BLOCK_N,\r\n                                    4 - STAGE, offs_m, offs_n\r\n                                                       ^\r\nAssertionError('First input (fp16) and second input (bf16) must have the same dtype!')\r\n\r\ncommit 6f9e4ff6477d51ef29e2f7eea9ff2bbd6986b007 (HEAD -> 1.5_test, origin/1.5_test)\r\nAuthor: kijai <40791699+kijai@users.noreply.github.com>\r\nDate:   Sun Nov 17 22:23:40 2024 +0200\r\n\r\n    Update custom_cogvideox_transformer_3d.py\r\n\r\ncommit e70da23ac2b4724624537e503b0cdaf93d24a74e\r\nAuthor: kijai <40791699+kijai@users.noreply.",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/245/comments",
    "author": "magicwang1111",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-11-18T08:09:53Z",
        "body": "I think I had this too with sageattention 1.0.5, reverting back to 1.0.3 made it work again. "
      },
      {
        "user": "magicwang1111",
        "created_at": "2024-11-18T08:13:33Z",
        "body": "> I think I had this too with sageattention 1.0.5, reverting back to 1.0.3 made it work again.\r\n\r\nthx,this error is fixed.you can add it in requirements."
      }
    ],
    "satisfaction_conditions": [
      "Ensure compatibility between SageAttention version and other components' data types",
      "Maintain consistent floating-point precision formats throughout the processing pipeline",
      "Provide clear version requirements for dependencies",
      "Resolve tensor dtype mismatches in attention mechanisms"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:50:13"
    }
  },
  {
    "number": 119,
    "title": "CogVideoX-Fun-V1.1",
    "created_at": "2024-10-01T21:00:40Z",
    "closed_at": "2024-10-03T18:05:59Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/119",
    "body": "is the CogVideoX-Fun-V1.1 not compatible with I2V like the first one? it completely distorts and warps images, no matter the noise strength \"wich i have no idea how it works\" or cfg, i had a poor cat turn into a strange matter akin to a demon from the depths of hell so many times that i had to delete the poor thing, i downloaded the model directly from HF, could that be the reason tho? i might re-download it from the loader tomorrow maybe.",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/119/comments",
    "author": "hassakajin",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-10-01T21:08:41Z",
        "body": "It's a lot stronger because of that noise, less noise would make it more stable. Generally the issue with the 1.0 was that it most of the time just generated no motion at all, just slow zooms, and their 1.1 was mostly to address this. I can't say I've seen anything like you describe yet though, but there's been some results with pretty much garbled motion.\r\n\r\nThat said, if you can run the model then it's working as intended, if there was any issue with download etc. it would simply error out."
      },
      {
        "user": "hassakajin",
        "created_at": "2024-10-01T21:38:00Z",
        "body": "> It's a lot stronger because of that noise, less noise would make it more stable. \r\n\r\nyeah really cranking the noise down + lowering cfg made it way more stable, thanks! i had the idea that normal noise would be like 0.4, but around 0.010 it started getting better!\r\n\r\n> Generally the issue with the 1.0 was that it most of the time just generated no motion at all, just slow zooms, and their 1.1 was mostly to address this.\r\n\r\nyeah that was truly an issue, though i think 1.1 was a bit overtuned in that aspect, maybe that's just me... 2b works wonders though strangely, time to get testing everything that works again i guess."
      },
      {
        "user": "cheezecrisp",
        "created_at": "2024-10-02T08:15:16Z",
        "body": "The v1.1 model does produce more significant human dynamics, but is unable to maintain proper human body structure. Attempted to reduce the 'noise_aug_strength' were not effective in improving this, and perhaps the v1.1 model needs further training."
      },
      {
        "user": "hassakajin",
        "created_at": "2024-10-03T06:35:30Z",
        "body": "Before anything else... what is the default \"noise_aug_strength\"? I've been trying to figure this out for the first fun5b model release, is it 0.7 or straight up 0? for me so far the previous model were giving me nice motion, so there is a little trick to it. Perhaps when i'm done setting up my workflow i can share with you, to use it however you like it.\r\n\r\nAlso in regards to the v1.1 version, it works well in pair with caption models like florence, it gives the image motion enough to no be still, but inputting any type of human motion like jumping or walking warps everything,"
      },
      {
        "user": "kijai",
        "created_at": "2024-10-03T09:01:26Z",
        "body": "> Before anything else... what is the default \"noise_aug_strength\"? I've been trying to figure this out for the first fun5b model release, is it 0.7 or straight up 0? for me so far the previous model were giving me nice motion, so there is a little trick to it. Perhaps when i'm done setting up my workflow i can share with you, to use it however you like it.\r\n> \r\n> Also in regards to the v1.1 version, it works well in pair with caption models like florence, it gives the image motion enough to no be still, but inputting any type of human motion like jumping or walking warps everything,\r\n\r\nThe default is the default when you create the node, so 0.056. The value is from the original code."
      },
      {
        "user": "hassakajin",
        "created_at": "2024-10-03T18:05:59Z",
        "body": "thanks"
      }
    ],
    "satisfaction_conditions": [
      "Guidance on balancing noise strength and CFG parameters to stabilize outputs while maintaining motion",
      "Clarification on model compatibility with I2V workflows",
      "Explanation of default parameter values and their impact on output quality"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:50:39"
    }
  },
  {
    "number": 95,
    "title": "OneDiff Support",
    "created_at": "2024-09-24T17:03:29Z",
    "closed_at": "2024-09-30T22:39:48Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/95",
    "body": "Hello! \r\nI was running all sample workflows on my system (Linux, PyTorch 2.4, onediff, RTX 4090). \r\nThey all work, but onediff is not being used. \r\n\r\nWhat do I need to trigger it? \r\n\r\nThank you! ",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/95/comments",
    "author": "sandor-lisn",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-09-24T17:06:27Z",
        "body": "You have enabled onediff as the compile option in the model loader node? Note that onediff doesn't work with GGUF so it's only available in the normal model loader node."
      },
      {
        "user": "sandor-lisn",
        "created_at": "2024-09-25T04:21:09Z",
        "body": "Hello, \r\nthank you very much for your lightning fast answer! \r\nMy bad, I should have seen this option myself...\r\n\r\nWhen I enable it, I get a python error. The stacktrace is at the botton of this message. \r\n\r\nI followed the instructions on your main project page. My conda environment contains: \r\n- torch 2.4.0.dev20240324+cu121\r\n- nexfort 0.1.dev271\r\n- onediff 1.2.1.dev24\r\n- onediffx 1.2.1.dev24\r\n\r\nDo you have a suggestion? Thank you!\r\n\r\n========\r\nUnable to load nexfort.{extension} module. Is it compatible with your PyTorch installation?\r\n!!! Exception during processing !!! /home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/nexfort/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK3c105Error4whatEv\r\nTraceback (most recent call last):\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 323, in execute\r\n    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 198, in get_output_data\r\n    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 169, in _map_node_over_list\r\n    process_inputs(input_dict, i)\r\n  File \"/home/sandor/workspace/ComfyUI-flux/execution.py\", line 158, in process_inputs\r\n    results.append(getattr(obj, func)(**inputs))\r\n  File \"/home/sandor/workspace/ComfyUI-flux/custom_nodes/ComfyUI-CogVideoXWrapper/nodes.py\", line 176, in loadmodel\r\n    pipe = compile_pipe(\r\n  File \"/home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/onediffx/compilers/diffusion_pipeline_compiler.py\", line 75, in compile_pipe\r\n    pipe = convert_pipe_to_memory_format(\r\n  File \"/home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/onediffx/compilers/diffusion_pipeline_compiler.py\", line 120, in convert_pipe_to_memory_format\r\n    from nexfort.utils.attributes import multi_recursive_apply\r\n  File \"/home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/nexfort/__init__.py\", line 22, in <module>\r\n    exec(f\"import nexfort.{extension} as {extension}\")\r\n  File \"<string>\", line 1, in <module>\r\nImportError: /home/sandor/anaconda3/envs/comfy-flux/lib/python3.10/site-packages/nexfort/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZNK3c105Error4whatEv\r\n"
      },
      {
        "user": "kijai",
        "created_at": "2024-09-25T11:02:20Z",
        "body": "Some issue with nexfort install, not seen that one before. Torch 2.4.1 is on stable release now so I'd suggest installing that, as it works for me using it."
      },
      {
        "user": "sandor-lisn",
        "created_at": "2024-09-26T10:11:16Z",
        "body": "Updating to Torch 2.4.1 did the trick. Thanks so much for your great volunteer efforts! "
      }
    ],
    "satisfaction_conditions": [
      "Resolves compatibility issues between OneDiff/NexFort and the user's PyTorch version",
      "Ensures OneDiff is properly enabled in the correct model loader configuration",
      "Provides actionable troubleshooting for dependency conflicts"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:50:53"
    }
  },
  {
    "number": 62,
    "title": "Error occurred when executing DownloadAndLoadCogVideoModel:  CogVideoXPatchEmbed( (proj): Conv2d(32, 3072, kernel_size=(2, 2), stride=(2, 2)) (text_proj): Linear(in_features=4096, out_features=3072, bias=True) ) does not have a parameter or a buffer named pos_embedding.",
    "created_at": "2024-09-18T21:05:16Z",
    "closed_at": "2024-09-18T21:15:36Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/62",
    "body": "Error occurred when executing DownloadAndLoadCogVideoModel:\r\n\r\nCogVideoXPatchEmbed(\r\n(proj): Conv2d(32, 3072, kernel_size=(2, 2), stride=(2, 2))\r\n(text_proj): Linear(in_features=4096, out_features=3072, bias=True)\r\n) does not have a parameter or a buffer named pos_embedding.\r\n\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 317, in execute\r\noutput_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 192, in get_output_data\r\nreturn_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 169, in _map_node_over_list\r\nprocess_inputs(input_dict, i)\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\execution.py\", line 158, in process_inputs\r\nresults.append(getattr(obj, func)(**inputs))\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\ComfyUI\\custom_nodes\\ComfyUI-CogVideoXWrapper\\nodes.py\", line 92, in loadmodel\r\ntransformer = CogVideoXTransformer3DModel.from_pretrained(base_path, subfolder=\"transformer\")\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\r\nreturn fn(*args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\diffusers\\models\\modeling_utils.py\", line 774, in from_pretrained\r\naccelerate.load_checkpoint_and_dispatch(\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\big_modeling.py\", line 613, in load_checkpoint_and_dispatch\r\nload_checkpoint_in_model(\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 1821, in load_checkpoint_in_model\r\nset_module_tensor_to_device(\r\nFile \"C:\\Users\\Gaming\\Desktop\\ComfyUI_windows_portable\\python_embeded\\Lib\\site-packages\\accelerate\\utils\\modeling.py\", line 341, in set_module_tensor_to_device\r\nraise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/62/comments",
    "author": "brad12d3",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-09-18T21:07:50Z",
        "body": "I2V model requires diffusers version 0.30.3 which came out yesterday."
      },
      {
        "user": "brad12d3",
        "created_at": "2024-09-18T21:15:34Z",
        "body": "Yup that was it, thanks. I guess I missed that.\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Identifies version incompatibility with the diffusers library",
      "Addresses missing parameters in model architecture",
      "Ensures compatibility with CogVideoXTransformer3DModel requirements"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:52:19"
    }
  },
  {
    "number": 9,
    "title": "Results from this are different to official CogVideoX demo and I can't figure out how to match it.",
    "created_at": "2024-08-10T07:37:18Z",
    "closed_at": "2024-08-11T06:55:11Z",
    "labels": [],
    "url": "https://github.com/kijai/ComfyUI-CogVideoXWrapper/issues/9",
    "body": "Using the default settings provided with the example workflow, with 16 frames at 8 fps, it only renders a two second video (as expected). But by default, CogVideoX does 6 second clips. However, upping the amount of frames to 48 does not have the intended effect and instead generates three disjointed clips, like three 2 second clips stuck together. \r\n\r\nHow do I get a single 6 second video like it's supposed to do?",
    "comments_url": "https://api.github.com/repos/kijai/ComfyUI-CogVideoXWrapper/issues/9/comments",
    "author": "Gyramuur",
    "comments": [
      {
        "user": "kijai",
        "created_at": "2024-08-10T14:12:41Z",
        "body": "Which workflow exactly? I added the feature to use temporal tiling (t_tile) as context windows, if the value is lower than your frame count it creates the video in what you call \"disjointed clips\", so you'd want to put that to 48 to get the max clips. The point of the t_tile is that you can also go above the 48, though the clips are usually not joined too well together, sometimes it works nicely."
      },
      {
        "user": "Gyramuur",
        "created_at": "2024-08-11T06:55:11Z",
        "body": "> Which workflow exactly? I added the feature to use temporal tiling (t_tile) as context windows, if the value is lower than your frame count it creates the video in what you call \"disjointed clips\", so you'd want to put that to 48 to get the max clips. The point of the t_tile is that you can also go above the 48, though the clips are usually not joined too well together, sometimes it works nicely.\r\n\r\nI'm using the default example_01.json provided in the examples folder. Also increasing t_tile_length along with num_frames fixes it, thank you. :) I wasn't sure what the t_tile_length parameter did.\r\n\r\nAlso it's unrelated to this issue, so I'll close it out anyway, but when you load the aforementioned workflow, it throws an error saying the t_tile_overlap of 2 is less than the minimum of 8."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how temporal tiling parameters (t_tile_length and t_tile_overlap) affect video continuity",
      "Guidance on parameter synchronization between frame count and temporal context settings",
      "Validation of parameter constraints to prevent errors"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:52:40"
    }
  }
]