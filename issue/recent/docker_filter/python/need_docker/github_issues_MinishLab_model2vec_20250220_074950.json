[
  {
    "number": 79,
    "title": "AttributeError: 'NoneType' object has no attribute 'get'",
    "created_at": "2024-10-12T19:01:40Z",
    "closed_at": "2024-10-12T19:37:09Z",
    "labels": [],
    "url": "https://github.com/MinishLab/model2vec/issues/79",
    "body": "If I try to use the distilled model with Sentence Transformers, I am getting the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[4], line 7\r\n      5 model_distilled = distill(model_name=model_name, pca_dims=256)\r\n      6 model_distilled.save_pretrained(\"./multilingual-e5-small-distilled\")\r\n----> 7 st_model_distilled = SentenceTransformer(\"multilingual-e5-small-distilled\")\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:306, in SentenceTransformer.__init__(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\r\n    294         modules, self.module_kwargs = self._load_sbert_model(\r\n    295             model_name_or_path,\r\n    296             token=token,\r\n   (...)\r\n    303             config_kwargs=config_kwargs,\r\n    304         )\r\n    305     else:\r\n--> 306         modules = self._load_auto_model(\r\n    307             model_name_or_path,\r\n    308             token=token,\r\n    309             cache_folder=cache_folder,\r\n    310             revision=revision,\r\n    311             trust_remote_code=trust_remote_code,\r\n    312             local_files_only=local_files_only,\r\n    313             model_kwargs=model_kwargs,\r\n    314             tokenizer_kwargs=tokenizer_kwargs,\r\n    315             config_kwargs=config_kwargs,\r\n    316         )\r\n    318 if modules is not None and not isinstance(modules, OrderedDict):\r\n    319     modules = OrderedDict([(str(idx), module) for idx, module in enumerate(modules)])\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1454, in SentenceTransformer._load_auto_model(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\r\n   1451 tokenizer_kwargs = shared_kwargs if tokenizer_kwargs is None else {**shared_kwargs, **tokenizer_kwargs}\r\n   1452 config_kwargs = shared_kwargs if config_kwargs is None else {**shared_kwargs, **config_kwargs}\r\n-> 1454 transformer_model = Transformer(\r\n   1455     model_name_or_path,\r\n   1456     cache_dir=cache_folder,\r\n   1457     model_args=model_kwargs,\r\n   1458     tokenizer_args=tokenizer_kwargs,\r\n   1459     config_args=config_kwargs,\r\n   1460 )\r\n   1461 pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \"mean\")\r\n   1462 self.model_card_data.set_base_model(model_name_or_path, revision=revision)\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:56, in Transformer.__init__(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\r\n     53     config_args = {}\r\n     55 config = AutoConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir)\r\n---> 56 self._load_model(model_name_or_path, config, cache_dir, **model_args)\r\n     58 if max_seq_length is not None and \"model_max_length\" not in tokenizer_args:\r\n     59     tokenizer_args[\"model_max_length\"] = max_seq_length\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:87, in Transformer._load_model(self, model_name_or_path, config, cache_dir, **model_args)\r\n     85     self._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\r\n     86 else:\r\n---> 87     self.auto_model = AutoModel.from_pretrained(\r\n     88         model_name_or_path, config=config, cache_dir=cache_dir, **model_args\r\n     89     )\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    562 elif type(config) in cls._model_mapping.keys():\r\n    563     model_class = _get_model_class(config, cls._model_mapping)\r\n--> 564     return model_class.from_pretrained(\r\n    565         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\r\n    566     )\r\n    567 raise ValueError(\r\n    568     f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\r\n    569     f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\r\n    570 )\r\n\r\nFile ~/PycharmProjects/product-search-custom-embedding/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3792, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\r\n   3789 with safe_open(resolved_archive_file, framework=\"pt\") as f:\r\n   3790     metadata = f.metadata()\r\n-> 3792 if metadata.get(\"format\") == \"pt\":\r\n   3793     pass\r\n   3794 elif metadata.get(\"format\") == \"tf\":\r\n\r\nAttributeError: 'NoneType' object has no attribute 'get'\r\n```\r\n\r\nUse the following code snippet to reproduce the error:\r\n\r\n```python\r\nfrom model2vec.distill import distill\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\nmodel_name = \"intfloat/multilingual-e5-small\"\r\nmodel_distilled = distill(model_name=model_name, pca_dims=256)\r\nmodel_distilled.save_pretrained(\"./multilingual-e5-small-distilled\")\r\nst_model_distilled = SentenceTransformer(\"multilingual-e5-small-distilled\")\r\n```\r\n",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/79/comments",
    "author": "aoezdTchibo",
    "comments": [
      {
        "user": "stephantul",
        "created_at": "2024-10-12T19:29:18Z",
        "body": "Hey, you are unfortunately not able to use `model2vec` like this. Please use the following snippet:\r\n\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom sentence_transformers.models import StaticEmbedding\r\n\r\nmodel_name = \"intfloat/multilingual-e5-small\"\r\nstatic_embedding = StaticEmbedding.from_distillation(model_name, device=\"cuda\")\r\nmodel = SentenceTransformer(modules=[static_embedding])\r\n```\r\n\r\nShould you have any further questions, please don't hesitate to reach out."
      },
      {
        "user": "aoezdTchibo",
        "created_at": "2024-10-12T19:37:09Z",
        "body": "Works with the provided snippet, thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates proper integration of a distilled model with SentenceTransformer initialization",
      "Uses components compatible with SentenceTransformer's architecture expectations",
      "Avoids deprecated or incompatible model serialization approaches",
      "Ensures all model components are properly initialized before use"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 01:46:56"
    }
  }
]