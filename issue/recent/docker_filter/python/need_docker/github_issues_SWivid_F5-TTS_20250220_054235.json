[
  {
    "number": 772,
    "title": "Improve prepare_csv_wavs.py",
    "created_at": "2025-02-07T20:13:05Z",
    "closed_at": "2025-02-09T06:34:11Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/772",
    "body": "This PR enhances the dataset preparation pipeline with major performance and reliability improvements. \r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/772/comments",
    "author": "hcsolakoglu",
    "comments": [
      {
        "user": "hcsolakoglu",
        "created_at": "2025-02-08T04:35:46Z",
        "body": "I actually wrote this for my own use. Goal here is to efficiently prepare large datasets for training. For example, in a dataset with 116,180 samples, the old version of the code took 4 hours, but now it has been reduced to 1 hour. If you're interested, I can finish adding it for everyone to use. @SWivid "
      },
      {
        "user": "SWivid",
        "created_at": "2025-02-08T06:06:48Z",
        "body": "Yes, if you are willing to share"
      },
      {
        "user": "justinjohn0306",
        "created_at": "2025-02-08T06:57:45Z",
        "body": "> I actually wrote this for my own use. Goal here is to efficiently prepare large datasets for training. For example, in a dataset with 116,180 samples, the old version of the code took 4 hours, but now it has been reduced to 1 hour. If you're interested, I can finish adding it for everyone to use. @SWivid\r\n\r\nI really love your PR. Please share the final thing, it would be much appreciated. Thank you :)"
      },
      {
        "user": "atlonxp",
        "created_at": "2025-02-08T19:01:24Z",
        "body": "looking forward to the merging"
      },
      {
        "user": "hcsolakoglu",
        "created_at": "2025-02-09T16:04:34Z",
        "body": "Since I was working, I couldn't return, but I see that @SWivid merged it for me. I wish everyone fast and successful training runs! @justinjohn0306 @atlonxp"
      }
    ],
    "satisfaction_conditions": [
      "Significant reduction in processing time for large datasets",
      "Maintains or improves reliability for dataset preparation",
      "Generalizable solution usable by multiple users"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 02:12:09"
    }
  },
  {
    "number": 459,
    "title": "load finetune data",
    "created_at": "2024-11-13T09:26:17Z",
    "closed_at": "2024-11-13T11:59:53Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/459",
    "body": "### Checks\n\n- [X] This template is only for usage issues encountered.\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Environment Details\n\ntrain on 4 a800\n\n### Steps to Reproduce\n\nI encountered an issue when trying to load my fine-tuned checkpoint. During the load_state_dict() call, I received the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 151, in <module>\r\n    infer_tts()\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\", line 72, in infer_tts\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 216, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, dtype=dtype, use_ema=use_ema)\r\n  File \"/remote-home1/ycyuan/TTS/F5-TTS/src/f5_tts/infer/utils_infer.py\", line 168, in load_checkpoint\r\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\r\n  File \"/remote-home1/ycyuan/conda/anaconda3/envs/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n    Missing key(s) in state_dict: \"transformer.time_embed.time_mlp.0.weight\", \"transformer.time_embed.time_mlp.0.bias\", \"transformer.time_embed.time_mlp.2.weight\", \"transformer.time_embed.time_mlp.2.bias\", \"transformer.text_embed.text_embed.weight\", \"transformer.text_embed.text_blocks.0.dwconv.weight\", \"transformer.text_embed.text_blocks.0.dwconv.bias\", \"transformer.text_embed.text_blocks.0.norm.weight\", \"transformer.text_embed.text_blocks.0.norm.bias\", \"transformer.text_embed.text_blocks.0.pwconv1.weight\", \"transformer.text_embed.text_blocks.0.pwconv1.bias\", \"transformer.text_embed.text_blocks.0.grn.gamma\", \"transformer.text_embed.text_blocks.0.grn.beta\", \"transformer.text_embed.text_blocks.0.pwconv2.weight\", \"transformer.text_embed.text_blocks.0.pwconv2.bias\", \"transformer.text_embed.text_blocks.1.dwconv.weight\", \"transformer.text_embed.text_blocks.1.dwconv.bias\", \"transformer.text_embed.text_blocks.1.norm.weight\", \"transformer.text_embed.text_blocks.1.norm.bias\"...\r\n    Unexpected key(s) in state_dict: \"module.transformer.time_embed.time_mlp.0.weight\", \"module.transformer.time_embed.time_mlp.0.bias\", \"module.transformer.time_embed.time_mlp.2.weight\", \"module.transformer.time_embed.time_mlp.2.bias\", \"module.transformer.text_embed.text_embed.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.weight\", \"module.transformer.text_embed.text_blocks.0.dwconv.bias\", \"module.transformer.text_embed.text_blocks.0.norm.weight\", \"module.transformer.text_embed.text_blocks.0.norm.bias\", \"module.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"module.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"module.transformer.text_embed.text_blocks.0.grn.gamma\", \"module.transformer.text_embed.text_blocks.0.grn.beta\",...\r\n```\r\nHowever, when I print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected:\r\n```\r\nOrderedDict([('transformer.time_embed.time_mlp.0.weight', tensor([[-0.0007, -0.0009, -0.0007,  ..., -0.0355,  0.0119,  0.0061], ...], device='cuda:3')),\r\n            ('transformer.time_embed.time_mlp.0.bias', tensor([ 0.0139,  0.0171,  0.0399,  ..., -0.0508,  0.0258, -0.0191], device='cuda:3'))]),....\r\n```\r\nHowever, when I attempt to load the checkpoint without any wrapping, the model loading fails due to the error above. If I wrap the model with DataParallel before loading the checkpoint (like this: model = torch.nn.DataParallel(model)), the loading works fine, but this leads to subsequent errors during inference.\r\n\r\nAdditionally, I have tried to remove the module prefix using the following function:\r\n```\r\ndef remove_module_prefix(state_dict):\r\n    new_state_dict = {}\r\n    for k, v in state_dict.items():\r\n        # Remove the 'module.' prefix\r\n        print(k)\r\n        new_k = k.replace('module.', '') if 'module.' in k else k\r\n        new_state_dict[new_k] = v\r\n    return new_state_dict\r\n```\r\nHowever, this approach does not work because, when I print the keys, they don't actually contain the module. prefix. Despite this, when loading the checkpoint, I still encounter a mismatch error regarding the module. prefix.\r\n\r\nI have not modified the checkpoint saving function. Here\u2019s the relevant code that I use to save checkpoints:\r\n```\r\ndef save_checkpoint(self, step, last=False):\r\n    self.accelerator.wait_for_everyone()\r\n    if self.is_main:\r\n        checkpoint = dict(\r\n            model_state_dict=self.accelerator.unwrap_model(self.model).state_dict(),\r\n            optimizer_state_dict=self.accelerator.unwrap_model(self.optimizer).state_dict(),\r\n            ema_model_state_dict=self.ema_model.state_dict(),\r\n            scheduler_state_dict=self.scheduler.state_dict(),\r\n            step=step,\r\n        )\r\n        if not os.path.exists(self.checkpoint_path):\r\n            os.makedirs(self.checkpoint_path)\r\n        rank = self.accelerator.process_index\r\n\r\n        if rank == 0:\r\n            if last:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_last.pt\")\r\n                print(f\"Saved last checkpoint at step {step}\")\r\n            else:\r\n                self.accelerator.save(checkpoint, f\"{self.save_ckpt_path}/model_{step}.pt\")\r\n```\r\n\r\nIs there a way to handle the mismatch of the module. prefix in the checkpoint file?\n\n### \u2714\ufe0f Expected Behavior\n\n_No response_\n\n### \u274c Actual Behavior\n\n_No response_",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/459/comments",
    "author": "southwindyong",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-13T09:33:23Z",
        "body": "maybe you could provide the additional `File \"/remote-home1/ycyuan/TTS/F5-TTS/inference.py\"` for more info to figure out the issue."
      },
      {
        "user": "southwindyong",
        "created_at": "2024-11-13T10:31:33Z",
        "body": "This is the file of ``/remote-home1/ycyuan/TTS/F5-TTS/inference.py``\r\n```\r\nimport os\r\nimport random\r\nimport json\r\nimport codecs\r\nimport numpy as np\r\nimport soundfile as sf\r\nimport tomli\r\nfrom pathlib import Path\r\nfrom f5_tts.infer.utils_infer import (\r\n    infer_process,\r\n    load_model,\r\n    load_vocoder,\r\n    preprocess_ref_audio_text,\r\n    remove_silence_for_generated_wav,\r\n)\r\nfrom f5_tts.model import DiT, UNetT\r\nimport re\r\n\r\ndef infer_tts():\r\n    # Directly specify the variables without argparse\r\n    print(\"\u5f00\u59cb TTS \u63a8\u7406\u8fc7\u7a0b...\")\r\n\r\n    config_path = Path(\"f5_tts\") / \"infer/examples/basic\" / \"basic.toml\"  \r\n    model = \"F5-TTS\"  # Choose \"F5-TTS\" or \"E2-TTS\"\r\n    ckpt_file = \"/remote-home1/ycyuan/TTS/F5-TTS/ckpts/F5TTS_Base/libritts_25s_1/model_last.pt\"  # Provide the path to checkpoint if necessary\r\n    vocab_file = \"\"  # Provide the vocab file path if necessary\r\n    text_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_text_2-6\"  # Text directory\r\n    audio_dir = \"/remote-home1/ycyuan/TTS/fish-speech/prompt/libritts_prompt_2_10s\"  # Audio directory\r\n    output_dir = \"./generate_data/libritts_25s_1\"  # Path to output folder\r\n    remove_silence = False  # Whether to remove silence from generated audio\r\n    vocoder_name = \"vocos\"  # Choose between \"vocos\" or \"bigvgan\"\r\n    load_vocoder_from_local = False  # Whether to load vocoder from local path\r\n    speed = 1.0  # Adjust audio generation speed (default is 1.0)\r\n\r\n    # Load configuration from the specified TOML file\r\n    print(f\"\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 {config_path}...\")\r\n    config = tomli.load(open(config_path, \"rb\"))\r\n    \r\n    # Use provided values or defaults from the config file\r\n    output_dir = output_dir if output_dir else config[\"output_dir\"]\r\n    model = model if model else config[\"model\"]\r\n    ckpt_file = ckpt_file if ckpt_file else \"\"\r\n    vocab_file = vocab_file if vocab_file else \"\"\r\n    remove_silence = remove_silence if remove_silence else config[\"remove_silence\"]\r\n    speed = speed\r\n    wave_path = Path(output_dir) / \"infer_cli_out.wav\"\r\n    \r\n    # Set vocoder local path based on the selected vocoder\r\n    print(f\"\u9009\u62e9\u7684 vocoder \u6a21\u578b: {vocoder_name}...\")\r\n    if vocoder_name == \"vocos\":\r\n        vocoder_local_path = \"../checkpoints/vocos-mel-24khz\"\r\n    elif vocoder_name == \"bigvgan\":\r\n        vocoder_local_path = \"../checkpoints/bigvgan_v2_24khz_100band_256x\"\r\n    \r\n    # Load the vocoder model\r\n    print(\"\u52a0\u8f7d vocoder \u6a21\u578b...\")\r\n    vocoder = load_vocoder(vocoder_name=vocoder_name, is_local=load_vocoder_from_local, local_path=vocoder_local_path)\r\n\r\n    # Load the TTS model\r\n    print(f\"\u52a0\u8f7d TTS \u6a21\u578b: {model}...\")\r\n    if model == \"F5-TTS\":\r\n        model_cls = DiT\r\n        model_cfg = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"F5-TTS\"\r\n            exp_name = \"F5TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    elif model == \"E2-TTS\":\r\n        model_cls = UNetT\r\n        model_cfg = dict(dim=1024, depth=24, heads=16, ff_mult=4)\r\n        if ckpt_file == \"\":\r\n            repo_name = \"E2-TTS\"\r\n            exp_name = \"E2TTS_Base\"\r\n            ckpt_step = 1200000\r\n            ckpt_file = str(cached_path(f\"hf://SWivid/{repo_name}/{exp_name}/model_{ckpt_step}.safetensors\"))\r\n    \r\n    # Initialize the model\r\n    print(f\"\u4f7f\u7528 {model} \u6a21\u578b\u8fdb\u884c\u63a8\u7406...\")\r\n    ema_model = load_model(model_cls, model_cfg, ckpt_file, mel_spec_type=vocoder_name, vocab_file=vocab_file)\r\n    print(\"\u6a21\u578b\u52a0\u8f7d\u6210\u529f\u3002\")\r\n\r\n    # Load a random JSON file containing text\r\n    print(f\"\u4ece {text_dir} \u4e2d\u52a0\u8f7d\u968f\u673a JSON \u6587\u4ef6...\")\r\n    json_files = [f for f in os.listdir(text_dir) if f.endswith('.json')]\r\n    random_json_file = random.choice(json_files)\r\n    json_file_path = os.path.join(text_dir, random_json_file)\r\n\r\n    with open(json_file_path, 'r') as file:\r\n        json_data = json.load(file)\r\n        text = json_data[\"\u5b8c\u6574\u6587\u672c\"]\r\n        text_turns = json_data[\"\u6587\u4ef6\u6570\"]\r\n\r\n    # From the audio directory, select a random folder and corresponding audio file\r\n    print(f\"\u4ece {audio_dir} \u4e2d\u9009\u62e9\u4e00\u4e2a\u968f\u673a\u6587\u4ef6\u5939...\")\r\n    folders = [f for f in os.listdir(audio_dir) if os.path.isdir(os.path.join(audio_dir, f))]\r\n    while True:\r\n        random_folder = random.choice(folders)\r\n        folder_path = os.path.join(audio_dir, random_folder)\r\n        lab_files = [f for f in os.listdir(folder_path) if f.endswith('.lab')]\r\n        if not lab_files:\r\n            continue  # Skip folders without `.lab` files\r\n\r\n        random_lab_file = random.choice(lab_files)\r\n        lab_file_path = os.path.join(folder_path, random_lab_file)\r\n\r\n        # Read the corresponding `.lab` file to get prompt text\r\n        with open(lab_file_path, 'r') as lab_file:\r\n            prompt_text = lab_file.read().strip()\r\n        prompt_turns = len(prompt_text.splitlines())  # Number of lines = number of turns\r\n\r\n        if text_turns % 2 == prompt_turns % 2 and prompt_turns <= 4:  # Ensure matching turns and prompt_turns <= 4\r\n            break\r\n\r\n    # Choose the corresponding audio file\r\n    print(f\"\u9009\u62e9\u7684\u97f3\u9891\u6587\u4ef6: {random_lab_file}...\")\r\n    audio_file = random_lab_file.replace('.lab', '.wav')\r\n    ref_audio = os.path.join(folder_path, audio_file)\r\n    if not os.path.exists(ref_audio):\r\n        raise FileNotFoundError(f\"\u672a\u627e\u5230\u97f3\u9891\u6587\u4ef6 {ref_audio}\")\r\n\r\n    # Call the inference process\r\n    def main_process(ref_audio, text, gen_text, model_obj, mel_spec_type, remove_silence, speed):\r\n        print(f\"\u6b63\u5728\u751f\u6210\u97f3\u9891: {text}\")\r\n        generated_audio_segments = []\r\n        reg1 = r\"(?=\\[\\w+\\])\"\r\n        chunks = re.split(reg1, gen_text)\r\n        reg2 = r\"\\[(\\w+)\\]\"\r\n\r\n        # Process each chunk of the generated text\r\n        for text_chunk in chunks:\r\n            if not text_chunk.strip():\r\n                continue\r\n            match = re.match(reg2, text_chunk)\r\n            if match:\r\n                voice = match[1]\r\n            else:\r\n                print(\"\u6ca1\u6709\u627e\u5230\u8bed\u97f3\u6807\u7b7e\uff0c\u4f7f\u7528\u9ed8\u8ba4\u8bed\u97f3\u3002\")\r\n                voice = \"main\"\r\n            \r\n            print(f\"\u8bed\u97f3\u9009\u62e9: {voice}\")\r\n            gen_text = text_chunk.strip()\r\n            audio, final_sample_rate, spectrogram = infer_process(\r\n                ref_audio, text, gen_text, model_obj, vocoder, mel_spec_type=mel_spec_type, speed=speed\r\n            )\r\n            generated_audio_segments.append(audio)\r\n\r\n        if generated_audio_segments:\r\n            final_wave = np.concatenate(generated_audio_segments)\r\n\r\n            if not os.path.exists(output_dir):\r\n                os.makedirs(output_dir)\r\n\r\n            with open(wave_path, \"wb\") as f:\r\n                sf.write(f.name, final_wave, final_sample_rate)\r\n                if remove_silence:\r\n                    print(\"\u6b63\u5728\u53bb\u9664\u751f\u6210\u97f3\u9891\u4e2d\u7684\u9759\u97f3\u90e8\u5206...\")\r\n                    remove_silence_for_generated_wav(f.name)\r\n                print(f\"\u751f\u6210\u7684\u97f3\u9891\u5df2\u4fdd\u5b58\u5230 {f.name}\")\r\n\r\n    main_process(ref_audio, text, text, ema_model, vocoder_name, remove_silence, speed)\r\n\r\n# Call the infer_tts function to start the process\r\nif __name__ == \"__main__\":\r\n    infer_tts()\r\n```\r\nI think the text and prompt wav file is not important because it's all right"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-13T10:58:49Z",
        "body": "> print the checkpoint['model_state_dict'], it seems correct and shows all the keys expected\r\n\r\nmaybe you have changed the code related with ema_model setup?\r\n\r\ntry do the removal of the module prefix using the function you got to `checkpoint['ema_model_state_dict']` see if works"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the checkpoint keys have 'module.' prefix despite no DataParallel usage",
      "Solution for loading checkpoints saved with distributed training artifacts",
      "Method to handle EMA model state_dict alignment",
      "Approach that maintains inference functionality after loading",
      "Clarification on accelerator.unwrap_model() behavior"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 02:13:23"
    }
  },
  {
    "number": 110,
    "title": "Offloading Whisper to CPU",
    "created_at": "2024-10-16T02:18:29Z",
    "closed_at": "2024-10-16T05:08:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/110",
    "body": "I'm using the Gradio app on a laptop with 6 GB VRAM.\n\nWith the current settings, the model overflows into system RAM reducing generation speed significantly, but if I offload Whisper to CPU, it fits in about 5.2 GB. Can we have a cmdline swich / config file setting / in-app setting to do this automatically? Or perhaps unload Whisper when not in use?\n\nThe model itself is amazing by the way.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/110/comments",
    "author": "athu16",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T03:02:54Z",
        "body": "Thanks for using. `inference-cli.py` may with your need"
      },
      {
        "user": "athu16",
        "created_at": "2024-10-16T05:08:40Z",
        "body": "Thanks, just what I needed!"
      }
    ],
    "satisfaction_conditions": [
      "Provides an automated way to offload Whisper to the CPU during inference",
      "Supports configuration via CLI, config file, or in-app settings",
      "Ensures the model remains within available VRAM limits during operation"
    ],
    "_classification": {
      "category": "Can be dockerized without any issue",
      "timestamp": "2025-04-05 02:15:56"
    }
  }
]