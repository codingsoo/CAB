[
  {
    "number": 53,
    "title": "LoRA scale=0.0 for non-Conditional tokens",
    "created_at": "2024-12-21T11:08:16Z",
    "closed_at": "2024-12-27T12:20:03Z",
    "labels": [],
    "url": "https://github.com/Yuanshi9815/OminiControl/issues/53",
    "body": "Hello @Yuanshi9815 !\r\nGreat work!\r\n\r\nI have a question regarding setting lora weight for non-conditional tokens to 0.0\r\nSee #3 \r\nCan you please mention where you do it in the code? In which module?\r\n\r\nKind regards,\r\nIgor ",
    "comments_url": "https://api.github.com/repos/Yuanshi9815/OminiControl/issues/53/comments",
    "author": "reallyigor",
    "comments": [
      {
        "user": "Yuanshi9815",
        "created_at": "2024-12-23T05:54:49Z",
        "body": "Hiii @reallyigor ,\r\n\r\nThanks for your attention. \r\n\r\nYou may find it in the file `src/lora_controller.py`. \ud83d\ude4c"
      },
      {
        "user": "reallyigor",
        "created_at": "2024-12-23T10:57:48Z",
        "body": "Thank you for such a swift answer!\r\ncan you please tell me the exact row?\r\n\r\ne.g. `set_lora_scale` function is not used in the code at all"
      },
      {
        "user": "Yuanshi9815",
        "created_at": "2024-12-25T04:06:39Z",
        "body": "Hi @reallyigor ,\r\n\r\nMerry Christmas! \ud83c\udf84\r\n\r\nThe LoRA weight control is implemented through the `enable_lora` context manager. For example, in `src/transformer.py ` line 98:\r\n  ```python\r\n  with enable_lora((self.x_embedder,), model_config.get(\"latent_lora\", False)):\r\n      hidden_states = self.x_embedder(hidden_states)\r\n  ```\r\nWhen`latent_lora` is `False`, the LoRA weights are set to 0 for the `x_embedder` module."
      },
      {
        "user": "reallyigor",
        "created_at": "2024-12-25T08:18:28Z",
        "body": "Thank you very much, @Yuanshi9815 !\r\n\ud83c\udf84 Merry Christmas! \ud83c\udf84\r\nHow this `model_config` controls conditional/non-conditional tokens? it enables/disables lora for the whole model, doesnt it? "
      },
      {
        "user": "Yuanshi9815",
        "created_at": "2024-12-27T04:45:38Z",
        "body": "Hi @reallyigor ,\r\n\r\nWe dynamically enable/disable LoRA during model execution. The `model_config` doesn't determine conditional/non-conditional tokens - it only controls whether to use LoRA on non-conditional tokens, which should match the training configuration. \r\n\r\nIn our released models, LoRA is used on conditional tokens but not on non-conditional tokens, hence the default False setting.\r\n\r\nXD"
      },
      {
        "user": "reallyigor",
        "created_at": "2024-12-27T12:20:03Z",
        "body": "Thank you! "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how LoRA weight control is dynamically applied to specific modules during execution",
      "Clarification of the relationship between model configuration and token type handling",
      "Identification of the control mechanism for non-conditional token LoRA weights",
      "Description of how training configuration alignment is maintained"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:52:58"
    }
  }
]