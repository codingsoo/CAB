[
  {
    "number": 137,
    "title": "Using Model2Vec for Token Classification",
    "created_at": "2024-11-29T01:57:11Z",
    "closed_at": "2024-11-29T18:36:13Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/MinishLab/model2vec/issues/137",
    "body": "Hi Model2Vec Team,\r\n\r\nThis is truely great work, thanks so much for the contribution! Awesome project!\r\n\r\nOne dumb question: Is it possible to apply model2vec to a token classification model, e.g., BERT model connected to a NER tagger?\r\n\r\nI've been playing with Model2Vec, and seems that for now it is designed to only output embedding for a whole sequence (e.g., [CLS]). It seems that Model2Vec currently does not support generating embeddings for every token in a sequence. Is this understanding correct?\r\n\r\nIf so, how would you recommend that I augment Model2Vec to support generating token-level embeddings, and potentially drop it into a NER tagger?\r\n\r\nThank you!",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/137/comments",
    "author": "kelayamatoz",
    "comments": [
      {
        "user": "Pringled",
        "created_at": "2024-11-29T07:12:16Z",
        "body": "Hi @kelayamatoz,\r\n\r\nThanks for the kind words! You can actually use Model2Vec for that purpose out of the box, the following should work:\r\n\r\n```python\r\nfrom model2vec import StaticModel\r\n\r\n# Load a model from the HuggingFace hub (in this case the potion-base-8M model)\r\nmodel = StaticModel.from_pretrained(\"minishlab/potion-base-8M\")\r\n\r\n# Make sequences of token embeddings\r\ntoken_embeddings = model.encode_as_sequence([\"It's dangerous to go alone!\", \"It's a secret to everybody.\"])\r\n```\r\n\r\nThis will give you the individual token embedding for every input token. Let me know if this works for you or if you have any other questions! "
      },
      {
        "user": "kelayamatoz",
        "created_at": "2024-11-29T18:36:14Z",
        "body": "This works perfectly -- thank you for the pointer! My apologies, I just realized that the tutorial does include token embedding examples, and I somehow overlooked them. Sorry about that!\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates how to generate token-level embeddings (not just sequence-level) using Model2Vec",
      "Shows compatibility with token classification architectures like NER taggers",
      "Confirms availability of built-in functionality without requiring model modifications",
      "Provides actionable method to extract embeddings per token rather than per sequence"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:45:59"
    }
  },
  {
    "number": 134,
    "title": "Add `model2vec` to config.json",
    "created_at": "2024-11-25T18:27:11Z",
    "closed_at": "2024-12-01T13:33:11Z",
    "labels": [
      "enhancement"
    ],
    "url": "https://github.com/MinishLab/model2vec/issues/134",
    "body": "Hello.\r\n\r\nI'm planning to add a change to txtai to autodetect model2vec models. The best idea I have right now is the read the config.json file and see if it has the keys `apply_pca` and `apply_zipf`. \r\n\r\nWhile I believe this will be pretty unique, have you guys considered adding something to the config.json file to signal it's a model2vec file?",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/134/comments",
    "author": "davidmezzetti",
    "comments": [
      {
        "user": "Pringled",
        "created_at": "2024-11-26T07:59:52Z",
        "body": "Hey @davidmezzetti,\r\n\r\nThat's a great suggestion! We will add the following to our config.json files:\r\n```\r\n\"model_type\": \"model2vec\",\r\n\"architectures\": [\r\n    \"StaticModel\"\r\n  ],\r\n  ```\r\n  \r\n  And then you can use the `model_type` key to check for model2vec models. I'll ping you once we've made that change."
      },
      {
        "user": "davidmezzetti",
        "created_at": "2024-11-26T10:42:56Z",
        "body": "Sounds great! This will make it easier in my case as txtai is working with multiple vectorization libraries. Once this change is in, txtai will be able to automatically infer the vectorization method for model2vec models.\r\n\r\n```python\r\nimport txtai\r\nembeddings = txtai.Embeddings(path=\"minishlab/potion-base-8M\")\r\n```\r\n"
      },
      {
        "user": "Pringled",
        "created_at": "2024-11-27T18:31:01Z",
        "body": "Hey @davidmezzetti, I just updated all our configs to include the changes. Let me know if everything works as intended!"
      },
      {
        "user": "davidmezzetti",
        "created_at": "2024-12-01T13:33:11Z",
        "body": "Excellent. Looks like this should work. I'll integrate this into txtai and report back. Thank you!\n\n---\n\nThis change has been made, thanks again!\r\n\r\nJust as an FYI, the potion-science models don't appear to have the updated config. But the other models are all working as expected. "
      },
      {
        "user": "Pringled",
        "created_at": "2024-12-01T14:04:40Z",
        "body": "Hey @davidmezzetti, great, no problem! I just updated the science models as well, thanks for pointing that out!"
      }
    ],
    "satisfaction_conditions": [
      "A standardized identifier in config.json to explicitly signal model2vec models",
      "Consistent implementation across all model configurations",
      "Machine-readable format for automatic detection",
      "Backward-compatible integration with existing workflows"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:46:07"
    }
  },
  {
    "number": 110,
    "title": "HFValidationError using custom model",
    "created_at": "2024-10-23T11:59:24Z",
    "closed_at": "2024-10-23T12:29:39Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/MinishLab/model2vec/issues/110",
    "body": "Hi,\r\n\r\nI am trying to use Model2Vec with a custom model we have.\r\nThe `distill` seems to run fine, until it tries to call `validate_repo_id` with the custom path I have (`./model`).\r\n\r\n1. I have a custom model that I can load successfully:\r\n\r\n```python\r\nfrom transformers import AutoModelForSequenceClassification\r\n\r\nmodel_save_path = \"./model\"\r\nmodel = AutoModelForSequenceClassification.from_pretrained(model_save_path)\r\n```\r\n\r\n2. I `distill` the custom model:\r\n\r\n```python\r\nfrom model2vec.distill import distill\r\n\r\nmodel_save_path = \"./model\"\r\nm2v_model = distill(model_name=model_save_path, pca_dims=256)\r\n```\r\n\r\n3. The following error is raised:\r\n\r\n```\r\nHFValidationError\r\n\r\n    This cell raised an exception: HFValidationError('Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './model'.')\r\n```\r\n\r\n",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/110/comments",
    "author": "tomsquest",
    "comments": [
      {
        "user": "stephantul",
        "created_at": "2024-10-23T12:04:42Z",
        "body": "Hi @tomsquest ,\r\n\r\nJust using `model` as a path should do the trick. Could you let us know whether that works? If that doesn't work, you can always load the model and tokenizer yourself, and use `distill_from_model`\r\n\r\n```python\r\nfrom model2vec.distill import distill_from_model\r\n\r\n# Some way to get your model and tokenizer.\r\nmodel, tokenizer = load_model_and_tokenizer(...)\r\n\r\nm2v_model = distill_from_model(model, tokenizer, pca_dims=256)\r\n```\r\n\r\nSt\u00e9phan"
      },
      {
        "user": "tomsquest",
        "created_at": "2024-10-23T12:27:31Z",
        "body": "Hi @stephantul ,\r\n\r\nYes, it's working using just `model` (instead of `./model`). That was my workaround. \ud83d\udc4d"
      },
      {
        "user": "stephantul",
        "created_at": "2024-10-23T12:29:39Z",
        "body": "Alright! Happy to be of service, I'm closing this for now. Let me know if you have other issues, and please reopen or make another issue if that's the case. Thanks for using model2vec! \ud83d\ude4f "
      }
    ],
    "satisfaction_conditions": [
      "Ensures the model path format complies with Hugging Face repository ID requirements",
      "Provides a method to use custom models without strict path validation",
      "Maintains compatibility with local model directories"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:46:15"
    }
  },
  {
    "number": 108,
    "title": "Number of tokens (151646) does not match number of vectors (151643)",
    "created_at": "2024-10-23T08:25:59Z",
    "closed_at": "2024-10-24T00:22:14Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/MinishLab/model2vec/issues/108",
    "body": "Hello.\r\n\r\nI tried testing `model2vec` in the following environment and encountered the error below. Is there a way to resolve this?\r\n\r\n### Environment\r\n\r\n```python\r\nmodel2vec==0.3.0\r\ntokenizers==0.19.1\r\n```\r\n\r\n### Code\r\n\r\n```python\r\nfrom model2vec.distill.distillation import distill\r\n\r\n# Choose a Sentence Transformer model\r\nmodel_id = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\r\n\r\n# Distill an output model with the chosen dimensions\r\nmodel = distill(model_name=model_id, device=\"cpu\", pca_dims=256)\r\n```\r\n\r\n### Error\r\n\r\n```python\r\nValue Error: Number of tokens (151646) does not match number of vectors (151643)\r\n```",
    "comments_url": "https://api.github.com/repos/MinishLab/model2vec/issues/108/comments",
    "author": "su-park",
    "comments": [
      {
        "user": "stephantul",
        "created_at": "2024-10-23T11:32:37Z",
        "body": "Hey @su-park ,\r\n\r\nThanks for reporting! This shouldn't be happening, I'll take a look as soon as possible. \r\n\r\nSt\u00e9phan\n\n---\n\nHey, I figured it out. Somehow, the number of tokens of the backend tokenizer and the number of tokens in the HF tokenizer doesn't match. \r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-Qwen2-7B-instruct\")\r\ntokenizer.vocab_size\r\n# 151643\r\ntokenizer.backend_tokenizer.get_vocab_size()\r\n# 151646\r\nlen(tokenizer.get_vocab())\r\n# 151646\r\n```\r\n\r\nI checked, and there are definitely 151646 tokens in the vocabulary, not 15643. So I think the `vocab_size` being off by three is actually a bug in Transformers, not a bug in `model2vec`. In the mean-time, we could rely on the length of the vocabulary instead. I'll open a PR to change this.\n\n---\n\nPR is here #109 . \r\n\r\nI'll see if we can push this together with #107 and release a bugfix release soon.\n\n---\n\nI accidentally closed this because I merged the PR. Sorry about that. If you want, you can pull main and retry. I think it should work."
      },
      {
        "user": "su-park",
        "created_at": "2024-10-24T00:22:14Z",
        "body": "Thank you for your swift support.\r\n\r\nIt worked!"
      }
    ],
    "satisfaction_conditions": [
      "Resolve the token count mismatch between the tokenizer's reported vocabulary size and actual vocabulary entries",
      "Handle inconsistencies between Hugging Face tokenizer attributes and backend implementations",
      "Provide a reliable method to obtain accurate vocabulary size for model distillation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 01:46:41"
    }
  }
]