[
  {
    "number": 676,
    "title": "Bug Fix: Parsing Argument --finetune always True",
    "created_at": "2024-12-28T11:19:46Z",
    "closed_at": "2024-12-29T13:29:50Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/676",
    "body": "I use f5-tts_finetune-gradio and found out that the finetune checkbox will always send finetune=true to the argument, eventhought the checkbox are off.\r\n\r\nAfter some investigation turns out using type=bool on ArgumentParser always return true because in python:\r\n`bool(\"False\")` are `True`.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/676/comments",
    "author": "hndrbrm",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-28T11:36:47Z",
        "body": "@hndrbrm seems bugs in first commit\r\n\r\nfeel free to test if works, if so will merge\r\nthanks again~"
      },
      {
        "user": "hndrbrm",
        "created_at": "2024-12-30T16:22:48Z",
        "body": "Its working, thanks."
      }
    ],
    "satisfaction_conditions": [
      "Ensures boolean arguments from UI checkboxes are parsed correctly",
      "Handles 'off' state mapping to False in argument values",
      "Maintains compatibility with existing argument parsing logic"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:12:16"
    }
  },
  {
    "number": 630,
    "title": "Checkpoint saving differences",
    "created_at": "2024-12-15T09:30:49Z",
    "closed_at": "2024-12-17T02:16:24Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/630",
    "body": "### Checks\r\n\r\n- [X] This template is only for question, not feature requests or bug reports.\r\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\r\n- [X] I have searched for existing issues, including closed ones, no similar questions.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Question details\r\n\r\nWhen having `grad_accumulation_steps` set to a different value than `1`, the checkpoint saving is a bit unexpected:\r\n\r\nIn `trainer.py` for setting `save_per_updates`:\r\n\r\n```python\r\nif global_step % (self.save_per_updates * self.grad_accumulation_steps) == 0:\r\n    self.save_checkpoint(global_step)\r\n```\r\n\r\nfor setting `last_per_steps`:\r\n\r\n```python\r\nif global_step % self.last_per_steps == 0:\r\n    self.save_checkpoint(global_step, last=True)\r\n```\r\n\r\nConsequently, for my global batch-size of `19200*8` the value of `save_per_updates` needs to be divided by `8` to be comparable to the setting of `last_per_steps` and the overall variable `global_step` shown via `tqdm`.\r\n\r\nIs this intended ?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/630/comments",
    "author": "lumpidu",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-15T10:11:43Z",
        "body": "yes, intended\r\nstep and update are different stuffs"
      },
      {
        "user": "lumpidu",
        "created_at": "2024-12-16T23:46:34Z",
        "body": "Similar subject as in #632, closing"
      }
    ],
    "satisfaction_conditions": [
      "Clarification of the intended relationship between gradient accumulation steps and checkpoint saving logic",
      "Explanation of the conceptual distinction between 'steps' and 'updates' in the training process",
      "Confirmation of whether the observed parameter scaling requirement is intentional design"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:12:22"
    }
  },
  {
    "number": 594,
    "title": "Does the vocabulary size impact the amount of data required for training?",
    "created_at": "2024-12-06T04:19:55Z",
    "closed_at": "2024-12-06T16:27:25Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/594",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI've been experimenting with this recently, there seems to be some great potential here.  It does clearly take a lot of data to work well.  I was wondering then, if it would train more quickly or with less data if the vocab.txt file is smaller?  Has anyone tried this out?  I realize it would reduce the multi-language capability of the resultant model.  ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/594/comments",
    "author": "DrBrule",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-12-06T04:58:31Z",
        "body": "no idea,\r\nthought the hard part is to learn speech-text alignment but word embed seems no conflict with that\r\nand if we use big vocab size but just use part of it, the gradient will just go for the used ones, thus the only bad thing is wasted params and sparse weights (low utilization rate)?"
      },
      {
        "user": "DrBrule",
        "created_at": "2024-12-06T16:27:25Z",
        "body": "That's clear, thank you!  Seems like it wouldn't make much of a difference in practice. "
      }
    ],
    "satisfaction_conditions": [
      "Explanation of how vocabulary size relates to training data requirements and computational efficiency",
      "Clarification of whether unused vocabulary entries impact model training dynamics",
      "Analysis of speech-text alignment challenges vs vocabulary-related factors in training difficulty"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:12:30"
    }
  },
  {
    "number": 545,
    "title": "issue training F5tts small model after new training code",
    "created_at": "2024-11-28T04:22:22Z",
    "closed_at": "2024-11-28T04:36:55Z",
    "labels": [
      "bug"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/545",
    "body": "### Checks\r\n\r\n- [X] This template is only for bug reports, usage problems go with 'Help Wanted'.\r\n- [X] I have thoroughly reviewed the project documentation but couldn't find information to solve my problem.\r\n- [X] I have searched for existing issues, including closed ones, and couldn't find a solution.\r\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\r\n\r\n### Environment Details\r\n\r\npython 3.10.12 (wsl)\r\n\r\n### Steps to Reproduce\r\n\r\n1. initialize training using the new config-based training code following the readme.\r\nIn my case I'm trying to train the f5tts small model and have modified the training config to point to my dataset.\r\nI receive the following error:\r\n\r\n```\r\nError executing job with overrides: []                                                                                  \r\nTraceback (most recent call last):                                                                                      \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/train/train.py\", line 35, in main                                                \r\n    model = CFM(                                                                                                        \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/model/cfm.py\", line 55, in __init__                                              \r\n    self.mel_spec = default(mel_spec_module, MelSpec(**mel_spec_kwargs))                                                \r\nTypeError: MelSpec.__init__() got an unexpected keyword argument 'is_local_vocoder'                                     \r\n```\r\nAnother issue I'm experiencing when using the new prepare_ljspeech script and trying to train using that dataset with char is the following, let me know if I should use a different tokenizer in this senario though.\r\n\r\n```\r\nTraceback (most recent call last):                                                                                      \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/train/train.py\", line 26, in main                                                \r\n    vocab_char_map, vocab_size = get_tokenizer(tokenizer_path, tokenizer)                                               \r\n  File \"/home/daniel/F5-TTS/src/f5_tts/model/utils.py\", line 118, in get_tokenizer                                      \r\n    assert vocab_char_map[\" \"] == 0, \"make sure space is of idx 0 in vocab.txt, cuz 0 is used for unknown char\"         \r\nAssertionError: make sure space is of idx 0 in vocab.txt, cuz 0 is used for unknown char                                \r\n```\r\n\r\nThanks for all of your work on these models.\r\n\r\n\r\n### \u2714\ufe0f Expected Behavior\r\n\r\nmodel training should proceed.\r\n\r\n### \u274c Actual Behavior\r\n\r\ntracebacks as above, which I assume are related to the new training code.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/545/comments",
    "author": "danielw97",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-28T04:37:54Z",
        "body": "Hi @danielw97\r\nThanks for reporting the bug!\r\nCheck if it works now"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T04:45:21Z",
        "body": "Thanks a lot for the quick fix!\r\nThat fixed my first issue with training, however am still having the second one with the prepare_ljspeech vocab.txt that I also mentioned.\r\nIf it's easier I can open another issue for that.\r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-28T04:53:23Z",
        "body": "It would be fine with this issue, we will check for the LJSpeech vocab"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T04:55:01Z",
        "body": "Thanks, in my tests I'm using ljspeech 1.1\r\nIf there's any more info you need, let me know."
      },
      {
        "user": "SWivid",
        "created_at": "2024-11-28T05:22:00Z",
        "body": "Hi @danielw97 \r\nthe second one is because the ljspeech has /n at end, which is not expected to include in the vocab\r\nfixed with adding strip() during processing\r\n"
      },
      {
        "user": "danielw97",
        "created_at": "2024-11-28T05:24:25Z",
        "body": "Thanks for the quick resolution, and explanation of my second issue."
      }
    ],
    "satisfaction_conditions": [
      "Resolve unexpected keyword argument errors in MelSpec initialization",
      "Ensure proper formatting of vocab.txt for tokenizer compatibility",
      "Maintain compatibility with LJSpeech dataset processing",
      "Provide clear error resolution guidance for config-based training flow"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:12:39"
    }
  },
  {
    "number": 471,
    "title": "Moving storage, but f5 still load from old location",
    "created_at": "2024-11-15T23:48:34Z",
    "closed_at": "2024-11-16T02:01:07Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/471",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\nI cloned my f5 installation to a new drive (E:\\ -> Y:\\) and ran f5-tts_finetune-gradio from the new location.\r\nSeems the path for my data checkpoints is still mapped to E:\\. I need help to re-map, so I can delete the data on the source drive.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/471/comments",
    "author": "AlpacaManAlpha",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-16T01:36:20Z",
        "body": "`pip uninstall f5-tts`\r\nDo `pip install -e .` at new path."
      },
      {
        "user": "AlpacaManAlpha",
        "created_at": "2024-11-16T02:00:54Z",
        "body": "This worked, thank you!"
      }
    ],
    "satisfaction_conditions": [
      "Ensures the application references the new installation path for data storage",
      "Provides a method to update internal path references without manual file editing",
      "Allows safe removal of original data from the old drive"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:12:46"
    }
  },
  {
    "number": 412,
    "title": "\u52a0\u8f7d\u897f\u73ed\u7259\u548c\u65e5\u8bed\u6a21\u578b\u7684\u65f6\u5019\uff0c\u62a5\u9519\uff1aRuntimeError: Error(s) in loading state_dict for CFM:",
    "created_at": "2024-11-07T02:20:30Z",
    "closed_at": "2024-11-07T05:19:54Z",
    "labels": [
      "help wanted"
    ],
    "url": "https://github.com/SWivid/F5-TTS/issues/412",
    "body": "### Checks\n\n- [X] This template is only for question, not feature requests or bug reports.\n- [X] I have thoroughly reviewed the project documentation and read the related paper(s).\n- [X] I have searched for existing issues, including closed ones, no similar questions.\n- [X] I confirm that I am using English to submit this report in order to facilitate communication.\n\n### Question details\n\n\u52a0\u8f7dDiscussions\u4e2d\u63d0\u4f9b\u7684\u8bad\u7ec3\u597d\u7684\u65e5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u6a21\u578b\u7684\u65f6\u5019\u62a5\u9519\uff1a\r\n\u65e5\u8bed\u62a5\u9519\uff1a\r\nmodel :  /root/F5-TTS/ckpts/.../model_1108224.pt\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\n\u897f\u73ed\u7259\u8bed\u62a5\u9519\uff1a\r\nmodel :  /root/F5-TTS/ckpts/jpgallegoar/model_1200000.safetensors\r\n\r\nRuntimeError: Error(s) in loading state_dict for CFM:\r\n        Missing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\". ",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/412/comments",
    "author": "liuhui881125",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-07T05:18:36Z",
        "body": "> RuntimeError: Error(s) in loading state_dict for CFM:\r\n> size mismatch for transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2743, 512]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n\r\nUse corresponding vocab.txt\r\n\r\n> RuntimeError: Error(s) in loading state_dict for CFM:\r\nMissing key(s) in state_dict: \"mel_spec.mel_stft.spectrogram.window\", \"mel_spec.mel_stft.mel_scale.fb\".\r\n\r\npull latest repo commit"
      },
      {
        "user": "liuhui881125",
        "created_at": "2024-11-07T05:19:54Z",
        "body": "ok.fix .good"
      }
    ],
    "satisfaction_conditions": [
      "Resolve vocabulary size mismatch between pretrained model and current implementation",
      "Ensure codebase compatibility with model architecture requirements"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:13:43"
    }
  },
  {
    "number": 359,
    "title": "small update gradio finetune",
    "created_at": "2024-11-01T09:23:40Z",
    "closed_at": "2024-11-01T10:22:47Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/pull/359",
    "body": "@SWivid just small fix stuff \r\nso now when audio stereo always get duraction mono and resample\r\nafter train take case stereo to mono and resample\r\nand just fix error speling the bf16",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/359/comments",
    "author": "lpscr",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-11-01T10:22:32Z",
        "body": "@lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification.\r\nJust remove the num_channel in eq is fine."
      },
      {
        "user": "lpscr",
        "created_at": "2024-11-01T10:28:24Z",
        "body": "> @lpscr there's no need to do resample for getting duration, and are using wrong sample_rate to caculate duration in previous modification. Just remove the num_channel in eq is fine.\r\n\r\nok so the first comit was ok then , yes after i confuse and chnage again thank you the fix"
      }
    ],
    "satisfaction_conditions": [
      "Ensures audio duration calculation does not involve resampling",
      "Uses correct sample rate for audio processing calculations",
      "Properly handles stereo-to-mono conversion without unnecessary processing steps",
      "Maintains correct parameter/variable naming conventions"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:13:52"
    }
  },
  {
    "number": 337,
    "title": "RuntimeError: Error(s) in loading state_dict for EMA",
    "created_at": "2024-10-30T17:01:15Z",
    "closed_at": "2024-10-31T01:26:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/337",
    "body": "\u6c42\u52a9\uff0c\u662f\u52a0\u8f7d\u6a21\u578b\u7684\u4ee3\u7801\u4e0d\u5bf9\u5417\uff0c\u6211\u53ea\u6539\u4e86cache_path\u4e3a\u672c\u5730\u7684\u7edd\u5bf9\u8def\u5f84\r\n\r\nTraceback (most recent call last):\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 161, in <module>\r\n    main()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/train/finetune_cli.py\", line 154, in main\r\n    trainer.train(\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 248, in train\r\n    start_step = self.load_checkpoint()\r\n  File \"/data/russell/F5-TTS/src/f5_tts/model/trainer.py\", line 168, in load_checkpoint\r\n    self.ema_model.load_state_dict(checkpoint[\"ema_model_state_dict\"])\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 512]) from checkpoint, the shape in current model is torch.Size([3413, 512]).\r\nTraceback (most recent call last):\r\n  File \"/data/env/f5-tts/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\r\n    args.func(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1168, in launch_command\r\n    simple_launcher(args)\r\n  File \"/data/env/f5-tts/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 763, in simple_launcher\r\n    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/data/env/f5-tts/bin/python', 'src/f5_tts/train/finetune_cli.py', '--exp_name', 'F5TTS_Base', '--learning_rate', '1e-05', '--batch_size_per_gpu', '2400', '--batch_size_type', 'frame', '--max_samples', '64', '--grad_accumulation_steps', '1', '--max_grad_norm', '1', '--epochs', '18107', '--num_warmup_updates', '14', '--save_per_updates', '26', '--last_per_steps', '6', '--dataset_name', 'yn_1030', '--finetune', 'True', '--tokenizer', 'pinyin', '--log_samples', 'True', '--logger', 'wandb']' returned non-zero exit status 1.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/337/comments",
    "author": "russell-shu",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T00:51:44Z",
        "body": "vocab\u5927\u5c0f\u4e0d\u5bf9\uff0c\u5982\u679c\u60f3finetune\u7684\u8bdd\uff0c\u4e0d\u8981\u66f4\u6539vocab"
      },
      {
        "user": "russell-shu",
        "created_at": "2024-10-31T01:11:57Z",
        "body": "\u975e\u5e38\u611f\u8c22\uff0c\u679c\u7136\u662f\u6211\u6539\u4e86vocab\u7684\u539f\u56e0\r\n\r\n> vocab\u5927\u5c0f\u4e0d\u5bf9\uff0c\u5982\u679c\u60f3finetune\u7684\u8bdd\uff0c\u4e0d\u8981\u66f4\u6539vocab\r\n\r\n"
      },
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-31T01:26:16Z",
        "body": "great"
      }
    ],
    "satisfaction_conditions": [
      "Ensures vocabulary size consistency between the pre-trained model and finetuning configuration",
      "Preserves original model architecture parameters during finetuning",
      "Explains relationship between tokenizer configuration and model architecture"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:14:09"
    }
  },
  {
    "number": 322,
    "title": "no such file f5-tts_infer-gradio",
    "created_at": "2024-10-30T01:20:47Z",
    "closed_at": "2024-10-30T11:16:59Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/322",
    "body": "# Launch a Gradio app (web interface)\r\nf5-tts_infer-gradio\r\nThere is no such file in the repo. cannot run it.",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/322/comments",
    "author": "michaeltran33",
    "comments": [
      {
        "user": "justinjohn0306",
        "created_at": "2024-10-30T02:26:23Z",
        "body": "> # Launch a Gradio app (web interface)\r\n> f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n\r\ncd F5-TTS\r\npip install -e ."
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T10:42:55Z",
        "body": "> > # Launch a Gradio app (web interface)\r\n> > f5-tts_infer-gradio There is no such file in the repo. cannot run it.\r\n> \r\n> cd F5-TTS pip install -e .\r\nno such file regardless. Best to review the repo. \r\n"
      },
      {
        "user": "danielw97",
        "created_at": "2024-10-30T10:46:01Z",
        "body": "Please note that this command is available when f5tts is installed as a pip package, not before.\r\nHope this helps a bit."
      },
      {
        "user": "michaeltran33",
        "created_at": "2024-10-30T11:17:00Z",
        "body": "lemme try. found it. it is exe form. thx. \n\n---\n\nit works now. thanks in a bunch."
      }
    ],
    "satisfaction_conditions": [
      "Clarify that the command requires installation of the package first",
      "Explain the relationship between the repository files and the installed package",
      "Address the executable availability mechanism"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:14:15"
    }
  },
  {
    "number": 285,
    "title": "Podcast generation",
    "created_at": "2024-10-26T11:00:31Z",
    "closed_at": "2024-10-26T12:10:26Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/285",
    "body": "I love this but a bit disappointed that the podcast generation has been taken out of the update really used that a lot, how can I get it back is there code for it I could include in the script.\r\nthanks for your work!!!!!",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/285/comments",
    "author": "Cal-DCosta",
    "comments": [
      {
        "user": "samni728",
        "created_at": "2024-10-26T11:18:18Z",
        "body": "in  Multiple Speech-Type Generation \r\n\r\nExample Input 2:\r\n{Speaker1_Happy} Hello, I'd like to order a sandwich please.\r\n{Speaker2_Regular} Sorry, we're out of bread.\r\n{Speaker1_Sad} I really wanted a sandwich though...\r\n{Speaker2_Whisper} I'll give you the last one I was hiding."
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-26T11:26:25Z",
        "body": "Hi @Cal-DCosta ~\r\nMultiple Speech-Type Generation is just for it, and you can generate for a talk (podcast) with more guests invited now lol"
      },
      {
        "user": "Cal-DCosta",
        "created_at": "2024-10-26T11:39:02Z",
        "body": "Ah! sorry did not realise, thanks!!!!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-26T12:10:26Z",
        "body": "@Cal-DCosta Yeah, have fun~\r\nWill close as solved, feel free to open if further questions"
      }
    ],
    "satisfaction_conditions": [
      "Confirmation that podcast-like functionality still exists in the system",
      "Clear documentation of multi-speaker interaction syntax",
      "Support for dynamic conversations with multiple participants"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:14:22"
    }
  },
  {
    "number": 197,
    "title": "Minimal GPU Memory Requirements for Running the Model??",
    "created_at": "2024-10-21T07:10:33Z",
    "closed_at": "2024-10-21T11:24:08Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/197",
    "body": "I would like to know:\r\n\r\nWhat is the minimal GPU memory requirement for running this model smoothly?\r\nIs there a recommended batch size or other configurations (like precision or memory optimizations) I should use to reduce memory usage?\r\ni managed to change batch_size of trainer.py script, utils_infer.py still encountering cuda oom!! can you tell me the minimal requirements of GPU mmeory??",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/197/comments",
    "author": "sachin-seisei",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:05:56Z",
        "body": "the minimal GPU memory is like 2G (just loading F5/E2 TTS model, and not leverage ASR model to do transcription)\r\nbtw you were to do training or inference.\r\n"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:08:13Z",
        "body": "@SWivid so inference only i am doing as of now!! thats why i want to know in inference also why its showing cuda oom!!\r\nPS: i am not leveraging ASR even i am providing ref text also and not an empty string!!"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:15:49Z",
        "body": "which script are you using? gradio_app.py or inference-cli.py"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T09:16:16Z",
        "body": "inference-cli.py \r\n"
      },
      {
        "user": "SWivid",
        "created_at": "2024-10-21T09:48:26Z",
        "body": "how many gpu mem do you have.\r\ni just tweak the `inference-cli.py` script, make sure it is not loading unneeded asr pipeline.\r\nit takes 1622M gpu mem for me running it"
      },
      {
        "user": "sachin-seisei",
        "created_at": "2024-10-21T11:24:08Z",
        "body": "the issue has been resolved that's why i am closing this.. thanks @SWivid "
      }
    ],
    "satisfaction_conditions": [
      "Clear specification of minimal GPU memory required for inference-only usage",
      "Guidance on avoiding unnecessary model component loading",
      "Identification of common pitfalls causing OOM despite meeting minimal specs"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:14:33"
    }
  },
  {
    "number": 192,
    "title": "RuntimeError: Input type (c10::Half) and bias type (float) should be the same",
    "created_at": "2024-10-21T01:57:10Z",
    "closed_at": "2024-10-21T17:55:39Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/192",
    "body": "I just started getting this error with the latest build:\r\n```\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/ttspod/speech/f5.py\", line 214, in infer_batch\r\n    generated_wave = self.vocos.decode(generated_mel_spec.cpu())\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/pretrained.py\", line 112, in decode\r\n    x = self.backbone(features_input, **kwargs)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/vocos/models.py\", line 79, in forward\r\n    x = self.embed(x)\r\n        ^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 308, in forward\r\n    return self._conv_forward(input, self.weight, self.bias)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/adam/ttspod/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 304, in _conv_forward\r\n    return F.conv1d(input, weight, bias, self.stride,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/192/comments",
    "author": "ajkessel",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-21T02:32:21Z",
        "body": "`generated = generated.to(torch.float32)` should be added as we done along with .half() for model and input\r\n```\r\n# Final result\r\ngenerated = generated.to(torch.float32)\r\ngenerated = generated[:, ref_audio_len:, :]\r\ngenerated_mel_spec = generated.permute(0, 2, 1)\r\ngenerated_wave = vocos.decode(generated_mel_spec.cpu())\r\n```"
      }
    ],
    "satisfaction_conditions": [
      "Resolve data type mismatch between model input and bias parameters",
      "Maintain compatibility with existing model quantization/half-precision usage",
      "Ensure tensor type alignment before decoding operations",
      "Address type consistency without breaking subsequent processing steps"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:14:45"
    }
  },
  {
    "number": 165,
    "title": "I get a mismatch error from using a finetune model while using the inference file",
    "created_at": "2024-10-18T17:51:27Z",
    "closed_at": "2024-10-18T20:22:53Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/165",
    "body": "I edit the inference-cli.py file to include the checkpoint path\r\n\r\n```\r\ndef load_model(repo_name, exp_name, model_cls, model_cfg, ckpt_step):\r\n    ckpt_path = f\"/home/user/F5-TTS/ckpts/Finetune/model_last.pt\" # .pt | \r\n.safetensors\r\n```\r\nI left this as it is\r\n\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Emilia_ZH_EN\", \"pinyin\")\r\n```\r\n\r\nThen I changed it after I got a message message\r\n```\r\nvocab_char_map, vocab_size = get_tokenizer(\"Finetune\", \"pinyin\")\r\n```\r\n\r\nThen I use the inference file\r\n```\r\npython inference-cli.py \\\r\n--ref_audio \"/home/user/F5-TTS Test/Template/Input.wav\" \\\r\n--ref_text \"Robinson Industries, the world's leading scientific research and design factory. My dad runs the company. They mass produce his inventions. His motto, keep moving forward. It's what he does.\" \\\r\n--gen_text \"I don't really care what you call me. I've been a silent spectator, watching species evolve, empires rise and fall. But always remember, I am mighty and enduring. Respect me and I'll nurture you; ignore me and you shall face the consequences.\"\r\n```\r\n\r\nThen I get this error message\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 392, in <module>\r\n    process(ref_audio, ref_text, gen_text, model, remove_silence)\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 374, in process\r\n    audio, spectragram = infer(ref_audio, ref_text, gen_text, model, remove_silence)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 346, in infer\r\n    return infer_batch((audio, sr), ref_text, gen_text_batches, model, remove_silence, cross_fade_duration)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 190, in infer_batch\r\n    ema_model = load_model(model, \"F5TTS_Base\", DiT, F5TTS_model_cfg, 1200000)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/inference-cli.py\", line 148, in load_model\r\n    model = load_checkpoint(model, ckpt_path, device, use_ema = True)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/user/F5-TTS/model/utils.py\", line 575, in load_checkpoint\r\n    ema_model.load_state_dict(checkpoint['ema_model_state_dict'])\r\n  File \"/home/user/F5-TTS/venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\r\n    raise RuntimeError(\r\nRuntimeError: Error(s) in loading state_dict for EMA:\r\n        Missing key(s) in state_dict: \"ema_model.transformer.text_embed.text_blocks.0.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.0.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.0.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.0.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.0.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.0.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.0.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.1.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.1.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.1.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.1.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.1.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.1.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.2.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.2.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.2.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.2.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.2.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.2.pwconv2.bias\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.weight\", \"ema_model.transformer.text_embed.text_blocks.3.dwconv.bias\", \"ema_model.transformer.text_embed.text_blocks.3.norm.weight\", \"ema_model.transformer.text_embed.text_blocks.3.norm.bias\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv1.bias\", \"ema_model.transformer.text_embed.text_blocks.3.grn.gamma\", \"ema_model.transformer.text_embed.text_blocks.3.grn.beta\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.weight\", \"ema_model.transformer.text_embed.text_blocks.3.pwconv2.bias\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.0.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.0.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.0.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.1.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.1.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.1.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.2.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.2.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.2.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.3.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.3.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.3.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.4.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.4.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.4.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.5.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.5.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.5.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.6.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.6.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.6.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.7.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.7.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.7.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.8.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.8.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.8.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.9.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.9.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.9.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.10.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.10.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.10.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.11.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.11.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.11.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.12.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.12.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.12.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.13.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.13.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.13.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.14.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.14.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.14.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.15.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.15.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.15.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.16.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.16.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.16.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.17.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.17.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.17.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.18.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.18.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.18.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.19.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.19.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.19.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.20.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.20.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.20.ff.ff.2.bias\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.weight\", \"ema_model.transformer.transformer_blocks.21.attn_norm.linear.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_q.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_k.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_v.bias\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.weight\", \"ema_model.transformer.transformer_blocks.21.attn.to_out.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.0.0.bias\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.weight\", \"ema_model.transformer.transformer_blocks.21.ff.ff.2.bias\", \"ema_model.transformer.norm_out.linear.weight\", \"ema_model.transformer.norm_out.linear.bias\". \r\n        Unexpected key(s) in state_dict: \"ema_model.transformer.layers.0.1.g\", \"ema_model.transformer.layers.0.2.to_q.weight\", \"ema_model.transformer.layers.0.2.to_q.bias\", \"ema_model.transformer.layers.0.2.to_k.weight\", \"ema_model.transformer.layers.0.2.to_k.bias\", \"ema_model.transformer.layers.0.2.to_v.weight\", \"ema_model.transformer.layers.0.2.to_v.bias\", \"ema_model.transformer.layers.0.2.to_out.0.weight\", \"ema_model.transformer.layers.0.2.to_out.0.bias\", \"ema_model.transformer.layers.0.3.g\", \"ema_model.transformer.layers.0.4.ff.0.0.weight\", \"ema_model.transformer.layers.0.4.ff.0.0.bias\", \"ema_model.transformer.layers.0.4.ff.2.weight\", \"ema_model.transformer.layers.0.4.ff.2.bias\", \"ema_model.transformer.layers.1.1.g\", \"ema_model.transformer.layers.1.2.to_q.weight\", \"ema_model.transformer.layers.1.2.to_q.bias\", \"ema_model.transformer.layers.1.2.to_k.weight\", \"ema_model.transformer.layers.1.2.to_k.bias\", \"ema_model.transformer.layers.1.2.to_v.weight\", \"ema_model.transformer.layers.1.2.to_v.bias\", \"ema_model.transformer.layers.1.2.to_out.0.weight\", \"ema_model.transformer.layers.1.2.to_out.0.bias\", \"ema_model.transformer.layers.1.3.g\", \"ema_model.transformer.layers.1.4.ff.0.0.weight\", \"ema_model.transformer.layers.1.4.ff.0.0.bias\", \"ema_model.transformer.layers.1.4.ff.2.weight\", \"ema_model.transformer.layers.1.4.ff.2.bias\", \"ema_model.transformer.layers.2.1.g\", \"ema_model.transformer.layers.2.2.to_q.weight\", \"ema_model.transformer.layers.2.2.to_q.bias\", \"ema_model.transformer.layers.2.2.to_k.weight\", \"ema_model.transformer.layers.2.2.to_k.bias\", \"ema_model.transformer.layers.2.2.to_v.weight\", \"ema_model.transformer.layers.2.2.to_v.bias\", \"ema_model.transformer.layers.2.2.to_out.0.weight\", \"ema_model.transformer.layers.2.2.to_out.0.bias\", \"ema_model.transformer.layers.2.3.g\", \"ema_model.transformer.layers.2.4.ff.0.0.weight\", \"ema_model.transformer.layers.2.4.ff.0.0.bias\", \"ema_model.transformer.layers.2.4.ff.2.weight\", \"ema_model.transformer.layers.2.4.ff.2.bias\", \"ema_model.transformer.layers.3.1.g\", \"ema_model.transformer.layers.3.2.to_q.weight\", \"ema_model.transformer.layers.3.2.to_q.bias\", \"ema_model.transformer.layers.3.2.to_k.weight\", \"ema_model.transformer.layers.3.2.to_k.bias\", \"ema_model.transformer.layers.3.2.to_v.weight\", \"ema_model.transformer.layers.3.2.to_v.bias\", \"ema_model.transformer.layers.3.2.to_out.0.weight\", \"ema_model.transformer.layers.3.2.to_out.0.bias\", \"ema_model.transformer.layers.3.3.g\", \"ema_model.transformer.layers.3.4.ff.0.0.weight\", \"ema_model.transformer.layers.3.4.ff.0.0.bias\", \"ema_model.transformer.layers.3.4.ff.2.weight\", \"ema_model.transformer.layers.3.4.ff.2.bias\", \"ema_model.transformer.layers.4.1.g\", \"ema_model.transformer.layers.4.2.to_q.weight\", \"ema_model.transformer.layers.4.2.to_q.bias\", \"ema_model.transformer.layers.4.2.to_k.weight\", \"ema_model.transformer.layers.4.2.to_k.bias\", \"ema_model.transformer.layers.4.2.to_v.weight\", \"ema_model.transformer.layers.4.2.to_v.bias\", \"ema_model.transformer.layers.4.2.to_out.0.weight\", \"ema_model.transformer.layers.4.2.to_out.0.bias\", \"ema_model.transformer.layers.4.3.g\", \"ema_model.transformer.layers.4.4.ff.0.0.weight\", \"ema_model.transformer.layers.4.4.ff.0.0.bias\", \"ema_model.transformer.layers.4.4.ff.2.weight\", \"ema_model.transformer.layers.4.4.ff.2.bias\", \"ema_model.transformer.layers.5.1.g\", \"ema_model.transformer.layers.5.2.to_q.weight\", \"ema_model.transformer.layers.5.2.to_q.bias\", \"ema_model.transformer.layers.5.2.to_k.weight\", \"ema_model.transformer.layers.5.2.to_k.bias\", \"ema_model.transformer.layers.5.2.to_v.weight\", \"ema_model.transformer.layers.5.2.to_v.bias\", \"ema_model.transformer.layers.5.2.to_out.0.weight\", \"ema_model.transformer.layers.5.2.to_out.0.bias\", \"ema_model.transformer.layers.5.3.g\", \"ema_model.transformer.layers.5.4.ff.0.0.weight\", \"ema_model.transformer.layers.5.4.ff.0.0.bias\", \"ema_model.transformer.layers.5.4.ff.2.weight\", \"ema_model.transformer.layers.5.4.ff.2.bias\", \"ema_model.transformer.layers.6.1.g\", \"ema_model.transformer.layers.6.2.to_q.weight\", \"ema_model.transformer.layers.6.2.to_q.bias\", \"ema_model.transformer.layers.6.2.to_k.weight\", \"ema_model.transformer.layers.6.2.to_k.bias\", \"ema_model.transformer.layers.6.2.to_v.weight\", \"ema_model.transformer.layers.6.2.to_v.bias\", \"ema_model.transformer.layers.6.2.to_out.0.weight\", \"ema_model.transformer.layers.6.2.to_out.0.bias\", \"ema_model.transformer.layers.6.3.g\", \"ema_model.transformer.layers.6.4.ff.0.0.weight\", \"ema_model.transformer.layers.6.4.ff.0.0.bias\", \"ema_model.transformer.layers.6.4.ff.2.weight\", \"ema_model.transformer.layers.6.4.ff.2.bias\", \"ema_model.transformer.layers.7.1.g\", \"ema_model.transformer.layers.7.2.to_q.weight\", \"ema_model.transformer.layers.7.2.to_q.bias\", \"ema_model.transformer.layers.7.2.to_k.weight\", \"ema_model.transformer.layers.7.2.to_k.bias\", \"ema_model.transformer.layers.7.2.to_v.weight\", \"ema_model.transformer.layers.7.2.to_v.bias\", \"ema_model.transformer.layers.7.2.to_out.0.weight\", \"ema_model.transformer.layers.7.2.to_out.0.bias\", \"ema_model.transformer.layers.7.3.g\", \"ema_model.transformer.layers.7.4.ff.0.0.weight\", \"ema_model.transformer.layers.7.4.ff.0.0.bias\", \"ema_model.transformer.layers.7.4.ff.2.weight\", \"ema_model.transformer.layers.7.4.ff.2.bias\", \"ema_model.transformer.layers.8.1.g\", \"ema_model.transformer.layers.8.2.to_q.weight\", \"ema_model.transformer.layers.8.2.to_q.bias\", \"ema_model.transformer.layers.8.2.to_k.weight\", \"ema_model.transformer.layers.8.2.to_k.bias\", \"ema_model.transformer.layers.8.2.to_v.weight\", \"ema_model.transformer.layers.8.2.to_v.bias\", \"ema_model.transformer.layers.8.2.to_out.0.weight\", \"ema_model.transformer.layers.8.2.to_out.0.bias\", \"ema_model.transformer.layers.8.3.g\", \"ema_model.transformer.layers.8.4.ff.0.0.weight\", \"ema_model.transformer.layers.8.4.ff.0.0.bias\", \"ema_model.transformer.layers.8.4.ff.2.weight\", \"ema_model.transformer.layers.8.4.ff.2.bias\", \"ema_model.transformer.layers.9.1.g\", \"ema_model.transformer.layers.9.2.to_q.weight\", \"ema_model.transformer.layers.9.2.to_q.bias\", \"ema_model.transformer.layers.9.2.to_k.weight\", \"ema_model.transformer.layers.9.2.to_k.bias\", \"ema_model.transformer.layers.9.2.to_v.weight\", \"ema_model.transformer.layers.9.2.to_v.bias\", \"ema_model.transformer.layers.9.2.to_out.0.weight\", \"ema_model.transformer.layers.9.2.to_out.0.bias\", \"ema_model.transformer.layers.9.3.g\", \"ema_model.transformer.layers.9.4.ff.0.0.weight\", \"ema_model.transformer.layers.9.4.ff.0.0.bias\", \"ema_model.transformer.layers.9.4.ff.2.weight\", \"ema_model.transformer.layers.9.4.ff.2.bias\", \"ema_model.transformer.layers.10.1.g\", \"ema_model.transformer.layers.10.2.to_q.weight\", \"ema_model.transformer.layers.10.2.to_q.bias\", \"ema_model.transformer.layers.10.2.to_k.weight\", \"ema_model.transformer.layers.10.2.to_k.bias\", \"ema_model.transformer.layers.10.2.to_v.weight\", \"ema_model.transformer.layers.10.2.to_v.bias\", \"ema_model.transformer.layers.10.2.to_out.0.weight\", \"ema_model.transformer.layers.10.2.to_out.0.bias\", \"ema_model.transformer.layers.10.3.g\", \"ema_model.transformer.layers.10.4.ff.0.0.weight\", \"ema_model.transformer.layers.10.4.ff.0.0.bias\", \"ema_model.transformer.layers.10.4.ff.2.weight\", \"ema_model.transformer.layers.10.4.ff.2.bias\", \"ema_model.transformer.layers.11.1.g\", \"ema_model.transformer.layers.11.2.to_q.weight\", \"ema_model.transformer.layers.11.2.to_q.bias\", \"ema_model.transformer.layers.11.2.to_k.weight\", \"ema_model.transformer.layers.11.2.to_k.bias\", \"ema_model.transformer.layers.11.2.to_v.weight\", \"ema_model.transformer.layers.11.2.to_v.bias\", \"ema_model.transformer.layers.11.2.to_out.0.weight\", \"ema_model.transformer.layers.11.2.to_out.0.bias\", \"ema_model.transformer.layers.11.3.g\", \"ema_model.transformer.layers.11.4.ff.0.0.weight\", \"ema_model.transformer.layers.11.4.ff.0.0.bias\", \"ema_model.transformer.layers.11.4.ff.2.weight\", \"ema_model.transformer.layers.11.4.ff.2.bias\", \"ema_model.transformer.layers.12.0.weight\", \"ema_model.transformer.layers.12.1.g\", \"ema_model.transformer.layers.12.2.to_q.weight\", \"ema_model.transformer.layers.12.2.to_q.bias\", \"ema_model.transformer.layers.12.2.to_k.weight\", \"ema_model.transformer.layers.12.2.to_k.bias\", \"ema_model.transformer.layers.12.2.to_v.weight\", \"ema_model.transformer.layers.12.2.to_v.bias\", \"ema_model.transformer.layers.12.2.to_out.0.weight\", \"ema_model.transformer.layers.12.2.to_out.0.bias\", \"ema_model.transformer.layers.12.3.g\", \"ema_model.transformer.layers.12.4.ff.0.0.weight\", \"ema_model.transformer.layers.12.4.ff.0.0.bias\", \"ema_model.transformer.layers.12.4.ff.2.weight\", \"ema_model.transformer.layers.12.4.ff.2.bias\", \"ema_model.transformer.layers.13.0.weight\", \"ema_model.transformer.layers.13.1.g\", \"ema_model.transformer.layers.13.2.to_q.weight\", \"ema_model.transformer.layers.13.2.to_q.bias\", \"ema_model.transformer.layers.13.2.to_k.weight\", \"ema_model.transformer.layers.13.2.to_k.bias\", \"ema_model.transformer.layers.13.2.to_v.weight\", \"ema_model.transformer.layers.13.2.to_v.bias\", \"ema_model.transformer.layers.13.2.to_out.0.weight\", \"ema_model.transformer.layers.13.2.to_out.0.bias\", \"ema_model.transformer.layers.13.3.g\", \"ema_model.transformer.layers.13.4.ff.0.0.weight\", \"ema_model.transformer.layers.13.4.ff.0.0.bias\", \"ema_model.transformer.layers.13.4.ff.2.weight\", \"ema_model.transformer.layers.13.4.ff.2.bias\", \"ema_model.transformer.layers.14.0.weight\", \"ema_model.transformer.layers.14.1.g\", \"ema_model.transformer.layers.14.2.to_q.weight\", \"ema_model.transformer.layers.14.2.to_q.bias\", \"ema_model.transformer.layers.14.2.to_k.weight\", \"ema_model.transformer.layers.14.2.to_k.bias\", \"ema_model.transformer.layers.14.2.to_v.weight\", \"ema_model.transformer.layers.14.2.to_v.bias\", \"ema_model.transformer.layers.14.2.to_out.0.weight\", \"ema_model.transformer.layers.14.2.to_out.0.bias\", \"ema_model.transformer.layers.14.3.g\", \"ema_model.transformer.layers.14.4.ff.0.0.weight\", \"ema_model.transformer.layers.14.4.ff.0.0.bias\", \"ema_model.transformer.layers.14.4.ff.2.weight\", \"ema_model.transformer.layers.14.4.ff.2.bias\", \"ema_model.transformer.layers.15.0.weight\", \"ema_model.transformer.layers.15.1.g\", \"ema_model.transformer.layers.15.2.to_q.weight\", \"ema_model.transformer.layers.15.2.to_q.bias\", \"ema_model.transformer.layers.15.2.to_k.weight\", \"ema_model.transformer.layers.15.2.to_k.bias\", \"ema_model.transformer.layers.15.2.to_v.weight\", \"ema_model.transformer.layers.15.2.to_v.bias\", \"ema_model.transformer.layers.15.2.to_out.0.weight\", \"ema_model.transformer.layers.15.2.to_out.0.bias\", \"ema_model.transformer.layers.15.3.g\", \"ema_model.transformer.layers.15.4.ff.0.0.weight\", \"ema_model.transformer.layers.15.4.ff.0.0.bias\", \"ema_model.transformer.layers.15.4.ff.2.weight\", \"ema_model.transformer.layers.15.4.ff.2.bias\", \"ema_model.transformer.layers.16.0.weight\", \"ema_model.transformer.layers.16.1.g\", \"ema_model.transformer.layers.16.2.to_q.weight\", \"ema_model.transformer.layers.16.2.to_q.bias\", \"ema_model.transformer.layers.16.2.to_k.weight\", \"ema_model.transformer.layers.16.2.to_k.bias\", \"ema_model.transformer.layers.16.2.to_v.weight\", \"ema_model.transformer.layers.16.2.to_v.bias\", \"ema_model.transformer.layers.16.2.to_out.0.weight\", \"ema_model.transformer.layers.16.2.to_out.0.bias\", \"ema_model.transformer.layers.16.3.g\", \"ema_model.transformer.layers.16.4.ff.0.0.weight\", \"ema_model.transformer.layers.16.4.ff.0.0.bias\", \"ema_model.transformer.layers.16.4.ff.2.weight\", \"ema_model.transformer.layers.16.4.ff.2.bias\", \"ema_model.transformer.layers.17.0.weight\", \"ema_model.transformer.layers.17.1.g\", \"ema_model.transformer.layers.17.2.to_q.weight\", \"ema_model.transformer.layers.17.2.to_q.bias\", \"ema_model.transformer.layers.17.2.to_k.weight\", \"ema_model.transformer.layers.17.2.to_k.bias\", \"ema_model.transformer.layers.17.2.to_v.weight\", \"ema_model.transformer.layers.17.2.to_v.bias\", \"ema_model.transformer.layers.17.2.to_out.0.weight\", \"ema_model.transformer.layers.17.2.to_out.0.bias\", \"ema_model.transformer.layers.17.3.g\", \"ema_model.transformer.layers.17.4.ff.0.0.weight\", \"ema_model.transformer.layers.17.4.ff.0.0.bias\", \"ema_model.transformer.layers.17.4.ff.2.weight\", \"ema_model.transformer.layers.17.4.ff.2.bias\", \"ema_model.transformer.layers.18.0.weight\", \"ema_model.transformer.layers.18.1.g\", \"ema_model.transformer.layers.18.2.to_q.weight\", \"ema_model.transformer.layers.18.2.to_q.bias\", \"ema_model.transformer.layers.18.2.to_k.weight\", \"ema_model.transformer.layers.18.2.to_k.bias\", \"ema_model.transformer.layers.18.2.to_v.weight\", \"ema_model.transformer.layers.18.2.to_v.bias\", \"ema_model.transformer.layers.18.2.to_out.0.weight\", \"ema_model.transformer.layers.18.2.to_out.0.bias\", \"ema_model.transformer.layers.18.3.g\", \"ema_model.transformer.layers.18.4.ff.0.0.weight\", \"ema_model.transformer.layers.18.4.ff.0.0.bias\", \"ema_model.transformer.layers.18.4.ff.2.weight\", \"ema_model.transformer.layers.18.4.ff.2.bias\", \"ema_model.transformer.layers.19.0.weight\", \"ema_model.transformer.layers.19.1.g\", \"ema_model.transformer.layers.19.2.to_q.weight\", \"ema_model.transformer.layers.19.2.to_q.bias\", \"ema_model.transformer.layers.19.2.to_k.weight\", \"ema_model.transformer.layers.19.2.to_k.bias\", \"ema_model.transformer.layers.19.2.to_v.weight\", \"ema_model.transformer.layers.19.2.to_v.bias\", \"ema_model.transformer.layers.19.2.to_out.0.weight\", \"ema_model.transformer.layers.19.2.to_out.0.bias\", \"ema_model.transformer.layers.19.3.g\", \"ema_model.transformer.layers.19.4.ff.0.0.weight\", \"ema_model.transformer.layers.19.4.ff.0.0.bias\", \"ema_model.transformer.layers.19.4.ff.2.weight\", \"ema_model.transformer.layers.19.4.ff.2.bias\", \"ema_model.transformer.layers.20.0.weight\", \"ema_model.transformer.layers.20.1.g\", \"ema_model.transformer.layers.20.2.to_q.weight\", \"ema_model.transformer.layers.20.2.to_q.bias\", \"ema_model.transformer.layers.20.2.to_k.weight\", \"ema_model.transformer.layers.20.2.to_k.bias\", \"ema_model.transformer.layers.20.2.to_v.weight\", \"ema_model.transformer.layers.20.2.to_v.bias\", \"ema_model.transformer.layers.20.2.to_out.0.weight\", \"ema_model.transformer.layers.20.2.to_out.0.bias\", \"ema_model.transformer.layers.20.3.g\", \"ema_model.transformer.layers.20.4.ff.0.0.weight\", \"ema_model.transformer.layers.20.4.ff.0.0.bias\", \"ema_model.transformer.layers.20.4.ff.2.weight\", \"ema_model.transformer.layers.20.4.ff.2.bias\", \"ema_model.transformer.layers.21.0.weight\", \"ema_model.transformer.layers.21.1.g\", \"ema_model.transformer.layers.21.2.to_q.weight\", \"ema_model.transformer.layers.21.2.to_q.bias\", \"ema_model.transformer.layers.21.2.to_k.weight\", \"ema_model.transformer.layers.21.2.to_k.bias\", \"ema_model.transformer.layers.21.2.to_v.weight\", \"ema_model.transformer.layers.21.2.to_v.bias\", \"ema_model.transformer.layers.21.2.to_out.0.weight\", \"ema_model.transformer.layers.21.2.to_out.0.bias\", \"ema_model.transformer.layers.21.3.g\", \"ema_model.transformer.layers.21.4.ff.0.0.weight\", \"ema_model.transformer.layers.21.4.ff.0.0.bias\", \"ema_model.transformer.layers.21.4.ff.2.weight\", \"ema_model.transformer.layers.21.4.ff.2.bias\", \"ema_model.transformer.layers.22.0.weight\", \"ema_model.transformer.layers.22.1.g\", \"ema_model.transformer.layers.22.2.to_q.weight\", \"ema_model.transformer.layers.22.2.to_q.bias\", \"ema_model.transformer.layers.22.2.to_k.weight\", \"ema_model.transformer.layers.22.2.to_k.bias\", \"ema_model.transformer.layers.22.2.to_v.weight\", \"ema_model.transformer.layers.22.2.to_v.bias\", \"ema_model.transformer.layers.22.2.to_out.0.weight\", \"ema_model.transformer.layers.22.2.to_out.0.bias\", \"ema_model.transformer.layers.22.3.g\", \"ema_model.transformer.layers.22.4.ff.0.0.weight\", \"ema_model.transformer.layers.22.4.ff.0.0.bias\", \"ema_model.transformer.layers.22.4.ff.2.weight\", \"ema_model.transformer.layers.22.4.ff.2.bias\", \"ema_model.transformer.layers.23.0.weight\", \"ema_model.transformer.layers.23.1.g\", \"ema_model.transformer.layers.23.2.to_q.weight\", \"ema_model.transformer.layers.23.2.to_q.bias\", \"ema_model.transformer.layers.23.2.to_k.weight\", \"ema_model.transformer.layers.23.2.to_k.bias\", \"ema_model.transformer.layers.23.2.to_v.weight\", \"ema_model.transformer.layers.23.2.to_v.bias\", \"ema_model.transformer.layers.23.2.to_out.0.weight\", \"ema_model.transformer.layers.23.2.to_out.0.bias\", \"ema_model.transformer.layers.23.3.g\", \"ema_model.transformer.layers.23.4.ff.0.0.weight\", \"ema_model.transformer.layers.23.4.ff.0.0.bias\", \"ema_model.transformer.layers.23.4.ff.2.weight\", \"ema_model.transformer.layers.23.4.ff.2.bias\", \"ema_model.transformer.norm_out.g\". \r\n        size mismatch for ema_model.transformer.text_embed.text_embed.weight: copying a param with shape torch.Size([2546, 100]) from checkpoint, the shape in current model is torch.Size([2546, 512]).\r\n        size mismatch for ema_model.transformer.input_embed.proj.weight: copying a param with shape torch.Size([1024, 300]) from checkpoint, the shape in current model is torch.Size([1024, 712]).\r\n```",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/165/comments",
    "author": "GUUser91",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-18T18:37:27Z",
        "body": "make sure you use corrrect model_type  also **F5TTS_Base** or **E2TTS_Base** this you finetune\r\n\r\n"
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T20:10:36Z",
        "body": "@lpscr\r\nI used the finetune_gradio.py file and selected E2TTS_Base for finetuning. Then I clicked on auto settings. Then I click on start training.\r\nI also used finetune-cli.py and the same thing happened.\n\n---\n\n@lpscr\r\nI used the finetune_gradio.py file again and finetuned with the F5TTS_Base model and now I don't have the error if I used this finetuned F5TTS_Base model. But I want to finetune with E2TTS_Base, can you try finetuning with E2TTS_Base on your end to see if it works?"
      },
      {
        "user": "lpscr",
        "created_at": "2024-10-18T20:13:30Z",
        "body": "yes because you need to change in\r\nstart the interface-cli -m E2-TTS\r\n\r\n\r\nparser.add_argument(\r\n    \"-m\",\r\n    \"--model\",\r\n    help=\"F5-TTS | E2-TTS\",\r\n)\n\n---\n\nwhat gpu you have and memory  ? because i working in auto setting , "
      },
      {
        "user": "GUUser91",
        "created_at": "2024-10-18T20:22:54Z",
        "body": "@lpscr\r\nThat did it. Thank you. I'm using a 4090. I have 64GB Ram."
      }
    ],
    "satisfaction_conditions": [
      "Explanation of model architecture compatibility between finetuning and inference",
      "Clarification of model type specification requirements"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:15:26"
    }
  },
  {
    "number": 145,
    "title": "Run time error while fine tuning",
    "created_at": "2024-10-17T15:26:06Z",
    "closed_at": "2024-10-21T07:00:03Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/145",
    "body": "I am trying to fine tune the model. And I am stuck with this error \r\n\r\n>`Epoch 1/10:  23% 34/145 [00:24<01:21,  1.36step/s, loss=1.92, step=34]\r\nTraceback (most recent call last):\r\n  File \"/content/F5-TTS/train.py\", line 94, in <module>\r\n    main()\r\n  File \"/content/F5-TTS/train.py\", line 88, in main\r\n    trainer.train(train_dataset,\r\n  File \"/content/F5-TTS/model/trainer.py\", line 229, in train\r\n    loss, cond, pred = self.model(mel_spec, text=text_inputs, lens=mel_lengths, noise_scheduler=self.noise_scheduler)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 820, in forward\r\n    return model_forward(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 808, in __call__\r\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 43, in decorate_autocast\r\n    return func(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/cfm.py\", line 273, in forward\r\n    pred = self.transformer(x = \u03c6, cond = cond, text = text, time = time, drop_audio_cond = drop_audio_cond, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 162, in forward\r\n    text_embed = self.text_embed(text, seq_len, drop_text = drop_text)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/content/F5-TTS/model/backbones/unett.py\", line 57, in forward\r\n    text = self.text_embed(text) # b n -> b n d\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\", line 164, in forward\r\n    return F.embedding(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2267, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)`\r\n\r\n\r\n\r\n\r\nthese are the inputs I am using.\r\n\r\ntarget_sample_rate = 24000\r\nn_mel_channels = 100\r\nhop_length = 256\r\n\r\ntokenizer = \"pinyin\" # 'pinyin', 'char', or 'custom'\r\ntokenizer_path = None # if tokenizer = 'custom', define the path to the tokenizer you want to use (should be vocab.txt)\r\ndataset_name = \"My_Dataset\"\r\n\r\n# -------------------------- Training Settings -------------------------- #\r\n\r\nexp_name = \"E2TTS_Base\"  # F5TTS_Base | E2TTS_Base\r\n\r\nlearning_rate = 5e-06\r\n\r\nbatch_size_per_gpu = 38400  # 8 GPUs, 8 * 38400 = 307200\r\nbatch_size_type = \"frame\"  # \"frame\" or \"sample\"\r\nmax_samples = 2  # max sequences per batch if use frame-wise batch_size. we set 32 for small models, 64 for base models\r\ngrad_accumulation_steps = 1  # note: updates = steps / grad_accumulation_steps\r\nmax_grad_norm = 1.\r\n\r\nepochs = 10  # use linear decay, thus epochs control the slope\r\nnum_warmup_updates = 20  # warmup steps\r\nsave_per_updates = 500  # save checkpoint per steps\r\nlast_per_steps = 5000  # save last checkpoint per steps\r\n\r\n\r\n\r\nIs this error something related to my dataset or my input? Can anyone help?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/145/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "ZhikangNiu",
        "created_at": "2024-10-19T02:12:00Z",
        "body": "'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\r\n\r\nmaybe you need to check the tensor's type"
      },
      {
        "user": "MilanaShhanukova",
        "created_at": "2024-10-19T19:48:36Z",
        "body": "@rasheed-aidetic I may guess that you have an empty text sample, check it out. If it is intended, you may add the type converter in TextEmbedding\r\n\r\n```\r\n        if text.dtype is not torch.long:\r\n            text = text.long()\r\n```\r\n\r\n"
      }
    ],
    "satisfaction_conditions": [
      "Identifies the root cause of the tensor type mismatch in the embedding layer",
      "Explains how to validate text input preprocessing",
      "Provides guidance on tensor type conversion requirements"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:15:34"
    }
  },
  {
    "number": 144,
    "title": "Shape mismatch error while fine tuning (non-singleton dimension 1)",
    "created_at": "2024-10-17T12:37:13Z",
    "closed_at": "2024-10-17T14:56:18Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/144",
    "body": "Hi This project is incredible guys. Great work.\r\n\r\nI am trying to finetune this model and facing an error on shape mismatch. Can anyone help?\r\n\r\n`Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\r\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/content/F5-TTS/model/dataset.py\", line 117, in __getitem__\r\n    mel_spec = rearrange(mel_spec, '1 d t -> d t')\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 591, in rearrange\r\n    return reduce(tensor, pattern, reduction=\"rearrange\", **axes_lengths)\r\n  File \"/usr/local/lib/python3.10/dist-packages/einops/einops.py\", line 533, in reduce\r\n    raise EinopsError(message + \"\\n {}\".format(e))\r\neinops.EinopsError:  Error while processing rearrange-reduction pattern \"1 d t -> d t\".\r\n Input tensor shape: torch.Size([2, 100, 1407]). Additional info: {}.\r\n Shape mismatch, 2 != 1`\r\n\r\nI am trying to fine tune the model with my own voice model. I have created the dataset using the prepare_csv_wavs.py script. Please let me know if I need to make any changes in the input section of train.py\r\n",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/144/comments",
    "author": "rasheed-aidetic",
    "comments": [
      {
        "user": "lpscr",
        "created_at": "2024-10-17T12:57:14Z",
        "body": "i think the problem is you have stereo you need to  resample your audio to **mono** **24000 hz**\r\n\r\nrun this script please first make back up because replace your original files !\r\n\r\n```\r\nimport os\r\nimport glob\r\nfrom pydub import AudioSegment\r\n\r\ndef convert_wav_to_mono(folder_path):\r\n    wav_files = glob.glob(os.path.join(folder_path, '*.wav'))\r\n    for file_path in wav_files:\r\n        filename = os.path.basename(file_path)\r\n        audio = AudioSegment.from_wav(file_path)\r\n        mono_audio = audio.set_channels(1)\r\n        mono_audio = mono_audio.set_frame_rate(24000)\r\n        new_file_path = os.path.join(folder_path, f\"mono_{filename}\")\r\n        mono_audio.export(new_file_path, format=\"wav\")\r\n\r\nfolder_path = 'your_folder_path'\r\nconvert_wav_to_mono(folder_path)\r\n```\r\n"
      },
      {
        "user": "rasheed-aidetic",
        "created_at": "2024-10-17T14:56:13Z",
        "body": "Working after the change. Thank you so much for the quick reply."
      }
    ],
    "satisfaction_conditions": [
      "Ensures audio input is mono-channel",
      "Validates audio sample rate compatibility",
      "Resolves tensor shape mismatch in mel spectrogram processing"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:15:40"
    }
  },
  {
    "number": 117,
    "title": "Python version",
    "created_at": "2024-10-16T08:06:20Z",
    "closed_at": "2024-10-16T08:13:26Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/117",
    "body": "It would be a good idea to specify the version of python. From my tests the project works for` >3.9`",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/117/comments",
    "author": "Mateleo",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-16T08:11:48Z",
        "body": "yes, we have the badge in readme showing that python 3.10 is recommended"
      }
    ],
    "satisfaction_conditions": [
      "Explicit documentation of Python version compatibility",
      "Clarification of minimum supported version vs. recommended version",
      "Visibility of version requirements in project documentation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:15:46"
    }
  },
  {
    "number": 33,
    "title": "precompute_max_pos = 4096 \u76ee\u524d\u6700\u5927\u53ea\u80fd\u652f\u6301\u52304096token\u7684\u6587\u672c\u957f\u5ea6\u4e48\uff1f",
    "created_at": "2024-10-13T13:20:08Z",
    "closed_at": "2024-10-14T02:46:10Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/33",
    "body": "      if conv_layers > 0:\r\n          self.extra_modeling = True\r\n          self.precompute_max_pos = 4096  # ~44s of 24khz audio\r\n          self.register_buffer(\"freqs_cis\", precompute_freqs_cis(text_dim, self.precompute_max_pos), persistent=False)\r\n          self.text_blocks = nn.Sequential(*[ConvNeXtV2Block(text_dim, text_dim * conv_mult) for _ in range(conv_layers)])\r\n\r\n\u8bf7\u95ee\u540e\u7eed\u662f\u5426\u6709\u8ba1\u5212\u652f\u6301\u66f4\u5927\u7684\u957f\u5ea6\u3002",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/33/comments",
    "author": "pengyong94",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-14T02:46:10Z",
        "body": "\u5b9e\u9645\u4e0a\u76ee\u524d\u8bad\u7ec3\u96c6\u6700\u957f\u662f30\u79d2\uff0c\u6240\u4ee54096 ~44s\u5b8c\u5168\u8986\u76d6\u4e86\uff08\u56e0\u4e3atext\u662fpad\u5230\u548cmel\u4e00\u6837\u957f\uff0c\u800cmel\u6700\u957f\u5c31\u662f30*24000/256\u76ee\u524d\uff09\r\n\u76ee\u524d\u901a\u8fc7\u5206\u5757\u751f\u6210\uff0c30s\u5e94\u8be5\u591f\u7528\uff0c\u4e4b\u540e\u5927\u6982\u7387\u4f1a\u5f80\u6d41\u5f0f\u7684\u65b9\u5411\u8d70\n\n---\n\nI'll close this issue, if you have other questions you can reopen this issue."
      },
      {
        "user": "pengyong94",
        "created_at": "2024-10-14T04:21:20Z",
        "body": "ok, thanks for your reply ,and your work is great !"
      }
    ],
    "satisfaction_conditions": [
      "Explanation of why the current text length limitation exists",
      "Clarification of future development direction for longer text handling",
      "Rationale for current sufficiency of the implementation"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:16:02"
    }
  },
  {
    "number": 6,
    "title": "Can it be used to edit speech?",
    "created_at": "2024-10-10T10:06:20Z",
    "closed_at": "2024-10-11T01:28:09Z",
    "labels": [],
    "url": "https://github.com/SWivid/F5-TTS/issues/6",
    "body": "Can this model to be used to edit speech?\r\n\r\nIt follow VoiceBox, but why E2-TTS do not mention its ability of redact speech, or it just can not. Or, I missed some thing?",
    "comments_url": "https://api.github.com/repos/SWivid/F5-TTS/issues/6/comments",
    "author": "chenht2021",
    "comments": [
      {
        "user": "SWivid",
        "created_at": "2024-10-10T16:43:40Z",
        "body": "As it was trained with in-filling task, editing ability should be inherited.\r\nSimply mask out the part for edit, synthesize and replace into this part.\r\n\r\nI'll try it out and if it works I'll add it to the script.\n\n---\n\n@chenht2021 It works well\r\nsee `python test_infer_single_edit.py`\r\nhave fun~"
      },
      {
        "user": "chenht2021",
        "created_at": "2024-10-11T01:28:02Z",
        "body": "Thanks, I figure out I made a mistake sending wrong mask to model.\r\nThanks for open source this model. Nice work!"
      }
    ],
    "satisfaction_conditions": [
      "Demonstrates the model's capability for speech editing through in-filling tasks",
      "Validates functionality through testable examples"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:16:06"
    }
  }
]