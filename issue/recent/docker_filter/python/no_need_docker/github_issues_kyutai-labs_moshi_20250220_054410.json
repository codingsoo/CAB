[
  {
    "number": 175,
    "title": "Semantic token inference for user stream",
    "created_at": "2024-12-26T01:50:39Z",
    "closed_at": "2024-12-26T08:43:31Z",
    "labels": [
      "question"
    ],
    "url": "https://github.com/kyutai-labs/moshi/issues/175",
    "body": "### Due diligence\n\n- [X] I have done my due diligence in trying to find the answer myself.\n\n### Topic\n\nThe PyTorch implementation\n\n### Question\n\n(My question is about the behavior of depformer in the training phase) I found that the semantic token inference for **moshi** stream is based on depformer's text emb and transformer_out \r\n\r\n```python\r\n        depformer_input = transformer_out\r\n        if self.depformer_multi_linear:\r\n            depformer_input = self.depformer_in[depformer_cb_index](depformer_input)  # depformer_in: cb_index \ubcc4\ub85c trasnforme_out encoding\r\n        else:\r\n            depformer_input = self.depformer_in[0](depformer_input)\r\n        if depformer_cb_index == 0:\r\n            last_token_input = self.depformer_text_emb(sequence[:, 0])\r\n        else:\r\n            last_token_input = self.depformer_emb[depformer_cb_index - 1](\r\n                sequence[:, 0]\r\n            )\r\n        depformer_input = depformer_input + last_token_input\r\n        assert depformer_input.shape[1] == 1\r\n        # depformer_input is [B, 1, depformer_dim].\r\n        # The streaming state of the depformer ensures that the proper layer is run.\r\n        dep_output = self.depformer(depformer_input)\r\n``` \r\n\r\nHowever, for the user stream, there is **no text_emb** so how can do the inference for semantic token? At the first time I see the paper, the author wrote that they simply concat the text, moshi and user stream along the codebook dimension so I guess the semantice token inference for user stream can be done with last acoustic token for moshi stream; however, in the code, there is no depformer_in for the last acoustic token with the comment \"# Only using up to dep_q - 1 because the last codebook is never an input to Depformer.\" ; so my guess is incorrect.\r\n\r\nCould you let me know how the user stream's semantic token inference can be done? did you insert some zero values between user and moshi stream? or use user's text stream only for the training phase?",
    "comments_url": "https://api.github.com/repos/kyutai-labs/moshi/issues/175/comments",
    "author": "jtkim-kaist",
    "comments": [
      {
        "user": "LaurentMazare",
        "created_at": "2024-12-26T07:41:11Z",
        "body": "I'm not sure to understand your guess correctly but I think it's correct.\r\n- The main transformer produces `transformer_out` and the text token.\r\n- The depformer produces 16 audio tokens, 8 for moshi, 8 for the user (in that order).\r\n- The first depformer layer uses the text token.\r\n- Each subsequent depformer layer uses the token from the previous layer.\r\nWith this in mind, you can see that the semantic token for moshi is based on `transformer_out` and the text token, and the semantic token for the user is based on `transformer_out` and the last acoustic token for moshi. However note that the depformer layers use a common kv cache so the layer predicting the semantic token for the user can attend to information about the text token.\r\nFinally the comment saying that \"Only using up to dep_q - 1 because the last codebook is never an input to Depformer.\", this means that the token generated by the last depformer layer (which is the last acoustic token for the user) is not used by any depformer layer as there is no subsequent layer."
      },
      {
        "user": "jtkim-kaist",
        "created_at": "2024-12-26T08:22:52Z",
        "body": "Thank you for the fast response! I attached the further detail of my question depending on your comments.\r\n----------------------------------------------------------------------------------------------------\r\nThere is a contradiction between your comments (also this is what I pointed out):\r\n\r\n_comment A_\r\n\"the semantic token for the user is based on transformer_out and the **last acoustic token for moshi.**\"\r\n\r\nand\r\n\r\n_comment B_\r\n\"this means that the token generated by **the last depformer layer** (which is the last acoustic token for the user) is not used by any depformer layer as there is no subsequent layer.\"\r\n\r\nBased on _comment A_, the last acoustic token for moshi is necessary for the user's semantic token inference, However, due to the absence of the last depformer layer, we cannot get the **last acoustic token embedding for moshi**, \r\n```python\r\nself.depformer_text_emb\r\nScaledEmbedding(32001, 1024)\r\n\r\nself.depformer_emb\r\nModuleList(\r\n  (0-6): 7 x ScaledEmbedding(2049, 1024)\r\n)\r\n``` \r\nIn the above code block, we can check that the total number of embedding layer is 8 (1 text, 7 acoustic); thus we only can get the 7th acoustic embedding, while the 8th (last) acoustic embedding for moshi is necessary for the user's semantic token inference.\r\n\r\nTherefore, the question is: how can I get the 8th (last) acoustic embedding for moshi for the user's semantic token inference without 8th (last) depformer_emb layer.\r\n \r\nAlso, cache related stuff may be out of scope in our discussion because caching technique may be used only for the inference phase, not for the training phase.  "
      },
      {
        "user": "LaurentMazare",
        "created_at": "2024-12-26T08:33:21Z",
        "body": "When training, the depformer uses 16 layers. However when running inference we only need to produce the tokens for Moshi so only run the first 8 layers, hence the released weights only use 8 layers and never generate the user audio tokens.\r\nWhat I meant by \"KV cache shared by the layers\" is the causal attention mechanism that is used by the depformer and applies both to training and inference (but indeed the term KV cache is not very appropriate here)."
      },
      {
        "user": "jtkim-kaist",
        "created_at": "2024-12-26T08:43:27Z",
        "body": "I perfectly understand the points from \"the depformer uses 16 layers\" Thank you for your great contributions and fast & kind answer!"
      }
    ],
    "satisfaction_conditions": [
      "Clarify how Depformer layers are structured during training vs inference phases",
      "Explain how user stream tokens are generated without explicit text embeddings",
      "Describe the relationship between moshi and user token generation in training",
      "Differentiate between training-time token generation and inference-time limitations"
    ],
    "_classification": {
      "category": "Does not need build environment",
      "timestamp": "2025-04-05 02:08:16"
    }
  }
]